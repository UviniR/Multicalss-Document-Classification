Duncan Edwards with Hannah Hudson, Colin Anderson, 
Rosie McGee and Karen BrockPROGRAMME LEARNING REPORTJANUARY 2018
Supporting innovation and the use 
of technologies in accountability 
initiatives: lessons from Making 
All Voices Count PROGRAMME  
LEARNING  REPORTSupporting innovation and the use of technologies in accountability 
initiatives: lessons from Making All Voices CountAuthors
At the time of writing, Duncan Edwards, Hannah Hudson, Colin Anderson, Rosie McGee and Karen Brock all 
worked for the research, evidence and learning component of Making All Voices Count, based at the Institute of Development Studies (IDS). 
Acknowledgements
Our thanks to John Gaventa, Director of Research at IDS, for his quick, thorough and thoughtful review of an earlier draft of this report; to Ally Ashlin, Francesca Feruglio, Erin Lewis, Nati Herbst and Cat Setchell, our colleagues in the final version of the research, evidence and learning team; to Eliska Champagne-Veselka and Cathy Shutt for their extensive contributions to facilitating and synthesising learning in the programme; and to all participants in and contributors to the qualitative assessment scorecard process, for their hard work and insights. 
Reference and copyright
IDS requests due acknowledgement and quotes from this publication to be referenced as: Edwards, D.; Hudson, H.; Anderson, C.; McGee, R. and Brock, K. (2018) Supporting innovation and the use of technologies in accountability initiatives: lessons from Making All Voices Count, Making All Voices Count Programme Learning Report, Brighton: IDS
© The Institute of Development Studies 2018FRONT COVER IMAGE:  PARTICIPANTS AT THE MAKING ALL VOICES COUNT POLICY AND PRACTICE DIALOGUE 2017, PHOTO: LANCE BELLERSPROGRAMME  
LEARNING  REPORTSupporting innovation and the use of technologies in accountability 
initiatives: lessons from Making All Voices CountSummary
Making All Voices Count was an international initiative that harnessed the power 
of innovation and new technologies to support effective, accountable governance. 
Focusing on six countries in Africa and Asia, the programme was implemented by a consortium comprising Hivos (the lead agency), the Institute of Development 
Studies (IDS) and Ushahidi. During its four-and-a-half-year life cycle (2013–17), 
it used funding from the Department for International Development (DFID), the Swedish International Development Cooperation Agency (Sida), the United States 
Agency for International Development (USAID) and the Omidyar Network to make 
grants to support new ideas that amplified the voices of citizens, and enabled 
governments to listen and respond. 
From the start, Making All Voices Count was also a learning programme. The 
objective of this learning was not only to bring about change during the programme’s life cycle, but also to leave a legacy that would help to ensure that future governance programmes and initiatives seeking to capitalise on the transformative potential of 
innovation and technology are more informed, inclusive and impactful. 
This programme learning report emerged from a wider process of analysing, 
discussing and synthesising the data from the programme, which wove together 
evidence-based learning about technology for accountable governance initiatives with experiential learning on how best to support such work. The report highlights five of 
the lessons that emerged from Making All Voices Count about how – and how not – 
to run large, complex programmes that intend to support innovation in governance.
The report opens by describing the context and design of Making All Voices 
Count, and describing how it evolved as it was implemented. It goes on to discuss the five lessons:
1. Take time at the start to get the basics right.
2. Trusting relationships at all levels are vital in successfully running a large, 
complex programme.
3. As they are inherently risky and uncertain, and require adaptive approaches to 
succeed, innovation programmes are not for all funders, or for all implementers.
4. What gets measured and monitored needs to be what matters for effectiveness 
and impact, complex as this may be.
5. Some types of knowledge and relationships needs to be viewed as valued outcomes to be pursued, rather than assumed to exist from the start.PROGRAMME  
LEARNING  REPORTSupporting innovation and the use of technologies in accountability 
initiatives: lessons from Making All Voices CountContents
Summary  3
About Making All Voices Count and this programme learning report  5
What did Making All Voices Count set out to do, and how did it evolve? 6
Lesson 1: Take time at the start to get the basics right 7
Lesson 2: Trusting relationships at all levels are vital in successfully running a  
large, complex programme 8
Lesson 3: As they are inherently risky and uncertain and require adaptive approaches  
to succeed, innovation programmes are not for all funders, nor all implementers 8
Lesson 4: What gets measured and monitored needs to be what matters for  
effectiveness and impact, complex as this may be  9
Lesson 5: Some types of knowledge and relationships need to be viewed as  
valued outcomes to be pursued, rather than assumed to exist from the start 11
Looking forward 14
References  14PROGRAMME  
LEARNING  REPORTSupporting innovation and the use of technologies in accountability 
initiatives: lessons from Making All Voices Count5
About Making All Voices Count and 
this programme learning report 
Making All Voices Count was an international initiative 
that harnessed the power of innovation and new 
technologies to support effective, accountable 
governance. Focusing on six countries in Africa 
and Asia,1 the programme was implemented by a 
consortium comprising Hivos (the lead agency), the 
Institute of Development Studies (IDS) and Ushahidi. 
During its four-and-a-half-year life cycle (2013–17), 
it used funding from the Department for International Development (DFID), the Swedish International Development Cooperation Agency (Sida), the United 
States Agency for International Development (USAID) 
and the Omidyar Network to make grants to support new ideas that amplified the voices of citizens, and 
enabled governments to listen and respond.
2
From the start, Making All Voices Count was also a 
learning programme. The objective of this learning was not only to bring about change during the programme’s 
life cycle, but also to leave a legacy that would help 
to ensure that future governance programmes and initiatives seeking to capitalise on the transformative potential of innovation and technology are more 
informed, inclusive and impactful. 
This emphasis on learning led to an exceptional core 
feature of Making All Voices Count: a substantial, 
embedded research, evidence and learning (REL) component that was an integral part of what was otherwise a mainly operational programme. The REL component led the process of building an evidence 
base on the roles that technologies play in securing 
responsive, accountable governance, and supporting practitioners to learn from this evidence. This process 
involved conducting research, and also issuing grants 
to both professional researchers and practitioners to study and reflect on what works, what doesn’t, and where, how and why.
The authors of this programme learning report were 
all members of the REL team, which was based at IDS. They wrote it in the very final days of the programme as part of a wider process of analysing, discussing and synthesising the data from the programme.
3 That 
process wove together evidence-based learning about technology (tech) for accountable governance with our own experiential learning on how best to support such work.
The key evidence-based findings from the programme 
– and a guide to the body of research publications it produced – are summarised in Appropriating technology for accountability ( McGee with Edwards, 
Anderson, Hudson and Feruglio 2018). Written in parallel, this programme learning report highlights five of the lessons we learned about how – and how not – to run large, complex programmes that intend to support innovation in governance. As the introduction to Appropriating technology for accountability points out, “[w]e ourselves are actors in the programme, part of its theory of change”. This learning report is, therefore, based on critical reflections on our own experiences as practitioners; it distils some of the many hundreds of conversations we have had with our colleagues, with Making All Voices Count grantees and with others working in this field. It draws on some of the challenges we encountered, and some of our successes. 
These lessons are intended for others who fund or run 
such programmes, and we hope that they contribute to improving performance and practice in the field of citizen voice, tech for transparency and accountability, and government responsiveness.
1 In order of prominence in the programme’s grants portfolio, these are: South Africa, Kenya, Philippines, Indonesia, Ghana and 
Tanzania. Making All Voices Count operated in a further six countries at a lower intensity (in terms of grants and accompanying activities): Liberia, Uganda, Pakistan, Nigeria, Mozambique and Bangladesh.
2 Making All Voices Count funded 178 projects: 72 innovation projects, 61 research projects, 38 scaling projects and 7 existing 
tech hubs.
3 The principal sources that we draw on here are: a cycle of reflective learning by programme staff, based on self-assessment using a 
qualitative assessment scorecard (QAS) methodology (described in Box 2); three programme-wide learning events, held in 2014, 2016 and 2017; identification and development of stories of change and impact, based on the Most Significant Change approach; a reflective learning trajectory with six partners during 2016–17; a series of practice papers produced through critical dialogues between research grant managers and grantees; and exit interviews conducted with grantees.PROGRAMME  
LEARNING  REPORTSupporting innovation and the use of technologies in accountability 
initiatives: lessons from Making All Voices Count6
4 Notes from Steering Committee meetings, 2013 and 2014.
5 Short message service, commonly known as text messages.What did Making All Voices Count set 
out to do, and how did it evolve?
Making All Voices Count was established to reveal, 
foster and support innovations that sought to increase 
government accountability and responsiveness. 
In programme Steering Committee meetings, the 
implementing consortium was told by its funders and advisers to “go out, take risks, seek out and support 
‘unusual suspects’” and to “find innovations which would shift the relationships between citizens and their 
governments”.
4 
In many cases, the programme’s combination of 
funding, mentoring, research, brokering relationships 
and learning was successful. It fostered exciting innovations, and supported them to shape changes in 
governance through improved service delivery, greater accountability to citizens and reduced corruption. The 
projects described in Box 1 illustrate some of what 
Making All Voices Count achieved.
Some elements of Making All Voices Count worked 
better than others. Indeed, a brief look at how the 
programme unfolded reflects the importance of that old principle of development: context matters. Within its relatively short lifetime, the programme’s political 
context underwent significant shifts and changes. 
It was conceived at a time of optimism about the 
prospects for deepening and transforming democracy, 
and of confidence in the power of technology to effect 
changes in accountability. Launched to great fanfare at the United Nations General Assembly in September 
In Liberia, TIMBY Productions created a user-
friendly tech platform through which citizens and citizen journalists have become involved in monitoring the corporate use of land, timber and mineral resources, and generating data visualisations of this use and accompanying 
narratives. Among the outcomes were: enhanced 
citizen capacity to monitor the use of natural resources; detection and prevention of corrupt transactions; and concrete examples of communities and clans using TIMBY-generated information to negotiate more environmentally and community-friendly deals with private sector actors.
In Ghana, the Oil Journey  project opened up data 
on community development projects funded by oil 
revenues. This raised citizens’ awareness about the use, misuse and lack of accountability around these funds. The data were also used by the state Public Interest and Accountability Committee, which formed an effective coalition with the African Centre for Energy Policy to continue working for the accountable use of these funds. 
In Pakistan’s Punjab region, the Bahawalpur Service 
Delivery Unit took an innovative approach to digitalising the health records managed by women health workers delivering antenatal care, using geographic information systems (GIS) mapping to develop an online monitoring dashboard. The project improved and extended service delivery (antenatal registration and care) to 31% more women, increased delivery by skilled birth attendants by 26%, and increased deliveries at government health facilities by 11%.  
In the Philippines, the Institute for War and Peace 
Reporting and the Citizen Action Network worked to enable data from ten local government units to be opened up, so that citizens could be involved in prioritising the allocation of public funds amid spending constraints. The project provided an SMS-based
5 online feedback mechanism for 
citizens to monitor local government performance anonymously, survey citizens’ assessments of their local government units, and collate their feedback on how government could be more responsive. 
In South Africa, Yowzit  successfully piloted and 
adapted a platform for citizens to rank, review and 
provide substantive feedback on the performance of local governments and service providers. The platform has been taken up by government and integrated into Customer Care Centres in Ekurhuleni Metro (Gauteng East). A partnership has been built between public officials and partner civil society organisations (CSOs), enhancing both sides’ capacity to engage with government regarding feedback – an important step in moving away from a historically antagonistic relationship to one based on constructive collaboration.Box 1. Innovations that helped to shape changes in governancePROGRAMME  
LEARNING  REPORTSupporting innovation and the use of technologies in accountability 
initiatives: lessons from Making All Voices Count7
2013, it began amid a rush to pledge aid funds to 
support the implementation of the recently formed 
Open Government Partnership.6
This background of optimism and urgency led Making 
All Voices Count to issue an immediate Open Call 
(September–November 2013) and a two-stage Global 
Innovation Competition (November 2013–April 2014), inviting proposals from across the world to harness 
new technologies to ‘fix the broken feedback loop 
between citizens and governments’. 
However, many of the early proposals and projects 
were weak, affording little transformative potential. As 
an early programme learning report notes, “[i]n the 
first year . . . few [of the projects set in motion by the granting process] were grappling with the politics of 
accountability, and even fewer were looking to the kind 
of transformative processes and relationships that will be needed to create more opportunities to express 
citizen demands, and new ways to enable citizens to 
work together and with government” (Brock, McGee 
and Besuijen  2014: 21).
By the end of the programme’s first year of operation, the implementing consortium and donors agreed 
that a more strategic and embedded approach was 
necessary to enable Making All Voices Count to support 
innovations that could play useful roles in specific accountability ecosystems in its six priority countries.
The new approach represented a substantial operational 
shift. It recognised that to surface high quality projects, 
support grantees effectively, and help them broker the 
relationships they needed to succeed, a ‘traditional’ 
innovation fund model was not going to work. Instead, the programme needed staff who understood and were 
embedded in the country contexts, and who could build 
the coalitions of diverse actors needed to support the 
combination of technology and civil action that would 
produce relevant innovations. In-country programme staff would also help to build the capacities of citizen 
groups and government actors to move towards 
transformative governance relationships, providing support through programme strategies and alliances (Brock et al. 2014).
Co-evolving with this new programme structure, the REL component adapted its learning model as the programme’s implementation began to take shape. It 
responded to new learning from across the tech for 
transparency and accountability field, and ensured that this was reflected in the research projects the 
programme funded (McGee et al. 2015). It also embellished its strategy to encourage critical curiosity 
and a cross-programme culture of learning, through the 
creation of a range of spaces and opportunities to learn at the international, programme, country and project 
levels (Brock, Shutt and Ashlin  2016).
It is also worth noting two significant shifts in the wider political context that took place while the programme 
unfolded (2013–17), which also shaped its evolution. Firstly, a number of trends and incidents contributed 
to a deflation of the hype and expectations concerning 
the emancipatory promise of technology: the Snowden revelations;
7 the use of technology to ‘manufacture’ 
political consent; the rise of right-wing populism; and 
alleged foreign influence in the US election in 2016 and 
the UK’s Brexit referendum. These contributed to a shift in the broadly positive narrative that had characterised 
the start of the programme, as it became more widely 
recognised that technology’s positive potential for 
deepening democracy is mirrored by its negative 
potential to control and oppress.
8
Secondly, shifts in the domestic politics of some Making All Voices Count donor countries – and in particular, 
their narratives about aid – changed both the level of 
financial scrutiny that some donors demanded and the levels of risk they were prepared to accept.
Lesson 1: Take time at the start to 
get the basics right
Making All Voices Count was complex and ambitious in 
both its design and its aspirations, bringing together 
diverse donors, implementers and grantees to address 
complicated political accountability problems. Amid its complexity and ambition, Making All Voices Count 
also faced pressure and impatience from donors and 
other actors in the transparency and accountability field to get funding out to civic tech initiatives across 
Africa and Asia. One result of this pressure was that the 
implementing consortium had just two months to set things up and establish its systems before launching 
the first Open Call for grants, described above, in 2013. 
There are certain basics that implementing partners 
and donors need to get right for the successful delivery 
of any complex development programme, whatever its nature or thematic focus. Many of these are obvious to 
experienced development practitioners: they include 
experienced strategic leadership, timely and strategic decision making, and effective budgeting and financial management. 
6 An international platform that aims to support champions of open government who are working to make their governments more 
accountable and responsive. See: www.opengovpartnership.org
7 Edward Snowden leaked US government documents revealing the extensive collection and analysis of communications data by the US National Security Agency, spotlighting mass digital surveillance and eroding trust in government.
8 See, for example, Treré (2016).PROGRAMME  
LEARNING  REPORTSupporting innovation and the use of technologies in accountability 
initiatives: lessons from Making All Voices Count8
In a complex programme with diverse stakeholders, 
time is needed to develop shared understandings of 
expectations and requirements between all donors 
and consortium members in relation to these basics. 
Equally, the consortium needs to develop a shared understanding of and mutual responsibility for the 
programme’s theory of change and theory of action – 
especially how different constituent components should complement one another. 
Making All Voices Count’s short inception period 
demonstrated the truth of the old adage ‘more haste, 
less speed’, and cast a long, problematic shadow over 
the programme’s life cycle.
Lesson 2: Trusting relationships at 
all levels are vital in successfully 
running a large, complex 
programme
The relationship between the donor consortium 
and implementing consortium is key to successful implementation. If one party – or the other, or both 
– fails to get the basics right, trust can erode as 
expectations aren’t met and decisions are delayed.
Delayed decision-making and unreliable funding flows 
can strain relationships between grantors and grantees. 
The strategic and embedded approach Making All 
Voices Count adopted in June 2014 (described on page 6), relied on programme staff building trusting relationships with a very wide and diverse range of 
grantees and other stakeholders in the accountability 
and tech ecosystems of their countries. What may seem like minor delays and insignificant missed deadlines at 
donor level can drastically erode trust, credibility and 
goodwill in these country-level relationships, on which successful programme delivery ultimately depends. 
Factors far beyond the control of front-line programme 
actors can do demoralising damage to their personal and professional reputations. 
Getting the basics wrong is extremely costly, both 
financially and in terms of reputation and effectiveness.
Lesson 3: As they are inherently 
risky and uncertain and require 
adaptive approaches to succeed, 
innovation programmes are not for all funders, nor all implementers
Innovation has become one of the buzzwords of 
international development, its association with technology taken for granted. But innovation is not the 
same thing as invention, and does not just mean new apps. In contrast with invention – the process of having 
an idea – innovation is the process by which society 
gains value from that idea (Fagerberg 2003). This is 
often a long, winding, learning journey, with multiple wrong turns and some doubling back along the way. 
Innovation can be ‘stop-start’, only demonstrating its 
value in the long term, once new elements have been added to the initial context.
Innovation is therefore inherently risky and uncertain. 
It will continue to happen the way that it happens, 
not in the way that ‘aidland’
9 might want it to happen. 
Different donors need to reflect on their different appetites and capacities for bearing these risks. In a 
multi-donor scenario, reflections need to be shared, 
reality-checked and managed effectively. 
Innovations funded by an aid programme cannot be 
guaranteed to achieve their intended outcomes in 
alignment with the programme’s linear timelines, plans, logframes and budgets. And managing and supporting 
innovation is all about minimising risk which, in such an 
uncertain venture, is best done by learning and adapting. 
Many of the governance problems Making All Voices 
Count’s grantee innovators tackled were highly 
complex and political, with limits to what could be 
known in advance; in most projects, it was unclear 
whether the innovators recognised this at the outset. 
Staff from 18 programme-funded projects formed part 
of a sample of 35 people interviewed for a study of 
tech for transparency and accountability projects in Kenya (Prieto Martin, Faith, Hernandez and Ramalingam 
2017). Some 79% of interviewees said that adapting 
to context had meant a lot to their project, and the 
majority had altered their plans from the beginning of 
project implementation, if not earlier.
This is echoed across the programme. Very few Making 
All Voices Count-supported projects did exactly what they initially set out to do; most adapted to the 
changing dynamics of each context, and in response 
to lessons that emerged as the projects unfolded. 
If the purpose of programme monitoring was the 
maximisation of effectiveness and impact, then these were the aspects that needed to be monitored. But in 
this programme, as in many others funded by official 
aid donors, financial accountability imperatives drove the programme’s monitoring system so strongly as to override learning objectives. Whether this would 
have happened if a more trusting donor–implementer 
relationship had prevailed can only be guessed at. 
Some of the programme’s donors set out wanting 
to ‘let a thousand flowers bloom’, favouring a model 
in which innovation projects were many and short: 6–24 months. Yet innovation – not to mention the 
9  International development, cf. Mosse (2011).PROGRAMME  
LEARNING  REPORTSupporting innovation and the use of technologies in accountability 
initiatives: lessons from Making All Voices Count9
accountability-claiming that was one of the intended 
outcomes of the programme – is often a longer-term 
process, and project outcomes could scarcely be 
expected with time frames this short. 
Many Making All Voices Count innovation and scaling 
projects did achieve governance outcomes – as highlighted in Box 1, and discussed more by McGee et 
al. 2018 – but often the ‘innovation projects’ themselves 
were only one episode in a much longer process of giving 
their invention social value. The only outcomes that can 
realistically be expected of these ‘innovation project’ 
episodes are knowledge outcomes: the harvesting of lessons from developing and testing the idea. 
A good example of this is one of projects in Box 1. The 
South African social enterprise Yowzit initially received 
an innovation grant to modify its existing ratings and responsiveness site – an Internet and mobile platform 
that gives citizens and consumers the opportunity to 
give feedback to businesses and service providers – to cover more local government services. One knowledge 
outcome of the innovation process – a lesson learned 
from critical reflection on developing and testing 
this idea – was that “online action will not translate 
into offline action without the necessary structures in place, such as a robust accountability framework 
within which government employees have the ability, 
capacity and interest to resolve citizen complaints” (Pegus 2016: 11). It took Yowzit two more years of 
work focused on building relationships and interest 
among both government actors and citizen groups before their efforts with the platform began to yield 
positive governance outcomes among staff in municipal 
government service delivery departments.
Binary notions of success and failure are problematic in 
themselves for numerous reasons, but in the innovation field, donors, practitioners and researchers need to 
recalibrate their expectations and consider innovation 
projects as failures only if they do not build on existing evidence and knowledge, or make available to future 
innovators knowledge generated by the project. This 
is clearly summarised by Czarina Medina-Guce, who 
worked with Making All Voices Count and the Union of 
Local Authorities of the Philippines to identify tech-based innovations for participatory budgeting:When it comes to innovation projects, 
exhaust all insights possible. An innovation project is short and it is a risk taken to test if a certain set of strategies could work. This means the project will most likely have ended 
before significant outcomes emerge. But 
your best contribution to the discourse is to 
offer as much evidence and insight as you 
can.
 Be the literature for those who want to 
build on what you have learned  
(Medina-Guce 2017).
It is widely recognised that complex programmes 
addressing complex problems need to work in ways that allow for learning and adaptive management.
10 
This is even more true when such programmes are also trying to foster innovation, for it is by documenting and communicating our learning, and adapting as we go, that we ‘become the literature’. Learning and adaptive 
management in turn require staff and grantees who are 
curious and disposed to learn, and a recognition that adaptation may call for adjustments in the staff mix to ensure that the necessary skills and competencies are available. They also require facilitated spaces for reflection, learning and adaptation. Box 2 on the following page illustrates two examples of how Making All Voices Count staff and grantees occupied learning spaces created and supported by the programme – one in the form of an internal learning process, and one in the form of a set of research grants.
Lesson 4: What gets measured and 
monitored needs to be what 
matters for effectiveness and 
impact, complex as this may be 
In Making All Voices Count, as in many large aid 
programmes, there were disjunctures between the kinds of monitoring and indicators that were useful at a project level and the kinds of performance data that were needed for aggregating upwards from project level to programme level, and from programme level to the donors. Such disjunctures of scale can create perverse incentives and distortions, and make learning-focused discussions difficult within and between the 
different layers of the funding chain. In a programme 
10 See, for example:  Valters, Cummings and Nixon (2016); Eyben, Guijt, Roche and Shutt (2015);  Doing Development Differently 
(2014); Andrews, Pritchett and Woolcock (2012).In a programme comprised of fundamentally different kinds of projects, 
outputs and outcomes, with a complex theory of change, uncritical slavery to 
an imperfect logframe can distort rather than illuminate.PROGRAMME  
LEARNING  REPORTSupporting innovation and the use of technologies in accountability 
initiatives: lessons from Making All Voices Count10
From the third year of the programme, Making 
All Voices Count used the qualitative assessment scorecard (QAS) methodology
11 to create a space for 
staff to reflect critically on programme performance. Three QAS workshops were held between April 2016 and August 2017, led by the programme’s monitoring, evaluation and learning manager. They brought together staff from across the programme, and offered an opportunity for the programme’s 
country engagement developers – responsible for 
implementing country plans – and research, evidence and learning staff to reflect together on their work. 
The country engagement developers gathered 
evidence from across their respective country portfolios and scored changes against logframe 
indicators drawn from assumptions in the 
programme’s theory of change. They analysed how the programme’s activities were working to produce these changes, what was helping or hindering change, and what could be done differently to 
improve projects. This process equipped programme 
staff to support innovation by learning and managing adaptively. Taking the time to reflect on the country portfolio provided an opportunity for programme 
staff to assess which projects were performing well 
and which needed further support, and to then take relevant action. The benefits of this approach 
were demonstrated by several examples of projects, 
which, during the first workshop, were facing significant challenges, but by the third workshop – after improvements were identified and changes in course implemented – were identified as best cases.
During the last 18 months of the programme, Making 
All Voices Count partnered with Global Integrity to support six Making All Voices Count grantees 
in ‘learning journeys’: co-creating and applying a 
participatory, learning-centred and adaptive approach 
to strengthening citizen engagement in governance processes in their contexts, especially with respect to the Open Government Partnership (OGP) (Moses 2017). Global Integrity facilitated and supported the 
learning journeys through four face-to-face, reflective 
learning workshops, and through ongoing virtual support throughout the full life cycle of each project. The workshops offered valuable spaces for grantees 
to learn and reflect on how their projects and contexts 
were changing, and to benefit from peer learning and support; they reported that the workshops were vital to 
supporting and managing adaptation of their projects. 
Planning this action-learning process into the life of the six projects created more flexibility for adaptation and learning, as grantees Patrick Lim of INCITEGov and Gladys Selosa of ANSA-EAP have written:
The adaptive learning approach gave us space to ask whether activities in the original project design were still strategic, and 
whether they would contribute to the intended 
project outcomes – especially in the face of contingencies and changing political realities. The assurance of flexibility by the partner 
donor made this possible, so that changes, 
if necessary, were permitted and indeed encouraged (Lim and Selosa 2017: 9).
Findings from this project are providing broad, important lessons for civil society actors, donors and OGP actors seeking to support agile open government reforms.Box 2. Creating space for staff and grantee learning
11 The qualitative assessment scorecard was adapted from frameworks that are the intellectual property of Jeremy Holland and Irene 
Guijt, and have been successfully piloted in the evaluation of GirlHub (www.girleffect.org).
12 Source: QAS workshop 3 scorecardscomprised of fundamentally different kinds of projects, 
outputs and outcomes, with a complex theory of change, uncritical slavery to an imperfect logframe can distort rather than illuminate.
For example, ‘reach’ was considered an important 
programme-wide indicator in Making All Voices Count’s logframe, with the implication that it needed to be maximised in all projects for the programme’s change 
goals to be met. Yet the programme’s theory of change 
was far more subtle and differentiated than this, giving the politically motivated ‘reach’ indicator weak validity as a measure of programme progress. During discussions at the third QAS workshop, several country-
level programme staff reflected that reach was not, 
in fact, an objective of all projects, and that in some 
cases, encouraging it made projects less effective, 
not more.
12 To give just one example, Black Sash’s 
work bringing citizen feedback into government health service monitoring in South Africa did not reach large 
numbers of citizens, but its use of direct community 
dialogues was extremely effective in engaging exactly 
the kind of marginalised citizens whose voices the programme sought to make count – far more so than 
some of the online platforms of other projects in the 
South Africa portfolio.PROGRAMME  
LEARNING  REPORTSupporting innovation and the use of technologies in accountability 
initiatives: lessons from Making All Voices Count11
Some donors required data on the users of Making All 
Voices Count-supported projects, including breakdown 
by sex – for the laudable purpose of ensuring gender-equitable outcomes. But in many projects, early feedback indicated that the value that users perceived 
in the tech that projects developed and used was 
precisely the opportunity to be anonymous and share no information about themselves. Capturing 
extra demographic data introduces the basic risk of putting people off, and also the potential danger to 
individual citizens, particularly in contexts where there 
is a closing of civil society space and where holding government to account entails high personal risk, 
including imprisonment. Such tensions and threats 
are real; they need to be taken seriously, discussed and mitigated in ways acceptable to both donors 
and implementers. Demands for disaggregated user 
data by donors need to be subjected to data ethics 
considerations and practices. 
In the case of Making All Voices Count, these problems 
might have been avoided had the programme’s theory 
of change been allowed to drive implementation more 
than the original logframe; and had logframe indicators 
been derived – or adjusted – based on a much more nuanced treatment of the theory of change. That this 
did not happen was largely due to the lack of trust in 
the donor–implementer relationship, alongside and variable donor attention to these issues throughout 
most of the programme’s life cycle.
Lesson 5: Some types of 
knowledge and relationships need 
to be viewed as valued outcomes 
to be pursued, rather than 
assumed to exist from the start
Making All Voices Count brought together varied 
actors from a range of backgrounds and countries, 
with diverse perspectives and epistemologies. They included grass-roots activists, national and local-level government officials, journalists, open data technologists, mobile app developers, staff of national 
and international CSOs, anthropologists and political 
scientists. All, in different ways, were pursuing the 
common goal of more accountable, responsive 
governance. 
On the one hand, this unusually diverse alliance led 
to some striking creativity and effective, impactful governance work. On the other hand, it posed 
significant challenges in terms of supporting staff and 
grantees to develop new skills and capacities, engage 
with evidence generated by themselves and others, and reflect and learn, so as to improve the effectiveness 
of their innovations. Different actors understand and describe the world in different ways, which can give 
rise to difficulties and tensions in co-working and 
co-learning. Each individual is uniquely attuned to the 
various elements that are crucial to an innovation’s 
success: while a technologist might not see the 
importance of understanding different dimensions of 
power, a civil society activist might not understand the 
different risks and affordances (potential uses) of a 
given technology. 
One way in which the programme aimed to address this 
diversity was to support learning via several different 
pathways, as summarised in Figure 1. Grantees and 
staff were facilitated to go beyond single-loop learning 
(‘are we doing it right?’) to double-loop learning 
(‘are we doing the right things? why do they or don’t 
they work?’),
13 as they uncovered and questioned 
assumptions. Much of the innovation mentoring 
provided to Making All Voices Count grantees enabled 
learning of the single-loop kind, focusing on quite 
functional organisational development and capacity 
support in areas such as financial management or 
choosing technologies – although there were instances 
of double-loop learning prompted and enabled by 
innovation mentors, illustrated by the examples in 
Box 3.
Across the programme, double-loop learning was 
best enabled through practitioner research and 
learning grants, interactions with Making All Voices Each individual is uniquely attuned to the various elements that are crucial to 
an innovation’s success: while a technologist might not see the importance of 
understanding different dimensions of power, a civil society activist might not 
understand the different risks and affordances of a given technology.
13 See Argyris and Schön (1974) and Brock, Shutt et al.  (2016).PROGRAMME  
LEARNING  REPORTSupporting innovation and the use of technologies in accountability 
initiatives: lessons from Making All Voices Count12
12Figure 1. Making All Voices Count’s research, evidence and learning pathways
Source: McGee et al.  2015
E
V
I
D
E
N
C
EMAVC PROGRAMME COUNTRY PLANS+
Kenya, Tanzania, Ghana, South Africa, 
Indonesia, PhilippinesPROJECTS & INITIATIVES •Knowledge and 
evidence from 
IDS and third-
party research, 
and MAVC 
learning 
processes
 •Knowledge and 
evidence from 
beyond MAVC 
fed in through 
REL outputs and 
eventsLEARNING
 •For periodic refreshing 
of programme theory
 •Annual reviews by 
EMU for DfID
 •Mid-term review
 •Final evaluation
 •Knowledge and 
evidence from 
IDS and third-
party research, 
and MAVC 
learning 
processes
 •Research and 
evidence from
fed in through 
REL outputs and 
eventsLEARNING
 •For adaptive management 
of country programme
 •About country contexts 
and MAVC’s possible 
contribution to changing 
aspects of context
 •EMU case studies
 •EMU quarterly 
reports
 •Monitoring & evaluation data
 •EMU case studies •Practitioners’ own 
experience and 
exposure
 •Research mentors’ 
experience and 
exposure
 •Other knowledge 
and evidence from 
beyond MAVC, 
brought in 
through REL 
outputs and 
eventsLEARNING
 •To share across grantees
 •To improve innovation, 
scaling and adaptation
 •To improve MAVC 
strategies and ways  of 
working
KEY
Learning arising at each level
Research funded and supported by MAVC
Data and evidence from MAVC shared with Evaluation Management Unit (EMU)
Research, knowledge, evidence and learning fed in at each level, of use to MAVC stakeholdersplan experience
reﬂ ect learnPractitioner research
plan experience
reﬂ ect learn
plan experience
reﬂ ect learnIDS/third-party research
CONTR IBUTION TO IMPACT 
Transfor med governance  arena: All people, including  
poor and marginali sed, are able to engage public and  
private  institutions and call them to account over 
their rights and the issues that matter most to them.  
IDS/third-party researchof change
beyond MAVC RESEARCH, EVIDENCE 
AND LEARNING PATHWAYSPROGRAMME  
LEARNING  REPORTSupporting innovation and the use of technologies in accountability 
initiatives: lessons from Making All Voices Count13
Innovation mentors helped to develop and 
strengthen the work of the project Informed Child Protection Planning in Tanzania, led by the Community for Children’s Rights (CCR). The 
project aimed to close the link between people’s 
experiences, evidence and public planning to respond to violence against children. 
After seeing the potential to strengthen CCR’s work 
by using technology in a more effective way, Making 
All Voices Count’s country engagement developer approached CCR, and connected the project staff to 
innovation and technology mentors within and outside 
of Making All Voices Count. The goal was to support 
CCR in reflecting on their work and interrogating the question of how technologies might help them to meet some of the challenges they were encountering. As a result of this innovation mentorship, CCR successfully integrated technology into their work for children’s rights through the development of a hotline that uses SMS to enable direct communication between citizens and councillors in Arusha.
Another example of double-loop learning through 
successful innovation mentoring, also from Tanzania, is the support that Deus Valentine from East African regional advocacy CSO Twaweza provided to Jamii 
Media on its Tushirikiane Project, which provided 
an online platform for citizens to engage with members of parliament around the implementation 
of election promises. The mentoring increased Jamii 
Media’s understanding of and reflection on the 
local governance context. The result was a change in approach – from applying the same model in all constituencies to designing an approach specific to each location.Box 3. Innovation mentoring in support of double-loop learning
Count country engagement developers, country-level 
communities of practice, and in three programme-wide 
learning and inspiration events.
Practitioner research and learning grants were 
designed to provide practitioners with the space and 
research mentoring to identify their own knowledge 
gaps and / or to uncover and examine their own 
assumptions for the purposes of learning and making 
their interventions more effective and impactful. These 
have been particularly effective in supporting deeper levels of reflection; many grantees have reported 
the significant value of working with their research 
mentors. One is Suzanne Johnson of the Foundation for 
Professional Development, whose project Thuthuzela 
Voices developed an app that users of government 
post-rape services in South Africa could use to give 
confidential feedback. She wrote of her practitioner 
research and learning mentor Beth Mills:
She really understands the role of the mentor . 
She never tried to take the wheel and drive the car . She asked us questions that made us think, 
helped us plan our next steps. She ensured 
that we owned our research, and it was really 
an empowering experience. She helped move 
the research forward when we were stuck. And 
pushed us to develop research outputs that we are proud of, which help us give a platform 
for the voices of the women who participated in our research (Pegus, Johnson and Mahlalela 
2017: 17).
Others are working to incorporate the knowledge generated by their practitioner research into their own future work, but also to utilise the skills and 
research relationships developed while doing it. Blair 
Glencorse, Director of Accountability Lab, whose 
project Next Generation Accountability in Liberia 
developed innovative approaches to engaging youth in 
governance work, summarises:
The Making All Voices Count practitioner research and learning grant the Accountability 
Lab received in Liberia has been catalytic. It 
allowed us to better understand how to work 
with young people to build accountability in 
Liberia, evaluate the impact of the Lab’s efforts, reinforce the evidence base for our work and 
begin to build a learning community around 
accountability on the ground. This helped to 
inform our new strategy and has bolstered 
its implementation. We are now using our co-working and innovation space, iCampus 
– also known as the local OpenGov Hub – as 
a place to build a community around these 
lessons going forwards, and to build on the 
knowledge we developed to better support 
the accountability ecosystem (Blair Glencorse, 
personal communication).
Programme-wide learning and inspiration events  
provided excellent spaces for bringing grantees 
and other stakeholders together to learn from one 
another. Carefully designed and facilitated for peer-to-peer learning, these three events involved using 
elements of governance concepts and theory as 
discussion frameworks for grantees working in very 
different contexts to discuss their own challenges 
and learn. Appropriately modulated, the theoretical PROGRAMME  
LEARNING  REPORTSupporting innovation and the use of technologies in accountability 
initiatives: lessons from Making All Voices Count14
frameworks brought practitioners new insights and 
ways of looking at the problems they were confronting 
in their own practice.14 As one participant in the 
second learning event observed in the post-event 
feedback survey:
I feel that important insights – some of which have been written about by researchers in 
the field – were explored and internalised by 
practitioners through peer interaction in an enabling space. I also realised through the 
interactions the importance of peer learning 
that crosses national boundaries. Gatherings 
like these bring second-degree connections 
together (response in a post-event feedback 
survey).
Country-level communities of practice such as the 
one seeded by Making All Voices Count in South Africa emerged from a need and desire among the 
programme’s South African grantees for a safe 
space to share their experiences and navigate both individual and shared challenges. Many members of 
the community of practice went on to describe it as 
invaluable in shaping their innovations and brokering the range of different relationships necessary for their work to be effective (Smit, de Lanerolle, Braam, Byrne 
and Legong 2017). The flexibility and responsiveness 
of the community of practice structure was particularly valued, as one of the original members, Indra de 
Lanerolle, commented:The community of practice meetings made sure 
learning was on the agenda every few months . . . the group found that whatever the programme design or implementation challenge, they 
were able to find advice and solutions through 
the members’ collective wealth of skills and experience (notes from a presentation about the 
South Africa community of practice; Making All Voices Count 2017).
This support was highly valued, as many grantees felt they were on new terrain with their ambitious and innovative projects.
Looking forward
The legacy of Making All Voices Count is not only the innovative ideas supported, but also, in large measure, the knowledge generated, the new relationships and partnerships developed, the communities of practice 
formed, and the new ways of working which will shape 
future governance innovations.
Researchers often finish reports suggesting more 
research is needed. In the case of the application of technology for the purposes of greater government accountability and responsiveness, more research is  
needed. But to be most effective for accountable future 
governance, it needs to be carried out by practitioners 
and researchers working closely together in an action-learning mode, helping to shape governance innovations as they emerge and evolve.
References
Andrews, M.; Pritchett, L. and Woolcock, M. (2012) 
Escaping Capability Traps through Problem-Driven 
Iterative Adaptation (PDIA), Harvard Kennedy School 
Faculty Research Working Paper Series, RWP12-036, Washington, DC: Center for Global Development, www.
cgdev.org/sites/default/files/1426292_file_Andrews_
Pritchett_Woolcock_traps_FINAL_0.pdf (accessed 12 December 2017)
Argyris, C. and Schön, D.A. (1974) Theory in Practice: 
Increasing Professional Effectiveness, San Francisco, 
CA: Jossey-Bass Inc. 
Brock, K. with McGee, R. (2017) More Accountable 
and Responsive Governance: How Do Technologies 
Help Make It Happen? Making All Voices Count Event 
Report, Brighton: IDS, https://opendocs.ids.ac.uk/opendocs/bitstream/handle/123456789/12975/
mavc_learning_event_FINAL.pdf (accessed 12 
December 2017) Brock, K.; McGee, R. and Besuijen, M. (2014) Making All Voices Count – Enabling Citizen Engagement 
and Responsive, Accountable Government: Strategy 
Synthesis, Making All Voices Count Programme Report, Brighton: IDS, https://opendocs.ids.ac.uk/opendocs/
bitstream/handle/123456789/12861/Making-All-
Voices-Count-Strategy-Synthesis-1.pdf (accessed 6 December 2017)
Brock, K.; Shutt, C. and Ashlin, A. (2016) 
Learning for Change in Accountable Governance 
Programming, Making All Voices Count Learning 
Review, Brighton: IDS, https://opendocs.ids.ac.uk/opendocs/bitstream/handle/123456789/12196/LearningForChange_FINAL.pdf (accessed 6 November 2017)
Doing Development Differently (2014) ‘The DDD 
manifesto’, http://doingdevelopmentdifferently.com 
(accessed 11 December 2017) 
14 These are reported in Brock with McGee (2017), Edwards, Brock and McGee (2016) and Wanjiku Kelbert (2014).PROGRAMME  
LEARNING  REPORTSupporting innovation and the use of technologies in accountability 
initiatives: lessons from Making All Voices Count15
Edwards, D.; Brock, K. and McGee, R. (2016) Transforming 
Governance: What Role for Technologies?, Making All Voices Count Learning Event Report, Brighton: IDS, https://opendocs.ids.ac.uk/opendocs/bitstream/
handle/123456789/11675/MAVC_Transforming_
Governance_final.pdf (accessed 12 December 2017)
Eyben, R.; Guijt, I.; Roche, C. and Shutt, C. (eds) (2015) 
The Politics of Evidence and Results in International 
Development: Playing the Game to Change the Rules?, 
Rugby: Practical Action Publishing
Fagerberg, J. (2003) ‘Innovation: a guide to the 
literature’, paper prepared for Statistics Canada workshop ‘The many guises of innovation: what we 
have learnt and where we are heading?’, October 
2004, http://folk.uio.no/janf/downloadable_
papers/03fagerberg_innovation_ottawa.pdf (accessed 
13 December 2017)
Lim. P . and Selosa, G. (2017) ‘Responding to Reality: 
People, Politics and Technology in Facilitating Local Involvement in OGP – A Case Study from the Philippines’, Global Integrity, ANSA-EAP and INCITEGov, Brighton: IDS, https://133421-384837-raikfcquaxqncofqfm.stackpathdns.com/wp-content/uploads/2017/11/Philippines_INCITEGov_ANSA-EAP .
pdf (accessed 12 December 2017)
Making All Voices Count (2017) ‘Greater than the Sum 
of its Parts: Impact of the South Africa Community 
of Practice’, news item, www.makingallvoicescount.
org/news/greater-sum-parts-impact-south-africa-community-practice (accessed 13 December 2017) 
McGee, R.; with Edwards, D.; Anderson, C.; Hudson, 
H. and Feruglio, F. (2018) Appropriating Technology for Accountability, Making All Voices Count Research Report, Brighton: IDS, https://opendocs.ids.ac.uk/opendocs/handle/123456789/13452
McGee, R.; with Edwards, D.; Minkley, G.; Pegus, 
C-M. and Brock, K. (2015) Making All Voices Count Research and Evidence Strategy, Brighton: IDS, https://opendocs.ids.ac.uk/opendocs/bitstream/
handle/123456789/11581/MAVC_Research_and_
Evidence_Strategy.pdf (accessed 12 December 2017)
Medina-Guce, C. (2017) ‘Insights that will Last: Making 
Innovation Projects Count’, Making All Voices Count 
Blog, 3 July, www.makingallvoicescount.org/blog/
insights-will-last-making-innovation-projects-count 
(accessed 13 December 2017) 
Moses, M. (2017) Learning to Make All Voices 
Count: Lessons and Reflections on Localising the Open Government Partnership, Making All Voices Count Research Report, Brighton: IDS, https://opendocs.ids.ac.uk/opendocs/bitstream/handle/123456789/13351/MAVC_RR_Moses%20
FINAL.pdf (accessed 12 December 2017)Mosse, D. (ed) (2011) Adventures in Aidland: The 
Anthropology of Professionals in International Development, Oxford: Berghahn Books
Pegus, C-M. (2016) Lessons from Yowzit’s 
Practitioner Research and Learning Process, Making 
All Voices Count Practice Paper, Brighton: IDS 
https://opendocs.ids.ac.uk/opendocs/bitstream/handle/123456789/12643/Yowzit_PracticePaper_FINAL_revised_24-11-16_proof.pdf (accessed 12 December 2017)
Pegus, C-M; Johnson, S. and Mahlalela, N.B. (2017) 
Giving Voice to Clients of Post-rape Services: Building and Piloting a Feedback Mechanism in Tshwane, Making All Voices Count Practice Paper, Brighton: IDS, https://opendocs.ids.ac.uk/opendocs/bitstream/handle/123456789/12961/FPD_PP%20WEB.pdf 
(accessed 12 December 2017)
Prieto-Martin, P .; Faith, B.; Hernandez. K. and 
Ramalingam, B. (2017) Doing Digital Development Differently: Lessons in Adaptive Management from Technology for Governance Initiatives in Kenya, Making All Voices Count Research Report, Brighton: IDS, https://opendocs.ids.ac.uk/opendocs/bitstream/handle/123456789/13285/MAVC_
DDD_RR_%28Pr4%29Final_WEB.pdf (accessed 12 
December 2017)
Smit, D.; de Lanerolle, I.; Braam, T.; Byrne, D. and 
Legong, G. (2016) I Know What I Know, but How Do 
I Know What I Don’t? Making All Voices Count South 
Africa, Community of Practice Workshop Report, Brighton: IDS, https://opendocs.ids.ac.uk/opendocs/bitstream/handle/123456789/12952/MAVC_I_Know_What_I_Know.pdf (accessed 12 December 2017)
Treré, E. (2016) ‘The Dark Side of Digital Politics: 
Understanding the Algorithmic Manufacturing of Consent and the Hindering of Online Dissidence’, IDS 
Bulletin 47.1: 127–38. https://opendocs.ids.ac.uk/
opendocs/bitstream/handle/123456789/7697/IDSB_47_1_10.190881968-2016.111.pdf (accessed 12 December 2017)
Valters, C.; Cummings, C. and Nixon, H. (2016) 
Putting Learning at the Centre: Adaptive Development Programming in Practice, London: Overseas Development Institute, www.odi.org/publications/10367-putting-learning-centre-adaptive-development-programming-practice (accessed 12 December 2017)
Wanjiku Kelbert, A. (2014) Bridging and Bonding: 
Improving the Links Between Transparency and Accountability Actors, Making All Voices Count Learning Event Report, Brighton: IDS, www.makingallvoicescount.org/publication/bridging-and-bonding-improving-the-links-between-transparency-and-accountability-actors (accessed 12 December 2017) PROGRAMME  
LEARNING  REPORTSupporting innovation and the use of technologies in accountability 
initiatives: lessons from Making All Voices Count
About Making All Voices Count
Making All Voices Count is a programme working towards a world in which open, effective and participatory 
governance is the norm and not the exception. It focuses global attention on creative and cutting-edge solutions to transform the relationship between citizens and their governments. The programme is inspired by and supports the goals of the Open Government Partnership.
Making All Voices Count is supported by the UK Department for International Development (DFID), the US Agency 
for International Development (USAID), the Swedish International Development Cooperation Agency (SIDA) and the Omidyar Network, and is implemented by a consortium consisting of Hivos, IDS and Ushahidi.
Research, evidence and learning component
The programme’s research, evidence and learning component, managed by IDS, contributes to improving performance and practice, and builds an evidence base in the field of citizen voice, government responsiveness, transparency and accountability (T&A) and technology for T&A (Tech4T&A).
Web www.makingallvoicescount.org
Email  info@makingallvoicescount.org
Twitter @allvoicescount
Disclaimer: This document has been produced with the financial support of the Omidyar Network, SIDA, UK aid 
from the UK Government, and USAID. The views expressed in this publication do not necessarily reflect the official policies of our funders.
This work is distributed under the terms of the Creative Commons Attribution 4.0 International 
licence, which permits unrestricted use, distribution and reproduction in any medium, provided the original authors and source are credited. http://creativecommons.org/licenses/by/4.0/legalcode
Implemented by:
EDITING : TIM WOODS (T.WOODS@GREENINK.CO.UK) AND CAROLYN MAHONEY (C.MAHONEYUK@GMAIL.COM)  LAYOUT : GREEN INK (WWW.GREENINK.CO.UK) 
