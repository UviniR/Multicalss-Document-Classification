 
Electoral Administration in Fledgling Democracies: 
Experimental Evidence from Kenya  
 
J. Andrew Harris , Catherine Kamindo , 
Peter van der Windt  
January  2020  
 Working Paper # 0036   
New York University Abu Dhabi , Saadiyat Island  P.O Box 129188 , Abu Dhabi, UAE  
http://nyuad.nyu.edu/en/academics/academic -divisions/social -science.html  Division of Social Science Working Paper Series  Electoral Administration in Fledgling Democracies:
Experimental Evidence from Kenya
J. Andrew Harris (NYU Abu Dhabi) *
Catherine Kamindo (IEBC)
Peter van der Windt (NYU Abu Dhabi)
12 January 2020
Abstract
We examine the effects of national voter registration policies on voting patterns
with a large-scale experimental study. Together with Kenya’s electoral commission,
we designed an experiment in which 1,674 communities were randomized to a status
quo or treatment group, receiving civic education on voter registration, SMS reminders
about registration opportunities, and/or local registration visits by election commis-
sion staff. We ﬁnd little evidence that civic education improves registration. Local
registration visits improve voter registration, a relationship that increases in poorer
communities. Moreover, local registration increased electoral competition and vote
preference diversity in down-ballot contests in the 2017 Kenyan elections. Our results
suggest that status quo voter registration policies constrain political participation and
competition, and that inexpensive policy changes may attenuate the effects of such
constraints.
*Conditionally Accepted at The Journal of Politics . Corresponding author: andy.harris@nyu.edu. We
thank Han Il Chang, Cornelius Lupao, Hannah Melville-Rea, and Cole Tanigawa-Lau for research support.
We thank Jim Alt, Daniel Posner, Kerstin Fisk, Guy Grossman, Macartan Humphreys, Ryan Moore, Vincent
Pons, and Kelly Zhang for comments. Thanks to participants attending presentations at EPSA, APSA,
NYU Abu Dhabi, and Leuven University for feedback. This research could not have been carried out
without the consistent support of Kenya’s Independent Electoral and Boundaries Commission, particularly
Andrew Limo, Decimah M’mayi, Michael Oyalo, William Kahindi, the Regional Election Coordinators, and
the Constituency Election Coordinators. New York University – Abu Dhabi provided ﬁnancial support.
This study was reviewed by the New York University Institutional Review Board and granted exemption
(039-2016). This study was pre-registered at http://egap.org/registration/2393.
11 Introduction
How elections are organized shapes political participation and election outcomes. For
instance, research in American politics demonstrates that policy choices like voter iden-
tiﬁcation requirements, voting technology, and precinct design shape who can and does
vote (Burden et al. 2014; Tomz and Van Houweling 2003). However, we know much
less about how election administration affects political behavior in ﬂedgling democracies,
where scholars have focused more on issues like vote buying (e.g., Kramon 2016), political
intimidation (e.g., Collier and Vicente 2012), ethnic mobilization (e.g., McCauley 2014),
and electoral fraud (e.g., Ichino and Schundeln 2012). Yet the choices and policies that
electoral commissions make about how to organize elections have the potential to affect
whether and how citizens participate in elections. In this paper, we present the results of
collaborative research with Kenya’s electoral commission, the Independent Electoral and
Boundaries Commission (IEBC), focusing on one key aspect of election administration:
voter registration.
Understanding how voter registration policies affect citizen participation is important
since, in many contexts, electoral commissions circumscribe which organizations can
engage in registration-related activities. This is especially true in ﬂedgling democracies,
which often centralize policy decisions related to election administration. This reality sits in
contrast to contexts like the United States, where election administration is decentralized,
and third-party civic organizations play an important role in voter registration.1Central-
ized election administration thus creates the possibility that existing policies systematically
exclude some citizens. Thus, examining how citizens respond to departures from status
quopolicies provides insight into how existing policies may shape the electorate.
1In Appendix A, we present descriptive data on the nature and centralization of voter registration
procedures across African countries, contrasting them with the relatively decentralized United States.
2Theoretically, election administration may shape citizen participation and electoral
outcomes via three channels (Harder and Krosnick 2008): citizen ability (e.g., under-
standing eligibility requirements); motivation (e.g., social pressure or norms regarding
participation); and difﬁculty (e.g., factors inﬂuencing the costs of participation). Election
commissions may target these channels through decisions about where, how, and how
often to register voters, as well as via their communication and education strategies. Geo-
graphic and neighborhood characteristics may moderate how such policies affect difﬁculty,
motivation, and ability, and, in turn, participation.
To study how changes in voter registration policies affect voter registration, we
designed a policy experiment with the IEBC to randomize information about and op-
portunities for voter registration. We developed three interventions, randomizing 1,674
polling stations into control and ﬁve treatment groups during November and December
2016. Control polling stations continued with the status quo, where citizens in the local
community had to travel to a constituency election ofﬁce to register to vote and received
no additional information or prompts from the IEBC. The ﬁrst intervention – localization –
reduced the cost, inconvenience, and difﬁculty of registration by sending IEBC staff with
mobile registration equipment to register citizens. The second intervention – canvassing
– involved IEBC civic educators providing citizens living near selected polling stations
with information on registration, potentially improving a citizen’s ability to participate
by providing them with basic information about the beneﬁts and requirements of regis-
tering to vote. The third intervention – SMS reminders – entailed sending mobile phone
messages to existing registered voters asking them to remind their unregistered friends
and acquaintances about registration opportunities, leveraging social relationships and
pressure to improve registration. The content of these interventions represent potential
alternative strategies that the electoral commission could use to register voters. As we
discuss in detail below, our experimental design allows us to learn about the causal effect
3of these interventions and their combinations, as well as how they operate across different
contexts. In addition, collaboration with the IEBC enabled access to administrative voter
registration records for measuring the effects of these interventions.
This paper contributes in several ways to the burgeoning literature on how elec-
tion administration shapes political behavior in ﬂedgling democracies (De Kadt 2017;
Neggers 2018; Ferree et al. 2018). First, we evince how changing centrally-organized
and implemented voter registration policies to improve access to the franchise affects
voter registration and election outcomes. Like Ichino and Schundeln (2012), we focus on
the pre-election administrative process of voter registration. While that work examined
registration-related fraud via random allocation of observers, our work shows how chang-
ing voter registration procedures affects voter registration patterns relative to the status
quo. Consistent with research on costs of voter registration in Western contexts (e.g., Dyck
and Gimpel 2005; Stein and Vonnahme 2008; McNulty, Dowling, and Ariotti 2009; H. E.
Brady and McNulty 2011; Bhatti 2012; Braconnier, Dormagen, and Pons 2017) and the
Harder and Krosnick (2008) theoretical framework, improving the accessibility of voting
registration opportunities increases voter registration by about 2% over status quo polling
stations. In contrast, observational evidence in Isaksson (2014) suggests that individual
resources do little to explain variation in voter turnout, a result that may be explained by
misreporting or social desirability bias in survey data (Kolstad and Wiig 2016; Adida et al.
2019).
Second, we ﬁnd differences in effects by context. By combining satellite data with
the spatial locations of polling stations, we blocked units on three prognostic covariates –
poverty, population density, and distance to the constituency election ofﬁce – allowing us
to efﬁciently estimate treatment effects across subgroups (Moore 2012). Similar to Nick-
erson (2015) and consistent with Harder and Krosnick (2008), we ﬁnd strong differences
4by context, with higher-poverty units seeing the greatest increases in registration. Local-
ization increases voter registration by approximately 4.4% in the poorest areas, relative
to 0.7% in the richest areas. In addition, our results span across urban and rural areas –
a feature heretofore unexplored in the experimental literature – and show that distant,
sparsely populated areas in particular beneﬁt from reducing the difﬁculty of registration.
These results suggest that some political inequalities relate to economic and geographical
inequalities.
Third, we examine how impersonal SMS reminders of registration opportunities shape
voter registration. Unlike previous work, we test the multiplicative nature of Harder and
Krosnick (2008) directly. We ﬁnd that when combined with localization, SMS reminders
increase voter registration by 2.4% relative to the status quo, 20% more than localization
alone. Without localization, SMS reminders have no effect. This contrasts with work
showing the positive effects of SMS across a range of outcomes, including voter turnout
(e.g., Dale and Strauss 2009; Malhotra et al. 2011; Marx, Pons, and Suri 2017), savings
behavior (e.g., Karlan et al. 2016), and adherence to antiretroviral therapy (Pop-Eleches et
al. 2011). However, our ﬁndings complement Bennion and Nickerson (2011), who ﬁnd
that SMS messaging increases voter registration in the United States, but only once an
individual possesses a voter registration form. This is analogous to our ﬁndings in the
(very different) Kenyan context: an SMS intervention is effective, but only once citizens
are confronted with a registration opportunity.
Fourth, we ask how civic education efforts affect voter registration, ﬁnding that
information alone does not increase voter registration. Speciﬁcally, we ﬁnd near-zero
effects for civic education, even when voter registration costs are reduced to zero.2We
2In Appendix B, we give an overview of other studies that explore the impact of civic education on
political behavior and knowledge, though these studies show little effect on behavioral outcomes like those
studied here.
5know of two other studies that explore the impact of civic education on registration.
Mvukiyehe and Samii (2017) ﬁnd no evidence that civic education via town hall discussions
improved registration in Liberia. Bratton et al. (1999), in contrast, examine multiple civic
education programs in Zambia and ﬁnd a positive impact on registration. Both studies rely
on survey-based outcomes to measure voter registration, in contrast to the administrative
data upon which this study builds.
Finally, we measure the effects of localization on Kenya’s 2017 General Elections,
which took place eight months after the interventions. Localization has a positive but
insigniﬁcant effect on the total number of registered voters and votes cast at polling stations.
In addition, localization decreased turnout rates, but this effect is indistinguishable from
zero. This ﬁnding suggests that those registered during the experiment are less likely
to vote. This result could be explained by a “double hurdle”: localization affected the
costs of registration, but not of getting to the polls on election day. We do ﬁnd that local
registration increased electoral competition and vote preference diversity in down-ballot
contests.
In the next section we anchor this study in the Kenyan context, describing voter
registration policies and barriers to registration. Section 3 discusses the interventions, data
and empirical strategy. Section 4 presents the results. We conclude in Section 5.
2 Electoral Administration and Barriers to Voter Registra-
tion in Kenya
Social tension, mistrust, and violence accompanied both the 2007 and 2013 General Elec-
tions in Kenya, which played out against a deeper background of ethnic politics and
6violence (Boone 2011; Kanyinga 2009; Mueller 2008; Cheeseman, Lynch, and Willis 2014).
The 2017 General Elections – held eight months after the close of the interventions studied
here – proved a continuation of these themes. Relative to problematic biometric voter
registration in the 2013 elections, improved implementation translated into a six-fold
decrease in the number of records missing biometric information and improvement in
election-day electronic voter identiﬁcation (Carter Center 2013; IEBC 2017; ELOG 2017).3
Despite these improvements, issues regarding results transmission and transparency led
the Supreme Court to annul the presidential election. The nulliﬁcation issued from qualita-
tive non-compliance with electoral laws, rather than a demonstration that irregularities
affected the electoral outcome.4The fresh elections, held in October 2017, were largely
boycotted by the opposition. In the election-related analyses below, we use data from the
August elections, as the October data were incomplete due to the boycott.
Existing studies of voter registration focus on contexts where third-party registration
is the norm, such as France (e.g., Braconnier, Dormagen, and Pons 2017) or the United
States (e.g., Nickerson 2015). Third-party registration decentralizes voter registration
policies by enabling outside groups to register voters (Herron and Smith 2013). In contrast,
Kenya’s IEBC is the sole body providing voter registration to citizens, an arrangement
common in many developing countries. Speciﬁcally, the Constitution mandates that the
IEBC provides continuous voter registration (CVR), giving citizens an opportunity to register
throughout the year. In practice, this means that an IEBC staff member is available at a
constituency election ofﬁce to provide registration year-round.5
3According to a legally-mandated audit prior to the 2017 elections, Kenya’s enrollment rate – the percent-
age of individuals registered divided by those eligible – stood at 78% by a measure deﬁning eligibility as the
estimated number of individuals possessing national identiﬁcation documents. According to that report,
this puts Kenya in 31st place among African countries in terms of citizen enrollment on the register.
4See Carter Center (2019) for a discussion of these legal debates.
5At the time of this experiment, Kenya had 290 constituencies, each of which had one constituency
election ofﬁce from which the IEBC organized election-related activities in that constituency.
7Anecdotal evidence and pre-experiment interviews with IEBC staff suggest that the
CVR policy may generate political inequalities in several ways. First, the stationary
approach to CVR puts the costs of voter registration on citizens.6Traveling to register is
costly, effectively barring poorer citizens from registering during CVR (IED 2007, xv).7
Furthermore, sparse population density may hinder information ﬂows about, as well as
citizen access to, voter registration opportunities when they do arise (IED 2002). Second, a
lack of funding for mobile registration means that IEBC staff can only provide registration
to a subset of relatively nearby communities.8Third, political interests may sponsor IEBC
voter registration drives in certain areas. This raises questions about the independence
of registration opportunities, since politicians would theoretically be reticent to fund
registration in areas where they have little political support.
Figure 1 complements this qualitative evidence with data. Using bivariate logistic
regression, the ﬁgure shows that poverty rates, distance from the election ofﬁce, and
population sparsity are negatively related to voter registration at polling stations from
March 16 to November 14, 2016 – the CVR period before the onset of the experiment. These
data suggest that the status quo voter registration policy systematically excludes poorer,
more remote, and more isolated polling stations from recruitment onto the voter register.
Our blocked design (discussed below) explicitly incorporates these three factors to explore
heterogeneous effects of the interventions.
Inequality in status quo voter registration policies appears to be a broader problem.
6In the counties studied here, the average constituency (the political unit with an IEBC registration ofﬁce)
serves 93 polling stations, with a minimum of 45 and a maximum of 213 polling stations. The average polling
station is 20 kilometers from the registration ofﬁce.
7We do not directly measure individual cost reductions in this paper. However, both discussions with
IEBC staff and grey literature on election administration in Kenya suggest that cost is a key barrier to
registration. For instance, IED (2007) (p. 12) reports: “Citizens sometimes must travel long distances to
register, losing work hours and incurring heavy travel expenses.”
8See IED (2007) (p. xii) and FIDA (2013). In pre-experiment interviews, constituency election coordinators
consistently identiﬁed CVR as ineffective given a lack of budget to take registration “to the people.”
8Figure 1: Voter Registration Decreases as Poverty, Distance and Population Sparseness
Increase
Note: Bivariate logistic regression results, where the dependent variable is the predicted
probability of at least one voter registering at a polling station during the period March 16
to November 14, 2016. Poverty, distance and population density are deﬁned in Appendix
C. Registration information from administrative data provided by the IEBC. Based on all
3,828 polling stations in the seven counties we study.
Bratton et al. (1999) discuss how geographic obstacles and questionable administrative
independence led to registration inequality in Zambia’s 1991 elections.9Bratton and
Lambright (2001) report similar issues with voter registration in Uganda’s 2000 referendum.
Elklit (1999) notes related issues in Nepal (1991), Kenya (1992), and South Africa (1999).
Pallister (2017) provides evidence of problems with inclusivity in voter registration in
Guatemala, El Salvador, and Nicaragua.
These problems are by no means limited to non-Western contexts. Writing on chal-
lenges of registering Black voters in the 1970’s U.S. South, a period where third-party
registration was relatively uncommon, Lewis and Allen (1972) ﬁnd problems strikingly
similar to Kenya’s: “In most of the southern states, registration is conducted only at the
county courthouse, requiring round-trip travel of 40 or 50 miles or more for residents in
9See Burnell (2002) for a similar discussion of Zambia’s 2001 elections.
9outlying areas of rural counties. The lack of dispersed registration centers, the reluctance
of election ofﬁcials to appoint deputy registrars, and the absence of mobile registration
stations also contribute to the suppression of initiative on the part of potential black
registrants.”
While the Kenyan context may be quite different than the United States, that does
not necessarily mean that we should expect different behavioral responses. Individuals
likely respond to decreased costs and additional information in similar ways, regardless of
context. Rather, our contribution lies in a carefully planned experiment in both a different
macro-context (i.e., where election administration is centralized), as well as a large variety
of micro-contexts (via blocking). In the United States, cost-reductions can be implemented
by any number of organizations seeking to register voters. In Kenya, the electoral com-
mission’s centralized control of registration means that they are uniquely responsible for
shaping the registration costs of citizens. While we may see similar behavioral responses
across contexts, the reasons for those changes and the counter-factuals that we can esti-
mate via an experiment, are fundamentally different. Moreover, existing work on voter
registration constraints focuses on urban contexts in high-income countries, whereas our
results examine both rural and (relatively) urban areas settings in a low-to-middle income
country (e.g., Nickerson 2015; Braconnier, Dormagen, and Pons 2017).
Understanding how status quo voter registration policies affect who can and cannot
register to vote strikes at questions of democratic fairness and access germane to political
processes worldwide. These questions are especially pressing in ﬂedgling democracies,
like Kenya, where often a single organization conducts voter registration in the presence
of multiple barriers to political participation. In the next section, we describe a research
design conceived to address such inequalities.
103 Experimental Design
Given CVR’s potential problems, we developed an experimental design with the IEBC
to study how changes to the status quo may improve voter registration, particularly in
poorer, more distant, and more sparsely populated communities. We study these problems
in seven Kenyan counties, which were selected based on being sufﬁciently similar (by
poverty, distance to registration ofﬁce and population density) and the level of government
support in the previous election.10These counties contain 3,828 polling stations. We
randomly selected 1,674 polling stations within these counties, following a strategy that
minimized the chances of spillovers, and blocked on poverty, distance and population
density.11
We deﬁne the units of analysis in this study as polling stations for several reasons.
First, as in Nickerson (2015), there is no population register enabling us to identify and
randomize across unregistered citizens. Nickerson (2015) addressed this problem by
randomizing across streets. With no address or well-documented street system outside of
large cities, this approach proved impossible. Polling stations provided a viable alternative.
Second, the polling station is the smallest unit at which registration and election data are
collected and available in Kenya. Third, polling stations are the natural randomization
unit in this context, since registration policies are organized around polling stations and
home registration is unavailable.
10County selection took place in collaboration with the IEBC to avoid perceptions of political bias. See
Appendix C for further details.
11Details about county and polling station selection, and information about the selected counties, can be
found in Appendix C. Ex post power calculations suggest that this study is well powered to observe even
small effects (Appendix D).
113.1 The Interventions
In addition to CVR, the IEBC implemented three types of interventions between 14 Novem-
ber 2016 and 23 December 2016. First, selected polling stations were visited for two days
by IEBC staff with portable voter registration equipment, which captures a citizen’s pho-
tograph, biometric information, and other required ﬁelds like gender, name, and date of
birth. IEBC staff identiﬁed a location within approximately 250 meters of the selected
polling station, setting up an IEBC banner and table for the registration equipment to
register voters from 9am until 5pm. Citizens in surrounding communities were able to
register at this location. We call this intervention localization .
The second intervention involved canvassing . IEBC staff visited the communities
surrounding the polling station for two days to provide basic education on the voter
registration process, to discuss the importance of electoral participation, and to answer any
remaining questions. IEBC staff also provided information on the practicalities of voter
registration, including the location of the nearest registration opportunity. To maximize
external validity, the IEBC used its standard training and implementation protocol for
canvassing by IEBC ﬁeld staff. The canvassing usually involved one-on-one household-
level discussions that lasted around ten minutes. If the opportunity arose, impromptu
discussions with groups of people lasted from ten to thirty minutes, depending on the
number of questions raised. The IEBC instructed staff to canvass from 9am to 5pm,
working outward from the polling station into the nearby communities.12
Third, the IEBC sent SMS messages to existing registered voters at a polling station.13
In Kenya, cellphone coverage and ownership is high.14While citizens are not required
12The IEBC voter education manual can be found online: https://www.iebc.or.ke/uploads/resources/
pdQMe3WKeV .pdf. Accessed: August 7, 2019.
13The reason we target existing voters is because there is no phone registry listing unregistered citizens.
14Information from the nationally representative 2016 Afrobarometer survey, for example, indicates that
98% of enumeration areas have cellphone coverage, and 85% of respondents personally own a mobile phone.
12to provide a phone number upon registering to vote, many do.15The message, which
clearly identiﬁed the IEBC as the sender, asked citizens to encourage their unregistered
friends and family to register and identiﬁed the nearest registration opportunity. Two
SMS messages per person were sent over a two-day period. In total, the IEBC sent almost
300,000 messages via SMS. Because the SMS messages were sent only to already registered
voters, this treatment could be considered conservative, relying on registered recipients
sharing the information with an unregistered person.16
Were the interventions implemented? Project documents kept by IEBC staff report
that all polling stations received their assigned treatment. Independent monitors made
unannounced visits to a random ten percent of polling stations, ﬁnding, under reasonable
assumptions given the mobility of canvassers, compliance of around 80% across treatment
arms.17
3.2 Random Assignment of the Interventions
We randomly assigned treatment to polling stations following a two-by-three factorial
design as in Table 1 below. In order to minimize the effect of spillovers, we ﬁrst sampled
1,674 polling stations from the sampling frame of 3,828 polling stations. Our sampling
algorithm ensured that polling stations were at least 1.5 kilometers apart. On average, sam-
pled polling stations were separated by 2.4 kilometers.18Each factorial cell contained 279
15Of the 338,988 previously registered voters in the sampled polling stations, 147,277 (43%) of records
listed a phone number, of which 143,704 (42% of all records) were valid and included in the bulk SMS
intervention. Note that registered voters providing their mobile numbers may be systematically different
(e.g., more likely to share information) than those not sharing their contact information.
16The need for the information to be shared to be effective implies that the treatment effect is likely to
differ across characteristics like a community’s network structure. We did not measure these characteristics,
but randomization implies that these should be balanced across treatment conditions.
17Appendix E discusses monitoring and compliance in more detail. In less than three percent of interven-
tions, implementation dates differed from those assigned. Our analyses presented below use the assigned
treatment dates, attenuating estimated treatment effects.
18Additional details related to randomization and treatment assignment can be found in Appendix C.
13polling stations. Pure control polling stations received no canvassing, SMS, or localization,
representing the status quo CVR registration policy. For canvassing-only polling stations,
citizens were provided with basic civic education and directions to the constituency elec-
tion ofﬁce for registration. For SMS-only polling stations, the text message reminded
citizens to register to vote at the constituency ofﬁce at any time.19For localization-only
polling stations, IEBC staff set up a portable registration site near the local polling station,
enabling the local community to register on the spot without travelling to the constituency
election ofﬁce, thus reducing the distance (and travel costs) required to register. Polling
stations assigned to both the localization and canvassing interventions received both
treatments simultaneously on two consecutive days. During this intervention, canvassers
directed citizens to the on-going local registration opportunity. For polling stations as-
signed localization and SMS interventions, the SMS was delivered for two consecutive
days at 6pm the evening before the localization intervention began.20
Table 1: Polling stations by treatment status
Canvassing SMS Nothing
Localization 279 279 279
No Localization 279 279 279
Note: Number of polling stations by treatment status.
To measure treatment effects across different contexts, we grouped polling stations
in blocks based on poverty, distance to the registration ofﬁce, and population density.21
Blocking has several advantages. First, randomizing within blocks improves balance
across treatments. Second, blocking allows efﬁcient estimation of treatment effects within
sub-groups (Duﬂo, Glennerster, and Kremer 2008).
19The SMS, translated from Kiswahili, read: “Thanks for being a registered voter! Those unregistered can
register at IEBC ofﬁce - (town/village). Please spread this message.” The SMS was customized to identify
the location of the closest IEBC ofﬁce where registration occurs.
20The SMS, translated from Kiswahili, read: “Thank you for registering as a voter. IEBC will visit polling
station [. . . ] tomorrow to register voters. Please spread this message.” The SMS was customized to indicate
the location of the local registration opportunity.
21Speciﬁcally, we use multivariate continuous blocking as in Moore (2012).
14Finally, ethical concerns are inherent in any ﬁeld experiment. Given that we worked
in collaboration with the IEBC, potential registrants were unaware that the intervention
was part of an experiment. Rather, it was viewed as part of the operations of the IEBC.
This approach ensured that citizens would respond normally to the intervention. Relative
to the status quo, the interventions pose little risk to citizens. Finally, the experiment
included counties showing both government and opposition support, ensuring political
impartiality.
3.3 Data
The target population is voting age individuals unregistered for the 2017 General Elections
at the onset of the interventions. Collaboration with the IEBC provided access to detailed
data on individual registration records. The data comprise individual-level records of a
person’s birth year, gender, cellphone number, the polling station where they wish to vote
in the 2017 elections, and the day registration occurred. From these data, we generate
outcome measures of registration at the polling station-day and polling station levels.
To measure the effects of the interventions on election day, we use publicly available
data from the August 2017 Kenyan General Elections. Voters selected candidates for six
different positions. Nation-wide, 8 candidates ran for president, 210 for governor, 256 for
senator, 299 for county women representative, 1,893 for member of parliament, and 11,857
for ward representative. From the IEBC website, we collected turnout and vote outcomes
at the polling station level for each election.
In addition, we used the spatial locations of polling stations to harvest spatial data on
poverty rates, distance to the local constituency election ofﬁce, and population density for
each polling station. Appendix C discusses these data.
154 Results
In this section, we present experimental results in three steps. First, we examine the
effects of the interventions on voter registration during the intervention period. Second,
we examine heterogeneous treatment effects across sub-groups of poverty, distance, and
population density. Third, we examine the downstream effects of the experiment on
election day outcomes.22
4.1 Proximate Impacts on Registration
We ﬁrst explore the proximate impact of the interventions on voter registration, focusing
on the period from 14 November 2016, the onset of the interventions, to 6 January 2017, ten
days after the last day of the interventions. In total, excluding weekends, the data include
66,560 polling station days across the six treatments. We focus on this period – which falls
during normal CVR operations – to test how simple deviations from status quo policies
affect voter registration.
While localization is likely to be effective only on the two days of the intervention
when registration is present, information from canvassers or via SMS may motivate action
in the days after treatment. To capture this, the (pre-registered) treatment variable deﬁnes
the localization treatment as only the two days when the registration kit was at a randomly
selected polling station. In contrast, the canvassing and SMS-only treatments are coded
as the two intervention days plus a ten day “tail.” The remaining polling station-days
constitute the control condition.23
22In Appendix F, we show that the randomization procedure was successful in ensuring substantive
balance between treatment groups.
23The ten-day tail and the asymmetry across interventions, although pre-registered and based on discus-
sions with the IEBC, is admittedly ad hoc. In Appendix G, we present results across a range of speciﬁcations,
which point to the same substantive conclusions. We ﬁnd no evidence for additional registrations beyond
16During the intervention period, polling station days assigned to the status quo con-
dition saw few individuals register: approximately 1 individual every 22.7 days (0.044
individuals per polling station per day). In total, of the 66,560 polling station days assigned
to the status quo condition, only 2,568 (3.86%) experienced nonzero registrations, 0.01% of
2013 registered voters.24These low numbers are consistent with prior CVR results (IREC
2008).
Table 2 presents the estimated treatment effects of the interventions, using a variety of
model speciﬁcations, which point to similar substantive conclusions. Column 1 presents
results from regressing the total number of individuals registered on the six treatment
conditions at the polling station-day level. Column 2 adds block, polling station and day
ﬁxed effects.25Column 3 adds controls for poverty, distance from the registration ofﬁce,
and population density, and weights by the inverse of the polling station-speciﬁc inclusion
probability to estimate population average treatment effects.
To better interpret effect sizes, column 4 measures the outcome as the number of
registered individuals at a polling station day divided by the total number of registered
individuals at that polling station in 2013.26Coefﬁcients in this column can be interpreted
as the effect of a single day of intervention on voters registered on that day, as a percentage
of 2013 registration.
Finally, in column 5, we collapse the temporal element of our data to the polling
station level. Summing across the entire intervention period decreases statistical power
the tail of the original deﬁnition.
24In Appendix H, we present summary information at baseline.
25This is our pre-registered equation: yit=a+b1Canvassingit+b2SMS it+b3Local it+b4Local it
Canvassingit+b5Local itSMS it+yb+gi+fw+eitwhere yitis the total number of voters registered at
polling station ion day t.Local ,Canvassing and SMS are dummy variables and equal to one if polling station i
received the respective treatment on day t, and zero otherwise. ybis a block ﬁxed effect; giis a polling station
ﬁxed effect; fwis a vector indicating the day of the week in order to account for weekly cyclical patterns that
might affect registration, such as market days; eitis an error term clustered at the polling station.
26Note that we are not able to measure the number of individuals registered by the eligible voting age
population, because these data are not available at the polling station level.
17but simpliﬁes the interpretation of results and takes away the possibility of incorrectly
deﬁning the treatment.27This column reports the effect of the two day intervention on
total registrations at the polling station during the entire intervention period as a share of
2013 registered voters.
Table 2: Impact of Interventions on Voter Registration
# Regs # Regs # Regs # Regs by 2013 # Regs by 2013
(1) (2) (3) (4) (5)
Canvassing effect 0.0520.0590.0550.00030.001
(0.018) (0.019) (0.018) (0.0001) (0.003)
SMS effect 0.001  0.002  0.003  0.00001  0.001
(0.008) (0.010) (0.011) (0.00003) (0.003)
Localization effect 3.1703.1632.9280.0100.020
(0.301) (0.305) (0.270) (0.002) (0.003)
Localization+Canvassing effect 3.6303.6273.6010.0110.020
(0.263) (0.266) (0.265) (0.001) (0.003)
Localization+SMS effect 4.7054.7014.0930.0120.024
(0.387) (0.391) (0.316) (0.001) (0.003)
Control average 0.0437 0.0437 0.0437 1e-04 0.0043
Level of analysis PS day PS day PS day PS day PS
Pre-registered No Yes No No No
Block FE No Yes Yes Yes Yes
Polling Station FE No Yes Yes Yes No
Day FE No Yes Yes Yes No
Controls No No Yes Yes Yes
Weights No No Yes Yes Yes
Observations 66,880 66,880 66,880 66,720 1,668
R20.222 0.254 0.261 0.171 0.350
Note:p<0.1;p<0.05;p<0.01. Clustered, robust standard errors in parentheses (columns 1 to 4).
Unit of analysis for columns 1 to 3 is the polling-station day and the outcome is the absolute number
of registered voters. Columns 4 and 5 divide the outcome by the number of registered voters in 2013,
measured at the polling station-day level and polling station-level, respectively. For Localization,
Localization+Canvassing and Localization+SMS, treatment are the two days at which the intervention
took place. For Canvassing-only and SMS-only, treatment are those two days at which the intervention
took place plus the ten following days. The remaining days plus polling station days assigned to the
control areas, are status quo. The full experimental sample involved 1,674 polling stations, based
on data provided by IEBC in 2016. However, six polling stations were dropped due to operational
reasons (e.g., stations were retired from service for the 2017 elections or had zero registered voters in
2013). This leaves us with 66,720 (1,668) observations in column 4 (5).
Table 2 demonstrates that localization drives most registration increases. This suggests
27The polling station-level equation is: yi=a+b1Canvassingi+b2SMS i+b3Local i+b4Local i
Canvassingi+b5Local iSMS i+yb+ei. This equation is identical to the equation above, except for
dropping weekday indicator variables and polling station ﬁxed effects.
18that costs and convenience, not information, constitute the main constraint to registration.
For ease of interpretation, we focus on column 5, the most conservative estimates.28
We ﬁnd little evidence that SMS or canvassing substantively increased the number of
registrations compared to control communities.29In contrast, localization increased voter
registration as a share of 2013 voters by 2 percent. While polling stations assigned to
the status quo (CVR) saw an increase in registered voters by 0.4 percent, polling stations
that received localization saw an increase of 2 percent.30The effect is substantial and
statistically signiﬁcant (p<0.01).
The combination of localization and SMS is especially effective at increasing registra-
tion, leading to a 2.4 percent increase relative to the status quo. The difference in voter
registration between areas with localization and SMS and localization (or localization plus
canvassing) is statistically signiﬁcant (p<0.01).31These results suggest that SMS reminders
of a local registration opportunity provide one cost-effective way to increase voter registra-
tion. However, information only appears to work in the presence of difﬁculty-reducing
interventions like localization.
28There are some differences in the effect of canvassing between columns 1 to 4 and that in column 5. In
columns 1 to 4, the effect of canvassing is statistically signiﬁcant but substantively quite small, while that in
column 5 is indistinguishable from zero. Similarly, columns 1 to 4 suggest a small increase in registration
due to canvassing, either in isolation or combination with localization. However, these effects are minor
relative to the difference between interventions with and without localization.
29In Appendix B, we give a summary of other studies exploring the impact of civic education interventions.
First, there is much variation in the type of interventions studied. Second, the average participant’s length
of exposure in the intervention we study here is shorter than the interventions explored by other studies.
Perhaps a longer treatment or different content may have increased registration. We discuss this further in
the conclusion.
30By election day, we ﬁnd that the intervention leads to a positive but insigniﬁcant increase in total
registered voters at the polling station level (see Section 4.3).
31In Appendix I, we show that all intervention effects, with the exception of localization versus localization
plus canvassing, are statistically distinguishable from from each other (p<0.05).
194.2 Effect Heterogeneity in Poverty, Distance, and Population Density
Section 2 revealed considerable heterogeneity across polling stations in poverty, distance,
and population density. Previous studies in the developed world have found that social
economic status is positively correlated with political participation and interest in general
(e.g. Verba, Scholzman, and Brady 1995), and voter registration in particular (Nickerson
2015). Distance from the polling place similarly hinders participation (Dyck and Gimpel
2005; H. E. Brady and McNulty 2011). Our blocked design makes it possible to examine the
heterogeneous effects of the interventions on registration as a function of levels of poverty,
distance to registration opportunities, and population density.
Figure 2’s top row shows the results of column 5 of Table 2 for different levels of
poverty. We calculate the average level of poverty for each of the 279 blocks, and then
subset the blocks into ﬁve equally sized groups based on these block-level means. The
blocks are organized from left to right increasing in poverty. The leftmost panel presents
the results from the richest quintile of blocks, while the rightmost panel focuses on the
poorest quintile. We highlight two main results. First, localization has a positive impact
in all ﬁve sub-groups. Second, the impact of localization is higher in poorer areas. The
impact of localization is six times as large in the poorest quintile relative to the richest
quintile (4.39% vs. 0.73%). This suggests that poorer communities in particular beneﬁt
from localization.32
We ﬁnd similar dynamics for distance and population density. The center row in
Figure 2 presents the effects of the interventions by quintiles of distance to the registration
ofﬁce, where the leftmost panel presents the results focusing on the quintile of blocks
closest to the registration ofﬁce, while the rightmost panel focuses on those farthest away.
32These results contrast with De Kadt (2017), who shows that the expansion of polling stations in South
Africa beneﬁted richer citizens. We discuss this difference in more detail below.
20Similar to poverty, localization has a positive effect across all panels. Moreover, the effect
of localization in the most distant polling stations is considerably larger as that in more
proximate polling stations (2.26% vs. 1.59%).
Finally, the bottom row of Figure 2 presents the results by quintiles of population
density. The leftmost panel presents results for the densest quintile of blocks, while the
rightmost panel focuses on the sparsest quintile. Again, localization has a positive effect
across all panels. Furthermore, the effect in sparsely populated areas is almost six times
larger than that in densely populated areas (5.34% vs. 0.99%).
These results suggest that registration is a much bigger problem among the poor,
as well as those living in distant and sparsely populated areas. Changes in status quo
registration policy are thus likely to have the most signiﬁcant impact on citizens in such
communities.
21Figure 2: Results by Poverty, Distance and Population Density
Note: Bars indicate 95% conﬁdence intervals. Dependent variable is the number of regis-
tered voters at a polling station during the intervention period divided by the total number
of registered voters at that polling station in 2013. For the top row we calculate the average
level of poverty for each of the 279 blocks, and then subset the blocks into ﬁve equally
sized groups based on these block-level means. We do the same for distance (center row)
and density (bottom row). The blocks are organized from left to right increasing in poverty
(top), distance (center row) and density (bottom row).224.3 Downstream Effects on Registration, Turnout, and Vote Choice
While localization has positive effects during the intervention period, what are the down-
stream effects of localization on election day?33Increases in voter registration may increase
turnout (Nickerson 2015). The interventions may also change the distribution of candidate
support at a polling station by registering citizens with different preferences (De Kadt 2017).
On the other hand, the effects presented thus far may be due to temporal displacement,
i.e., individuals that registered to vote due to the interventions would have registered later
in the intervention’s absence.
Our intervention period represents a sliver of the entire ﬁve-year cycle of voter
registration. Thus, the two-day interventions may not be of sufﬁcient magnitude to
affect election day outcomes. To address this, we combine polling stations receiving
any localization – which induced virtually all of the total estimated effects during the
intervention period – and compare them to polling stations that did not receive localization.
Figure 3 examines the effect of localization across all six races on election day out-
comes: total registered voters, total turnout, turnout rate, vote margin, and preference
diversity. Bars indicate 95% conﬁdence intervals, and the dependent variables are stan-
dardized to enable comparison across outcomes. First, we explore the overall number of
registered voters for the 2017 election relative to that in 2013. Kenyans have to register
only once and the effects are thus the same across races. Relative to control, localization
increased 2017 registration relative to 2013 by 1.4% or 0.052 standard deviations. Thus,
localization temporally displaced the registration decision of some voters, causing them
to register earlier than they otherwise would have. Relative to the 2% effect at the end of
our intervention period, the estimated election day effect of localization becomes 1.4%.
33This experiment was not originally designed to detect downstream effects; thus, the analyses in this
section are not pre-registered.
23This implies that 0.6% of the overall increase consists of individuals who would have
registered in the absence of the intervention. While these ﬁndings are consonant with our
expectations, they are not statistically signiﬁcant.
Second, we explore voter turnout and turnout rates. Because Kenyans vote for all
six races at the same time (using separate ballots), the effects are again very similar
across races. On average, localization increased voter turnout by around 0.04 standard
deviations relative to control polling stations. However, localization depressed turnout
rates, measured as the number of votes cast divided by the number of registered voters in
2017, by an average of 0.03 standard deviations. This result is consistent with the notion of
a “double hurdle”: localization relaxed constraints related to registration but not voting.
Again, these results remain statistically insigniﬁcant.
Finally, we explore the impact of localization on vote margins and vote fractional-
ization.34We ﬁnd that localization led to a 2.8% (0.09 standard deviation) decrease in
vote margins at the polling station level in the women’s representative race (p<0.01). This
suggests that improving access to voter registration may increase political competition.
Moreover, localization increased the diversity of expressed political preferences in the
women’s representative race by 0.014 (p<0.05) or 0.06 standard deviations. In the context
of a tight two candidate race, this is equivalent to adding a third party candidate receiv-
ing 1.4 percent of the vote. The increase in vote fractionalization (and decrease in vote
margins) means that localization increases political competition by recruiting new voters
that are more likely to support secondary candidates than voters recruited via status quo
processes.35The statistically signiﬁcant ﬁndings, however, are conﬁned to the women’s
representative race. On the one hand, this makes theoretical sense: if the interventions
34Vote margin is measured as the percentage of votes for the winner minus the percentage of the votes for
the second place candidate. Vote fractionalization is measured as one minus the sum of candidates’ squared
vote shares.
35As a reviewer noted, this may not necessarily be a normative good if it leads to more spoiler candidates.
24were to have any effect on the kind of preferences recruited to the voter register, it would
likely be preferences over newer, less-salient, and down-ballot contests like the women’s
representative and senator. Both of these contests exhibit similar point estimates for vote
margin and preference diversity. On the other hand, Figure 3 presents 30 estimates and
one would expect, given the 95% conﬁdence intervals, just under two of thirty tests to be
signiﬁcant, simply by chance. Thus, we remain somewhat circumspect on the substantive
interpretation of the effects of localization on the election.
Figure 3: Downstream Effects of Localization on Voter Registration and Election Outcomes
Note: Effect of localization across all six August 2017 races on total registered voters, total
turnout, turnout rate, vote margin, and preference diversity. Bars indicate 95% conﬁdence
intervals. Dependent variables are standardized to enable comparison across outcomes.
Turnout rate is the number of votes divided by number of registered voters. Preference
diversity is measured using fractionalization.
255 Discussion and Conclusion
In this paper, we explore two questions related to election administration in ﬂedgling
democracies. First, do status quo policies for voter registration create systematic barriers
to political participation? Second, if such barriers exist, how can they be overcome? We
partnered with the Kenyan electoral commission to implement a large-scale experiment
providing citizens with information about and access to voter registration to address
these questions. We ﬁnd little evidence that providing knowledge or reminders about
registration encourages participation. Rather, we ﬁnd that material costs related to reg-
istration functionally disenfranchise poorer, more distant, and more sparsely populated
communities. Our study is perhaps most similar in spirit to Nickerson (2015), who ﬁnds
that average registration increases by about 4.4% in response to a door-to-door registration
drive. The magnitude we report is approximately half of this, although the effect size
varies by context. The poorest subgroup in Nickerson (2015) reported a 7.3% increase
in registration, relative to a 4.4% effect in the poorest block in this study. Interestingly,
Nickerson (2015) (p. 95) notes that the average effect of 4.4% arises “in the contemporary
[United States] setting where voter registration is moderately easy.” This raises the ques-
tion as to whether the relatively modest results we observe (relative to Nickerson 2015)
might be driven by the more onerous nature of Kenya’s voter registration process and the
environment in which it is carried out.36
Our results have important implications for electoral administration and democracy
promotion in the developing world. While there is limited evidence that civic education
improves registration or turnout, existing research in lower-income contexts does ﬁnd
36Appendix J reviews related work on voter registration and GOTV experiments, and discusses the
magnitude of our results relative to the broader literature. It is also worth noting that, in contrast to
Nickerson (2015), the localization intervention did not use door-to-door registration. Localization thus
decreased the cost of the intervention, but did not bring it to zero.
26that civic education may improve knowledge and other forms of political participation.37
Finkel, Horowitz, and Rojo-Mendoza (2012), for instance, ﬁnd that in-person civic edu-
cation programs in Kenya led to increases in a broad measure of local – but not national
– political participation. Similarly, Mvukiyehe and Samii (2017) ﬁnd positive effects of
experimentally-allocated civic education interventions on both self-reported and behav-
ioral measures of participation in Sierra Leone. Bratton et al. (1999) ﬁnd similar results in
Zambia. We contribute to this body of work by juxtaposing the beneﬁts of civic education
with alternative interventions on voter registration, ﬁnding that the largest beneﬁts to
voter registration may lie in changing convenience and cost, not providing civic education.
One implication is that election administrators and donors should focus their efforts on
cost-reducing measures like the localization of registration examined in our study. Doing
so may increase political participation and, as a result, electoral competition.
Moreover, the localization results complement existing work showing how accessi-
bility affects political participation (McNulty, Dowling, and Ariotti 2009; H. E. Brady and
McNulty 2011; Bhatti 2012). Our results present experimental evidence that existing voter
registration policies may functionally disenfranchise certain populations in the developing
world by making it more difﬁcult for some citizens to register. While this effect may not be
intentional, a lack of viable registration options disadvantages citizens living in poorer,
more remote, and more sparsely-populated areas. In some respects, these results contrast
with the careful observational analysis in De Kadt (2017), which ﬁnds a three to ﬁve percent
increase in voter turnout across 15 years due to the expansion of polling stations in South
Africa. Citizens in higher socio-economic groups were particularly responsive to changes
in access, in apparent contradiction to our results where citizens from poorer areas were
more responsive. Two differing aspects of the studies may explain this seeming disparity.
37See Appendix B for a careful exposition of other civic education studies. A robust literature from high-
income countries suggests that face-to-face canvassing and voter education cause increased participation in
those contexts as well (e.g., Braconnier, Dormagen, and Pons 2017; Green and Gerber 2015).
27First, De Kadt (2017) studies how the addition of new polling stations generates participa-
tion, whereas our work examines random localization at existing polling stations. Thus,
while we can estimate an average treatment effect representing a deviation from an existing
policy, the estimates in De Kadt (2017) represent an estimate of an expansion policy which
may serve urban – and richer – citizens with greater demand for additional polling station
capacity. Second, the interventions tested here focus on unregistered citizens near exist-
ing polling stations, while the De Kadt (2017) study engages with a different population
entirely: both registered subpopulations whose access was eased, as well as unregistered
populations engaged by newly established stations. As a result of these differences, we are
hesitant to suggest that our results contradict those in De Kadt (2017), but rather speak to
the complicated nature of election administration as it relates to political behavior.
One objection to improving access to registration is that it is not cost effective. Lo-
calization, however, is relatively cost-effective. An average cost of $25 per localization
day translates into a cost per registration of $8.33 (average) or $3.57 (highest poverty
block). This compares favorably to status quo costs per registered voter of US$13.74 in
an election year, with non-election years signiﬁcantly higher due to lower registration
numbers but identical ﬁxed costs (IREC 2008, 44). Adding SMS to localization improves
this cost comparison, given the relative effectiveness and low cost of SMS bulk messaging
relative to canvassing,38bringing the electoral commission’s registration costs well within
sustainable per registration cost targets (IFES 2005, 173).39Moreover, the estimates assume
hiring additional IEBC personnel; implementation with existing employees carrying out
regular localization may drive costs even lower.
This study is not without limitations. Perhaps most pressing, the present study
38For the cost of one canvasser sent to a polling station, we could send over 2,000 SMS messages, which
would cover almost four average sized polling stations.
39In addition, these estimates are likely an upper bound, given that our intention-to-treat analysis probably
underestimates the actual effect of localization.
28examines the effect of one relatively light-touch form of civic education, face-to-face
canvassing, which has been effective in other contexts. In terms of Harder and Krosnick
(2008), it may be that the reduction in difﬁculty due to localization may vastly outstrip the
increases in citizen ability due to our civic information treatment, and that stronger versions
of the latter may generate a detectable effect. Future work might consider variations on
the civic education treatment studied here, varying the content of the educational material
and the depth or frequency at which it is delivered. Second, our blocking strategy focuses
on poverty, one important element of economic context. Recent work demonstrates that
also another important element, economic inequality, may function to condition political
preferences (Sands 2017). Future work might examine whether the effects of inequality on
political preferences extend to formal political participation. Finally, voter registration in
ﬂedgling democracies may be conditioned by ethnic or religious contexts, local institutions,
and networks. Future work could explore more complex designs aimed at differentiating
effects due to social-contextual factors from those due to geography.
Finally, to what extent do the results from this study generalize beyond our study
site? There are good grounds to expect external validity with respect to other polling
stations in the seven counties since our polling stations were drawn randomly from
larger populations. In Appendix K, we compare the national distribution of poverty and
population density with that of our sampled polling stations.40Although the sample
excludes the extreme polling stations, it overlaps with the vast majority of the national
distributions. Furthermore, our results suggest that scaling-up the policy to even poorer
and more sparsely populated areas may lead to even higher registration gains. Would
our results replicate beyond Kenya?41We have little direct empirical leverage over this
40We do not have data on the locations of the constituency ofﬁces in constituencies not included in our
study. Thus, we do not present sample-to-population comparisons to our third blocking variable, distance to
the constituency election ofﬁce.
41Evrensel (2010) mentions similar voter registration problems related to cost/distance-based disenfran-
chisement in Ghana (pp. 2, 14), DRC (pp. 58, 81, 91, 95), Mozambique (pp. 224, 228), and South Africa (in
29question. It is worth noting, however, that the environment of our study is similar to
that found in many developing countries on some key dimensions. In most developing
countries, policy decisions related to election administration are centralized; state capacity
and bureaucratic outreach remain weak; and citizens are poor, living far away from
registration opportunities or in sparsely populated areas. For example, among the 46
countries classiﬁed by the World Bank as Sub Saharan Africa, Kenya ranks as only the
30th least densely populated country.42In other words, the factors central to this study are
clearly a more general phenomenon in the developing world.
6 References
Adida, Claire, Jessica Gottlieb, Eric Kramon, and Gwyneth Mcclendon. 2019. “Response
Bias in Survey Measures of Voter Behavior: Implications for Measurement and Inference.”
Journal of Experimental Political Science forthcoming: 1–7.
Bennion, Elizabeth A., and David W. Nickerson. 2011. “The Cost of Convenience: An
Experiment Showing E-Mail Outreach Decreases Voter Registration.” Political Research
Quarterly 64 (4): 858–69.
Bhatti, Yosef. 2012. “Distance and Voting: Evidence from Danish Municipalities.”
Scandinavian Political Studies 35 (2): 141–58.
Boone, Catherine. 2011. “Politically Allocated Land Rights and the Geography of
Electoral Violence: The Case of Kenya in the 1990s.” Comparative Political Studies 44 (10):
1311–42.
Braconnier, Celine, Jean-Yves Dormagen, and Vincent Pons. 2017. “Voter Registration
2004, p. 345), suggesting that the broader problems explored here are not uncommon.
42These are 2017 population density estimates. Source: https://data.worldbank.org/.
30Costs and Disenfranchisement: Experimental Evidence from France.” American Political
Science Review 111 (3): 584–604.
Brady, Henry E., and John E. McNulty. 2011. “Turning Out to Vote: The Costs of
Finding and Getting to the Polling Place.” American Political Science Review 105 (1): 115–34.
Bratton, Michael, Philip Alderfer, Georgia Bowser, and Joseph Temba. 1999. “The
Effects of Civic Education on Political Culture: Evidence from Zambia.” World Development
27 (5): 807–24.
Bratton, Michael, and Gina Lambright. 2001. “Uganda’s Referendum 2000: The Silent
Boycott.” African Affairs 100: 429–52.
Burden, Barry C., David T. Canon, Kenneth R. Mayer, and Donald P . Moynihan. 2014.
“Election Laws, Mobilization, and Turnout: The Unanticipated Consequences of Election
Reform.” American Journal of Political Science 58 (1): 95–109.
Burnell, Peter. 2002. “Zambia’s 2001 Elections: The Tyranny of Small Decisions,
’Non-Decisions’ and ’Not Decisions’.” Third World Quarterly 23 (6): 1103–20.
Carter Center. 2013. “Observing Kenya’s March 2013 National Elections Final Report.”
Atlanta: Carter Center.
———. 2019. “Report on Legal Issues from Kenya 2017 Presidential Election.” Atlanta,
Georgia: Carter Center.
Cheeseman, Nic, Gabrielle Lynch, and Justin Willis. 2014. “Democracy and Its
Discontents: Understanding Kenya’s 2013 Elections.” Journal of Eastern African Studies 8
(1): 2–24.
Collier, Paul, and Pedro C. Vicente. 2012. “Violence, Bribery, and Fraud: The Political
31Economy of Elections in Sub-Saharan Africa.” Public Choice 153 (1-2): 117–47.
Dale, Allison, and Aaron Strauss. 2009. “Don’t Forget to Vote: Text Message Re-
minders as a Mobilization Tool.” American Journal of Political Science 53 (4): 787–804.
De Kadt, Daniel. 2017. “Bringing the Polls to the People: How Electoral Access
Encourages Turnout But Exacerbates Political Inequality.” Working Paper .
Duﬂo, Esther, Rachel Glennerster, and Michael Kremer. 2008. “Using Randomization
in Development Economics Research: A Toolkit.” In Handbook of Development Economics ,
edited by T. Paul Schultz, 3895–3962. Amsterdam: Elsevier.
Dyck, Joshua J., and James G. Gimpel. 2005. “Distance, Turnout, and the Convenience
of Voting.” Social Science Quarterly 86 (3): 531–48.
Elklit, Jørgen. 1999. “Electoral Institutional Change and Democratization: You Can
Lead a Horse to Water, But You Can’t Make it Drink.” Democratization 6 (4): 28–51.
ELOG. 2017. “One Country, Two Elections, Many Voices!: The Kenya 2017 General
Elections and The Historic Fresh Presidential Election.” Nairobi: Elections Observation
Group.
Evrensel, Astrid. 2010. Voter Registration in Africa . Johannesburg: EISA.
Ferree, Karen E, Danielle F Jung, Robert A Dowd, and Clark C Gibson. 2018. “Election
Ink and Turnout in a Partial Democracy.” British Journal of Political Science forthcoming:
1–17.
FIDA. 2013. “Key Gains and Challenges: A Gender Audit of Kenya’s 2013 Election
Process.” Nairobi: Federation of Women Lawyers Kenya.
Finkel, Steven E, Jeremy Horowitz, and Reynaldo T Rojo-Mendoza. 2012. “Civic
Education and Democratic Backsliding in the Wake of Kenya’s Post-2007 Election Violence.”
32The Journal of Politics 74 (1): 52–65.
Green, Donald P , and Alan S Gerber. 2015. “What Works, What Doesn’t, and What’s
Next.” In Get Out the Vote . Washington, D.C.: Brookings Institution Press.
Harder, Joshua, and Jon A Krosnick. 2008. “Why Do People Vote? A Psychological
Analysis of the Causes of Voter Turnout.” Journal of Social Issues 64 (3): 525–49.
Herron, Michael C, and Daniel A Smith. 2013. “The Effects of House Bill 1355 on
Voter Registration in Florida.” State Politics & Policy Quarterly 13 (3): 279–305.
Ichino, Nahomi, and Matthias Schundeln. 2012. “Deterring or Displacing Electoral
Irregularities? Spillover Effects of Observers in a Randomized Field Experiment in Ghana.”
The Journal of Politics 74 (01): 292–307.
IEBC. 2017. “Media Release: Report on Audit of the Register of Voters.” Nairobi.
IED. 2002. “Registration of Voters in 2002: An Audit Report.” Nairobi: Institute for
Education In Democracy.
———. 2007. “Pre-Elections Observation: Registration of Voters in 2007 an Audit.”
Nairobi: Institute for Education In Democracy.
IFES. 2005. Getting to the CORE: A global survey on the cost of registration and elections .
Washington, D.C.
IREC. 2008. Report of the Independent Review Commission on the General Elections Held in
Kenya on 27 December 2007 . Nairobi: Government Printer.
Isaksson, Ann-Soﬁe. 2014. “Political Participation in Africa: The Role of Individual
Resources.” Electoral Studies 34: 244–60.
Kanyinga, Karuti. 2009. “The Legacy of the White Highlands: Land Rights, Ethnicity
and the Post-2007 Election Violence in Kenya.” Journal of Contemporary African Studies 27
33(3): 325–44.
Karlan, Dean, Margaret McConnell, Sendhil Mullainathan, and Jonathan Zinman.
2016. “Getting To the Top of Mind: How Reminders Increase Saving.” Management Science
62 (12): 3393–3411.
Kolstad, Ivar, and Arne Wiig. 2016. “Education and Electoral Participation: Reported
Versus Actual Voting Behaviour.” Applied Economics Letters 23 (13): 908–11.
Kramon, Eric. 2016. “Electoral Handouts as Information: Explaining Unmonitored
Vote Buying.” World Politics 68 (3): 454–98.
Lewis, John, and Archie E Allen. 1972. “Black Voter Registration Efforts in the South.”
Notre Dame Law Review 48: 105–32.
Malhotra, Neil, Melissa R Michelson, Todd Rogers, and Ali Adam Valenzuela. 2011.
“Text Messages as Mobilization Tools: The Conditional Effect of Habitual Voting and
Election Salience.” American Politics Research 39 (4): 664–81.
Marx, Benjamin, Vincent Pons, and Tavneet Suri. 2017. “The Perils of Voter Mobiliza-
tion.” Vol. 23946. NBER Working Paper Series.
McCauley, John F. 2014. “The Political Mobilization of Ethnic and Religious Identities
in Africa.” American Political Science Review 108 (4): 801–16.
McNulty, John E., Conor M. Dowling, and Margaret H. Ariotti. 2009. “Driving Saints
to Sin: How Increasing the Difﬁculty of Voting Dissuades Even the Most Motivated Voters.”
Political Analysis 17 (1): 435–55.
Moore, Ryan T. 2012. “Multivariate Continuous Blocking to Improve Political Science
Experiments.” Political Analysis 29: 460–79.
Mueller, Susanne D. 2008. “The Political Economy of Kenya’s Crisis.” Journal of Eastern
34African Studies 2 (2): 1753–1055.
Mvukiyehe, Eric, and Cyrus Samii. 2017. “Promoting Democracy in Fragile States:
Field Experimental Evidence from Liberia.” World Development 95: 254–67.
Neggers, Yusuf. 2018. “Enfranchising Your Own? Experimental Evidence on Polling
Ofﬁcer Identity and Electoral Outcomes in India.” American Economic Review 108 (6):
1288–1321.
Nickerson, David W. 2015. “Do Voter Registration Drives Increase Participation? For
Whom and When?” Journal of Politics 77 (1): 88–101.
Pallister, Kevin. 2017. Election Administration and the Politics of Voter Access . London:
Routledge.
Pop-Eleches, Cristian, Harsha Thirumurthy, James P Habyarimana, Joshua G Zivin,
Markus P Goldstein, Damien de Walque, Leslie MacKeen, et al. 2011. “Mobile Phone
Technologies Improve Adherence To Antiretroviral Treatment In a Resource-limited Setting:
A Randomized Controlled Trial of Text Message Reminders.” AIDS 25 (6): 825–34.
Sands, Melissa. 2017. “Exposure to Inequality Affects Support for Redistribution.”
Proceedings of the National Academy of Sciences 114 (4): 663–68.
Stein, Robert M., and Greg Vonnahme. 2008. “Engaging the Unengaged Voter: Vote
Centers and Voter Turnout.” The Journal of Politics 70 (2): 487–97.
Tomz, Michael, and Robert P Van Houweling. 2003. “How Does Voting Equipment
Affect the Racial Gap in Voided Ballots?” American Journal of Political Science 47 (1): 46–60.
Verba, Sidney, Kay Scholzman, and Henry Brady. 1995. Voice and Equality: Civic
Voluntarism and American Politics . Cambridge: Harvard University Press.
35Appendix for:
Electoral Administration in Fledgling Democracies:
Experimental Evidence from Kenya
J. Andrew Harris
Catherine Kamindo
Peter van der Windt1
January 12, 2020
1Corresponding author: andy.harris@nyu.edu
1A Voter Registration Procedures Across Africa
Table A1 presents attributes of election administration and voter registration processes in
the United States, Kenya and other Africa countries. Broadly construed, the table makes
two main points. First, election administration in the USA – where most current studies
on voter registration take place – is much more decentralized than the Kenyan and most
African contexts. Second, the Kenyan context mirrors the median African context in terms
of voter registration and administration across a number of dimensions. Below, we discuss
each column in turn.
•Registering authority This column tracks the body in charge of registering voters.
EMB refers to a national electoral management body, distinct from existing govern-
ment structures. Local/Central Government refers to situations where a government
body – local or national – registers voters. State/County refers to the decentralized
registration in the United States where a state body or county ofﬁce handles voter
registration.
•Registration required to vote Is a citizen required to register in order to vote? This
tracks whether a citizen must be listed on a distinct voter register in order to partici-
pate. In some contexts, national citizenship records may be used in place of a voter
register.
•Method for creation of electoral register Continuous refers to a process where the
voter register is open for changes and transfers throughout the electoral cycle, and
closes at some point prior to an upcoming election. Periodic refers to a registration
process where a voter register is compiled for each election, and discarded afterwards.
•Voting registration technology This column categorizes the methods use to compile
a register. Digital/Computer means that the registering body uses speciﬁc computer
2hardware/software for voter registration. Paper/Scanning refers to a process that
starts with paper forms, which are then digitized to compile a digital register. None
means that the voter registration process remains manual. Mobile System means that
voter registration can occur via an online system and/or SMS technology.
•Biometric data captured Biometric data include ﬁngerprints, eye scans or other types
of personal unique identiﬁers captured by the registering authority.
•Voting age This column reports the minimum age a citizen must be in order to vote.
•Compulsory voting This column reports whether or not voting is required of all
eligible citizens.
•EMB conducts civic education This column reports whether or not the electoral
management body (or other body responsible for conduct of elections) conducts civic
education related to elections.
3Table A1: Comparative Data on Voter Registration, Civic Education, Registration Require-
ments, and EMB Structure
Country Registering Registration Method for Voting registration Biometric Voting Compulsory EMB conducts
authority required creation of technology data age voting civic
to vote electoral register captured education
USA State/County Varies Continuous Varies Varies 18 No Varies
Kenya EMB Yes Continuous Digital/Computer Yes 18 No Yes
Median African County EMB Yes Continuous Digital/Computer Yes 18 No Yes
Algeria Local Gov. Yes Continuous Digital/Computer Yes 18 No No
Angola EMB Varies Continuous Digital/Computer Yes 18 Yes Yes
Benin EMB Yes Continuous Digital/Computer Yes 18 No No
Botswana EMB Yes Continuous Paper/Scanning No 18 No No
Burkina Faso EMB Yes Continuous Paper/Scanning Yes 18 No Yes
Burundi EMB Yes Periodic None No 18 No Yes
Cameroon EMB Yes Continuous Digital/Computer Yes 20 No -
Cape Verde EMB Yes Continuous Digital/Computer Yes 18 No -
CAR EMB Yes Continuous None No 18 No Yes
Chad EMB Yes Continuous Digital/Computer Yes 18 No No
Comoros Local Gov. Yes Continuous Digital/Computer Yes 18 No No
DRC EMB Yes Continuous Digital/Computer Yes 18 No Yes
Cote d’Ivoire EMB Yes Continuous Digital/Computer Yes 18 No Yes
Djibouti EMB Yes Continuous None No 18 No No
Egypt EMB No NA NA NA 18 Yes Yes
Eq. Guinea Central Gov. No NA NA NA 18 No No
Eritrea EMB Yes Continuous None No 18 No Yes
Ethiopia EMB Yes Periodic None No 18 No Yes
Gabon Local Gov. Yes - Digital/Computer Yes 18 No No
Gambia EMB Yes Periodic Digital/Computer Yes 18 No Yes
Ghana EMB Yes Continuous Digital/Computer Yes 18 No Yes
Guinea EMB Yes Continuous Digital/Computer Yes 18 No Yes
Guinea-Bissau EMB Yes Periodic Digital/Computer Yes 18 No Yes
Lesotho EMB Yes Continuous Digital/Computer Yes 18 No Yes
Liberia EMB Yes Periodic Paper/Scanning Yes 18 No Yes
Libya EMB Yes Continuous Mobile System No 18 No -
Madagascar Central Gov. Yes Continuous None No 18 No Yes
Malawi EMB Yes Continuous Digital/Computer Yes 18 No Yes
Mali EMB Varies Continuous NA Yes 18 No No
Mauritania EMB Yes Continuous Digital/Computer Yes 18 No Yes
Mauritius EMB Yes Continuous None No 18 No -
Morocco Central Gov. Yes Continuous Digital/Computer Yes 18 No No
Mozambique EMB Yes Periodic Digital/Computer Yes 18 No Yes
Namibia EMB Yes Continuous Paper/Scanning Yes 18 No Yes
Niger EMB Yes Continuous None No 18 No No
Nigeria EMB Yes Continuous Digital/Computer Yes 18 No Yes
Congo Brazzaville Central Gov. Yes - - - 18 No No
Rwanda EMB Yes Periodic Digital/Computer Yes 18 No Yes
Sao Tome and Principe - Yes Continuous Digital/Computer Yes 18 No -
Senegal Central Gov. Varies Continuous Digital/Computer Yes 18 No No
Seychelles Central Gov. Yes Continuous Digital/Computer No 18 No No
Sierra Leone EMB Yes Periodic Digital/Computer Yes 18 No Yes
Somalia EMB Yes Continuous Digital/Computer Yes 18 No -
South Africa EMB Yes Continuous Paper/Scanning No 18 No Yes
South Sudan - Yes Continuous None No - - -
Sudan EMB Yes Continuous None No 18 No Yes
Swaziland EMB Yes Periodic Paper/Scanning Yes 18 No Yes
Tanzania EMB Yes Continuous Paper/Scanning Yes 18 No Yes
Togo EMB Yes Continuous Digital/Computer Yes 18 No No
Tunisia EMB Yes Continuous Mobile System No 18 No Yes
Uganda EMB Yes Continuous Digital/Computer Yes 18 No Yes
Zambia EMB Yes Continuous Digital/Computer Yes 18 No Yes
Zimbabwe EMB Yes Continuous Digital/Computer Yes 18 No Yes
Note: Data are drawn from IDEA Electoral Management Design Database, IDEA ICTs in Elections Database, ACE Project
Comparative Database, and the authors’ own research. Missing data are denoted by a dash.
4B Literature Overview: Civic Education
This section provides an overview of the empirical literature on the relationship between
civic education and political participation and knowledge. Given the wide variation in
civic education interventions, the intention of this appendix is to better situate our civic
education intervention in the broader literature.
Table A2 gives summary information about other studies that investigate the impact
of civic education on political behavior and knowledge, speciﬁcally: voter registration
(Reg.), turnout (Turnout), other forms of political participation like participation in election
campaigns, contact with public ofﬁcials, etc. (Other) and/or political knowledge (Know.).
The table indicates the mode delivery: Canvassing (C), School-based curriculum (S),
townhall meetings (T), workshops (W), the introduction of exams (E) and other modes like
drama, media spots, concerts, and puppet shows (O). We indicate whether the intervention
targeted village populations (V), students (S), or speciﬁc leaders such as women leaders or
village chiefs (L). Finally, we report the typical length of participant exposure to the civic
education intervention. Subsequently, the table includes whether a study’s outcome is
based on administrative (A), behavioral (B) or self-reported (S) data, and whether the study
is experimental (E) or observational (O) in design. The table also shows whether the study’s
unit of analysis is the individual (I) or the polling station (P). The table also summarizes
the results of civic education across the four outcomes of interest: “+” indicates an overall
positive impact; “-” a negative impact; “0” no evidence for impact; and “.” indicates that
the outcome was not explored. Finally, we list what subgroups the study explores.
The inclusion criteria were as follows. We focus on articles in the following seven
journals: American Journal of Political Science, American Political Science Review, British
Journal of Political Science, Comparative Political Studies, Journal of Politics, Quarterly
5Journal of Political Science, and World Development. We searched for the term “civic
education” in Google Scholar and in each of the journals individually. We kept those
articles that have civic education as independent variable and at least one of the earlier-
mentioned four dependent variables. In addition, we also report such articles from other
journals when they have 50 or more citations in Google Scholar (on August 7, 2019). Table
A2 includes only studies that investigate the impact of civic education. That is, we include
studies that focus on civics courses or civic education programs, but exclude studies that
explore the impact of education more broadly (e.g. Sondheimer et al 2010).2
We highlight ﬁve observations from Table A2. First, there are only two other studies
that explore the impact of civic education on voter registration (the key outcome in this
study).
Second, there is much variation in “civic education”. Speciﬁcally, nine out of fourteen
studies explore interventions that target (often high school) students via their curriculum
or the introduction of civics exams. In contrast, seven out of fourteen studies explore
interventions that target other populations via townhall meetings, workshops, or other
modes of delivery. The interventions under study also differ considerably in the average
participant’s length of exposure. While many curriculum-based interventions target
students for terms at a time, other modes of delivery are often shorter like two or three
half day sessions or one hour town hall meetings.
Third, the civic education intervention we explore is similar to the mode of delivery
in only two other studies. Furthermore, the canvassing intervention we explore here
involved one-on-one household-level discussions that lasted around ten to thirty minutes.
The average participant’s length of exposure is shorter than the interventions explored by
2Note also that we include studies that provide general information about the importance and/or the
process of political participation, but exclude those studies that provide information about the performance
of political representatives. We thus did not include the recent set of studies conducted in Dunning et al
(2019), which ﬁnd no evidence that such voter information campaigns shape voter behavior.
6other studies.
Fourth, few other studies build on administrative data and an experimental design.
In fact, ten of the fourteen studies build solely on self-reported data to measure outcomes,
and only three of the fourteen studies build on an experimental design.
Finally, the null result we ﬁnd related to civic education and registration does not
stand out from other studies. Of the two other studies that explore the impact of civic
education on registration one ﬁnds a null result, and one ﬁnds a positive result. The
latter, however, does not build on an experimental design and makes use of self-reported
data. Moving to results related to turnout, other forms of political participation and
knowledge, we observe that there is also little evidence that civic education improves
turnout. In contrast, there seems to be quite consistent evidence that civic education leads
to improvements in other forms of political participation and political knowledge.
7Table A2: Studies on the Relationship between Civic Education and Political Behavior and Knowledge
# Study Journal Citations Country Mode Target Length Outcome Design Unit Reg. Turnout Other Know. Subgroup
NA This study NA NA Kenya C V One time 10-30 minutes A E I;P 0 0 . . Poverty; Isolation; Density
1 Langton et al (1968) APSR 600 USA S S 1 school year S O I . . . 0 Race
2 Pasek et al (2018) ADS 219 USA S S 1 or 2 semesters S O I . 0 . + None
3 Finkel (2002) JOP 173 Dom. Rep.; SA C;S;O V;L Multiple S O I . . + + Exposure; Pol. resources
4 Finkel et al (2005) PP 162 South Africa S S >1 per week for 2 years S O I . . . + None
5 Finkel et al (2000) WD 127 Dom. Rep. C;S;O V;L Multiple S O I . . . + None
6 Finkel et al (2011) AJPS 118 Kenya W;T;O V Multiple S O I . . + + Exposure; Pol. resources
7 Bratton et al (1999) WD 107 Zambia C;S;O V;S Multiple S O I + . + + None
8 Gottlieb (2016) AJPS 93 Mali W V 2 or 3 halfday sessions S;B E I . . + + None
9 Slomcyznski et al (1998) PP 75 Poland S S 30 hours S O I . . . + None
10 Finkel et al (2012) JOP 58 Kenya W;O V Multiple S O I . . + + Exposure to violence
11 Green et al (2011) JOP 55 USA S S 21 classroom lessons S E I . . . + None
12 Persson et al (2010) SPS 53 Sweden S S 100 hours S;B O I . 0 0 0 None
13 Campbell et al (2016) APSR 22 USA E S NA B O I . . . + Incentivized; Race
14 Mvukiyehe et al (2017) WD 21 Liberia T V >1 hour S;B;A E I;P 0 0 + + None
Note: Journal: ADS = Applied Development Science; WD = World Development; SPS = Scandinavian Political Science; APSR = American Political Science Review; JOP = Journal of
Politics; PP = Political Psychology; AJPS = American Journal of Political Science. Citation counts from Google Scholar (August 7, 2019). Mode: C = Canvassing, S = School-based
curriculum, T = Townhall meetings, W = Workshops, E = Exams, O = Other. Target: V = Village populations, S = Students, L = Leaders. Outcome: A = Administrative; S =
Self-reported; B = Behavioral. Design: E = Experiment; O = Observational. Unit: P = Polling station; I = Individual. Reported results relate to the full sample, not subgroup
analyses.
8C County and Polling Station Selection and Assignment to
Treatment
C.1 County Selection
The study took place in seven of Kenya’s 47 counties: Bungoma, Kwale, Kisumu, Nyamira,
Nyandarua, Kericho, and Makueni. The selection of these counties took place in two
steps. First, we choose counties that have polling stations that are similar across three
characteristics: poverty; distance from the registration ofﬁce; and population density.
Speciﬁcally, we use a covariance matrix to select counties that are similar on these three
characteristics. Then, we select seven counties that have a cosine-distance higher than 0.9
and taking into account IEBC’s political sensitivities. We measure these variables in the
following way:
1.Poverty The percentage of people below the poverty line at that polling station,
average across county. Information on the local poverty rates for each polling station
comes from http://www.worldpop.org. The data are at a high geographic resolution.
We used a Voronoi diagram to measure a polling station’s level of poverty.
2.Distance to registration ofﬁce Normally, citizens register at the registration ofﬁce. Each
constituency has one registration ofﬁce. We obtained information from the IEBC
about all polling station locations. From this we create a distance indicator between
the polling station and the registration ofﬁce based on three measures: 1) kilometer
distance as the bird ﬂies, 2) kilometer walking distance from Googlemaps, and 3)
kilometer driving distance from Googlemaps.
3.Population density Measures as the population density within a 500 meter radius
around a polling station. Datasource: http://www.worldpop.org.
9Table A3 presents information about poverty, distance and population density for
each county.
Table A3: County Information
County Bungoma Kericho Kisumu Kwale Makueni Nyamira Nyandarua
Total # of Polling Stations 806 530 528 415 866 333 350
# of Sampled Stations 252 252 252 252 252 162 252
Sample as % of Total 31 48 48 61 29 49 72
Poverty 0.46 0.44 0.46 0.52 0.43 0.38 0.27
Distance 17.25 18.31 20.68 27.98 27.03 13.62 18.99
Population Density 73 50 84 28 27 92 31
Note: Poverty is the percentage of the population below poverty line; distance is kilometers distance
from the registration ofﬁce; population Density is per km2.
Second, conditional on being sufﬁciently similar, we selected seven counties based
upon their support for the government. Speciﬁcally, we choose two pro-government
counties (Kericho, Nyandarua), three pro-opposition counties (Kisumu, Kwale, Makueni),
and two split counties (Bungoma, Nyamira). For the county selection we collaborated
with the IEBC in order to avoid perceptions of political bias.
C.2 Polling Station Selection and Assignment to Treatment
The sampling frame is a complete list of the 3,828 polling stations in the selected seven
counties. For all counties except Nyamira, we sampled 426=252polling stations to
be part of our study. Given the higher population density of Nyamira County (and thus
higher spatial density of polling stations), we only sampled 276=162, in order to
maintain sufﬁcient distance between treated polling stations to minimize spillovers. The
study thus targets a total of 1,674 polling stations across seven counties in Kenya. Our
strategy to sample polling stations and assign them to treatment had three main goals:
1) sample the number of polling stations as indicated above, 2) minimize the chances of
spillovers, and 3) randomly assign the six treatments to polling stations within blocks that
consist out of similar polling stations. We undertook the follow nine steps for each county:
101. We randomly select 1 polling station out of all possible polling stations;
2. Drop those units within 1.5 kilometers distance of the selected polling station;3
3. Randomly select a second polling station among those that are left;
4. Drop those units within 1.5 kilometers distance of this second unit;
5.Etc. Until we are left with exactly 252 (or 162 for Nyamira) polling stations sampled.
6.From those selection polling stations we create blocks of six that are similar based on
the above three characteristics (poverty, distance and population density), using the
technique discussed in Moore (2012);
7. Subsequently, within each block of six polling stations we assign the six treatments.
8. Conduct steps 1 to 8 a total of 1,000,000 times.
9.Of the 1,000,000 outputs, we choose the one with the best balance on the above set of
blocking covariates.
Two ﬁnal notes are in order. First, we choose not to do a simple random sample of
252 polling stations (or 162 for Nyamiria) because there is only a small probability that we
get units where all selected polling stations are more than 1.5 kilometers apart from each
other. Second, we conduct the steps within county, not within constituency (the electoral
unit below county). The latter would have given blocks consisting of more diverse polling
stations and thus less efﬁcient estimates.
3We choose 1.5 kilometers as a threshold after consultation with the IEBC regarding the minimum distance
that would create a functional buffer between local communities.
11D Statistical Power
In this appendix, we discuss statistical power. We did not include power calculation in the
pre-analysis plan as we had little ex ante information or strong priors about the magnitude
of treatment effects or their standard deviations. That said, the main results in Table 2
suggest that the study is powered to detect small effects. Column 2, our pre-registered
analysis, for example, identiﬁes signiﬁcant differences between the status quo and four of
the ﬁve treatment conditions. The only intervention for which we are not able to identify
such an effect is the SMS effect, of which the effect is extremely small.
In addition, this appendix presents the results of an ex post power calculation. Speciﬁ-
cally, we explore how large the sample size must be to detect an effect at at least the 0.05
level, given the observed data. To do so, we simulate our experiment 5,000 times across
a number of sample sizes, randomly drawing observations with replacement from the
observed data. Speciﬁcally, for each simulation, we randomly sample a dataset of sample
size Nfrom the actual data, and then use this sample to regress the dependent variable on
the treatment vector, recording whether or not the estimated p-value for that simulation
for each factorial treatment is less than or equal to 0.05. In this way, we use the observed
data to address questions related to sample size and the detection of effects for a given
alpha level. We repeat simulations for Nranging from 10 to 575 per treatment.
Figure A1 plots Non the x-axis and power – the percentage of p-values less than or
equal to 0.05 – for each factorial treatment on the y-axis. The results show that, for the
magnitude of the localization-related interventions, we are well-powered and, given our
blocked design, have sample size to spare. Related to the effects of canvassing and SMS
only interventions, even if we had increased our sample size by any appreciable amount,
we would reach the same conclusions.
12Figure A1: Ex Post Power Calculations
●●●●●●●●●●●●●●●●●●●●●●●
0.000.250.500.751.00
0 200 400
Sample Size (per cell)PowerTreatment
●Canvass
SMS
Local
Local+Canvass
Local+SMS
Note: Power given sample size conditional on the observed data. For each cell sample size
N, we randomly select Nobservations for each factorial cell, for a total sample size of 6 N.
We then regress the sampled outcome vector on the sampled treatment vector, recording
the p-values for each coefﬁcient. We do this 5,000 times for each N. Next, for each N, we
calculate the power (y-axis) by calculating the proportion of p-values less than or equal to
0.05, the chosen alpha-level. Vertical line indicates the number of units per treatment in
this study (279).
13E Monitoring
This section describes the monitoring effort that was carried out alongside the project. We
randomly selected ten percent of the 1,674 polling stations to be monitored.4We hired one
person per county to carry out the monitoring, given funding constraints. Because of this,
not all monitoring visits could be completed, simply due to logistical constraints of getting
from one randomly selected intervention site to another in a given day across an entire
county. As a result, of the 167 planned monitoring visits, 133 (80%) were carried out. Table
A4 presents descriptive statistics on the monitoring effort.
Table A4: Descriptive Statistics on Project Monitoring.
Canvassing Local Local+Canvassing Local+SMS
Assigned Visits 52 42 36 37
Completion Rate 73% 79% 81% 86%
Observed Compliance 61% 91% 79% 88%
Top row reports the number of monitoring visits assigned to a given treatment. Second row
presents the percentage of those visits completed. Third row presents the percentage of visits
where treatment compliance was directly observed by monitors.
For the interventions including localization, the monitors observed the correct inter-
vention 86% ((91+79+88)/3) of the time. For the canvassing-only intervention, monitors
directly observed compliance in 61% (23) of the 38 monitored interventions. For 12 of
the 38 monitored canvassing interventions, monitors did not directly observe the IEBC
canvassers in the ﬁeld. Given the mobility of canvassers, this could mean one of two things.
First, it could mean that the IEBC canvassers were not doing the intervention. Second, it
could mean that the IEBC canvassers were in the ﬁeld, but that the monitors could not
locate them. We cannot distinguish between these two possibilities, and report the directly
observed compliance above. If we assume that the two scenarios are equally likely, then
4In fact, we selected from the polling stations receiving localization and/or canvassing related interven-
tions. We did not aim to monitor the SMS intervention in the ﬁeld because this was provided directly by a
bulk SMS provider in Nairobi.
14canvassing would have been correctly implemented about 82% of the time, which is on
par with the localization interventions (86% compliance).
For 3 of the 38 monitored canvassing interventions, our monitors observed registration
occuring alongside canvassing. Our data during the intervention period corroborates this,
showing that a total of 31 individuals were registered during those two non-compliant
interventions, which is consistent with the effects of localization. This raises issues of
compliance, and how non-compliance may affect our results. This is especially a concern
in the canvassing intervention. More speciﬁcally, the small estimated effects could be the
result of noncompliance, rather than true treatment effect near zero. As discussed above,
there were two potential types of non-compliance – canvassers not showing up (which
would attenuate the estimated effect of canvassing) and canvassers showing up with voter
registration kits (which would magnify the effect of canvassing, assuming that localization
has some positive effect).
As a rough check of the within-intervention averages in the monitored units, the
veriﬁed canvassing treatment elicited 1.5 registered voters on average; veriﬁed localization
interventions yielded 10 registered voters on average. This comparison of the responses
observed in veriﬁed interventions suggests two things. First, the magnitude of the dif-
ference in response between canvassing and localization interventions is large. Second,
small amounts of non-compliance in the canvassing group in the form of non-compliant
localization could easily bias canvassing estimates upward, given the magnitude of this
difference. Put another way, upward bias from small amounts of non-compliance via local-
ization is likely to swamp downward bias from even moderate amounts of unimplemented
canvassing. This may be one reason why we see small positive effects to canvassing in
some speciﬁcations: given that CEC’s hope to meet registration quotas, they occassionally
sent registration equipment out with canvassers.
15More broadly, our compliance rates are in line with other work in the social sciences.5
Table A5 below presents compliance rates in a set of recent social science studies from
economics, public health, and political science, demonstrating compliance rates in those
studies. With respect to the speciﬁc context of working within ﬁeld ofﬁces of a national
quasi-government organization, the rates of compliance we observe are on par with
analogous rates of absenteeism in the education sector, which stood at 22% in 2015 (Jones
et al 2017).6This comparison supports the assertion that our intent-to-treat estimates are
based on credible real-world compliance (i.e., people showing up to work and doing their
job) if the interventions studied here were implemented as policy.
Table A5: Compliance in a Set of Recent Social Science Experiments
Citation Journal Intervention Compliance Rate
Bail et al. (2018) PNAS Twitter Bots 62%
Ichino and Schundeln (2012) JOP Observers 85%
Pop-Eleches et al. (2011) AIDS SMS 83%
Panagopoulos (2009) PRQ GOTV Calls 58% - 63%
Nickerson (2008) APSR Canvassing 33% - 46%
Kremer and Miguel (2004) Econometrica Medical Treatment 55% - 80%
Table reports information on treatment compliance as reported by authors in a set
of recent experimental papers. A number of the papers above reported compliance
information for multiple experiments, leading us to report a range of compliance
percentages.
5Our search of recent experimental work revealed that few experiments actually discuss or report on
compliance in detail.
6Chaudhury et al (2006) surveys the broader state-of-research on absenteeism in healthcare and education
in the developing world, and ﬁnding average rates of absenteeism at 19% in primary schools and 35% in
health centers.
16F Balance
The analyses in this paper rely on randomization, which guarantees that the treatment
and control areas are similar in expectation. In practice, however, it is possible for them to
differ simply by virtue of unlucky draws. To test this, we compare the different treatment
conditions across 19 variables:
•Poverty See Section C.1.
•Population Density See Section C.1.
•Distance See Section C.1.
•Nighttime Lights Version 4 DMSP-OLS Nighttime Lights Time Series, 2013, average
stable lights. Source: https://ngdc.noaa.gov/eog/dmsp/downloadV4composites.
html.
•Terrain Derived from SRTM DEM using raster package in R.
•Slope Derived from SRTM DEM using raster package in R.
•Elevation SRTM elevation data. Source: https://www2.jpl.nasa.gov/srtm/
•Health The probability of skilled birth attendance (SBA) during delivery. Source:
http://www.worldpop.org.
•Dependency Ratio Sub-national dependency ratios. The ratio of dependents (both
young, 0 to 14, and old, 65+) upon the working age population. Source: http:
//www.worldpop.org.
•Literacy Proportion of women aged 15-49 classed as literate in 2008. Source: http:
//www.worldpop.org.
•Mean Age Age of individuals that were registered to voter in the 2013 elections.
17Source: IEBC data.
•SD Age Standard deviation of age of individuals that were registered to voter in the
2013 elections. Source: IEBC data.
•Mean Pct. Youth Share of youth (individuals younger than 35 years old) that were
registered to vote in the 2013 elections. Source: IEBC data.
•Mean Pct. Female Share of women that were registered to vote in the 2013 elections.
Source: IEBC data.
•Votes Cast Total number of votes cast during the 2013 elections. Source: IEBC data.
•Kenyatta VS Share of votes cast for Kenyatta during the 2013 elections. Source: IEBC
data.
•Odinga VS Share of votes cast for Odinga during the 2013 elections. Source: IEBC
data.
•Turnout Percent of registered voters who voted in the 2013 elections. Source: IEBC
data.
Table A6 contains covariates stored as a raster extracted from a 500 meter radius of
the polling station. Table A7 contains covariates relating to the 2013 elections. Columns
1 and 2 in both tables state the intervention groups being compared. In total there are
15 comparisons between all combinations of treatment groups. The remaining columns
present bootstrapped Kolgorov-Smirnoff statistic p-values (Sekhon 2011), comparing the
distributions of covariates across interventions groups. The p-values suggest that our
samples are well-balanced, which is consistent with what is to be expected given the
random assignment.
18Table A6: Kolgorov-Smirnoff Balance Tests: Spatial Covariates
Group 1 Group 2 Poverty Pop. Dens. Distance Lights Terrain Slope Elev. Health Dep. Ratio Literacy
Canvass Control 0.99 1.00 0.99 0.16 0.67 0.67 0.86 0.50 1.00 0.70
SMS Control 1.00 0.95 0.98 0.52 0.73 0.66 0.99 0.86 0.99 0.73
Local Control 0.99 0.98 0.91 0.93 0.85 0.85 0.63 0.91 0.96 0.91
Loc.+Canv. Control 0.99 0.85 0.87 0.82 0.62 0.55 0.79 0.60 1.00 0.79
Loc.+SMS Control 0.97 0.99 0.94 0.74 0.95 0.94 0.98 0.67 0.70 0.80
SMS Canvass 0.98 0.93 0.98 0.54 0.95 0.98 0.95 0.65 0.99 0.19
Local Canvass 0.99 0.96 0.94 0.44 0.81 0.68 0.72 0.35 0.93 0.57
Loc.+Canv. Canvass 0.92 0.74 0.99 0.45 0.92 0.87 0.97 0.75 1.00 0.23
Loc.+SMS Canvass 0.95 0.98 0.98 0.17 0.44 0.39 0.91 0.33 0.97 0.39
Local SMS 0.99 0.94 0.95 0.55 0.57 0.44 0.94 0.60 0.98 0.87
Loc.+Canv. SMS 0.89 0.97 0.97 0.85 0.91 0.85 0.85 0.15 0.98 0.95
Loc.+SMS SMS 0.96 0.88 1.00 0.20 0.74 0.67 0.98 0.96 0.76 0.65
Loc.+Canv. Local 0.95 0.86 0.91 0.95 0.91 0.86 0.86 0.59 0.99 0.91
Loc.+SMS Local 0.93 0.66 0.93 0.81 0.82 0.88 0.97 0.44 0.53 0.78
Loc.+SMS Loc.+Canv. 0.87 0.86 0.96 0.45 0.93 0.92 0.86 0.07 0.91 0.98
Note: Columns 1 and 2 state the intervention groups being compared. Remaining columns present bootstrapped
KS statistic p-values, comparing the distributions of covariates across treatment arms.
19Table A7: Kolgorov-Smirnoff Balance Tests: 2013 Election-Related Covariates
Group 1 Group 2 Mean age SD age Pct. Youth Pct. Female Votes Cast Kenyatta VS Odinga VS Turnout Reg. Voters
Canvass Control 0.97 0.85 0.61 0.57 0.28 0.78 0.94 0.69 0.17
SMS Control 0.83 0.43 0.70 0.88 0.97 0.83 0.87 0.24 0.79
Local Control 0.99 0.97 0.60 0.95 0.71 0.92 0.98 0.65 0.78
Loc.+Canv. Control 0.93 0.39 0.84 0.08 0.12 0.89 0.84 0.53 0.10
Loc.+SMS Control 0.79 0.96 0.95 0.09 0.69 0.98 0.95 0.70 0.81
SMS Canvass 0.89 0.41 0.63 0.87 0.45 0.75 0.66 0.79 0.36
Local Canvass 0.84 0.94 0.96 0.58 0.34 0.97 0.97 0.97 0.65
Loc.+Canv. Canvass 0.80 0.12 0.86 0.21 0.69 0.90 0.76 0.80 0.48
Loc.+SMS Canvass 0.78 0.86 0.82 0.65 0.39 0.75 0.99 0.83 0.28
Local SMS 0.86 0.73 0.59 0.91 0.76 0.88 0.82 0.93 0.33
Loc.+Canv. SMS 0.77 0.73 0.77 0.11 0.11 0.45 0.76 0.50 0.01
Loc.+SMS SMS 0.98 0.62 0.84 0.27 0.38 0.97 0.75 0.73 0.40
Loc.+Canv. Local 0.99 0.34 0.84 0.28 0.25 0.96 0.89 0.91 0.59
Loc.+SMS Local 0.84 0.86 0.81 0.27 0.44 0.97 0.98 0.68 0.28
Loc.+SMS Loc.+Canv. 0.96 0.31 0.97 0.33 0.26 0.60 0.74 0.49 0.35
Note: Columns 1 and 2 state the intervention groups being compared. Remaining columns present bootstrapped KS statistic
p-values, comparing the distributions of covariates across treatment arms.
20G Treatment Effects Over Time
While localization is likely to be effective only on the two days of the intervention when
registration is present, information from canvassers or via SMS may motivate action in the
days after treatment. To capture this, the (pre-registered) treatment variable deﬁnes the
localization treatment as only the two days when the registration kit was at a randomly
selected polling station. In contrast, the canvassing and SMS-only treatments are coded
as the two intervention days plus a ten day “tail.” The remaining polling station-days
constitute the control condition. The ten-day tail and the asymmetry across interventions,
although pre-registered and based on discussions with the IEBC, is admittedly ad hoc. As
a result, in this appendix, we present results across a range of treatment speciﬁcations.
Figure A2 re-estimates the model used in Table 2’s columns 4 for different treatment
windows, respectively: 2 days of the intervention, 2 days of the intervention plus a ten-day
tail, 2 days of the intervention plus a twenty-day tail, etc.7Note that the dependent
variable is the average registrations per polling station per day. In other words, the total
number of registered individuals averaged over 2 days if there is no tail, averaged over 12
days if there is a 10 day tail, etc.
We ﬁnd that the effect of the interventions involving canvassing or SMS remains
similar as those report in Table 2. We also ﬁnd that the localization effect is driven solely
by the two-day period when the registration kit was present at the polling station. That is,
the effect is strong in the two days when the intervention takes place and then averages
out over time, with the effect approaching zero as more non-intervention days are added
to the tail.
7June 9, 2017 (207 days after the start of the intervention) was the last day that Kenyans could register to
vote. The ﬁgure therefore continues to two days plus a tail of 210 days.
21Figure A2: Treatment Effects for Different Time Windows
●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
●
●
●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
●
●
●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●
●
●
●● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ● ●Canvas SMS Local Local+Canvas Local+SMS2
12
22
32
42
52
62
72
82
92
102
112
122
132
142
152
162
172
182
192
202
2120.0000.0050.010
0.0000.0050.010
0.0000.0050.010
0.0000.0050.010
0.0000.0050.010
Number of Treatment DaysIntervention Effect
Note: Dependent variable is the average registrations per polling station per day. The
X-axis indicate the two intervention days plus the length of the "tail". Bars indicate 95%
conﬁdence interval.
22H Baseline Summary Information
Table A8 presents descriptive statistics for the three outcome variables and the number
of registered voters at a polling station, based on the month before the onset of the
intervention. The table also presents descriptive statistics for the number of registered
voters at a polling station in 2013 and the three blocking variables.
Table A8: Baseline Information
Mean SD Min Max Obs.
# Registrations per PS day 0.0293 0.4585 0.0000 33.0000 35112
# Registrations per PS day / 2013 reg. voters 0.0001 0.0009 0.0000 0.0833 35112
# Registrations per PS / 2013 reg. voters 0.0011 0.0044 0.0000 0.0833 1668
# Registrations per PS 0.6160 2.3608 0.0000 33.0000 1672
# Registered voters in 2013 540.5264 429.0525 2.0000 6764.0000 1668
Poverty 0.4267 0.1095 0.1810 0.8989 1668
Distance 20.9136 16.2380 0.0390 179.0235 1668
Density (x10,000) 5.2724 9.0415 0.0000 231.9146 1668
Note: PS = polling station. Polling station day information is based on the month preceding
the start of the intervention (November 14, 2016). Poverty, Distance and Density are deﬁned as
before.
As expected, the average daily number of registrations per polling station in the
month before the onset of the intervention (0.0293) is quite similar as those in the control
group during the intervention (row “Control mean” of columns 1 to 3 in Table 2: 0.0437).
23I Comparison Intervention Effects
Table 2 presents the size of the intervention effects. Do these effects differ from each
other? To answer this question, Table A9 presents p-values from Wald tests, comparing
the effects of the different interventions. The tests build on the model speciﬁcation that we
pre-registered (column 2 in Table 2). We ﬁnd that across all intervention effect comparisons,
with the exception of localization versus localization plus canvassing, are signiﬁcantly
different from each other.
Table A9: Comparison Intervention Effects
Intervention 1 Intervention 2 P-value
SMS Canvassing 0.00
Localization Canvassing 0.00
Localization SMS 0.00
Localization+Canvassing Canvassing 0.00
Localization+Canvassing SMS 0.00
Localization+Canvassing Localization 0.25
Localization+SMS Canvassing 0.00
Localization+SMS SMS 0.00
Localization+SMS Localization 0.00
Localization+SMS Localization+Canvassing 0.02
Note: P-values from Wald tests to compare intervention effects.
24J Literature Overview: Registration and Turnout
In this section, we summarize existing literature on improving voter registration and
Get Out To Vote (GOTV), and discuss the ﬁndings of those literatures in relation to our
results. Appendix Table A10 summarizes features of twenty papers from these two distinct
but related literatures. First, the table presents information on twelve (descriptive or
experimental) studies examining the determinants of voter registration. Inclusion criteria
were articles published in political science journals that have voter registration as an
outcome. Second, to situate the magnitude of our results in the much larger experimental
GOTV literature, the table summarizes an additional eight GOTV papers. Inclusion criteria
were publication since 2000, at least 100 citations on Google Scholar, and publication in the
AJPS, APSR or the JOP .8
The interventions or independent variables in the registration literature fall into three
categories: civic educational interventions, traditional GOTV interventions, and a residual
category. The civic educational interventions in Mvukiyehe et al (2017), Bennion et al
(2016), and Bratton et al (1999) vary widely. Mvukiyehe et al (2017) administered a civic
education program in Liberia, consisting of a presentation by educators and a town-hall
style discussion. The intervention in Bennion et al (2016) consisted of presentations to
students by professors or peers. Bratton et al (1999) evaluates a trio of diverse civic
education programs in Zambia, ranging from a formal course to community events. The
GOTV interventions encompass face-to-face canvassing (Nickerson 2015, Braconnier et al
2017) and mail/email (Bennion et al 2011, Nickerson et al 2007, Gosnell 1926, Mann et al
2019). The residual category examines the effect of election observers (Ichino et al 2012,
law changes (Mitchell et al 1995), and a lottery (John et al 2015).
8We considered less restrictive inclusion criteria (e.g., no time restriction, inclusion of observational
studies, or fewer citations), but the list quickly ballooned to over 50 GOTV studies.
25Broadly construed, the effects of experimental interventions on registration mirror
ﬁndings in the GOTV literature. Less expensive efforts like mail or email tend to generate
little effect (a maximum of 2.2% in Mann et al (2019)).9Civic educational interventions
have potentially larger effects (3.5% on average), while the canvassing studies report the
largest effects, averaging almost 9%. Note that this broad pattern is based on relatively few
studies testing a wide array of interventions in diverse contexts. Future work is needed to
verify the broad pattern. That said, the pattern mirrors the ﬁndings of our work: intensive
localization brings the largest gains, with less expensive canvassing and SMS providing
limited or no beneﬁt.
The magnitude of effects we ﬁnd are in line with the experimental effects found in
the small registration literature. Our study is perhaps most similar in spirit to Nickerson
(2015), who ﬁnds that registrations increase, on average, by about 4.4% in response to a
door-to-door registration drive. The magnitude we report is approximately half of this,
although the effect size varies by context. The poorest subgroup in Nickerson (2015)
reported a16.4
225=0.073 (7.3%) increase in registration, relatively to an effect of about 4.4%
in the poorest block in our intervention. Interestingly, Nickerson (2015) (p. 95) notes that
the average effect of 4.4% arises, “in the contemporary setting where voter registration
is moderately easy.” This raises the question as to whether the relatively modest results
we observe (relative to Nickerson (2015)) might be driven by the more onerous nature of
Kenya’s voter registration processes.
Registration effects, according to the table, seem to be small relative to turnout effects
reported in the GOTV literature. This may be for several reasons. First, GOTV interventions
are implemented on a different population of citizens than registration studies: registered
voters versus the broader eligible population. Registered voters may be more responsive
9We exclude the effects of the mail intervention in Gosnell (1926) from discussion, given that it provides
little of the standard statistical reporting like standard errors present in modern studies.
26to interventions, given that they have already cleared the registration hurdle. Second,
registration may constitute a more substantial barrier to participation for the eligible-but-
unregistered population relative to voting with respect to registered voters. As such, we
urge caution in directly comparing magnitudes of effects in the GOTV literature with the
registration literature.
27Table A10: Studies on Voter Registration and Turnout Outcomes
# Paper Journal Year Citations Country Treatment Reg? TO? RegATE TOATE Type RCT? Unit Design
1 Mvukiyehe et al (2017) WD 2017 21 Liberia Civic Ed. Yes Yes 1% 2% S/A Yes Indiv.; agg. Blocked
2 Ichino et al (2012) JOP 2012 167 Ghana Observers Yes No -4.1% NA A Yes EA Blocked
3 Mann et al (2019) ES 2019 1 USA Mail Yes Yes 2.2%* 2%* A Yes Indiv. Simple
4 Bennion et al (2016) PSP 2016 5 USA Presentation Yes Yes 6%* 2.6%* A Yes Indiv. Simple
5 Nickerson (2015) JOP 2015 43 USA Canvassing Yes Yes 4.4%* 2%* A Yes Street Simple
6 Gosnell (1926) APSR 1926 60 USA Mail Yes Yes 10% 10% A Yes Indiv. Simple
7 Mitchell et al (1995) PB 1995 163 USA Law Change Yes Yes 4.5% 3.8% S/A No Indiv.; State -
8 Nickerson (2007) QJPS 2007 119 USA Email Yes Yes 0% 0% A Yes Agg. Simple
9 Braconnier et al (2017) APSR 2017 45 France Canvassing Yes Yes 13%* 0% A Yes Apartment; Indiv. Simple
10 Bratton et al (1999) WD 1999 107 Zambia Civic Ed. Yes Yes 26.1% 13.6% S No Indiv. -
11 Bennion et al (2011) PRQ 2011 51 USA Email Yes No -0.3% NA A Yes Indiv. Simple
12 John et al (2015) ES 2015 17 USA Lottery Yes No 1.9% NA A Yes Household Simple
13 Chong et al (2014) JOP 2014 177 Mexico Info. No Yes NA -1.3%* S/A Yes Indiv.; agg. Blocked
14 Gerber et al (2000) APSR 2000 1381 USA Mail No Yes NA 8.7%* A Yes Indiv.; agg. Simple
15 Gerber et al (2003) AJPS 2003 685 USA Mail No Yes NA 10.2% A Yes Indiv. Simple
16 Gerber et al (2008) APSR 2008 1145 USA Mail No Yes NA 8.1%* A Yes Indiv. Simple
17 Lassen (2005) AJPS 2005 365 Denmark Info. No Yes NA 9.5%* S No Indiv. -
18 Krasno et al (2008) JOP 2008 181 USA TV No Yes NA 0.7% A No Market -
19 Gerber et al (2000) JOP 2000 168 USA Leaﬂet No Yes NA 7.2%* A Yes Indiv. Simple
20 De La O (2013) AJPS 2013 227 Mexico CCT No Yes NA 5.3%* A Yes Precinct Simple
Note: Inclusion criteria for GOTV papers are papers published in the last 10 years with at least 100 citations in AJPS, APSR, or JOP . For registration
studies (a much smaller literature), we included a wider range of papers with no time restriction, searching for research examining voter regis-
tration as an outcome. Citations are taken from Google Scholar, August 7, 2019. Journal: WD = World Development, JOP = Journal of Politics,
ES = Electoral Studies, PSP = Political Science and Politics, APSR = American Political Science Review, PB = Political Behavior, QJPS = Quarterly
Journal of Political Science, PRQ = Political Research Quarterly, AJPS = American Journal of Political Science. means that the estimate is signif-
icantly different from zero. Cells without a were either insigniﬁcant or did not report standard errors. Outcome type includes survey (S) and
administrative (A).
28K External Validity
Figures A3 shows the joint distribution of poverty (horizontal axis) and logged population
density (vertical axis) for the selected polling stations (white circles) and the full population
of polling stations nationally (gray circles). This shows that the population of polling
stations that we sampled from largely overlap the distribution of all pollings stations. The
areas that do not overlap are urban areas (where things like localization are unlikely to
be effective given the relatively close nature of registration opportunities) and pastoralist
areas (where localization is problematic given the mobile nature of populations.)
29Figure A3: Comparison National and Sample Distributions
Note: Polling stations in the experimental sample are colored in white; out of sample
polling stations for the rest of the country are in black. Given that we do not have data for
out-of-sample polling stations on distance to the constituency election ofﬁce, we present
only the joint distribution of poverty and population density here.
30L Deviations from Pre-Analysis Plan
This study was preregistered at the EGAP registry (ID: 20170307AA). The registration took
place prior to researcher access to outcome data. In this section, we discuss deviations
from the pre-analysis plan, additional analyses not preregistered and items that were
preregistered but not undertaken.
First, we note one deviation from the pre-analysis plan.
•Initially, we envisioned the model used in column 2 of Table 2 to be our main
speciﬁcation. Instead, the paper builds further on the model used for column 5.
First, column 5 measures the outcome as the percentage of registered voters at a
polling station in 2013, which simpliﬁes interpretation of effect sizes. Second, column
5 collapses the temporal element of our data to the polling station level, which
provides more conservative estimates. As we discuss in the text, both sets of results
lead to the same substantive conclusions.
Second, the following analyses were not pre-registered:
•Initially, we aimed to understand only the short-term effects of the interventions.
However, because the interventions may have very different long run implications
we also added Section 4.3, and explore the downstream effects of localization on
election day eight months later.
•In Section 4.2 we conduct block-level analyses based on levels of poverty, distance
and density. We did not initially pre-register density as a subgroup. This was an
omission, because the design is blocked across these three factors.
•The pre-analysis plan did not include a discussion around statistical power. We have
added such a discussion, including ex post power calculations, in Appendix D.
31Finally, we list items that we preregistered but did not include in the document.
•We explore differential treatment effects by distance, poverty and density. We also
suggested to conduct subgroup analysis by voter characteristics (i.e. gender and age
groups), county characteristics (i.e. level of opposition support), and other project
design features (i.e. treatment dosage of SMS message, IEBC staff characteristics,
day of implementation). In the document we only focus on distance, poverty and
density for two reasons. First, the design explicitly blocked for these characteristics.
Second, including all of the possible pre-registered analyses in one paper would be
impossible, given journal word limits.
•We suggested testing for geographic spillovers; that is, whether the interventions
have effects on polling station areas not directly targeted by the IEBC. We have not
done so, and explicitly note in our design strategy above how experimental design
aims to avoid spillovers.
•We measure the dependent variable in two ways: as the raw counts of registered
voters, and as the percentage of registered voters at a polling station in 2013. Initially,
we also suggested to measure the outcome as raw counts divided by population
density in an 1.5 kilometer radius around the polling station. However, two factors
prevent this approach. First, satellite data used to measure population density is not
sufﬁciently accurate to facilitate such an analysis. Second, aggregating spatial popu-
lation density data to the polling station level requires some reasonable deﬁnition
of the catchment area of the polling station. Since IEBC does not deﬁne – and the
law does not require – residency within a particular surveyed catchment area, this
approach seemed fraught with ad hoc decisions about how to deﬁne catchment areas
across regions with massive geographic variation. As a result, we did not proceed
with this analysis, as the assumptions and decisions we would have to make seemed
32indefensible, given the reality on the ground.
33M References Appendix
•Bail, Christopher A, Lisa P Argyle, Taylor W Brown, John P Bumpus, Haohan Chen,
Fallin Hunzaker, Jaemin Lee, Marcus Mann, Friedolin Merhout, and Alexander
Volfovsky. 2018. “Exposure to Opposing Views on Social Media can Increase Political
Polarization.” Proceedings of the National Academy of Sciences 115 (37): 9216-21.
•Bennion, Elizabeth A., and David W. Nickerson. 2011. “The Cost of Convenience:
An Experiment Showing E-Mail Outreach Decreases Voter Registration.” Political
Research Quarterly 64 (4): 858-69.
•Bennion, Elizabeth A., and David W. Nickerson. 2016. “I Will Register and Vote,
if You Teach Me How: A Field Experiment Testing Voter Registration in College
Classrooms.” PS - Political Science and Politics 49 (4): 867-71.
•Braconnier, Celine, Jean-Yves Dormagen, and Vincent Pons. 2017. “Voter Registration
Costs and Disenfranchisement: Experimental Evidence from France.” American
Political Science Review 111 (3): 584-604.
•Bratton, Michael, Philip Alderfer, Georgia Bowser, and Joseph Temba. 1999. “The
Effects of Civic Education on Political Culture: Evidence from Zambia.” World
Development 27 (5): 807-24.
•Campbell, David E., and Richard G. Niemi. 2016. “Testing Civics: State-Level
Civic Education Requirements and Political Knowledge.” American Political Science
Review 110(3): 495-511.
•Chaudhury, Nazmul, Jeffrey Hammer, Michael Kremer, Karthik Muralidharan, and
F Halsey Rogers. 2006. “Missing in Action: Teacher and Health Worker Absence
in Developing Countries Background on Education and Health Care Systems in
34Developing Countries.” Journal of Economic Perspectives 20 (1): 91–116.
•Dunning, Thad, Guy Grossman, Macartan Humphreys, Susan D Hyde, Craig Mcin-
tosh, Gareth Nellis, Claire L Adida, et al. 2019. “Voter Information Campaigns and
Political Accountability: Cumulative Findings from a Preregistered Meta-Analysis of
Coordinated Trials.” Science Advances 5: 1-11.
•Finkel, Steve E., Christopher A. Sabatini, and Gwendolyn G. Bevis. 2000. “Civic
Education, Civil Society, and Political Mistrust in a Developing Democracy: The Case
of the Dominican Republic.” World Development 28 (11): 1851-74.
•Finkel, Steven E. 2002. “Civic Education and the Mobilization of Political Participa-
tion in Developing Democracies.” The Journal of Politics 64 (4): 994-1020.
•Finkel, Steven E. 2005. “Civic Education in Post-Apartheid South Africa: Alternative
Paths to the Development of Political Knowledge and Democratic Values.” Political
Psychology 26 (3): 333-64.
•Finkel, Steven E, Jeremy Horowitz, and Reynaldo T Rojo-Mendoza. 2012. “Civic
Education and Democratic Backsliding in the Wake of Kenya’s Post-2007 Election
Violence.” The Journal of Politics 74 (1): 52-65.
•Finkel, Steven E., and Amy Erica Smith. 2011. “Civic Education, Political Discus-
sion, and the Social Transmission of Democratic Knowledge and Values in a New
Democracy: Kenya 2002.” American Journal of Political Science 55 (2): 417-35.
•Gerber, Alan S, and Donald P Green. 2000. “The Effect of a Nonpartisan Get-Out-
the-Vote Drive: An Experimental Study of Leaﬂetting.” Journal of Politics 62 (3):
846-57.
•Gerber, Alan S, Donald P Green, and Christopher W Larimer. 2008. “Social Pressure
35and Voter Turnout: Evidence from a Large-Scale Field Experiment.” American
Political Science Review 102 (1): 33-48.
•Gottlieb, Jessica. 2016. “Greater Expectations: A Field Experiment to Improve
Accountability in Mali” American Journal of Political Science 60 (1): 143-57.
•Gosnell, Harold F. 1924. “An Experiment in the Stimulation of Voting.” American
Political Science Review 20 (4): 869-74.
•Green, Donald P ., Peter M. Aronow, Daniel E. Bergan, Pamela Greene, Celia Paris,
and Beth I. Weinberger. 2011. “Does Knowledge of Constitutional Principles Increase
Support for Civil Liberties? Results from a Randomized Field Experiment.” Journal
of Politics 73 (2): 463-76.
•Ichino, Nahomi, and Matthias Schundeln. 2012. “Deterring or Displacing Electoral
Irregularities? Spillover Effects of Observers in a Randomized Field Experiment in
Ghana.” The Journal of Politics 74 (01): 292-307.
•John, Peter, Elizabeth MacDonald, and Michael Sanders. 2015. “Targeting Voter
Registration with Incentives: A Randomized Controlled Trial of a Lottery in a London
Borough.” Electoral Studies 40: 170-75.
•Jones, Jessica, Rachel Jones, and Andres Mojica. 2017. “Teacher Allocation and
Absenteeism in Kenya, Tanzania, and Uganda: 2015.” Georgetown University
McCourt School of Public Policy Research Report.
•Krasno, Jonathan S., and Donald P . Green. 2008. “Do Televised Presidential Ads
Increase Voter Turnout? Evidence from a Natural Experiment.” Journal of Politics 70
(1): 245-61.
•Langton, Kenneth P ., and M. Kent Jennings. 1968. “Political Socialization and the
36High School Civics Curriculum in the United States.” American Political Science
Review 62(3): 852-67.
• Lassen, David Dreyer. 2005. “The Effect of Information on Voter Turnout: Evidence
from a Natural Experiment.” American Journal of Political Science 49 (1): 103-18.
•Mann, Christopher B., and Lisa A. Bryant. 2019. “If you Ask, They Will Come (to
Register and Vote): Field Experiments with State Election Agencies on Encouraging
Voter Registration.” Electoral Studies forthcoming: 1-10.
•Miguel, Edward, and Michael Kremer. 2004. “Worms: Identifying Impacts on
Education and Health in the Presence of Treatment Externalities.” Econometrica 72
(1): 159-217.
•Mitchell, Glenn E., and Christopher Wlezien. 1995. “The Impact Constraints of Legal
on Voter of the Composition American Electorate.” Political Behavior 17 (2): 179-202.
•Mvukiyehe, Eric, and Cyrus Samii. 2017. “Promoting Democracy in Fragile States:
Field Experimental Evidence from Liberia.” World Development 95: 254-67.
•Nickerson, David W. 2007. “Does Email Boost Turnout?” Quarterly Journal of
Political Science 2 (4): 369-79.
•Nickerson, David W. 2008. “Is Voting Contagious? Evidence from Two Field Experi-
ments.” American Political Science Review 102 (1): 49–57.
•Nickerson, David W. 2015. “Do Voter Registration Drives Increase Participation? For
Whom and When?” Journal of Politics 77 (1): 88-101.
•Panagopoulos, Costas. 2009. “Partisan and Nonpartisan Message Content and Voter
Mobilization Field Experimental Evidence.” Political Research Quarterly 62: 70-76.
•Pasek, Josh, Lauren Feldman, Daniel Romer, and Kathleen Hall Jamieson. 2008.
37“Schools as Incubators of Democratic Participation: Building Long-Term Political
Efﬁcacy with Civic Education.” Applied Developmental Science 12 (1): 26-37.
•Persson, Mikael, and Henrik Oscarsson. 2010. “Did the Egalitarian Reforms of the
Swedish Educational System Equalise Levels of Democratic Citizenship?” Scandina-
vian Political Studies 33 (2): 135-63.
• Pop-Eleches, Cristian, Harsha Thirumurthy, James P Habyarimana, Joshua G Zivin,
Markus P Goldstein, Damien de Walque, Leslie MacKeen, et al. 2011. “Mobile Phone
Technologies Improve Adherence To Antiretroviral Treatment In a Resource-limited
Setting: A Randomized Controlled Trial of Text Message Reminders.” AIDS 25 (6):
825-34.
•Sekhon, Jasjeet S. 2011. “Multivariate and Propensity Score Matching Software with
Automated Balance Optimization: The Matching Package for R.” Journal of Statistical
Software 42 (7): 1-52.
•Sondheimer, Rachel Milstein, and Donald P Green. 2010. “Using Experiments to
Estimate the Effects of Education on Voter Turnout.” American Journal of Political
Science 54 (1): 174-89.
•Slomcyznski, Kazimierz M., and Goldie Shabad. 2003. “Can Support for Democracy
and the Market Be Learned in School? A Natural Experiment in Post-Communist
Poland.” Political Psychology 19 (4): 749-79.
38