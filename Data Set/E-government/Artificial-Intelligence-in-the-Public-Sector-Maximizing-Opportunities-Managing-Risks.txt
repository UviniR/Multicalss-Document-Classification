Artificial Intelligence 
in the Public Sector
Maximizing Opportunities, Managing Risks
GOVERNANCE
GOVERNANCE
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT 
Supported by the GovT ech Global Partnership: www.worldbank.org/govtech
Republic of Korea
Public Disclosure Authorized Public Disclosure Authorized Public Disclosure Authorized Public Disclosure Authorized© 2020 International Bank for Reconstruction and Development / The World Bank
1818 H Street NW, Washington DC 20433
Telephone: 202-473-1000; Internet: www.worldbank.org
Some rights reserved.
This work is a product of the staff of The World Bank with external contributions. The findings, 
interpretations, and conclusions expressed in this work do not necessarily reflect the views of 
The World Bank, its Board of Executive Directors, or the governments they represent. The World 
Bank does not guarantee the accuracy of the data included in this work. The boundaries, colors, 
denominations, and other information shown on any map in this work do not imply any judgment 
on the part of The World Bank concerning the legal status of any territory or the endorsement or 
acceptance of such boundaries.
Nothing herein shall constitute or be considered to be a limitation upon or waiver of the privileges 
and immunities of The World Bank, all of which are specifically reserved.
Rights and Permissions
This work is available under the Creative Commons Attribution 3.0 IGO license (CC BY 3.0 IGO),  
http://creativecommons.org/licenses/by/3.0/igo. Under the Creative Commons Attribution 
license, you are free to copy, distribute, transmit, and adapt this work, including for commercial 
purposes, under the following conditions:
Attribution—Please cite the work as follows: 2020. Artificial Intelligence in the Public Sector | 
Maximizing Opportunities, Managing Risks. EFI Insight-Governance. Washington, DC: World Bank.
Translations—If you create a translation of this work, please add the following disclaimer along 
with the attribution: This translation was not created by The World Bank and should not be 
considered an official World Bank translation. The World Bank shall not be liable for any content 
or error in this translation.
Adaptations—If you create an adaptation of this work, please add the following disclaimer along 
with the attribution: This is an adaptation of an original work by The World Bank. Views and 
opinions expressed in the adaptation are the sole responsibility of the author or authors of the 
adaptation and are not endorsed by The World Bank.
Third-party content—The World Bank does not necessarily own each component of the content 
contained within the work. The World Bank therefore does not warrant that the use of any third-
party-owned individual component or part contained in the work will not infringe on the rights 
of those third parties. The risk of claims resulting from such infringement rests solely with you. 
If you wish to reuse a component of the work, it is your responsibility to determine whether 
permission is needed for that reuse and to obtain permission from the copyright owner. Examples 
of components can include, but are not limited to, tables, figures, or images.
All queries on rights and licenses should be addressed to World Bank Publications, The World 
Bank Group, 1818 H Street NW, Washington, DC 20433, USA; e-mail: pubrights@worldbank.org.
Cover design and layout: Diego Catto / www.diegocatto.comContents>>>
Foreword 5
Acknowledgments 6
Executive Summary 7
Priorities Going Forward 9
Abbreviations 11
1. Introduction 12
Methodology and Scope 14
2. AI Opportunities 15
Use Cases 19
AI in Corruption 20
AI for Citizen Engagement 22
AI in Customs 23
AI in Health 23
AI in the Judicial Sector 26
AI In Procurement 27
AI in T ax Compliance 28
AI in T ax Policy 30
AI in Audit 32
3. AI Risks 33
Performance, Trust, and Bias 33
Cybersecurity 36
Control 37
Privacy 37
4. AI Governance and Operations 38
AI Ethical Principles 38
Role of a Central Government Agency 44
AI Operations Framework 45
Innovative Procurement Examples 51
Role of the Public Sector in Society 52
AI Operationalization in World Bank Projects 53
5. Ethical Considerations 54
Inequality 55
Control 55
Concentration 56
6. Government’s AI Building Blocks 57
Whole-of-Government Architecture 57
Interoperability Patterns 61
Data Standards 62
7. Conclusions 64
Priorities Going Forward 65
Appendix A. AI T echnical Primer 67
Appendix B. AI and the Sectors 92
Glossary 101
References 102Box 1. Actionable Insight:
Adopt Principles of AI and Issue an AI Governance Model41
Box 2. Private Sector AI Principles 43
Box 3. Procurement: Important Steps to Consider 53
Box 4. Data Fabric in Brief 58
Box 5. Blockchain: Distributed Ledger T echnology 61
Box 6. Actionable Insight: Data Fabrics Can Overcome Silos 61
Box 7. Actionable Insight: Governments Should Standardize Data 62Boxes
Figure 1 - Fixed Broadband Subscriptions per 100 Inhabitants, 2001–2019 17
Figure 2: The disparity in ICT Skills across the Regions 18
Figure 3. Wuhan Neural Network Model with Quarantine Control 24
Figure 4. Italy Neural Network Model with Quarantine Control 24
Figure 5. South Korea Neural Network Model with Quarantine Control 24
Figure 6. U.S. Neural Network Model with Quarantine Control 24
Figure 7. Results of COVID-19 Analysis by AI 25
Figure 8. An Optimal T ax Policy Optimizes a Balance between Equality and 
Productivity31
Figure 9. Global Consensus on the Principles of AI 40
Figure 10. AI Business Case Assessment 47
Figure 11. Operationalizing AI 48
Figure 12. Singapore Procurement Model 51
Figure 13. General Data Fabric Architecture for Whole-of-Government Use 59
Figure 14. High-Level Data Fabric Architecture 60Figures
T able 1. AI Readiness Index 16
T able 2. Role of humans - Five Levels of AI Adoption 19
T able 3. AI Risk Mitigation Framework 35T ablesForeword>>>
Disruptive technologies like artificial intelligence (AI), mobile apps, Internet of Things, block-
chain, cloud computing, and data analytics have the potential to transform governments by 
enhancing personalized service delivery experience, improve back-end process efficiencies, 
and strengthening policy compliance. One of the most promising disruptive technologies, AI is 
already being adopted by the digitally advanced governments to maximize its potential benefits. 
And this trend is catching up with other governments as well. More than 50 governments have 
issued or are in the process of issuing AI strategies in recent years. 
However, in many of our client countries, the public sector’s ability to adopt AI is hampered by 
low access to digital skills, insufficient foundational digital technologies, and inadequate digital 
data as well as a lack of awareness of the potential of AI. These differences in the pace of AI 
adoption in the public sector could further exacerbate inequalities between the rich and the poor 
countries. To promote wider AI adoption in our client governments, this paper provides a prelimi -
nary synthesis of the existing opportunities, risks, and building blocks required for implementing 
and integrating AI in their operations. The paper also highlights policy, governance and people 
aspects necessary for AI implementation, as there are no shortcuts to technology adoption. The 
use of technology cannot be fast-tracked as many of the analog complements needed for adop -
tion are not yet in place (World Bank 2016).
To better understand the role AI can play in public sector transformation, the World Bank pro-
duced this paper in partnership with the Swiss State Secretariat for Economic Affairs. This paper 
aims to distill the existing knowledge on the use of AI in the public sector and summarize the 
lessons learned from early adopters. It draws on the accumulated literature, case studies, and 
emerging trends to provide guidance to our teams working in this field. The World Bank’s tech-
nical team benefited from a panel of experts from inside the World Bank and from the industry 
who shared their insights and enriched the paper. The goal is to alert our staff and clients to the 
opportunities, risks, and the potential to foster AI for public sector transformation.
Edward Olowo-Okere
Global Director 
Governance and Institutions
5
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Acknowledgments>>>
This paper was prepared by a World Bank team consisting of Khuram Farooq (Senior Gover-
nance Specialist in the Global Governance Practice (GGP) and Bartosz Sołowiej (Consultant).
The team received valuable guidance from Edward Olowo-Okere (Global Director, GGP) who 
is leading the GovTech agenda in the Bank; Tracey Marie Lane and Adenike Sherifat Oyeyiola 
(Practice Managers, GGP); Kimberly Johns (Senior Governance Specialist); and Cem Dener 
(Lead Governance Specialist, GGP).
The team benefited from the comments of external peer reviewers: Aaron Moffatt (Chief Technol -
ogy Officer, Immersion Analytics) and Brittan Heller (Global Head of AI, Foley Hoag LLP, Harvard 
Tech, and Human Rights AI Fellow 2019). The team is also grateful for contributions from the 
World Bank’s reviewers: Aki Ilari Enkenberg (Senior Digital Development Specialist, IDD02), 
David Santos (Senior Public Sector Specialist, ELCG2), Jana Kunicova (Senior Public Sector 
Specialist, EA1G2), Parminder P.S. Brar (Lead Governance Specialist, GGP), and Trevor Mon-
roe (Senior Operations Officer, DECAT).
The team also wishes to express its thanks to Barbara Joan Rice (Consultant, World Bank) 
and Mary A. Kent (Working Copy Editor) for their editorial support; to Jasmine N. Brown (Intern, 
Foley Hoag LLP) for her research support, and Angela Hawkins (Team Assistant) for her format-
ting expertise. Finally, our thanks to Richard Crabbe for his editorial work and communications 
advice.
This paper was financed by the State Secretariat for Economic Affairs of Switzerland (SECO). We 
gratefully acknowledge this excellent partnership with SECO to promote the GovTech agenda.
6
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Executive Summary>>>
Disruptive technologies like Artificial Intelligence (AI) offer new opportunities to governments 
facing development challenges, especially now as fiscal stress is causing many governments to 
find new solutions to improving services without increasing the costs. Artificial Intelligence can 
be defined as the ability of the software systems to carry out tasks that usually require human 
intelligence: vision, speech, language, knowledge, and search. 
Many governments view AI as a strategic resource for competitiveness and growth and are em-
bracing it with speed and priority. According to Bughin et al. (2018), AI can potentially contribute 
$13 trillion to the global economy by 2030.1 At least 50 governments have developed or are in 
the process of developing an AI strategy. However, the pace of AI adoption is uneven, and most 
countries are not ready for AI adoption. There is no country from Africa or Latin America in the list 
of the top 20 countries on the AI Readiness Index developed by Oxford Insights.2 Except for four 
economies, Asia-Pacific is also one of the worst-performing regions on this list. Slower adoption 
of AI in our client countries may lead to further inequality between the rich and the poor nations
To reduce these inequalities, opportunities should be explored through the initiation of AI proj-
ects in areas of strategic impact and priority. Chapter 2 on AI opportunities provides examples of 
AI use from around the world. Moreover, it provides operational guidance on AI implementation 
on fundamental questions relating to developing country contexts. It broadens the perspective 
to explore opportunities for implementation. Government AI deployments exist in every sector. A 
common pattern of use cases includes citizen engagement, compliance and risk management, 
fraud and anti-corruption, business process automation, service delivery, asset management, 
and analytics for decision-making and policy design.
While AI should be explored to solve complex problems, associated adverse consequences in 
client contexts should also be fully understood and managed as AI comes with additional risks 
that could exacerbate the problems facing the public sector. Chapter 3 summarizes these risks. 
The ethical use of AI is fundamental to managing the adverse consequences of AI use in public 
policy. The ethical use of AI means that these systems should not harm humans. Rather, they 
are used to enhance overall human wellbeing. For example, an AI system that renders people 
jobless on a wide-scale, makes a biased decision against an ethnic minority applicant on eligibil -
ity for government welfare assistance or is used to propagate fake news on social media would 
be unethical. On the other hand, however, an AI system that improves anti-fraud measures 
through the reconciliation of multiple large data sets, facilitates medical diagnosis through image 
recognition, or enhances learning outcomes through tailored access to learning material, would 
be considered ethical and therefore, human-centered AI. 
National level public policy response is needed to address these ethical issues. Inequality could 
rise due to unemployment, the lowering of wages for low-skilled workers, and the vulnerability 
of some communities to bias in AI-based automatic decisions. Control could increase due to 
1. Bughin et al., 2018.
2. Government AI Readiness Index 2019.
7
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>state surveillance of citizens, robot-induced propaganda and 
fake news on social media and use of AI-enabled weapons 
like drones. The concentration of wealth could accentuate mo-
nopolies as a few firms with the AI resources could dominate 
the market and lead to net resource flows from the developing 
to the developed countries where these firms are based.
To manage the risks and maximize the opportunities of adopt-
ing AI in the public sector, the government should prepare an 
AI policy and governance frameworks to help guide the ethical 
use of AI and to provide clarity about AI principles and priori -
ties. Following the adoption of AI policy and the development of 
a roadmap, an operating framework will anchor the principles 
as the use of AI is rolled out. Chapter 4 on governance and 
operations provides more details of the models and AI com-
pliance frameworks currently in use. The models are funda-
mentally important to help guide the government in protecting 
the sanctity of human life throughout phases of AI adoption in 
the public sector. Governments also adopt basic principles to 
promote human-centered use of AI. These principles include 
personal data privacy, accountability, cybersecurity, transpar-
ency and explainability, fairness and non-discrimination, hu-
man control of technology, and human values. 
A central innovation hub for AI could help pool scarce resourc-
es to support the initiatives of line ministries. In the use cases, 
most governments have set-up the main hub for AI that serves 
as a central authority over decentralized projects among line 
agencies. The AI hub helps them in several ways. It central-
izes talent that guides and supports the line agency, connects 
industry expertise to the line agency, promotes research, and 
builds alliances with academic institutions and the private sec-
tor. It also helps connect with AI organizations internationally 
to exchange knowledge and resources. Neighboring countries 
that have a forum for coordination at the political level can 
develop regional AI innovation hubs suitable to many of the 
World Bank’s client countries.
Innovation procurement frameworks provide agility for ex-
perimentation. In digitally advanced governments, a problem-
driven request for proposal (RFP), rather than a tender with 
solution specifications, is developed and launched under inno -
vative procurement methods. These methods allow an initial 
award of a small scope proof-of-concept contract to more than 
one vendor to compare a range of solution options and decide 
the best option for further scale-up. World Bank task teams 
could adopt these approaches in consultation with procure -
ment colleagues and other available technical resources in 
the Bank, such as the GovTech team, Innovation Lab, Digital 
Development teams, Innovations in Big Data and Analytics for 
Development Program, and other sector colleagues.Most early adopters are embracing a design thinking frame-
work and agile methodology. These include a staged iterative 
approach to implementation—ideation (problem definition), 
conceptualization, proposal, procurement, prototype, testing, 
deployment, and scaling up. A feedback learning loop is built 
into the design at every stage.
Adopting a government-wide data fabric architecture will help 
governments leverage cutting-edge technologies to address 
data silos in a cost-efficient manner. The initial focus should be 
on foundational technologies, interoperability, open data, and 
standardization of data across government. Chapter 6 on AI 
building blocks illustrates the technology foundations for this 
architecture. The data fabric architecture will serve as the com-
mon denominator for standardized data interchange among 
the multitudes of subject-area specific applications such as an 
integrated financial management information system, payroll, 
tax administration systems, e-procurement, health manage -
ment system, population census, and geographical informa-
tion systems, among others. This architecture should be built 
on agile principles, evolve organically, and engender trust. 
Cloud computing offers immense opportunities to harness the 
power of such an architecture with agility. Inadequate founda -
tional digital technologies, quality of data, and digital skills are 
the major barriers to AI adoption in developing countries and 
constitute critical elements of the digital divide.
AI threats during implementation need to be carefully as-
sessed and mitigation actions planned. Threats include per-
formance and bias, cybersecurity, control, and privacy. These 
risks should be managed at the implementation agency level, 
while broader ethical issues need policy action at higher lev-
els. Chapter 3 offers steps toward risk mitigation. Involving 
stakeholders is crucial to mitigating risk, especially among 
groups most vulnerable to bias. Additionally, transparency and 
explainability could strengthen accountability. Compliance 
with privacy and data protection regulations is critical.
Governments and world leaders are instrumental in guiding 
the transition to automation and AI. They can provide lead -
ership to influence the trajectory of AI adoption among citi-
zens at national and international levels. This will help avoid 
adverse consequences and reap productivity gains. National 
governments could choose global guiding principles that will 
inevitably shape the acceptance or rejection of AI. Since AI will 
have a profound influence on service delivery, citizen engage -
ment, and core operations, it is imperative to help formulate 
a cohesive governance model that supports the process of 
ethical implementation.
8
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Based on the issues highlighted in the discussion, several pri-
orities could be considered by policymakers.
• Governments must adopt policies and governance 
frameworks that promote human-centric AI while 
maximizing opportunities.  A few aspects of the policy 
framework are mentioned below:
 »AI policy anchored in ethical principles would 
be essential. It could be tailored to specific settings 
but should be approved at the policy level to provide 
the authorizing environment. Governments in many 
settings have issued AI strategies approved by the 
parliament, president, prime minister, or the cabinet. 
These policies should be based on ethical principles. 
Governance and operational framework are essential 
to specify broad guidelines and institutional arrange -
ments. An innovation hub could be established to pool 
talent, establish partnerships with academia and the 
private sector, promote research, and facilitate ex-
perimentation by line ministries. The innovation hub 
should source the best talent through adequate in-
centives. Innovative procurement approaches should 
be adopted to leverage private sector skills with agility 
to allow iterative, problem-driven approaches to the 
RFP. The implementation teams should also manage 
the risks associated with AI, including bias, security, 
and unintended consequences, among others.
 »Promote transparency and accountability through 
inclusion and multi-stakeholder engagement at 
every step of the AI policy design and implemen-
tation. Affected communities and populations should 
be informed and provided with avenues for contesting 
AI logic without delays and hurdles. »Adverse ethical implications of AI could be man-
aged through broader economic policies. These 
could include industrial policy, tax policy, competition 
policy, human capital policy, among others. These 
policies should aim to develop human capital, ensure 
fair competition, incentivize human-enhancing AI so-
lutions, among others. 
 »These policies should also promote digital skills, 
and broader education in science, technology, 
engineering, and mathematics (STEM) to support 
people as they adjust to the shifting nature of work 
in the coming decades. Unskilled people and disad-
vantaged groups should be given special attention.
 »The regulatory framework to fight online propa-
ganda, misinformation, libel, and cybercrimes 
should be given priority. Also, governments could 
establish agency mandates to monitor policy compli-
ance and track, prevent, and investigate disinformation 
to protect their citizens. Engagement with social media 
Big Tech—Facebook, Instagram, and Twitter—should 
aim at encouraging the deployment of AI tools and pro-
fessional fact-check partnerships to take down content 
that is malicious, hateful, propagandist, and false.
 »Strengthen privacy, data protection, and civil lib -
erties and monitor compliance, which is typically 
weak in most settings. Promoting full disclosure of 
information being tracked by AI and robots through 
transparency frameworks should also be strength-
ened. Civil liberties and privacy are at a particular risk 
of infringement, which should be addressed through 
these regulatory frameworks.Priorities Going Forward
9
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>• Investments should be made in human capital and 
digital infrastructure. AI research, digital skills, AI entre-
preneurship, and foundational digital technologies could 
be prioritized.
 »Investments should be directed to fund research, 
education, and digital skills development pro-
grams in general and in AI in particular. They 
could include scholarships, apprenticeships, and re-
search funding in AI, computer science, STEM educa -
tion, and AI-related disciplines such as data science. 
Special emphasis could be given to disadvantaged 
groups such as women, minorities, and those at risk 
of being left behind.
 »Innovative entrepreneurship could be promoted.  
This could be done through an innovation fund, loan 
programs through state development banks, income-
contingent loans for students or others, and small 
business loan programs. Variations of these funding 
modalities are already used in Brazil, China, Denmark, 
the European Union, Finland, Germany, Israel, and the 
United States (Mazzucato, 2015). AI could be one of 
the areas to be incentivized through these programs.
 »The innovation hub should be staffed with the 
best talent on market-based salaries. These skills 
are in high demand and could easily drain overseas if 
not attracted and retained with appropriate incentives.  »Data fabric architecture, including interoperabil-
ity, should be considered for investments. This 
will overcome silos, and leverage data assets for de-
cision-making, compliance monitoring, and analytics. 
The initial focus should be on interoperability, open 
data, and data standardization. A hybrid cloud option, 
which combines on-prem data and cloud computing 
in a hybrid envirionment, should be explored to lever-
age the computing power at much lesser costs to pilot 
AI solutions.
 »Proof-of-concept and pilot AI projects could be 
the starting point for exploring opportunities.  
Many governments have deployed AI to solve prob-
lems. Key use cases include citizen engagement, ser-
vice delivery, regulatory compliance, decision analyt-
ics, fraud, and anti-corruption. Hackathons promote 
emerging talents and start-ups as seen in Austria, Es-
tonia, India, Pakistan, Poland, and the United States.
• Risks should be identified and managed, rather than 
avoided. Good algorithm impact assessment framework 
models exist, which can be tailored to suit a country’s 
context. The details could vary from context to context, 
but fundamental principles of risk mitigation are common. 
These include self-assessments, peer reviews, inclusion, 
and transparency. 
10
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Abbreviations>>>
ACL Access Control Layers
AI Artificial Intelligence
ANN Artificial Neural Network
API Application Programming Interface
COTS Commercial Off-The-Shelf
CPU Central Processing Unit
DLT Distributed Ledger Technology
FedRAMP Federal Risk and Authorization Management Program
FMIS Financial Management Information System
FOSS Free Open Source Solutions
GAN Artificial Neural Network
ICT Information and Communication Technology
IoT Internet of Things
IPC Inter-Process Communication
IRS Internal Revenue Service
ITU International Telecommunication Union
ML Machine Learning
MoH Ministry Of Health
NGFM New Generation Fiscal Machines
NGO Nongovernmental Organization
NLP Natural Language Processing
OECD Organisation For Economic Co-Operation And Development
RFP Request For Proposal
RL Reinforcement Learning
SRT Solicitation Review Tool
STEM Science, Technology, Engineering, And Mathematics
TCO Total Cost of Ownership
UN United Nations
UNCTAD United Nations Conference on Trade And Development
UNESCO United Nations Educational, Scientific, and Cultural Organization 
VPC Virtual Private Cloud
11
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>1.Introduction >>>
The World Bank launched the GovTech3 Global Partnership in 2019 to support the mod-
ernization of client governments through the use of technology. To promote this effort, the 
Swiss State Secretariat for Economic Affairs partnered with the World Bank to produce a series 
of papers. This paper on artificial intelligence (AI) in the public sector, one in this initial series, 
offers insights drawn from the existing uses of AI in the public sector. The target audience is non-
technical staff and policymakers who are developing and supporting the implementation of digi -
tal strategies for the public sector and drawn into conversations on the role of AI in modernizing 
the public sector. It refers to some fundamental technical concepts and provides more in-depth 
technical explanations in the appendices.
In recent years, governments have begun to investigate ways of leveraging artificial in-
telligence (AI) in public policy to better serve citizens, enhance compliance, and reduce 
fraud. The development of an appropriate policy and legal environment for AI could help coun-
tries stay ahead in commercial innovation, competitiveness, and international trade. The aca-
demic and professional research on AI ethics, policy, and regulatory reforms provides empirical 
and quantitative evidence on the opportunities and risks of AI adoption in the public sector The 
objective of this paper is to help World Bank’s client governments understand the ethical issues 
and policy options associated with AI to promote ethical AI and to elaborate on the opportunities 
for AI adoption in the public sector.
3. GovTech is a whole-of-government approach to public sector modernization that promotes simple, accessible, and efficient 
government. It aims to promote the use of technology to transform the public sector, improve service delivery to citizens and 
businesses, and increase efficiency, transparency and accountability.
12
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Advanced digital economies are increasingly adopting 
AI in both the private and the public sectors as part of 
their digital strategies. According to Bughin et al. (2018), AI 
can potentially contribute $13 trillion to the global economy by 
2030. Use cases provide case studies for learning and also 
illustrate the potential for adopting AI in the public sector to 
enhance efficiency and quality of service delivery. The World 
Bank’s client governments frequently request support on how 
to design digital transformation programs that can increase 
efficiency and quality of service delivery, improve citizen en-
gagement, and modernize core government operations. One 
of the important areas of support is AI. With careful execution, 
AI programs can help a government to deliver services faster 
and more tailored to the needs of beneficiaries and citizens 
and the public administration charged with delivering them. 
Public administrations that lack data collection capa-
bilities, technical skills in the civil service and digital 
infrastructure are unlikely to be able to manage AI data 
requirements or benefit from the application of AI. But, 
generally, the volume of information produced and stored 
daily by people’s movements, activities, and transactions is 
increasing, and combined with more computing power, such 
data can be used for effective analysis and policymaking. 
The speed of AI innovation and adoption has been fast; AI 
computation has been doubling every three months.4 Govern-
ments could create readiness conditions to fully leverage the 
potential of AI as both the speed of government digitalization, 
store of data, and AI innovation evolve. The paper describes 
readiness conditions, such as governance arrangements, 
availability of digital data, local and international data source 
integrations, technical capacity, and infrastructure, for wider 
AI adoption and guides assessing these conditions. While 
this paper touches briefly on the policy as it relates to AI, a 
more detailed paper is forthcoming on AI policy aspects and 
elaborates on a comprehensive framework for policy domains. 
Also, this paper does not cover the re-engineering of business 
processes or project management aspects as they relate to AI 
adoption. Regulations and policies on data, privacy, security, 
transparency, and accountability, in addition to the business 
process review, must precede the actual implementation of AI.
The adoption of AI in government requires interagency 
oversight, coordination among interdisciplinary teams of 
policymakers, and requires the adoption of overarching policies to guide its use. In recent years, the public sector 
made impressive headway developing counsels and policies 
on AI applications, procurement, and adoption. The United 
Kingdom and Bahrain launched AI procurement guidelines 
across their governments (ANI 2019). The U.K. government 
published “A Guide to Using Artificial Intelligence in the Public 
Sector” (GDS and OAI 2019). Singapore issued the “Model 
AI Governance Framework” (PDPC 2020). The United Arab 
Emirates established the National Program for Artificial Intel-
ligence.5 The Organization for Economic Co-operation and 
Development (OECD) published the “Recommendation of the 
Council on Artificial Intelligence” (OECD 2019).
The use of AI poses substantial risks as models and data 
may be substandard or inaccurate leading to bias. Data pri-
vacy and security, and ethical use of AI pose major concerns 
in all contexts, but this is likely to be even more of a concern 
where there is a lack of transparency more generally, concerns 
over human rights, or what might be considered a “poor gover-
nance” environment. AI software is a “black box” that is opaque 
to policymakers. This means that algorithm opacity—the inabil-
ity to detect design bias in constructing the algorithm—poses a 
major challenge for policymakers and auditors.
The adoption of digital solutions in government will re-
quire an investment in digital skills. The shift in the public 
sector needs from low-skilled to high-skilled workers will take 
place gradually over the long term, but it is a key consider -
ation because building digital skills in the public sector and 
overcoming skills shortages more generally also takes time. 
The use of AI in the public sector may shift the characteris -
tics of public sector employment and potentially result in job 
losses as more decision making becomes automated through 
the use of machine learning and models. However, the impact 
of adopting AI is likely to be less of a concern where the public 
sector wage bill is manageable, and the cost of labor is low. In 
some cases, demand for lower-skilled labor will decrease but 
whole scale substitution of professions with an AI program or 
machine is unlikely as the expert judgment will still be needed. 
The demand for high-skilled labor will likely increase. In some 
contexts, AI can automate systems of bureaucracy and create 
new job opportunities in, for example, policymaking, auditing, 
and resource management, jobs that require more analytical 
skills and judgment.
4. This is six times faster than the Moore’s Law on processor speed doubling every two years – now closer to 18 months. See Artificial Intelligence Index 2019 Annual Re -
port, 2019 Stanford Report, produced in partnership with McKinsey & Company , Google, PwC, OpenAI, Genpact and AI21Labs (Artificial Intelligence Index 2019 Annual 
Report, 2019). 
5. For more information, visit the website of the National Program for Artificial Intelligence at https://ai.gov.ae/about-us/.
13
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>This paper aims to provide some indications of the opportu -
nities and risks of AI adoption in the public sector. It distills 
knowledge and guidance on the use of AI in the public sec-
tor. The AI use cases discussed in the paper demonstrate the 
potential to improve government services and create new op-
portunities to strengthen engagement with citizens.
The paper curates knowledge residing in public documents 
and aims to distill lessons learned on how to adopt and use AI 
as part of a public sector modernization strategy. The paper’s 
primary scope is on governance-related aspects. Chapter 2 
elaborates on the opportunities being availed by governments 
around the world through the use of AI. These opportunities 
should be availed while managing associated risks, which 
are discussed in Chapter 3. For maximizing opportunities and 
managing risks, governments need to adopt AI ethical prin-
ciples and institutional arrangements, discussed in Chapter 
4. Chapter 5 discusses the ethical dimensions that need a 
broader policy response at the national level. Chapter 6 enu-
merates the building blocks necessary for a successful long-
term AI strategy. 
The appendices contain information for practitioners. Appen -
dix A provides technical information and additional resources 
for further support, and Appendix B highlights solutions that 
rely on AI for improvements in efficiency, scientific analysis, and prediction within the disciplines. To fully comprehend the 
impact that AI might have on governments, it is necessary to 
develop a solid understanding of key AI concepts. The paper 
does not offer in-depth coverage of work in specific sectors.
The findings in the paper were validated through interviews 
with industry experts. Special efforts have been made to en-
sure the architectural design approaches discussed in the pa-
per incorporate the best industry knowledge. The paper goes 
to great lengths to maintain a practical approach, with “hands-
on” examples of architectures and applications. 
The paper has limitations and AI adoption is not widespread. 
Actionable lessons in AI use are rare among client govern -
ments. Furthermore, there are limitations to the level of de-
tailed, in-depth information, and availability of use cases from 
public resources. 
Chapter 2 provides 14 use case examples of how AI has al-
ready been adopted in the public sector to address public sec-
tor issues such as how to control corruption. The associated 
risks of AI adoption are elaborated in Chapter 3. However, to 
harness the opportunities from AI governments need to de-
velop the governance frameworks, address the ethical con-
siderations and develop the building blocks of a government-
wide AI architecture, issues discussed in Chapters 4, 5, and 
6, respectively. Methodology and Scope
14
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>2.AI Opportunities>>>
The public sector in advanced digital economies is rapidly adopting AI, notably in Austria, 
Brazil, China, Estonia, Israel, Mexico, Republic of Korea, Singapore, the United States, 
and the United Kingdom, among others. Noteworthy examples are also surfacing in Bank cli-
ent countries. In this chapter, several AI use cases are provided to demonstrate the opportunities 
of AI already being harnessed in the public sector. Developing governments can also harness 
these opportunities to address some of the complex developmental challenges. However, gov-
ernments in these countries need to address some of these challenges to maximize opportuni -
ties. The biggest bottlenecks in AI adoption are the availability of quality data, expertise, budget, 
and mindset for experimentation and problem-solving. Sectors or agencies that are more likely 
to adopt AI primarily have well-developed data infrastructures. These agencies are typically 
well resourced, experience compliance pressures, have a mission-critical need for analytical 
information for decision-making, or consider citizen engagement as an important element of 
the policy design. The role of leadership initiative is also important. Silos and closed systems 
with poor or inaccessible data impede AI development. Governments need to first evaluate the 
strengths and weaknesses of their data, procedures and AI policy framework before embarking 
on AI solutions.
Wider AI adoption in the public sector typically follows once prerequisites like sufficient 
digital infrastructure, adequate digital skills, enabling legal frameworks, and digital strat-
egies are in place. The Oxford Insights’ Government AI Readiness Index scores the govern -
ments of 194 countries according to their preparedness to use AI in the delivery of public ser-
vices. The overall score is comprised of 11 metrics grouped under governance, infrastructure 
and data, skills and education, and government and public services. The data is derived from a 
variety of resources including desk research and the UN eGovernment Development Index. As 
presented in Table 1, the 2019 AI Readiness Index shows that Singapore comes first, with the 
rest of the top 20 mostly Western European countries.6 
6.  Government AI readiness indicators: https://www.oxfordinsights.com/ai-readiness2019.
15
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>GOVERNMENT AI READINESS INDEX 2019
Rank Government
1 Singapore
2 United Kingdom
3 Germany
4 United States of America
5 Finland
6 Sweden
7 Canada
8 France
9 Denmark
10 Japan
11 Australia
12 Norway
13 New Zealand
14 Netherlands
15 Italy
16 Austria
17 India
18 Switzerland
19 United Arab Emirates
20 China> > >
TABLE 1 - AI Readiness Index
Source: Oxford Insights 
Investments in data infrastructure, APIs, open standards,  
and data governance arrangements are all required for suc-
cessful AI strategies in government, as discussed in the fol-
lowing chapters.
A digital divide exists across countries in terms of ful-
filling the prerequisites for AI adoption. Most World Bank 
client countries are still far behind compared to the developed 
countries in terms of access to broadband, availability of digital 
skills, and adoption of relevant policies and legislation. Access 
to fixed broadband is significantly higher in more advanced 
economies, and the gap between the developed and develop -
ing countries has increased in the last 20 years according to 
the ITU (See Figure 1). 
16
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>> > >
FIGURE 1 - Fixed Broadband Subscriptions per 100 Inhabitants, 2001–2019
Fixed broadband subscriptions per 100 inhabitants, 2001-2019*
35
30
25
20
15
10
5
0
2001
2007
20172004
201420102006
20162003
20132009
20192002
20122008
20182005
20152011Per 100 Inhabitants
World Developing Developed33.6
14.9
11.2
Only 14.9 percent of inhabitants in developing countries have 
access to fixed broadband compared to 33.6 percent in de-
veloped countries (ITU 2019). Internet usage is limited to only 
19 percent of the population in least-developed countries, 
compared to 87 percent in developed countries. There are 
only 67 data centers in 13 countries in Africa—of which 21 
are in South Africa—compared to 1,237 data centers in 23 
Western European countries. Advanced digital skills, such as 
writing software using a programming language, are also con-
centrated in a few rich countries. The disparity in information 
and communication technology (ICT) skills around the world 
is shown in Figure2. Europe is far ahead in terms of ICT skills, compared to Asia and the Pacific, Arab states, and Africa (ITU 
2019). More generally, skills in data science and technology 
are scarce in low-income countries. Capacity constraint is an 
important issue. Those that have already adopted AI have pro-
moted the adoption of additional AI capacity in government 
through sponsoring government officials to attend programs in 
academic institutions, introducing training programs in-coun -
try, or partnering with the private sector to provide expertise. 
Creating an innovation hub or a central AI unit as part of the 
centralized digital agency or as an independent agency helps 
maximize the use of scarce expertise.
17
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>> > >
FIGURE 2 - The disparity in ICT Skills across the Regions
The Percentage of People with Advanced IT Skills 2014-2018
Source: ITU, 2019
0-5
5-10
10-15
15-20
Data not available
The legislative framework for data protection and privacy 
is relatively widely enacted, but policies that would allow 
accessibility to government-held data are mostly not in 
place in most developing countries. According to UNCTAD 
2020, 132 of 194 countries, including 50 percent of the African 
countries and 57 percent countries in Asia-Pacific, have adopt-
ed data protection and privacy legislation. However, only seven 
governments out of 115 include a statement on open data by 
default in their current data management policies. Worldwide, 
only 7 percent of government-held data is fully open, and only 
one in every two datasets is machine-readable (Open Data Ba-
rometer 2020). There is also a significant lack of data sharing 
and interoperability within the government. Open, machine-
readable, and interoperable data are some of the important 
preconditions for wider AI adoption in government.
AI use in government is therefore typically in a few ad-
vanced countries, and being taken up by digitally more 
advanced World Bank client countries. Some countries 
have adopted an AI strategy as a signal of the government’s 
commitment to AI. At least 50 governments, in addition to the European Union, have developed or are in the process of 
developing a national AI strategy. Out of these, 37 have or 
plan to have either separate strategies in place for the public 
sector or a dedicated public sector focus embedded within a 
broader strategy (Berryhill et al. 2019). AI strategies are being 
adopted in some developing and emerging economies around 
the world including in India, Kenya, Malaysia, Mexico, Poland, 
Taiwan, and Tunisia (Dutton 2018). 
AI patent applications (279,145) are also predominantly in 
the USA (55 percent), Europe, China, and Japan (Statistica, 
2019). Similarly, AI research publications are dominated by 
developed countries (Microsoft Academic Graph, 2019). 
Governments in some less advanced digital economies 
have started deploying AI to improve government effec-
tiveness. While the scope and abundance of digital resourc-
es—talent, capital, infrastructure, and data—may be relatively 
limited, some developing governments have started piloting AI 
to address their development challenges. 
18
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>The remainder of the chapter reviews several use cases where 
AI has been used in the public sector to address specific chal-
lenges: corruption, citizen engagement, customs compliance, 
health pandemic response, consistent judicial decisions, pro-
curement compliance, taxation compliance, and policy, and 
audit efficiencies. Regardless of the stage of development, 
countries can develop AI initiatives based on their most im-
mediate needs, but it is also recommended that the approach 
to AI should be part of the planning and accounting for fu-
ture digital initiatives with a whole-of-government approach to 
infrastructure, standardization, governance, and execution.7 
The pattern of government adoption typically follows this ty-
pology of use cases:
• CITIZEN ENGAGEMENT. The introduction of AI tools 
such as chatbots that answer citizen queries. For exam-
ple: Where is my ballot? Where is the nearest emergency 
department? How can I apply for social welfare benefits? 
Additionally, aggregation and pattern determination can 
be used to collect feedback from millions of citizens on a 
draft policy published online.
• COMPLIANCE AND RISK MANAGEMENT. AI systems 
are used to cross-reference and reconcile terabytes of data 
from multiple sources to create alerts for noncompliance. 
For example, financial intelligence units and central banks 
use AI to track illicit fund flow and beneficial ownership as 
well as terrorism financing to comply with the Financial Ac-
tion Task Force. Tax authorities can use AI to track tax filers 
who use duplicate profiles to avoid taxation.
• FRAUD DETECTION, PREVENTION, AND INVESTI-
GATION.  Closely related to compliance AI can be used 
to detect and prevent fraud for example by procurement 
agencies, anti-corruption units, or audit agencies.
• BUSINESS PROCESS AUTOMATION.  AI automation 
tools can scan websites to get currency exchange rates 
and present information.
• PERSONALIZED SERVICE DELIVERY. Based on a pro-
file, AI sends automatic alerts such as when to renew a 
driving license.
• ASSET MANAGEMENT. AI can be used to tracking asset 
movements across multitudes of systems, aggregating 
data from the Internet of Things devices.• ANALYTICS AND DECISION-MAKING. AI or machine 
learning helps aggregate and cross-reference data such 
as household survey data with information on school 
enrollment, address changes, satellite images of floods, 
mosquito swamps, and pandemics to produce policy in-
sights and identify areas needing greatest attention for 
targeted policy actions.
Use Cases
The following use cases illustrate real-world applications 
and opportunities for AI in the public sector in a range of 
contexts. The use cases provide a summary of AI initiatives 
to tackle corruption in China and Brazil, to engage citizens in 
Nigeria and Uganda, to improve efficiency and compliance in 
the United States customs administration, to tackle the CO-
VID-19 response in Singapore and China, to improve public 
procurement in South Korea and the United States, to improve 
the effectiveness of the justice sector in China and the UK, the 
tax administration in Armenia, Mexico, and the UK, and audit 
in Canada and UK. The use case of health pandemic, CO-
VID19, is developed in-depth to elaborate the concepts and 
the details of AI logic. Using the typology developed by Oxford 
Insights, each use case brief states the role of humans on the 
level of AI adoption in each application area. Table 2 describes 
the five levels of AI adoption. For examples of additional AI 
use cases, see Appendix B.
Level Description
Level 5A fully automated system that never requires 
human intervention.
Level 4Automation: A public service runs itself unless 
it hits an extreme case where it requires human 
intervention.
Level 3Semi-autonomous: Computers monitoring and 
running (e.g., a regulatory sys-tem).
Level 2Close supervision: Routine administration of 
systems (e.g., energy networks with difficult 
decisions referred to a human).
Level 1Simple augmentation: Entering data, process-
ing, identifying clusters of activity, and profiling, 
among others (e.g., in fraud detection).
Level 0 No automation: People-powered public services.> > >
TABLE 2 -Role of humans - Five Levels of AI Adoption
Source: Oxford Insights. 
7. Governments can adopt an incremental approach when making investments to avoid the huge costs of full-scale infrastructure before development begins. Cloud solutions 
can provide any opportunity to reduce the total cost of ownership (TCO) for nascent projects. Cloud solutions enable incremental growth because they offer on-demand 
services at scale, without upfront investments or buying any on-premise servers. More information about cloud solutions and infrastructure is available in Appendix A.
19
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>In most instances, multiple AI methods and techniques, de-
scribed below, are in use. A more detailed description of these 
techniques is provided in Appendix A on the AI Technical Primer.
Natural Language Processing (NLP): processing large 
amounts of natural language data by the AI systems. For ex-
ample, NLP refers to the ability of an AI algorithm to read a text, 
convert speech into text, or vice versa. Specific use of NLP is 
chatbots, applications used to support online chat conversation 
using text or text-speech, typically used as customer support. 
Data Mining:  The ability of the AI algorithm to examine large 
amounts of raw data to determine patterns. For example, 
analyzing millions of comments from citizen feedback on an 
online policy document, and converting these comments into 
patterns of suggestions, approval, disapproval, etc. Common 
uses are:
• CLUSTER ANALYSIS: Clusters of similar objects or in-
formation are grouped to find patterns. For example, clus-
ter analysis of tax filings to identify the same warehouse 
or same names of employees used by the same firm but 
using different registration numbers and titles to avoid or 
evade taxes. 
• FEATURE ENGINEERING: Features are extracted from 
raw data to recognize patterns and classify information. 
For example, drone pictures of community rooftops could 
be used by the AI to identify types of roofs – thatch, cor-
rugated, cement – and determine patterns of poverty for 
more targeted policy interventions.
Artificial Neural Network (ANN): AI algorithms that recog-
nize relationships between different data sets similar to how 
the human brain analyzes such information. For example, rec-
onciliation of two or more data sets to detect fraud patterns, 
medical image analysis to relate a specific feature in an image 
to a diagnosis and improving the diagnosis through adaptive 
learning. ANN techniques are also used in data mining. 
Convolutional neural networks: A type of deep neural net-
work, most commonly applied in visual imagery. 
Generative Adversarial Network (GAN): Use of two or more 
neural networks to produce a realistic output. For example, 
fake videos can be made about some celebrity or popular fig-
ure by synthesizing two videos to misinform and manipulate 
public opinion. AI in Corruption
Brazil
Governance Risk Assessment System
Use Case 
BriefWorld Bank Artificial Intelligence 
Governance Risk Assessment System
Level 5It is estimated that Brazil might be losing 
between 3 to 5 percent of GDP annually due to 
corruption. Over 48,000 companies tendered 
in public bidding processes between 2016-
2018 in the State of São Paulo alone. Brazilian 
Government agencies can systematically iden-
tify public expenditure risks at this scale only 
through advanced digital technolo-gies. 
Level 4Government agencies do not have the tools 
or capacity to conduct sys-tematic fraud risk 
assessments. The current approach, which 
depends on manual input to a large extent, is 
time consuming, inefficient, and ineffective. 
Level 3Graph theory, clusterization, regression analy-
sis, and supervised ma-chine learning.
Level 2
Level 1Level 3-4: Users must interpret the evidence 
concerning high-risk firms and agencies. The 
System analyzes complex networks of poten-
tial fraud with minimal effort. 
Level 0
Source: World Bank.
The World Bank Team in Brazil, with funding from the Dis-
ruptive Technologies for Development (DT4D) Trust Fund, 
developed an AI System that identifies 225 red flags of 
potential fraud in public procurement processes and can 
help improve expenditures. The World Bank partnered with 
the City of Sao Paulo, the States of Rio de Janeiro and Mato 
Grosso, and the Federal Ministry of Health to leverage the 
vast amounts of unused data to build a system to help improve 
their investigative and expenditure capabilities. 
As part of the project, the World Bank created one of the 
world’s largest data lakes, which currently includes 27 data-
sets with over 250 million data points and more than R$500 
billion in public expenditure (approximately US $100 billion). 
This includes numerous sources and types of data: expen -
diture databases; electoral databases; beneficiaries of social 
20
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Use Case 
BriefWorld Bank Artificial Intelligence 
Governance Risk Assessment System
Strategic 
context President Xi Jinping’s policy of promoting tech-
nological innovations such as Big Data and AI in 
government reform. China has faced enor-mous 
challenges of controlling corruption and has 50 
million employ-ees on the government payroll. 
Problem 
statementThe extent of operational corruption among 
public officials.
AI 
methodsNatural language processing; Big Data; data 
mining; anomaly detection.
Role of 
humansLevel 2-3programs databases; blacklisted firms’ databases; and elec-
tronic invoices. Overall, the system builds on:
 
• Analysis of over R$500 billion in public procurement in 
Brazil from 12 States and Federal Level.
• Analysis of over 15 million electronic invoices.
• Analyzed and geo-referenced over 750,000 firms and a 
Public Registry Dataset containing details about 30 mil-
lion firms – HQ address, partners, data of incorporation, 
economic sector.
• Incorporated over 30,000 news feeds about corruption.
• Data on 20 million social program beneficiaries.
• Data on 30,000+ blacklisted firms.
• Data from 20 million politicians and 800,000 political do-
nations.
The system optimizes the process of detecting fraud in public 
expenditure substantially, saving valuable resources – time 
and money – and increasing the effectiveness of audits and 
investigations. The system has, so far, led to the exposure of 
numerous high-risk cases, including: 
• Identified over 420 firms that won bids against companies 
that have a high likelihood of being shell companies and 
reflecting potential bid-rigging. The winning firms have 
more than R$ 600 million in public contracts. 
• Identified 857 companies that won bidding processes against 
firms that share at least one partner in common. These firms 
have executed at least R$ 800 million in contracts. 
• 450 firms whose partners are beneficiaries of the con-
ditional cash transfer program, Bolsa Família, which in-
dicates that these individuals are potentially strawmen.
These companies have more than R$ 600 million in con-
tracts. 
• Identified more than 500 firms owned by public servants 
working at the same government agency that has exe-
cuted the contract. These cases amount to over R$ 4.5 
billion in contracts.
The technology has a high potential for scalability across 
Brazil and beyond through the implementation of Scalable 
Data Unification, which drastically reduces the marginal 
cost of replicating the implementation of the algorithms 
and the system. This approach reduces the cost of replica -
tion by building a global public expenditure database schema 
upfront, based on identifying and converting local schemas—
State’s public procurement dataset—into that global schema. Therefore, instead of adapting all the complex algorithms nec-
essary for extracting the 225+ red flags to match the schema 
of a single public expenditure dataset, the team did the op-
posite and now every new public expenditure dataset is con-
verted to the global schema and the risk detection algorithms 
are implemented directly. 
China
Zero Trust
Zero Trust was developed by the Chinese Academy of Scienc-
es and the Chinese Communist Party’s internal control institu-
tions to monitor, evaluate, and scrutinize the work and lives 
of public servants. Zero Trust can cross-reference more than 
150 databases in central and local government systems. The 
system detects an individual’s property transfers, infrastruc-
ture, construction, land purchases, and house demolitions. 
Zero Trust also detects unusual increases in a civil servant’s 
bank savings, new car purchases, and if an official is bid-
ding for government contracts or is doing so under the name 
of family members or friends. The system then calculates a 
probability that those actions are corrupt and alerts officials to 
highly probable cases of corruption.
Zero Trust was rolled out in 30 counties and cities and identi -
fied 8,721 government officials suspected of engaging in em-
bezzlement, abuse of power, misuse of government funds, 
and nepotism. Some of these cases resulted in a prison sen-
tence, most were allowed to keep their jobs after receiving a 
warning or minor punishment (Chen 2019). The future of Zero 
Trust is uncertain; the system faces backlash from public of-
ficials, and it may be decommissioned (Chen 2019).Source: World Bank.
21
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Use Case 
BriefPublic Spending 
Observatory AI Apps
Strategic 
context Citizen engagement and feedback are helpful 
tools to complement formal mechanisms 
of accountability as they offer compelling 
insights for monitoring and evaluation of poli-
cies, project designs, and imple-mentation.
Problem 
description The limited capacity of the agencies to receive, 
analyze, and respond to citizen feedback.
AI design Natural language processing text matching.
Role of 
humansLevel 1-2
A World Bank team is working in Edo State, Nigeria with Data 
Science Nigeria (DSN) to pilot an AI solution for citizen feed-
back to monitor project progress in sample locations. DSN has 
a mobile app called DataCrowd that is based on AI. The pilot 
was done over four weeks in May 2020. Its scope covered 
77 locations in the state and collected citizen’s feedback for 
the project, State Employment, and Expenditure for Results 
(SEEFOR). After initial positive results, the project is planned 
to scale up to cover three more states and about 350 loca-
tions. The AI solution has several features; the following were 
included in the Edo pilot:
AI-powered tag cloud.  DataCrowd can summarize text and 
sentences, such as citizens’ feedback through mobile phones, 
and instantly shows the keywords and their relevance. This 
AI feature was used on citizen feedback received during the 
Edo pilot.8AI-powered geofencing.  This feature instantly rejects a sub-
mission made outside of a geofenced location. This feature 
was used during the Edo pilot.
AI-powered image classifier. This feature can classify the 
contents of a picture. For instance, if a picture of a person 
is taken, the AI model can tell if it is a male or a female. Un-
like the tag cloud and sentiment analyzer features, the image 
classifier feature is custom-trained based on the image data 
collected for a particular project, which always requires a lot 
of images to train. In the case of SEEFOR, the Research and 
Development Team is working on training the image classi-
fier model to classify some of the SEEFOR images, especially 
under the public works category. This feature is particularly 
useful for quality assurance checks and when many images 
are being collected.
AI-powered image matching. This feature will allow DataC-
rowd to instantly match an existing image with a new image 
and report if they are the same or not. This feature is in devel -
opment. It is expected to be useful as first-level data verifica-
tion and validation when many images are being submitted by 
data collectors.
AI-powered opinion mining and sentiment analyzer. The 
sentiment analyzer feature can measure the sentiment pulse 
of text data, such as citizen feedback, and categorize sentenc-
es into negative, neutral, and positive sentiments. Although 
this feature exists on DataCrowd, it was not included in the 
pilot. It is useful for understanding the sentiment expressed in 
all citizen feedback and could be used potentially for scale-up.
In the pilot, the project authorities were able to obtain citizen 
feedback on civil works and confirm various aspects of the 
project’s implementation progress, including location, perfor-
mance, quality, and completion.Nigeria
DataCrowd
Source: World Bank.AI for Citizen Engagement
8. The tag cloud is available at https://datasciencenigeria.github.io/DataCrowd/.
22
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>AI in Customs
United States
Northern Border Surveillance Systemlos exacerbate the issue, leaving data locked up and inacces -
sible to communities.
AI can sort through regional data and identify which aspects 
of the overarching infrastructure have the greatest impact on 
resilience. It can simulate various disaster events in a region 
to uncover vulnerabilities and assist with the formulation of 
disaster recovery plans. A data fabric can hold data from si-
los and enhance disaster preparation by coordinating emer-
gency information exchange capabilities. During a disaster, 
predefined use cases can equip first responders with better 
tools for understanding the local context to take more precise 
action. Reinforcement learning (RL) is a strong candidate for 
this type of future simulation.
On a more practical level, in light of the COVID-19 pandemic, 
AI methods are being employed in earnest to model potential 
effects of quarantine models and screen patients for potential 
infections using facial and thermal recognition models. Figures 
4-7 demonstrate recent modeling of quarantine methods using 
artificial neural networks (ANNs) from several countries with 
varying degrees of quarantine policy. Predicted data are mod-
eled in using solid lines, while actual observed data uses dots. 
Note the relative accuracy of the predictions for most sources 
and the detection of possible disparity of infection due to poten-
tial under-reporting (Dandekar and Barbastathis 2020).Use Case 
BriefNorthern Border Remote 
Video Surveillance System 
Strategic 
context The US Customs and Border Patrol is one of 
the world's largest law en-forcement organi-
zations and is charged with keeping terror -
ists and their weapons out of the U.S. while 
facilitating lawful international travel and 
trade There are 300 ports of entry into the 
United States that need to be secured without 
disrupting trade and transit.
Problem 
statementConcerns of illegal trade, including drug smug-
gling and human traffick-ing, and weapons 
entering the US under the mandate of the U.S. 
Cus-toms and Border Protection Agency. 
AI methodsConvolutional neural network, computer vi-
sion, pattern matching, anomaly detection, 
prediction.
Role of 
humansLevel 2Use Case 
BriefContact Tracing and T emperature 
Detecting Camera Apps
(Sense-
Time, 
Megvii, 
WeChat)The US Customs and Border Patrol is one of 
the world's largest law en-forcement organi-
zations and is charged with keeping terror -
ists and their weapons out of the U.S. while 
facilitating lawful international travel and 
trade There are 300 ports of entry into the 
United States that need to be secured without 
disrupting trade and transit.
Strategic 
context Contact tracing and screening to target policy 
response on quarantine for minimum disrup-
tion on economic life and contain the spread of 
COVID-19.
Problem 
statementThe economic shutdown to contain COVID-19 
has impacted jobs and growth and has trig-
gered an unprecedented economic recession 
in many economies. Smarter and targeted 
response on quarantine and social distancing 
policy could save economies from economic 
disasters. 
AI methodsArtificial neural network, reinforcement learn-
ing, data mining, predic-tion.
Role of 
humansLevel 3Source: World Bank.
Source: World Bank.Border patrols require vigilance to stem illicit trade including 
drug smuggling and human trafficking. The use of AI to com-
bat illicit activities is on the rise. The U.S. Customs and Border 
Protection Agency uses the Northern Border Remote Video 
Surveillance System (NBRVSS). The NBRVSS can detect and 
monitor vessels from miles away and alert authorities when it 
recognizes unusual vessel movements. It commenced before 
2016 and utilizes many radio towers equipped with computer 
vision that spot anomalies in vessel behavior and allow agents 
on the ground to intercept potential sources of contraband en-
tering the United States from the Canadian border.
AI in Health
The unforeseen rise of unseen global threats to human health 
and safety has put AI on the frontlines of disaster response 
efforts. Furthermore, sudden changes in the behavior of the 
human population challenge existing models and stressed 
predictive AI systems to the breaking point (World Economic 
Forum 2018). The speed of response to disaster events sub-
stantially impacts the extent of economic losses and human 
suffering. Delays occur due to a lack of information, analytics, 
and predictive modeling of the best course of action. Data si-
23
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>> > >
FIGURE 3 - Wuhan Neural Network Model with Quarantine Control
Source: Dandekar and Barbastathis 2020.0 0 0 20 20 20 10 10 10 30 30 30 40 40 408x104
6x104
4x104
2x104
01.1
1.0
0.9
0.8
0.72.0
1.5
1.0
0.5
PredictionData: Recovered
PredictionData: Infected
R1 = 1Effective reproduction numberQuarantine StrenghtWuhan: Number of cases
Q (t)Rt
Days post 500 infected Days post 500 infected Days post 500 infected
> > >
FIGURE 4 - Italy Neural Network Model with Quarantine Control
Source: Dandekar and Barbastathis 2020.0 0 0 10 10 15 5 5 10 5 20 15 20 20 25 25 15 251x105
8x104
6x104
4x104
2x104
01.0
0.8
0.6
0.42.0
1.5
1.0
0.5
PredictionData: Recovered
PredictionData: Infected
R1 = 1Effective reproduction numberQuarantine StrenghtItaly: Number of cases
Q (t)Rt
Days post 500 infected Days post 500 infected Days post 500 infected
> > >
FIGURE 5 - South Korea Neural Network Model with Quarantine Control
Source: Dandekar and Barbastathis 2020.0 0 0 10 10 15 5 5 10 5 20 15 20 20 15 252.0x104
1.5x104
1.0x104
5.0x103
00.75
0.70
0.65
0.60
0.55
0.50
0.452.0
1.5
1.0
0.5
PredictionData: Recovered
PredictionData: Infected
R1 = 1Effective reproduction numberQuarantine StrenghtKorea: Number of cases
Q (t)Rt
Days post 500 infected Days post 500 infected Days post 500 infected
> > >
FIGURE 6 - U.S. Neural Network Model with Quarantine Control
Source: Dandekar and Barbastathis 2020.0 0 0 10 10 15 5 5 10 5 15 15 20 20 20 252.0x105
1.5x105
1.0x105
5.0x104
00.7
0.6
0.5
0.4
0.32.0
1.5
1.0
0.5
PredictionData: Recovered
PredictionData: Infected
R1 = 1Effective reproduction numberQuarantine StrenghtUS: Number of cases
Q (t)Rt
Days post 500 infected Days post 500 infected Days post 500 infected
24
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>The impact that AI can have on pandemic mitigation continues with additional AI methods that are in place, beginning with facial 
recognition—it uses face scans to detect symptoms. Upon entering the Tampa General Hospital, patients are given an automatic 
face scan that determines signs of fever, including sweating and increased skin temperature within 0.3 degrees of variance over 
1-3 seconds. In another modeling example, RL models learn to combat the illness using policies of quarantine and hospitalization 
to identify the most successful policy model (Chilamkurthy 2020). Figure 8 illustrates the results of the AI analysis, revealing the 
potential to thwart the progression of the pandemic within 50 days.
This is the bast action policy, where the agent brings down the whole virus in 50 days. At some point, this agent also 
allows increase in virus spread (25th to 29th day).
Distribution of action space {7: 3, 11: 4,5: 21,15: 4,13: 2,12: 2,0: 2,3: 1,1: 4, 10: 1,14: 1,9: 1,4: 2, 2: 2}400
300
200
100
0
0 10 20 30 40
Hospitalized Death Recognized Contagious> > >
FIGURE 7 - Results of COVID-19 Analysis by AI
Source: Chilamkurthy, 2020.
Lastly, contact tracing applications are emerging on the 
front lines of halting the spread of infectious disease. One 
notable example taps into Bluetooth communication broadcasts 
from smartphone devices. In this system, data from a confirmed 
infected person’s cell phone can be extracted to list the Blue-
tooth broadcast “chirps” detected within the phone’s database. 
By uploading this information to an interoperable data platform, 
the signatures of the chirps can be cross-referenced with chirps 
from other reported infections. If the information is made avail-
able through an application interface on any smartphone, then 
the general public can determine whether they have come in 
contact with known sources of infection and can take measures 
to mitigate the risk of further exposure and potentially seek 
treatment, if the potential of infection is high due to repeated or 
multiple contacts. While this concept is possible to implement 
on a local basis and efforts to implement this technology are 
documented by both Google and Apple in partnership, no suc-
cessful implementation exists for the general public at this time. 
The key problem is interoperability using a large-scale data fab-
ric solution, though the two tech giants assure the public that a 
solution will exist in the coming months (Apple 2020).Use Case 
BriefHospital and health information app for 
doctors and front-line health workers
Strategic 
context Doctors and front-line health workers need in-
formation on the latest health protocols, staff 
rosters, operational directives, and dosage to 
effectively manage the COVID-19 pandemic.
Problem 
statementHealth facilities are under immense pressure to 
respond to the un-precedentedly high volume 
of COVID-19 patients. An effective re-sponse 
needs timely information for a coordinated 
team effort.
AI methodsNatural language processing, data mining, 
chatbot, search.
Role of 
humansLevel 1
Source: Bot MD.Singapore
Bot MD
25
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Bot MD is an AI Chatbot mobile app that acts like ‘google’ 
for hospital and clinical information on COVID19 for doctors 
and frontline health workers. Developed in Singapore, more 
than 13,000 doctors in 52 countries are now using the app. 
The doctors, front-line health workers, and Ministry of Health 
(MoH) officials can type a question and the app can provide 
information on staff rosters, health protocols, drug formulary 
information, disease guidelines, operational directives, and 
latest MoH circulars. The app was developed by Tan Tock 
Seng Hospital (TTSH), and the MoH’s IT team in 2018. The 
system uses AI to predict situations before they occur, provide 
information for decision-making on resource allocation to deal 
with the pressures. These resources could include manpower, 
equipment, supplies, medicines, hospital beds, intake centers, 
etc. (The Straits Times, April 2020).
AI in the Judicial Sector
The inconsistent application of law and long pendency of  
cases due to excessive workloads plagues the judicial sector. 
AI has the potential to enhance consistency and efficiency in 
the judiciary.
China
Similar Cases Push Systemdicial decisions. Initially, a system called “Review and Approval 
of Judgement System” was implemented through which supe-
rior courts would review the judgments of the courts submitted 
online through the system. However, this led to inappropriate 
interference and delays. The use of this technology, there-
fore, was canceled. Under the new guidelines, the principle 
of self-accountability and independence was established, un-
der which the final judgment is issued by the concerned judge 
without higher-level approval. However, this has led to the risk 
of inconsistent judgments across jurisdictions. SPC policies 
now require judges to research similar cases and cite these 
cases in judgments to ensure consistency.
To support this research, the Chinese judiciary is piloting AI in 
some provinces to improve consistency. Under this implemen -
tation, all prior judgments were digitized and stored in a da-
tabase. Next, the SPC deployed NLP AI capabilities, through 
the Similar Cases Push System, to match key text relevant 
to pending cases using the database. The system presents 
relevant judgments before a judge using a pre-populated judg -
ment template that the judge reviews and edits. The system 
reduces the time it takes to formulate a written judgment and 
all legal procedural documents by 70 percent and 90 percent, 
respectively (China Daily 2019).
Also, an AI pilot program records court proceedings. Some 
courts in China are now using AI speech recognition products 
to directly translate the court hearing recordings into texts in 
real-time and convert these into written court proceedings us-
ing Speech-to-Text NLP methods.
United Kingdom
Legal AI Tools and Bots
Before harnessing AI, the Chinese judiciary adopted poli -
cy measures that enforced the use of technology. China’s 
Supreme People’s Court (SPC) issued a policy of “Similar 
Judgments in Similar Cases” to promote consistency in the ju-Use Case 
BriefSimilar Cases Push System
Strategic 
context China’s Supreme People’s Court is promoting 
the policy of “Similar Judgments in Similar 
Cases” to promote consistency in judicial 
deci-sions.
Problem 
statementInconsistent application of law during judicial 
decisions.
AI methodsNatural language processing, Big Data, data 
mining, and automation.
Role of 
humansLevel 1
Source: World Bank.Use Case 
BriefRobot Lawyer—DoNotPay App
Strategic 
context Legal document processing in cases of litiga-
tion.
Problem 
statementAn AI legal assistant is necessary for im-
provements in the analysis of legal contracts; 
support of private legal bureaucracy among 
citizens; and guided legal advice.
AI methods Natural language processing, chatbots.
Role of 
humansLevel 4
Source: World Bank.
26
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Automated AI legal assistants and lawyers have sur-
passed human-level accuracy. An AI bot performed better 
than human lawyers in competitions for accuracy and efficien-
cy held in London and Tokyo. In London, human lawyers from 
prominent law firms in the United Kingdom predicted whether 
the Financial Ombudsman would allow an insurance claim. 
Of the 775 total predictions, the AI “Case Cruncher” emerged 
on top with an 86.6 percent accuracy rate compared to 66.3 
percent among 100 human lawyers (BBC News 2017).
DoNotPay, touted as the world’s first robot lawyer, helps 
users dispute parking tickets. In one month, post-launch, 
DoNoPay.com helped people overturn 160,000 of 250,000 
parking tickets—a success rate of 64 percent (King, n.d.). 
DoNotPay has now expanded its offerings to airline ticketing 
disputes and subscriptions. Other lawyer bots are also in op-
eration. These include Ross (United States) for cash research 
powered by Watson AI APIs; Billy Bot (United States), which 
takes the role of a junior clerk to guide users to free online 
resources and to find legal representation; and i-LIS, South 
Korea’s first intelligent legal assistant for legal research.
AI In Procurement
Central procurement agencies in governments face chal-
lenges when ensuring regulatory compliance of procurement 
among a large number of government entities. Central pro-
curement agencies cannot manage the magnitude of procure -
ment activities occurring across the government because the 
capacity of human agents is limited.
United States
Solicitation Review ToolThe U.S. government is harnessing the power of AI to 
strengthen procurement compliance. The U.S. General 
Services Administration (GSA) has an Office of Government-
wide Policy, which developed a new pilot using AI for scanning 
bidding documents to determine regulatory compliance. The 
tool is known as the Solicitation Review Tool (SRT).
The SRT AI platform uses NLP, text mining, and machine 
learning (ML) algorithms to scan and review whether federal 
solicitations posted on fbo.gov are compliant with Section 508 
of the Rehabilitation Act. It alerts responsible parties of non-
compliance and flags the need for corrective actions. Through 
the independent review, the predictions have an accuracy of 
95 percent.
This innovation substantially alleviates the human re-
sources needed to identify, audit, and enforce compliance. 
The SRT platform is innovative because it helps the GSA focus 
on limited available resources on noncompliant solicitations. 
The SRT AI platform has expanded to predict whether solicita-
tions comply with other federal regulatory requirements, such 
as cybersecurity or sustainability (GSA 2018). 
Korea
Bid Rigging Indicator Analysis System
Korea is cracking down on bid-rigging through the use 
of AI. Officials converted a manual process that was in place 
since 2004 to detect bid-rigging cases using AI. The introduc-
tion of the AI system greatly increased speed and effectiveness.
Bid rigging refers to collusion between procurement of-
ficials and a pre-ordained vendor to award a contract 
using corrupt practices. Bid rigging can take various forms, 
including short bid submission windows, split procurements to 
capture funds below detectable thresholds, significant change Use Case 
BriefSolicitation Review T ool
Strategic 
contextLegal compliance in the tender documents 
with Section 508 of the Rehabilitation Act.
Problem 
statementReviewing hundreds of complex and volumi-
nous bidding documents, issued by the federal 
agencies, to ensure compliance with regula-
tions.
AI methodsNatural language processing, Big Data, data 
mining, feature engineer-ing, and automation.
Role of 
humansLevel 3
Source: World Bank.Use Case 
BriefKorea’s Fair Trade Commission’s 
Bid Rigging Indicator Analysis System
Strategic 
contextThe Fair Trade Commission ensures fair 
competition in procurement practices in the 
government. 
Problem 
statementUnfair practices in procurement, using bid-
rigging, to beat the compe-tition.
AI methodsNatural language processing, Big Data, data 
mining, feature engineer-ing, automation.
Role of 
humansLevel 2-3
Source: World Bank.
27
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>orders, and substitution of low priced items with high priced 
items after the award. Korea’s Fair Trade Commission (KFTC) 
is leveraging an AI and analytics platform, the Bid Rigging In-
dicator Analysis System (BRIAS) to combat corrupt practices.
Before the introduction of the automated AI solution,  the 
KFTC collected and manually analyzed hard copies of bid-re-
lated documents from major public organizations such as the 
Public Procurement Service, Korea Expressway Corporation, 
and Korea Electric Power Corporation, which issue large-scale 
public projects. Presently, the KFTC collects and analyzes this 
information electronically and flags cases of suspicious bid-
rigging activities.
In total, 322 public organizations must report their bids to 
the KFTC. Construction projects over ₩5 billion and tenders 
for procurement of goods and services over ₩500 million must 
report to the KFTC. The affected public organizations must re-
port related data into BRIAS within 30 days of selecting a bid-
der. The organizations that use internal bidding systems may 
transmit bid data to the KFTC in real-time using BRIAS APIs. 
The others must report bid information to the KFTC portal. The 
information submitted includes the following features:
• The organization’s information on the executive agency 
and issuing agency.
• Procurement information: types and methods of tenders, 
the date and contents of tender notices, and the estimated 
price set out by issuing organizations before tender no-
tice, which serves as a benchmark to determine the tender 
amount for the successful bidder.
• Bid evaluation information: the ratio of bidding price to the 
estimated price, the number of bidders, bidder-based ten-
der details, company information for successful bidders, 
and the number of unsuccessful bids.
• Contract execution information: the number of estimated 
price increments and alterations to bids.
The KFTC weights the features according to a preset for-
mula and uses the data to analyze the probability of bid-
rigging quantitatively. An automated system calculates and 
assigns a score between 0 and 100 to the procurement item or 
contract. The higher the score, the more likely the concerned 
bid is rigged. The KFTC sends flagged bids to external depart-
ments for further investigation. In one example involving 12 
construction companies for the Seoul subway, the KFTC de-
tected bid-rigging, and the government imposed a surcharge 
amount of ₩5.108 billion.AI in T ax Compliance
Tax administration authorities in governments consistently 
grapple with the challenge of ensuring reasonable tax compli -
ance. Tax authorities are better positioned to pilot AI tools to 
strengthen their mandate on compliance for several reasons.
• Generally, tax agencies have more data assets than other 
agencies. The capacity is generally higher.
• Tax compliance and collections directly affect fiscal sus-
tainability targets and the political agenda of most govern -
ments with an interest in funding capital and social proj-
ects as promised in their manifestos.
• Many tax agencies across the world deploy data ware-
houses, data analytics, and, lately, AI projects to leverage 
the power of technology to promote their mandate through 
a shift to risk-based auditing techniques for tax compliance.
More than 32 tax administrations worldwide have changed 
their strategies from a traditional data-oriented audit to a risk-
based, cooperative compliance approach that relies heavily 
on analytics during the assessment process (Microsoft, PWC 
2018). The huge data assets typically process a historical 
record of tax payments, electronic value-added tax (VAT) in-
voices, income tax returns, and personal and company infor-
mation. By deploying a Big Data architecture, capturing all the 
relevant structured and unstructured information in one data-
base, and running AI and analytics tools, tax administrations 
may significantly improve their effectiveness. These solutions 
offer a complete picture of businesses or individuals using 
risk-based compliance assessments. Examples highlight how 
tax authorities are deploying AI and analytics for common 
problems faced by most tax authorities.
28
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Armenia
AI Use in Tax Administrationl
Tax evasion is carried out in many ways. One of the most 
common practices among small businesses and individuals of 
lower-income is to remain below revenue thresholds to benefit 
from lower tax rates. An existing business may open a new 
business when the existing firm reaches the threshold. Simi-
larly, a business will split into several small businesses, often 
using the names of friends and relatives. The aim is to avail 
of the lower rate compared to the appropriate VAT. To combat 
this, tax administrators will analyze tax data and identify the 
interconnectedness of split entities. Armenia’s tax authorities 
handled this issue using several techniques.
Single administrative document (SAD). Producing a SAD 
about importers of goods is one way of having a fuller view. 
Analytics detect whether a taxpayer is always importing the 
same goods from the same country and the same enterprises 
repeatedly. Moreover, electronic invoices help detect groups 
of taxpayers that use identical storage for imported goods. 
The tax administration investigates the anomalies.
Cross-matching of sales and invoices. Cross-referencing 
sales and invoice data provides important insights into various 
sellers’ revenues. Armenia’s tax authority collects data from 
the registration database—new generation fiscal machines 
(NGFM) or cash registers connected to the agency’s serv-
ers—and invoice databases. The invoice database detects 
when a variety of entities are selling goods from the same warehouse. The NGFM data reveals when a group of taxpay-
ers use a variety of fiscal machines at the same location. The 
registration database reveals when different enterprises have 
the same founders. Such suspicious anomalies are subject 
to detailed audits, which may not necessarily be tax fraud but 
need deeper scrutiny.
Taxpayer’s employees. AI and analytics can detect suspi-
cious cases in which different employers declare identical 
groups of employees in income tax filings or when a closed 
and reopened business hires identical employees. Employee 
information is obtained by linking a social security number and 
person identification database using Big Data infrastructure.
AI and analytics on sales data from the cash registers. 
The Monitoring Center leverages information received from 
NGFM. Some patterns call for further scrutiny. For example, 
if a fiscal machine does not work throughout the day, and the 
taxpayer prints 100 or 200 receipts within one or two hours, 
this flags a falsified fiscal amount with false receipts without 
actual sales. Some taxpayers print a single receipt with an 
unrealistic amount at the end of the day. All such cases are 
under control since the Monitoring Center automatically sends 
notifications and requires explanations. If no reasonable ex-
planation is given, the case goes to audit.
Comparison of data from utility providers. The data from 
water, electricity, and gas, for example, reveals enterprise ex-
penses, which demonstrate a logical correlation with the to-
tal amount of reported sales for a particular line of business. 
Again, this cross-referencing and correlation shows valuable 
insights.
The outcomes of Big Data analysis. Using targeted audits 
conducted during the first years of implementation of NGFM, 
the tax administration reduced the number of audit cases by 
about 2.5 times over recent years. The effectiveness, mea-
sured by the average amount of additional tax per audit, grew 
constantly over recent years. Also, the agency achieved sub-
stantial cost savings due to the rapid reduction of the num-
ber of local tax administration offices. Armenia reduced the 
number of local tax offices from 52 in 2009 to only two offices 
in 2017 (IOTA 2018). These are the departments for large, 
medium, and small taxpayers.Use Case 
BriefNew Generation Fiscal Machines
Strategic 
contextT ax evasion among businesses and individu-
als.
Problem 
statementT ax evasion practices remain undetected as 
evasive practices fail to cross-reference fiscal 
records that may reveal correlations resulting 
in the detection of tax reporting anomalies. 
AI methodsNatural language processing, Big Data, data 
mining, and cluster analysis.
Role of 
humansLevel 2
Source: World Bank.
29
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>United States
Palantir Gotham Platform
In 2011, the Internal Revenue Service (IRS) created the Of-
fice of Compliance Analytics (OCA) to construct analytics 
programs that could identify potential refund fraud, detect 
taxpayer identity theft, and handle noncompliance issues ef-
ficiently. OCA leverages an advanced analytics program that 
relies on the use of Big Data and predictive algorithms to re-
duce tax fraud. In 2016, significant organizational changes 
took place when the OCA and Research, Analysis, and Sta-
tistics merged to create the Research Applied Analytics and 
Statistics (RAAS) division. RAAS leads a data-driven culture 
through innovative and strategic research, analytics, statistics, 
and technology services in partnership with internal and ex-
ternal stakeholders. By combining AI and advanced analytics 
platforms, RAAS extracts value by leveraging vast amounts of 
proprietary data stored within the IRS legacy computers.
The IRS uses the Palantir Gotham platform to run its Lead 
and Case Analytics (LCA) service.  Special agents and in-
vestigative analysts in IRS Criminal Investigations use LCA 
to “generate leads, identify schemes, uncover tax fraud, and 
conduct money laundering and forfeiture investigative activi-
ties” (Federico and Thompson 2019).
The various divisions of the IRS have access to several 
data mining applications. These include the Investigative 
Data Examination Application—formerly known as Investiga-
tive Data Analytics; LCA; Return Review Program (RRP); Fi-
nancial Crimes Enforcement Network Query; and Compliance 
Data Warehouse. In 2016, RRP generated more than 693,000 
identity theft leads, with a 62 percent accuracy rate and more 
than 103,000 other nonidentity fraud leads with a 49 percent 
accuracy rate (U.S. Department of the Treasury 2017).Modeling data-driven tax policies in most developing coun-
tries is hampered by a lack of reliable data, forecasting skills, 
and robust models. These impediments could be overcome 
through the use of emerging AI tools if concomitant analog 
complements are in place. The challenge in most settings is 
devising a tax policy that optimizes equity and productivity. 
The AI Economist employs AI models based on RL algorithms 
to model and predict tax policy design through data-driven 
simulations using a two-level RL framework composed of 
agents (workers) and tax policy to model and learn the effects 
of dynamic tax policies in principled economic simulations. 
The framework does not use prior world knowledge or make 
any modeling assumptions. It can optimize for any socioeco -
nomic objective. It learns from observable data alone. Though 
the framework is not yet deployed in government, results 
show that the AI Economist can improve opportunity costs 
and trade-offs between equality and productivity by 16 per-
cent when compared to a prominent tax framework proposed 
by Emmanuel Saez, professor of economics and Director of 
the Center for Equitable Growth at the University of California 
at Berkeley. The framework captures even larger gains over 
an adaptation of U.S. federal income tax in the free market 
(Zheng et. al. 2020).AI in T ax Policy
United States
AI EconomistUse Case 
BriefPalantir Gotham Platform
Strategic 
context T ax refund fraud, identity theft, and compli-
ance.
Problem 
statementAI is necessary in detecting tax evasion and 
conducting criminal inves-tigations in cases of 
tax fraud and identity theft.
AI methodsCloud computing, Big Data, analytics, aggre-
gation, and automation.
Role of 
humansLevel 1-2
Source: World Bank.Use Case 
BriefAI Economist
Strategic 
contextMacro-fiscal policymakers need better data 
and analytical reporting to design data-driven 
policies. 
Problem 
statementData-driven macro-economic policies are 
hampered by a lack of data, skills, and robust 
models.
AI methodsArtificial neural networks, cloud computing, 
Mechanical Turk, and au-tomation.
Role of 
humansLevel 1
Source: World Bank.
30
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>100%Equality
Productivity> > >
FIGURE 8 - An Optimal T ax Policy Optimizes a Balance between Equality and Productivity
Source: Zheng et al. (2020).a. Equality-productivity trade-off b. Wealth distribution by T ax Model
Free Market T op Agent US Federal Typical Agent Saez Formula AI EconomistPareto Boundary
0%
Low HighFree
MarketUS
FederalSaez
FormulaAI
Economist
Notably, the AI Economist leveraged real-world human actors 
in the roles of workers competing with AI-driven policy models 
that evolved based on human interactions. Figure 9 compares 
the overall results of the study. They take into account the 
Pareto boundary, which is the event horizon where marginal 
benefit and cost trade-offs result in reduced productivity. Note the parity in wealth distribution among sectors of society and 
the overall gain in productivity due to the tax policies enacted 
by the AI Economist model. The AI Economist is in active de-
velopment with plans for open-source distribution and govern -
ment engagements shortly.
31
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Canada, UK
MindBridge for AI Auditor
Use Case 
BriefMindBridge AI Auditor
Strategic 
context Oversight, assessment of the effectiveness of 
risk management, con-trols, and governance 
through external and internal audit.
Problem 
statementAI models help maximize the efficiency of docu-
ment analysis for legal infraction detection 
and policy audits.
AI methodsNatural language processing, Big Data, data 
mining, anomaly detection.
Role of 
humansLevel 1-2
Source: World Bank.
Private sector audit and assurance firms are the primary 
adopters of AI in the audit. Their goal is to maximize ef-
ficiency, minimize the costs of audit work, and enhance the 
coverage of audit procedures. Specifically, these procedures 
require two functions:
• Analyze contract documents—leases, rental agreements, 
etc.—for pre-identified keywords, such as key clauses, 
dates, persons, and relevant terms.
• Present potential anomalies for further human investigation.
Because these documents may be several thousand pages 
long, they are often reviewed on a sample basis due to limita-
tions associated with manual labor.
However, AI allows document analysis at a fraction of the time 
cost. In some cases, it reduces the time cost by more than 90 
percent. Furthermore, the quality of risk assessment is also 
vastly improved.When detecting anomalies, AI produces a risk score using 
general ledger entries with financial features to meet compli -
ance and assurance parameters. Some of these features are:
• Materiality levels
• All urgent payments
• Unbalanced debits and credits
• Rare flows
• Cash to bad-debt conversions
• All payments that went through multiple adjustments or 
reversals
• Journal entries beyond a threshold
• Open invoices beyond a period
• Sudden spikes in otherwise dormant vendors
• High-value transactions for a historically low-value vendor
• Duplicate entries
• End of the year or end of the period procedures
• Uncleared bank reconciliation entries
• Multiple changes to the bank account information of a 
vendor.
AI processes allow auditors to extract and load account-
ing and finance data directly from financial management 
information systems (FMIS) or underlying enterprise re-
source planning (ERP) systems.  Human auditors use a 
dashboard to visualize the risk scores and investigate anom-
alies externally. Auditors can flag data and trigger ML algo -
rithms to refine scores. In minutes, AI can do work that will 
otherwise cost several auditors for many weeks. Some tools 
are compliant with international audit standards like SAS 99, 
CAS 240, and ISA 240, such as the MindBridge AI Auditor, an 
application developed by a private Canadian firm. The UK and 
Canadian federal governments are testing the tool for wider 
applicability and adoption (MindBridge). AI in Audit
32
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>3.AI Risks>>>
For all the potential benefits, there are also significant potential risks that will need to be miti-
gated for the adoption of AI as part of a government’s digital transformation. The risks and their 
mitigating measures discussed here are primarily at the project-level, while policy-level ethical 
issues for society at large are discussed in Chapter 5. 
Performance, Trust, and Bias
Negative bias is an inherent problem in AI that arises as a result of many factors, includ -
ing incomplete, inaccurate, or corrupt data (statistical bias) which cause a predictive 
outcome that is in favor of or against one or more groups of people.  There are well-known 
cases of how harmful such negative bias can be leading for example to unfair access to public 
services such as housing and social benefits or unfair incarceration. For example, analysis of 
the Correctional Offender Management Profiling for Alternative Sanctions (COMPAS) software 
used by U.S. courts and police to forecast which criminals are most likely to re-offend found it 
was biased against African Americans. The COMPAS algorithm provided information to police 
and judges to make decisions on defendants and convicts, for example setting bail amounts and 
sentences. The analysis found that the software was twice as likely to falsely label black defen-
dants as future criminals than white defendants.9 
9. Venkateswaran 2020
33
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Some bias is inherent in AI models because data are fi-
nite, even when made available at scale. AI systems need 
to continuously be refined and improved AI systems as da-
tasets and tools evolve or weaknesses emerge. Even with 
considerable preparation, sources of bias can be difficult to 
identify preemptively. As a result, AI results can be deceptively 
rational, even when biased (Ntoutsi et al. 2020). Sometimes, 
the AI team of developers or data scientists carries some in-
herent bias (cognitive bias), which should also be carefully 
monitored. Also, AI firms voluntarily manipulate data and algo -
rithms to maximize profits (economic bias), which should also 
be addressed through policy action and public scrutiny. 
To manage the risks of bias and the impact on access to 
services, a policy framework needs to address these is-
sues. The full disclosure of the datasets and algorithms 
used in AI is the key to managing bias. Data and algorithm 
disclosure can aid in building trust and also aids the produc -
tion, collection, and engineering of “good” data, which is de-
fined as follows:
• Good data are available in abundance. The more data, 
the better.
• Good data have explainable features that relate to the 
problem statement. Raw unprocessed data contains sim-
ple, human-readable values.
• Good data are extensible. In other words, new features 
(data points or parameters to the layman) can be added 
to each record as models evolve. Feature engineering is 
possible with good data, which involves using existing 
features to derive additional information about each re-
cord of data (see Annex A).
• Good data are normally distributed. A normally distributed 
sample of the population is easily derived using random 
selection methods during training and testing. The values 
of data are not random; however, the selected members 
of a broader population are random.
• Good data are complete. Data are not missing key fea-
tures that are critical to the problem statement.• Good data can be traced back to the origins. Data can ob-
fuscate sensitive personal information about people. Data 
come from official government sources.
Nonetheless, bias can emerge throughout the AI project 
life cycle, often unconsciously, through selective data 
gathering, requiring additional policies to oversee data 
selection processes. For example, data scientists may 
choose to collect data from groups that are perceived to be 
relevant, but these groups may be selected as a matter of 
personal preference. This is a classical polling technique that 
yields favorable results from a population-based selection of 
data around information on gender, race, ethnic origin, zip 
code, color, and disability. 
Bias is best mitigated by policies and processes that en-
sure inclusion, conscientious oversight, transparency, 
disclosure, and contestability. Where models may influence 
public policy or mission-critical outcomes, the publication of 
data collection criteria as well as the release of open-source 
code for the implemented frameworks may mitigate the risk 
of producing nefarious outcomes. Even more so, the democ-
ratization of data and policymaking can improve the practical 
outcomes of AI frameworks and enhance trust in AI infrastruc-
ture in government.
Additionally, governments should develop competing AI 
systems that focus on the same problem statement. By 
employing multiple solutions on one problem statement, a 
practice adopted in Singapore and Israel, governments can 
significantly improve the likelihood of a positive outcome. Two 
systems with varying degrees of bias help reduce the likeli -
hood of unintended outcomes by converging on results in dif-
ferent ways. Also, AI systems could be developed to identify 
bias – use the same tool to fight bias that caused the bias in 
the first place. 
Human oversight could provide an additional safeguard 
against machine-invoked bias. Introducing human oversight 
can help detect skewed results from influences such as train-
ing data manipulation, forgery, and intentional bias. 
9. Venkateswaran 2020
34
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Implementing agencies could develop risk mitigation frameworks. Many governments have already developed model AI risk 
mitigation frameworks, which can be tailored to the local context. The Government of Canada developed an Algorithm Impact As-
sessment10 for implementing agencies that consist of an online questionnaire and scoring scheme to assess the level of risk and 
mitigate the risk. The U.S. Department of Homeland Security has developed an AI risk assessment framework that is also useful 
in mitigating risks in AI performance. Key aspects of this framework are summarized below:
10.  https://www.canada.ca/en/government/system/digital-government/digital-government-innovations/responsible-use-ai/algorithmic-impact-assessment.html.Types of 
StandardHow Applicable to AIWhere Standards 
Are AppliedHow It Can Reduce 
AI Risk from an Adversary
Analytics and 
re-searchStandards that evaluate 
the quality of analysis and 
scrutability of algorithmsBack end: explainability 
and transparencyIdentify faulty logic or reasoning, increase the 
difficulty of deceiving and/or manipulating 
analysis from AI
Determine how much to trust system inputs and 
outputs
Legal and 
regulatoryStandards-based 
on govern-ance and 
regulatory over-sight into 
preserving privacy and 
consentFront end: usability 
and personalization; 
back end: standardized 
architectureChange understanding of liability for mistakes 
and enhance attribution 
Transform the notion of the jury of peers and 
evolve crime and punishment
Moral and ethicalStandards that prevent AI 
from performing actions 
that are contrary to a 
moral or ethical normBack end: fail-safesReduce the likelihood that AI will do the “wrong 
thing” (i.e., immoral or unethical behavior) if 
exploited or infiltrated by an adversary
T echnical and 
indus-tryStandards to measure 
the performance of an 
algorithm on relevant 
tasksFront end: performanceMeet appropriate tech-nical specifications (e.g., 
low number of false posi-tives) to be robust 
against adversary denial and deception activities
Data and 
Information 
securityStandards for the 
protection, sharing, or use 
of data relevant to a taskFront end: training; back 
end: data integrity and 
availabilityLimiting access to and information about how an 
AI system works to appropriate people could help 
prevent exploitation by an adversary 
Preventing manipulation of training data> > >
TABLE 3 - AI Risk Mitigation Framework
Source: Oxford Insights 
35
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Hacking poses a serious risk in AI systems. Forged data and 
bad actors can impair training algorithms to cause harm. One 
of the most common hacking techniques to exploit security 
vulnerabilities in AI is phishing. 
Spear-phishing tactics include the practice of delivering 
malicious code or gaining unauthorized access through 
socially engineered messages. The best-known example of 
a general phishing attack is that of a digital hustler – a foreign 
prince offering unclaimed money in a foreign bank account in 
exchange for a small cash advance or a bank account number. 
In this example, broad stroke methods of AI message creation 
leverage socially desirable outcomes, which AI in spam filters 
have become adept at detecting. Propagation of malicious 
code that spreads itself across systems, networks, and even 
‘networkless’ devices offers exponential reach in offensive cy-
ber operations. The most notable example is the infiltration 
of a country’s uranium enrichment program called Stuxnet, 
where a targeted propagation attack led a centrifuge to spoil its 
payload. Another is NotPetya, which relied on password theft 
and caused over $10 billion in damages across hundreds of 
thousands of computers in more than 100 countries. NotPetya 
was later repurposed by the National Security Agency of the 
U.S. Department of Defense to rip through targeted networks 
in seconds or minutes, making it one of the fastest-spreading 
pieces of malicious code in history. The adage goes, “by the 
second you saw it, your data center was already gone.” Not-
Petya did not leverage even the slightest bit of AI.
Solid governance practices help mitigate the risks by 
imposing explainability, transparency, and validation in 
AI systems, in addition to the security best practices at the 
technical level. Governments can prevent adversarial attacks 
on data sources and computing resources with the use of 
security best practices, such as access-control lists (ACLs) 
and API tokens for inter-process communication (IPC) and 
human-facing endpoints. These practices are standard rules 
among corporations. Government systems are no exception 
to these rules of practice.
Prevent common patterns that kill critical processes. Pro-
active cybersecurity operations conceptualize the kill chain—the 
sequence of steps that hackers cycle through to achieve nefari-
ous goals. Both hackers and defenders have a vested interest 
in finding vulnerabilities in AI systems; the former to exploit, the 
latter to remediate. AI is useful in vulnerability discovery.
Mitigate zero-day exploits—those with no patches—that 
are the targets of cyberattacks.  Cybersecurity and AI teams also use tools known as fuzzers to discover errors and secu-
rity loopholes by inputting massive amounts of data (called 
fuzz) to the system in an attempt to make it crash.
The implementing teams should also ensure back up data 
with redundant systems and enforce no single point of 
failure (SPOF). Wiping attacks that erase or overwrite oth-
erwise benevolent files on computing systems are difficult to 
detect because their effect is known only after they propagate 
and execute. However, through the use of learning-enabled AI, 
engineers can develop defenses against these types of propa-
gation attacks, though no known examples of such AI systems 
exist in the public domain. Obfuscation and anti-forensics em-
ploy methods of detection avoidance. AI can be quite beneficial 
in detecting obfuscation attacks as well as creating them. De-
structive attacks are unlikely candidates for AI prevention.
AI holds great promise in cybersecurity defense. How-
ever, given the fact that destructive propagation attacks can 
proliferate and remain dormant for months, even years, the 
detection of these attacks may be limited in scope. Still, the 
effort to detect security breaches remains a key focus of AI 
systems in cybersecurity.
It stands to reason that if AI can learn to detect threats, it can 
also alter them to further delay their effects if not block them 
altogether. Furthermore, attribution mechanisms that detect 
external sources of threats through AI clustering techniques 
are proving themselves in identifying sources for threats in 
geographic regions. In some cases, NLP can detect grammat-
ical nuances in source codes that allow defenders to home in 
on geographical regions for further investigation.
All told, the many methods of subterfuge and espionage em-
ployed by hackers and defenders are writhe with theory and are 
unclear to the general public. Sometimes, tools designed at the 
hands of government defense departments are responsible for 
the greatest defenses and offenses. It is in the domain of a gov-
ernment’s responsibility to determine the degree of impact that its 
defensive strategies have on the safety of the general population.
Remain proactive. Not all offenses are the source of political 
cyberwarfare. Many still emerge from obscure corners of the 
internet, to “prove” that the vulnerabilities of government and 
commercial organizations are real. Although they are effec-
tive in advancing the evolution of cybersecurity best practices, 
they are more often than not isolated incidents that fall under 
the jurisdictions of international authorities and garner stern 
responses from enforcement officers and legislators.Cybersecurity
36
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Control
Because many AI systems operate autonomously and inter-
act behind the scenes with one another using IPC, machine-
centric feedback loops can cause unintended consequences. 
In 2010, stock exchanges that allow high-frequency trading 
experienced a flash crash caused by AI algorithms that went 
awry in competition with one another. This led to unintended 
artificial financial market inflation. Moreover, chatbots interact-
ing with one another can create their language that humans 
cannot understand.
Proactive control, monitoring, testing, and validation are nec-
essary to control the outcomes of rogue AI systems and pre-
vent edge cases in software development from getting the 
best of humanity, if only on a rare occasion.
Privacy
The use of data fabrics and Big Data, growing reliance on au-
tomation and decision-making, and the gradual reduction of 
human involvement in human processes raise concerns about 
fairness, responsibility, and respect for human rights. More-
over, AI data policy raises concerns for privacy and individual 
identity. Group and community-driven AI has the potential to 
increase the risk of harm by what Carl Jung describes as the 
collective unconscious of humanity, a shadowy force or dark 
side of personality that collectively propels human digressions 
at a macro level. AI is no exception.
Protect privacy and human identity. Yet, despite all the 
foreboding ethical predictions, ethical influences begin with 
the protection of individual identity within large-scale datasets, 
access control, and policies. This prevents the arbitrary ex-
ploitation of identity recognition systems. In the United States, municipalities are enforcing policies to ban facial recognition 
technology altogether. The use of AI in some countries to de-
tect fever from facial recognition software in cameras installed 
at public places carried the risk of human surveillance and 
infringement of privacy. In Singapore, the GovTech agency 
and the Ministry of Health (MoH) have co-developed an app, 
TraceTogether, that can trace individuals without infringing on 
privacy. Citizens download the app, turn on the Bluetooth, and 
allow push notifications and location services. The app can 
exchange signals in a short distance of 2-5 meters with other 
app users, exchange anonymized identifications (IDs), and 
store anonymized data locally of all the persons in the proxim-
ity of the app users. If the user allows on the app, the MoH 
will contact the user by sending a code. MoH will then be able 
to decrypt the random IDs of individuals with whom the user 
came into contact. The authorities comply with the privacy and 
data protection laws, as no personal details are collected ex-
cept the phone number. 
Furthermore, policies can enforce limitations on group infer-
ence models that lead to individual discrimination. For ex-
ample, organizations are choosing to obfuscate individual 
identity to mitigate against the risk of fraud due to unauthor -
ized access to data. Rather than use names and ID numbers, 
data systems are using salted cryptographic hash functions 
to “scramble” identifiable information. Because the use of a 
salted hash function is idempotent—it always yields the same 
result for a given input—systems can protect exploitable data 
and retain uniqueness for algorithmic purposes.
Privacy legislation and regulatory framework provide a 
solid legal basis for mitigation privacy risks. Governance 
frameworks that promote self-assessment, peer review, and 
public inclusion could strengthen compliance with these le-
gal frameworks. The details could be adopted based on the 
context and existing mechanisms of transparency, citizen en-
gagement, and accountability. However, the value of public 
inclusion is critical in this process.
37
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>4.AI Governance and Operations>>>
Most advanced digital governments have issued governance frameworks, including ethical prin-
ciples for the use of AI. An overview of these governance models is presented in this chapter, 
which discusses three aspects of governance models: ethical principles, the role of a central 
agency, and operational framework.
AI Ethical Principles
The risk mitigation for AI requires the adoption of some ethical principles and several of the 
key ethical considerations. Several advanced digital economies are adopting AI governance 
models and policies developed by an interagency team of policymakers and AI experts, this 
chapter summarizes those principles, identifies good practices for the institutional design for 
adopting AI in the public sector, and shares innovative procurement practices for acquiring AI 
implementation services. The models of AI governance typically include bias, privacy, algorithm 
opacity, limited data access, security, citizen consent, and inadequate supervision. National gov-
ernments, including Australia, Canada, China, Japan, Singapore, United Arab Emirates, and 
the United States as well as international organizations including the European Commission 
(EC), the Institute of Electrical and Electronics Engineers (IEEE), International Organization for 
38
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Standardization (ISO), UN, and World Economic Forum, are 
actively proposing governance models for AI that emphasize 
common principles:
• PRIVACY AND DATA PROTECTION.  AI solutions should 
respect an individual’s right to privacy and civil liberties. 
Individuals should have control over their data. Individual 
consent is necessary for using and re-distributing their 
data. They should have the right to restrict the processing 
of their data, rectification, and erasure.
• ACCOUNTABILITY. Mechanisms must ensure account-
able behavior during the life cycle of AI design and im-
plementation. Impact assessment frameworks should be 
done to identify accountability at every step of the pro-
cess. An agency or body should be responsible for moni-
toring accountability.
• SAFETY AND SECURITY. Cybersecurity is critical. AI so-
lutions should have predictable behavior. Leaders must en-
sure the well-being of society at large and private individual 
humans.
• TRANSPARENCY AND EXPLAINABILITY. The algo -
rithm, business case, data collection, design, and policy 
information must be transparent to stakeholders and 
those impacted. Open-source data algorithms could en-
hance transparency. Individuals should get notifications 
when interacting with AI or when AI decides for him or 
her. There should be regular Reporting requirements on 
transparency. The rights of citizens to information are im-
portant. Data should be of high quality and representative.
• FAIRNESS. AI solutions should minimize bias and iden -
tify and manage risk. Inclusiveness should be ensured in 
design and impact.• HUMAN CONTROL OF TECHNOLOGY. The AI should 
be under human control. People should review automated 
decisions. Individuals should be allowed to opt-out of au-
tomated decisions.
• PROFESSIONAL RESPONSIBILITY. Multistakeholder 
collaboration, accuracy, and scientific integrity of the solu-
tion should be ensured.
• PROMOTION OF HUMAN VALUES. AI should be hu-
man-centric. It should promote human values and benefit 
society.
Some governance models and guidelines emphasize common 
program and project management practices like cost-benefit 
analysis, legal and regulatory compliance, risk management, 
flexibility, and the use of an agile approach.
Ensuring compliance with these principles would require 
a careful balance between oversight and agility.11
These principles are given a different level of emphasis in dif-
ferent settings. The Berkman Klein Center for Internet Society 
at Harvard University tracks and maps the global consensus 
on ethical principles for AI. Figure 10 is adapted from their 
work which shows the global adoption of these principles and 
the level of emphasis of each principle. Despite different levels 
of emphasis on different principles, there is a consensus that 
ultimate control of AI must remain with people. AI must not be 
a regulatory means unto itself.
11. To enforce policies, the European Union (EU) is considering establishing a standards body, similar in composition to the U.S. Food and Drug Administration, to assess 
the impact of algorithmic processes before release. There is a key problem here since algorithmic innovation occurs at such a speed that it outpaces the government’s 
ability to evaluate every potential outcome. The agency may even become a bottleneck that developers simply bypass due to capital constraints. Instead, some propose 
that such validation should be part of a certification process that is executed through peer review .
39
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>> > >
FIGURE 9 - Global Consensus on the Principles of AI
Source: Fjeld et al. (2020).
40
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Country Examples of 
AI Governance Systems
The Australian Government Department of Industry Innovation 
and Science funded research into the ethical principles of AI us-
age in government in 2018 and published a white paper on it in 
2019. The core principles in its AI governance framework are:12 
1. GENERATES NET-BENEFITS. The Al system must gen-
erate benefits for people that are greater than the costs.
2. NOT HARM. Civilian Al systems must not be designed to 
harm or deceive people and should be implemented in 
ways that minimize any negative outcomes.
3. REGULATORY AND LEGAL COMPLIANCE. The Al 
system must comply with all relevant international and 
Australian local, state, territory, and federal government 
obligations, regulations, and laws.
4. PRIVACY PROTECTION.  Any system, including Al sys-
tems, must ensure people’s private data are protected 
and kept confidential plus prevent data breaches which 
could cause reputational, psychological, financial, profes-
sional, or other types of harm.
5. FAIRNESS. The development or use of the Al system 
must not result in unfair discrimination against individuals, 
communities, or groups. This requires particular attention 
to ensure the “training data” is free from bias or character-
istics which may cause the algorithm to behave unfairly.
6. TRANSPARENCY AND EXPLAINABILITY. People must 
be informed when an algorithm is being used that impacts 
them and they should be provided with information about 
what information the algorithm uses to make decisions.
7. CONTESTABILITY. When an algorithm impacts a person 
there must be an efficient process to allow that person to 
challenge the use or output of the algorithm.8. ACCOUNTABILITY. People and organizations respon -
sible for the creation and implementation of Al algorithms 
should be identifiable and accountable for the impacts of 
that algorithm, even if the impacts are unintended. 
The Canadian government’s 2019 Directive on Automated 
Decision-Making13 guiding principles for the ethical application 
of AI governance are:
• Understand and measure the impact of using AI by devel -
oping and sharing tools and approaches.
• Be transparent about how and when to use AI, starting 
with a clear user need and public benefit.
• Provide meaningful explanations about AI decision mak-
ing, while also offering opportunities to review results and 
challenge these decisions.
• Be as open as possible by sharing source code, training 
data, and other relevant information, all while protecting 
personal information, system integration, and national se-
curity and defense.
• 
• Provide sufficient training so that government employees 
developing and using AI solutions have the responsible 
design, function, and implementation skills needed to 
make AI-based public services better.
Furthermore, the Canadian government formulated a compre-
hensive analysis and exposition of the key government pro-
cesses in play across the entire government. The document 
includes objectives and expected results, definitions, and 
rules for semi-annual re-evaluation, which is crucial in light 
of the rapid pace of AI development. The government also 
developed an Algorithm Impact Assessment (AIA), which is 
a questionnaire designed to assist agencies in assessing and 
mitigating their risks.14
12. https://www.industry.gov.au/news-media/towards-an-artificial-intelligence-ethics-framework.
13. https://www.tbs-sct.gc.ca/pol/doc-eng.aspx?id=32592.
14. For more information, visit the “Responsible Use of Artificial Intelligence (AI)” on the website of the Government of Canada at https://www.canada.ca/en/government/sys -
tem/digital-government/modern-emerging-technologies/responsible-use-ai.html#toc1.> > >
BOX 1 - Actionable Insight: Adopt Principles of AI and Issue an AI Governance Model
The central digital agency should adopt the common principles of ethical AI and prepare a governance model. The model 
should formulate operational arrangements, including an innovation hub, data governance, data standards, collaboration 
with the private sector, skills development, adoption in the public sector, and partnership with nonprofit and academia to 
promote AI research, among others. 
41
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Singapore
Most of the themes discussed have been incorporated into the 
Model Governance Framework, issued by the Government of 
Singapore (PDPC 2020).15 Singapore maintains an active 
leading role in the strategic development of integrated gov-
ernment AI systems around the world. Singapore is actively 
investing in AI policy and process standards among partner 
nations to support global AI development in trade and com-
merce. Transparency affords opportunities for the successful 
development of systems impacting key strategic international 
partners. The willful commitment to long-term execution pro-
vides a global foundation that extends far beyond Singapore’s 
borders. Singapore is also experimenting with policy enforce-
ment using AI-powered robotics and contact tracing since the 
start of the COVID-19 pandemic. The government’s gover-
nance model is driven by two fundamental guiding principles:
• EXPLAINABLE, TRANSPARENT, AND FAIR PROCESS.  
The organizations using AI should ensure the decision-
making process is explainable, transparent, and fair.
• HUMAN-CENTRIC AI. AI SOLUTIONS ARE HUMAN-
CENTRIC. AI helps amplify human capabilities and pro-
tects human interests.
The model advocates that organizations should embrace four 
key measures in their quest for AI adoption:
1. INTERNAL GOVERNANCE STRUCTURE. The involve -
ment of top officials and their sponsorship of AI initiatives 
is critical. This ensures ethical considerations are intro-
duced in the decision-making process and monitored reg-
ularly at the highest levels.
2. DETERMINING THE LEVEL OF HUMAN INVOLVE-
MENT IN AI-AUGMENTED DECISION-MAKING. AI 
algorithms can support processes with or without the in-
volvement of humans. Any process that affects human 
beings must involve humans “in-the-loop.”
3. OPERATIONAL MANAGEMENT. This aspect includes 
data management, talent, skills, and procurement. Or-
ganizations must ensure data governance arrangements 
are in place to ensure integrity, consistency, transparen -
cy, security, interoperability, and accountability for data. 
Also, organizations must strive to incorporate relevant talent through proactive partnerships between academia, 
private sector firms, and start-ups, which are fundamen -
tal pipelines leading to the success of AI initiatives. Pro-
curement must provide flexible experimentation, produce 
proofs of concept over multiple iterations, and scale up 
with an acceptable risk of failure.
4. STAKEHOLDER INTERACTION AND COMMUNICA -
TION.  Strategies must ensure consistent and transparent 
communication with the key stakeholders and manage re-
lationships with them. In the public sector, public scrutiny 
and transparency are critical aspects of AI initiatives.
The U.K. Office for Artificial Intelligence is responsible for 
overseeing the implementation of A I and has produced sev-
eral Reports.16 The agency is a joint effort of the Department 
for Business, Energy, and Industrial Strategy and the Depart-
ment for Digital, Culture, Media, and Sport.
AI and the Multilaterals
The EC has formed a high-level expert group to prepare the 
ethics guidelines which were circulated for comments, testing, 
and assessment in 2019 and being vetted by many organiza-
tions (EC 2019). The EC envisions developing an AI ecosystem 
that brings benefits to citizens and businesses for improved 
service delivery, promotes new products and services, and em-
phasizes sustainability while ensuring safeguards, rights, and 
freedoms. EC is promoting a common European approach to 
reach scale and avoid fragmentation of the single market. Ac-
cording to these guidelines, trustworthy AI should be:
• Lawful, comply with all applicable laws and regulations.
• Ethical, respect ethical principles and values.
• Robust, both from the technical and social perspective.
The guidelines put forward seven key requirements that AI 
systems should meet. They incorporate the ethical principles 
promoted by the EC and include human agency and over-
sight, privacy and data governance, transparency, diversity, 
nondiscrimination and fairness, societal and environmental 
well-being, and accountability.
In 2019, the United Nations (UN) launched its Centre on Ar-
tificial Intelligence and Robotics, under the UN Interregional 
Crime and Justice Research Institute (UNICRI), to monitor 
developments in AI and robotics, with the support of the gov-
15. https://www.pdpc.gov.sg/Help-and-Resources/2020/01/Model-AI-Governance-Framework.
16. Understanding artificial intelligence—GOV.UK. This is an introduction to using AI in the public sector. The Data Ethics Framework. A Guide to Using AI in the Public 
Sector enables public bodies to adopt AI systems in a way that works for everyone in society (GDS and OAI 2019). Guidelines for AI procurement—GOV.UK. These new 
procurement guidelines will inform and empower buyers in the public sector , helping them to evaluate suppliers, then confidently and responsibly procure AI technologies 
for the benefit of citizens.
42
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>ernment of the Netherlands. The center, based in The Hague, helps focus expertise on AI throughout the UN in a single agency. 
UNICRI initiated its program on AI and robotics in 2015. One of the leading agencies of the UN, the United Nations Educational, 
Scientific, and Cultural Organization (UNESCO) recently appointed an international expert group to draft internationally applicable 
global recommendations on the ethics of AI (UNESCO 2020). This action follows the decision by UNESCO’s 193 member states 
during its last General Conference in November 2019.
> > >
BOX 2 - Arivate Sector AI Principles
There is broad convergence on the adoption of AI principles in the public and the private sector. Several private organiza -
tions have adopted principles to enhance trust and transparency in the process of developing AI applications:
• IBM’s principles of trust and transparency state that AI should augment human intelligence rather than replace it, trust 
is key to adoption, and data policies should be transparent (Dignan 2017).
• Google’s principles on AI state that AI should protect the privacy of citizens and be socially beneficial, be fair, be safe, 
and accountable to people. 
• The Asilomar AI Principles were outlined at the 2017 Conference on Beneficial AI organized by the Future of Life 
Institute and cover research, ethics, and values in AI. The 23 principles have been adopted and signed by 1,273 re-
searchers and 2,541 other interested parties, including Elon Musk and the late Stephen Hawking. 
• Organizations interested in joining the Partnership on AI must endeavor to uphold eight tenets and support the Part-
nership’s purpose. They include calls for an open and collaborative environment to discuss AI best practices, social 
responsibility on the part of companies delivering AI, explainability, and a culture of trust, cooperation, and openness 
among scientists and engineers. 
• The AI4PEOPLE principles and recommendations are concrete recommendations for European policymakers to 
facilitate the advance of AI in Europe (Floridi et al. 2018).
• The World Economic Forum’s five principles for ethical AI cover the purpose of AI, its fairness and intelligibility, data 
protection, the right for all to exploit AI for their well-being, and the opposition to autonomous weapons (O’Brien et 
al. 2020).
• The IEEE’s set of principles place AI within a human rights framework with references to well-being, accountability, 
corporate responsibility, value by design, and ethical AI (IEEE 2019, 17–35).
The Institute for Ethical AI & Machine Learning adopted eight principles of responsible ML development to provide a 
practical framework to support technologists when designing, developing, or maintaining systems that learn from data.17 
17. https://ethical.institute/principles.html.
43
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>The use of AI in many advanced digital governments is seen 
as a broader effort for the citizen-centric digital transformation 
of public services. A central coordinating agency is typically 
established and responsible for issuing the ethical principles 
and guidelines for trustworthy AI. It develops government-wide 
data strategies and policies to harness the power of AI. This 
is the essential first step in making advances in this domain to ensure commitment, governance, line-of-sight, and monitor-
ing for the acceptable use of AI in the public sector. The AI 
policy should address key policy domains: research, talent, 
entrepreneur ecosystem, ethical standards, data access, AI in 
government, AI in sectors, and governance capabilities (World 
Bank 2020).Role of a Central Government Agency or AI Hub
Country Agency or Program Role
CanadaCIFAR (formerly the 
Canadi-an Institute 
for Advanced Research)Leads the strategy in close partnership with the Canadian government and three 
new AI institutes: the Alberta Machine Intelligence Institute in Edmonton, the 
Vector Institute in T oronto, and Mila in Montreal. It is primarily a research and 
talent promoting institute, while the implementation of AI in the government is 
decentralized.
FinlandAurora AI National
AI ProgramThe program seeks to provide a holistic set of personalized AI-driven government 
services for citizens and businesses in a way that is human-centric and works 
toward their well-being as its ultimate goal, instead of being driven by the needs 
of the public authorities.
Finnish Center for AIA joint partnership by Aalto and Helsinki Uni-versities to promote AI research, 
talent, and industry collaboration. It also supports an AI accelerator pilot 
program and the integration of AI in the public service.
FranceJoint Center of 
Excellence for AIState-level agency to help recruit AI talent and to serve as an advisor and lab for 
public policy design.
Inter-ministerial 
coordina-torThe coordinator’s role is to implement France’s AI strategy, including public sector 
AI transformation efforts, and serving as an interface between the public and 
private sectors.
GermanyGerman Research
Center for AIA major actor in this pursuit and provides funding for application-oriented 
research. 
Plattform 
Lernende Sys-temeBrings together experts from science, indus-try, politics, and civic organizations 
to devel-op practical recommendations for the gov-ernment.
IndiaNational Institution for 
Transforming India—
Aayog programAayog adopted a three-pronged approach: (a) undertaking exploratory proof-of-
concept AI projects in various areas; (b) crafting a na-tional strategy for building 
a vibrant AI eco-system in India; and (c) collaborating with various experts and 
stakeholders.
Saudi ArabiaSaudi Data and Artificial 
In-telligence AuthorityStrategy approved in 2019 provides a core mandate to drive and own the national 
data and AI agenda to help achieve the govern-ment’s Vision 2030’s goals. T o 
fulfill this mandate, the Authority and its sub-entities—National Information 
Center, National Data Management Office, and National Center for AI—will deliver 
on the promise to create a data-driven and AI-supported government.
Singapore Digital Government OfficeOne of the leading agencies on AI, which also brings together research institutions 
and the private sector.
United States The White House Issued a memorandum to the agencies providing guidance on ethical principles 
and operating framework.
U.S. Commerce Depart-
ment’s National T echnical 
Information ServiceDelivers a fed-to-fed framework for data sci-ence innovation through 
partnerships with industry, universities, and nonprofits at the velocity of the 
government’s needs. > > >
TABLE 4 - The Role of a Central Agency in AI
Source: World Bank.
44
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>The institutional arrangements for the implementation of 
AI could be centralized or decentralized. In several jurisdic -
tions such as Canada and the USA, implementation of AI is 
delegated to the agency level, while the central agency issues 
the AI ethical principles, AI data strategy, and operating frame-
work. The central agency may partner with the private sec-
tor and academia to bring in talent and do research. Table 1 
presents an overview of the role of a central agency in several 
countries. Under centralized arrangements, governments cre-
ate a hub within a central digitization agency to implement the 
AI strategy. The central hub pools scarce talent, partners with 
the line agencies, provides an AI lab, and develops alliances 
with academia, the private sector, and start-ups. Governments 
typically view themselves not only as the service providers for 
citizens and businesses but also as an orchestrator of public 
services through expanding public-private partnerships. This 
model is adopted by many economies such as Austria, Es-
tonia, Israel, Saudi Arabia, Singapore, United Arab Emirates, 
and the United Kingdom.
In the United States, AI is both centralized under the federal 
government and decentralized among state governments. 
Centralization is enabled through the National Technical Infor-
mation Service (NTIS) under the U.S. Commerce Department 
and the Federal Risk and Authorization Management Program 
(FedRAMP). The former is responsible for helping federal 
agencies rapidly analyze, manage, and implement scalable 
data solutions by leveraging an extensive NTIS network of 
technical talent from private industry, which is often difficult to 
locate in today’s competitive information technology landscape. 
FedRAMP’s mission is to promote the adoption of secure cloud 
services across the federal government by providing a stan-
dardized approach to security and risk assessment.
The central agency encourages and promotes agency-, 
ministry-, and department-level initiatives. U.S. agencies 
such as the IRS, Treasury, and General Services Administra -
tion (GSA), have their centers of excellence focused on agen -
cy-specific AI solutions. The National Security Commission on 
AI Strategy focuses on defense, security, and war. Regard -
less, state and municipal levels aggressively pursue indepen -
dent AI initiatives, primarily for land management, tax revenue 
management, and fraud detection.
The Canadian government tapped CIFAR (formerly the Ca-
nadian Institute for Advanced Research), a global research 
organization based in Canada, to lead the development of its 
Pan-Canadian AI Strategy. CIFAR is focused on ethics, re-
search, and talent promotion, while implementation is done at 
the government agency level.AI Operations Framework
The central agency responsible for leading the AI initia -
tives generally provides an operating framework. It guides 
agencies and departments through steps for operationalizing: 
defining the idea with a problem statement; conceptualizing 
the problem with experts; proposing a solution to the prob-
lem; developing a proof of concept; and implementing this 
idea through iterative stages. The framework focuses on in-
tegrating AI into operations to produce efficiencies, enhance 
the quality or augment data-driven policy capabilities. It also 
accounts for ways in which the solution will augment human 
decision-making capabilities by increasing the breadth of data 
beyond human comprehension. A key example is using NLP 
to analyze millions of policy documents from citizen sources 
and public records. The operating framework typically guides 
key implementation steps. Governments may customize the 
framework contextually, but overall, it could include six com-
ponents as presented below in Table 5).
Component Description
IdeateThe problem statement is produced in 
detail.
The statement is agnostic to technology.
ConceptualizeThe project manager coordinates discus-
sions between small and medi-um enter -
prises and Al experts.
ProposeA detailed proposal is prepared. It contains 
the problem statement, po-tential solution 
options, and a checklist with a brief de-
scription of each to ensure alignment with 
legal, policy, and ethics risks, mitigation 
ac-tion, and expected results. A separate 
section on data sources is critical. Manage-
ment approves.
Develop a 
prototypeThe project manager ensures technol-
ogy teams work together with Small and 
Medium Enterprises (SMEs) seamlessly to 
develop a proof of concept. A prototype 
visualizes the solution with or without code.
T est SMEs and technical teams test the system.
Develop and 
deployThe system is developed full scale, tested 
again, and deployed for oper-ational use. It 
is also integrated with the environment.
Source: World Bank.> > >
TABLE 5 - Operating Framework
45
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>The implementation steps are summarized as follows:
• IDEATE. The detailed problem statement involves le-
veraging subject matter experts. The problem statement 
should be technology agnostic. It captures sufficient detail 
and contextualizes the overall strategy and vision to main-
tain a clear line-of-sight.
• CONCEPTUALIZE. Domain, subject, and technology ex-
perts enter into discussion and conceptualize technical 
components to the problem statement. These experts are 
either from the center of excellence in the government or 
the private sector. The output of this stage is a conceptual 
Report that details how the solution will address the prob-
lem statement.
• PROPOSE. In this stage, the team formulates a proposal 
for the implementation. Typically, implementation partners 
are private sector firms, including start-ups, nongovern -
mental organizations (NGOs), legislative, and human 
rights experts with experience and knowledge of these so-
lutions. The procurement framework engages these firms 
with flexibility; without detailed specifications, but rather 
based on problem statements and a high-level solution 
concept, amenable to change based on market response.
• DEVELOP A PROTOTYPE. The team selects an imple -
mentation partner and requests a working proof of con-
cept. This software demonstrates how the solution will 
work as a vertical, without pursuing full-scale production 
deployment, customization, and data migration.• TEST, DEVELOP, AND DEPLOY. Proof of concept typi-
cally goes through several iterations, leading up to imple -
mentation, based on working feedback from the subject 
matter and domain experts participating since the early 
planning stages. Upon maturation, the solution is ready 
for go-live production as a pilot capable of scaling hori-
zontally. An important operational issue in procuring AI is 
experimenting at the big-data scale. Traditional approach -
es to linear solution silos require detailed specifications 
that interfere with AI innovation, which involves many it-
erations, much experimentation, optimization, and itera-
tive learning from performance tuning based on unprec -
edented results due to the immense scale of AI modeling 
beyond the scope of human capabilities.
Justification at the 
Conceptualization Stage
AI is not the solution to every problem. How should an organi -
zation evaluate the scope and needs of a problem statement 
to determine whether AI fits the playbill or is little more than 
a theater act? The American Council for Technology and In-
dustry Advisory Council (ACT-IAC) AI playbook for the U.S. 
government offers a questionnaire for assessing the necessity 
and fitness of AI solutions. Figure 11 illustrates the full scope 
of the playbook consisting of five phases. “Phase 1, Problem 
Assessment” stipulates that a government must “[d]develop a 
vision and business objectives through various assessments 
to ensure the AI solution addresses a specific use case and 
delivers results that optimize services and operational deliv -
ery” (ACT-IAC 2020). In more detail, the inputs and outputs of 
this assessment are shown in Figure 11.
46
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>> > >
FIGURE 10 - AI Business Case Assessment
Source: ACT-IAC (2020).
On a granular level, a 14-point questionnaire accompanies the assessment phase, which asks questions of stakeholders and key 
decision-makers. Answers fall on a scale of zero (not at all) to five (critical). A score of 18 or less indicates limited applicability 
and low return on investment; 19 to 40 indicates that AI could be applicable, but not without more in-depth analysis, and over 41 
represents compelling applicability and significant benefits from a potential AI solution. The questions are:
• Does the use case clearly and accurately describe the 
problem to be solved?
• Does the use case accurately outline current processes 
in place?
• Does the use case align the goals and objectives with de-
sired outcomes?
• Does the use case identify what data are required and 
available, accessible, and accurate?
• Does the use case need greater insight from the data?
• Has sufficient data been identified for the use case?
• Are the data from the use of case annotated and curated? 
(Does the data contain meta-information?)
• Does your use case largely need manual process auto-
mation? (That is to determine if only RPA [robotic process 
automation] is needed)
• Is there a predictive element to the use case? (Assump-
tions and testing made based on prior data)• Have other technologies successfully been applied to ad-
dress elements of the use case? (Could you somewhat 
solve your use case with an existing solution?)
• Does the data fit for purpose (descriptive modeling), and 
is it operationally relevant (predictive modeling)?
• Are the authoritative data sources of the use case orga-
nized, structured, deconflicted, and matriculated?
• Could the result of the use case change how conformance 
requirements need to be applied—for example, person -
ally identifiable information (PII), classified, etc?
• Does the use case contain ethical considerations, and is 
there a potential for bias, for example in the data, algo -
rithms, or aggregation process?
The implementation agency should assess the high-level gov-
ernance conditions in Figure 12.Goal: Determine if AI is the appropriate technology to solve my problem
ASSESSMENT
KEY ACTIVITIES
KEY OUTCOMESOUTPUTS INPUTS
Management People Process T echnology Acquisition
• Establish an 
AI Inventory 
& Definition 
Set
• Capture the 
Need and 
Use Cases 
for problem
• Document 
the objective 
trying to be 
achieved
• Program/mission 
office executive 
and rank and file• An AI solution is 
applicable (selection 
is not defined in this 
phase)• The ROI permits 
MGT, procurement 
options exist• Define who 
will use the AI
• Workforce 
Readiness 
(knowledge & 
Capability & 
Skill)
• Willingness 
(Perception 
of Value 
Benefit vs 
Consequence)• Map the use 
to the AI
• Define the 
ethical 
boundaries 
for the AI
• What is the 
impact of 
the AI• Assess how 
sophistication 
and maturity 
of the AI
• Evaluate the 
AI’s fitness for 
the intended 
use
• Identify 
capability 
differentiators• Capture the 
solution value 
and outcomes
• Define your 
constraints 
(Cost & 
Schedule)• Primer: Basic 
understanding 
of AIAI
• T echnical Vision
• Non-functional 
Requirements
• Type/Quality of 
DataAI
• Problem 
Statement
• Use Case IdeasBusiness Need
• Validated Use 
Cases
• Future State Vision
• Stakeholder 
AnalysisBusiness Need
• Checklist
• Applicable 
government-
wide and agency 
specific policy and 
compliance
• High-level risk 
analysisGRCAwareness of:
• Applicable NIST 
Guidance - FISMA,  
800-53 (Security), 
800-63 (Identify)
• Agency specific 
compliance
• Government-wide 
& agency-specific 
policiesBusiness Need
Engaged Defined Planned
47
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>> > >
FIGURE 11 - Operationalizing AI
Source: The World Bank.LeadershipIs Leadership committed? 1
Problem StatementWhat is the problem, is it strategic enough to have impact? 2
DataIs data available, complete and of good quality; can it be shared/interoperable? 3
CoordinationIs there internal coordination machanims with institutions who share the date? 4
ExpertiseHow can I access resources to fine-tune the problem statement, develop the idea, 
engage the AI expertise to test the idea through the proof-of-concept (PoC)? 5
The operating framework should also address the issues of organizational roles and responsibilities. Entities implementing AI 
must identify key roles and responsibilities when designing the internal organization for managing AI, suitable to their context. At 
a minimum, these roles include:
• EXECUTIVE SPONSOR. Depending on the context, this 
role is the head of an agency, chief information officer, 
or department director. This role ensures compliance and 
alignment with the broader legal framework, policy objec -
tives, strategies, and ethical considerations for AI. Also, 
this role develops coordination mechanisms with involved 
agencies.
• WORKING GROUP. Stakeholders from different depart-
ments whose data will be used, or who will be impacted 
by AI or have a stake in the solution, should be consulted 
at every step.
• SUBJECT MATTER EXPERT. Someone that under -
stands the business process and its data, core nature of 
the qualitative objectives, and key results required for the 
successful implementation of an AI solution. This person 
does not need a background in AI to fulfill this role.
• DEVELOPER, AI ENGINEER, AND DATA ARCHITECT. 
An engineer with a mind for understanding the practical 
implementation of the AI infrastructure and engineering requirements. This person needs a background in AI soft-
ware systems engineering.
• DATA SCIENTIST. A quantitative engineer that under -
stands the data requirements for the project based on 
both qualitative and quantitative best practices that lever-
age statistical methods for assessing inbound and out-
bound data for bias and qualitative excellence. This per-
son needs a background in AI modeling and should be a 
champion for data interoperability.
• PROJECT MANAGER. A project manager who manages 
teams, resources, results, and procurement in project 
planning at all stages of the project life cycle. He or she 
needs to be versed in AI systems engineering at a level of 
competency that will allow for the proper scoping of team 
objectives and key results. This person must also take 
the overall responsibility of aligning expectations from the 
subject matter expert, developer, and data scientist so the 
policy liaison can properly construct a policy plan that en-
sures on-time delivery and overall project integrity.
48
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>These roles and responsibilities can be tailored to the  
context, but essentially, they should cover the contextual ar-
eas of activity:
• Oversight of the various stages of AI planning, budgeting, 
design, development, legislation, and operations.
• Integration of roles and responsibilities defined by an in-
ternal risk management framework.
• Procedures for data governance, transparency, and dis-
closure.
• Policies for information governance, which enforce securi-
ty, interoperability, and access control among stakeholders.
• Oversight of data science and AI modeling procedures 
that emphasize documentation and explainability to 
stakeholders.
Stages of T echnical Solution Development
The following concepts are important components that the 
stages of AI implementation must address.
• DATABASE COLLECTION. Collected data must be 
cleaned and checked for bias.• SOFTWARE AND ALGORITHM DEVELOPMENT. Mul-
timodal data recognition must be implemented to reduce 
discrimination, bias, and unjust consequences. Algorithm 
transparency must disclose the steps taken to explain  
the results.
• MODEL TRAINING AND EXCHANGE. Standardization 
and consistency offer practitioners the opportunity to ex-
change trained models without revealing sensitive data, 
yet offering explainable disclosures for the practical pur-
pose of understanding results.
• TESTING AND VALIDATION. Fairness and bias testing 
must be evaluated against standardized test sets created 
with oversight from representatives of affected popula -
tions and stakeholders.
Procurement
Most governments acquire expertise from the private sector 
through innovative procurement methods. The private sec-
tor, in particular start-ups, brings cutting-edge expertise to 
solve the complex public sector problems through AI. The 
implementation team must produce a broad overview of how 
they will customize procurement to these initiatives by using 
the procurement framework. Governments should consider 
adopting a set of guidelines and principles published by the 
World Economic Forum (see Table 6).
49
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Guideline Principles
1. 1. Prescribe a procurement 
process that defines the 
scope of problems and op-
portunities while allowing 
room for iteration.b. Allow innovative procurement processes for AI systems development.
c. Develop a clear focus with a specific problem statement.
d. Avoid putting any energy toward defining the details of the solution.
e. Support an iterative approach to product development.
2. Produce an RFP that pub-
licly defines the benefits 
and costs associated with 
an AI solution while assess-
ing risks.a. Assess why AI is relevant to the problem. Be open to alternative technical solutions.
b. Explain which public benefits are the main drivers in the decision-making process when 
assessing proposals. Consult with external experts if needed.
c. Conduct an initial AI risk and impact assessment before starting the procurement pro-
cess. Ensure that interim findings inform the RFP and revisit the initial assess-ment at 
key decision points.
3. Align procurement with 
relevant existing govern-
mental strategies and 
contribute to their further 
improvement.a. Consult relevant government AI initiatives on national, innovation, or industrial strategies. 
Review any guid-ance documents informing public policy about emerg-ing technologies.
b. Collaborate with other relevant government bodies and institutions to share insights and 
knowledge.
4. Incorporate potentially rel -
evant legislation, policies, 
and codes of practice in 
the RFP.a. Conduct a review of relevant legislation, rights, admin-istrative rules, and other relevant 
norms that govern the types of data and kinds of applications in scope for the project.
b. Consider the appropriate confidentiality, trade-secret protection, and data privacy best 
practices that may be relevant to AI systems deployment.
5. Articulate the technical 
and administrative feasi-
bility of accessing relevant 
data.a. Implement the proper data governance mechanisms at the start of the procurement process.
b. Assess whether relevant data will be readily available for the project.
c. Define data sharing policies for the vendor(s) during the procurement initiative and sub -
sequent project.
6. Highlight the technical 
and ethical limitations of 
intended data uses to mini-
mize issues with bias.a. Consider the susceptibility of data and if the usage of the data is fair.
b. Highlight known limitations (e.g., quality) of the data by consulting domain experts and 
require bidder(s) to describe strategies for addressing these shortcomings.
c. Have a plan for addressing relevant limitations as they arise.
7. Work with a diverse, multi-
disciplinary team.a. Develop ideas and make decisions throughout the procurement process in a multidisci -
plinary team.
b. Require the successful bidder(s) to assemble a team with the right skillset and consult 
with the established domain experts.
8. Focus on mechanisms of 
algorithmic accountability 
and of transparency norms 
throughout the procure-
ment process.a. Promote a culture of accountability across AI-powered solutions.
b. Ensure that AI decision-making is as transparent as possible.
c. Explore mechanisms to enable the interpretability of the algorithms internally and exter -
nally as a means of establishing accountability and contestability.
9. Implement a process for 
the continued engagement 
of the AI provider with the 
acquiring entity for knowl-
edge transfer and long-
term risk assessment.a. Consider that acquiring a tool that includes AI is not a one-time decision. T esting the 
application over its lifespan, adapting to new models, and extending to new datasets is 
crucial to success.
b. Ask the AI provider to ensure that knowledge transfer and training are part of the en-
gagement.
c. Ask the AI provider for insights on how to manage the appropriate use of the application 
by nonspecialists.
10. Create the conditions for a 
level and fair playing field 
among AI solution provid-
ers.a. Discover a wide variety of AI solution providers.
b. Engage vendors early and frequently throughout the process.
c. Ensure interoperability of AI solutions and require open licensing terms to avoid vendor 
lock-in.
Source: WEF (2019).> > >
TABLE 6 - Innovative Procurement Guidelines
The procurement of AI expertise should be done within the procurement framework of the government, exploring flexibilities within the 
framework to ensure the best value for money. Practitioners should adopt an iterative and agile approach to developing a solution.
50
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>The U.S. government has launched two programs to facilitate 
the procurement of innovative solutions: FASt Lane and Start-
up Springboard. The FASt Lane program aims to facilitate and 
streamline the process for younger, innovative companies and 
suppliers to do business with the government. Under this pro-
gram, the suppliers get shorter processing times for specified 
contract categories (e.g., IT Schedule 70 contracts) including 
a 48-hour turnaround for contract modifications and a turn-
around as quickly as 45 days for new contract offers.
Under the Startup Springboard program, if a start-up does not 
have the required experience, it can use the experience of its 
executives and key professionals as a substitute for two years 
of corporate experience. Startup Springboard has one primary 
objective: helping federal agencies quickly gain access to the 
latest innovative technologies from fresh, vibrant private sec-
tor firms (Nakasone 2018).Smaller digital economies also offer similar flexibility in their 
procurements. In Israel, the government issues challenge ten-
ders that outline the problem statements, without the solution 
specifications. 
The Government of Singapore launched a process called Call 
for Solutions. It entails sourcing ICT innovations through the 
evaluation of working prototypes and awarding contracts by 
stages to one or more suppliers. These multiple solutions are 
assessed in parallel through a series of pilot trials when the 
preceding stage or pilot proves successful. Facilitated by the 
Infocomm Development Authority of Singapore, this process 
will allow government agencies to collaborate more closely 
with the industry on ICT innovation needs. The EC adopted a 
similarly innovative approach.18 Figure 13 depicts the process 
in Singapore.Innovative Procurement Examples
18. For more information, visit Shaping Europe’s Digital Future on the website of the European Commission at https://ec.europa.eu/digital-single-market/en/innovation-procurement.> > >
FIGURE 12 - Singapore Procurement Model
Source: Reproduced from Annex A: Innovation Procurement for Singapore Government, Infocomm Development Authority of Singapore, available 
at https://www.imda.gov.sg/-/media/Imda/Files/Inner/Archive/News-and-Events/News_and_Events_Level2/20120531094015/AnnexA.pdf.Conduced via an open contest for crowd-sourcing 
of ICT innovation proposals
Phase 1
Open tender process where the procurement 
principles of Value-for-Money, Competition 
and Transparency are adhered toA single process for contracting 
from prototyping to Implementation
(where applicable)Phase 2 Typical durantion: 6 monthsStage one:
Issue Request-
for-Proposal 
(RFP) in GeBIZStage two:
Shortlist 
ProposalsStage three:
Evaluate devel -
oped working 
prototypesStage four:
Pilot
TrialsStage five:
Implementation
Supplier A
Supplier B
Supplier C
Supplier DSupplier B
Supplier C
Supplier DSupplier C
Supplier D Supplier D
51
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>The public sector can have a much wider role in governing 
AI outside the government for society at large. Its many fac-
ets include promoter of science, technology, and innovation 
culture to be a source of talent for AI; promoter of research 
in academic institutions; a regulatory body to regulate the AI 
developments in the private sector; and a promoter of AI by 
opening up its administrative and sectoral data to the private 
sector in machine-readable and downloadable formats to pro-
mote innovative use of these data. In this manner, the public 
sector can set the direction for the development of technology 
and set the rules for its application.For example, in the United States, the White House issued 
Executive Order 13859—Maintaining American Leadership 
in Artificial Intelligence—to federal agencies to guide them 
on regulatory and nonregulatory oversight of AI applications 
developed and deployed outside of the federal government. 
The memo encourages the agencies to avoid regulatory and 
nonregulatory actions that needlessly hamper AI growth. It 
also provides guidelines on new regulations to ensure the 
principles of AI, as described in this paper, are adhered to in 
the private sector as well. It calls on agencies to facilitate the 
private sector innovation and growth by giving the public ac-
cess to agency data. This access should be open, public, and 
electronic according to the Public, Electronic, and Necessary 
Government Data Act.Role of the Public Sector in Society
52
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Task teams sometimes support Bank clients in experimentation and proof of concept for AI within the scope of World Bank proj-
ects. These engagements may be development policy operations, investment project financing, or advisory services, and analyt-
ics. The Bank’s New Procurement Framework is flexible enough to allow experimentation with agile approaches, customized to 
the context; see Box 3.AI Operationalization in World Bank Projects
> > >
BOX 3 - Procurement: Important Steps to Consider
A few steps for developing the RFP and designing the procurement process are given below:
• Outline the problem, not the solution specifications. The problem must be agnostic to technology. Special consider -
ations should be given to sources of data and their quality.
• Define the benefits or results, which are of strategic importance and impact.
• Align with existing legal frameworks, public policies, and government strategies. Ethics and associated risks should 
be assessed together with mitigation strategies. Risks should be managed, as it is difficult to eliminate or avoid risks.
• Constitute a working group or multidisciplinary team.
• Establish mechanisms for transparency and accountability of AI systems.
• Ensure knowledge transfer from the AI vendor.
• Ensure value for money and fairness through competition, especially for scaling up AI that will involve large invest-
ments.
• Ensure code ownership. AI vendors could standardize the code, make it agnostic to client context, and resell the 
license, as with any technology, to create win-win. Consider opportunities for open-source code sharing.
Source: WEF (2020).
53
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>5.Ethical Considerations>>>
Managing AI ethics is important and unavoidable for the productive use of AI in either the 
public or private sector. Failure to address ethical considerations, in government and private-
sector AI solutions alike, leads to public mistrust and potential backlash. Most of the discourse 
on AI is dominated by the power of technology to process data faster, learn faster, and propose 
or take actions automatically to increase efficiencies and effectiveness. However, the societal 
implications of wider AI adoption have ethical dimensions that need to be understood and ad-
dressed at the outset. This chapter focuses on those ethical dimensions needing national-level 
policy response, while the technical risks to be mitigated at the implementing agencies level 
were discussed in Chapter 3 on AI risks. 
AI harbors the inherent risks of automating poor decision-making and hiding complex decisions 
behind opaque algorithmic logic. AI can also do harm, for example, through AI-generated dis-
information campaigns on social media. Malicious actors may leverage AI to further strengthen 
their influence over society.
Policy-level concerns on the ethical use of AI are can fall under the following three categories:19 
 
• INEQUALITY. Bias in the use of algorithms, or as a result of a biased data pool may en-
hance negative bias toward vulnerable and weak communities and exacerbate inequalities; 
AI could lead to more demand for higher-skilled labor and exacerbate the returns to educa -
tion which may not be equally accessed in the first place.
• CONTROL. AI could increase the misuse of information, surveillance, and use in defense 
systems.
• CONCENTRATION.  The concentration of power and wealth in a few actors could be aggra -
vated through the net flow of resources into a few firms, and success in achieving singularity 
when machines become equal or better than human general intelligence. 
The detailed discussion on policy level ethical issues is given below. 
19.  See WDR World Bank 2016.
54
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Inequality 
AI may lead to specific job losses in both public and pri-
vate sectors, more likely among lower-skilled workers, 
which has implications for the government to skill-up its 
workforce and to introduce policies that manage this tran-
sition. It is estimated that as much as 30 percent of today’s 
jobs will be replaced by AI and automation by 2030, and up 
to 375 million workers in both the private and public sectors 
worldwide could be affected by emerging technologies (McKin-
sey Global Institute 2017). While the impact in the governments 
of the World Bank’s client countries is likely to take place time, 
it will also take a longer period to prepare the workforce for the 
future. According to one study, 50 percent of the activities peo-
ple do can be automated by adapting currently demonstrated 
technologies. This could have significant implications for the 
use of automation in the public sector. To manage this change, 
a distinction should be made between human-replacing AIs 
and human-assisting AIs. Government policies should promote 
human-assisting AIs, rather than human-replacing ones. To off-
set the effects of AI, unskilled labor should be progressively di-
verted to sectors needing personal attention and care, including 
health, education, and hospitality sectors.20 
The potential threat to low skilled jobs in the private sec-
tor from AI is also a potential issue for the World Bank’s 
client governments whose comparative advantage eco-
nomically stems from a large unskilled and semi-skilled 
labor force. Unlike the innovations of the past, AI solutions 
could be more labor-replacing than human-enhancing. Ger-
man robots have already begun replacing workers of garment 
factories in Bangladesh.21 Chatbots are increasingly taking 
over call center work. It is estimated that 80 percent of cus-
tomer interaction will be managed without human interaction. 
Autonomous vehicles could soon become a reality, with po-
tential erosion of jobs for the taxi, bus, and Uber drivers in all 
countries. On the optimistic side, countries could potentially 
increase productivity in sectors like agriculture, health, educa -
tion, and climate change through human-enhancing use of AI. 
For example, AI can improve diagnosis through image recog-
nition, increase crop yield through monitoring soil and crop 
health using drone-generated data on farming, strengthen the 
fight against fraud and corruption through the reconciliation of 
data from multiple data sources. 
To manage the labor market transition, the policy frame -
work needs to be developed to show how investments in human capital and skills will be deployed and how equal-
ity of access to skills enhancement opportunities will be 
managed. Priority should be given to research, education, 
and skill development programs. Investing in such skills now 
for the future use of AI in the public sector is also important. 
Special emphasis should be given to managing equality of 
access and reaching groups vulnerable to missing these op-
portunities. This could include scholarships, apprenticeships, 
and research funding in computer science, STEM education, 
and AI-related disciplines such as data science for skill devel -
opment. Governments could also create an innovation fund, 
loan programs through state development banks, and income-
contingent student loans. Variations are already used in China 
and Brazil, and examples can be drawn from the experiences 
of Denmark, the European Union, Finland, Germany, Israel, 
and the United States (Mazzucato 2015). Governments could 
also initiate hackathons to promote opportunities for emerg-
ing talent and start-ups, as is being done in many countries, 
including Austria, Estonia, India, Poland, Pakistan, and the 
United States.
Control
One of the potential risks introduced by AI is who has 
control over the information and how it can be manipu-
lated for certain outcomes. Developing policies early on to 
deal with the use of AI to misinform or mislead groups is an im-
portant issue. The use of fake news and targeted but distorted 
newsfeeds can have several consequences leading to polar -
ization of ideas and groups in society and influencing political 
choices. AI-enabled social media bots can analyze millions 
of personality profiles by using cookies to track websites that 
people visit and deliver tailored news, including fake news, 
suitable to the profile. Fake or selected news can be used as 
a tool for manipulating political outcomes and discrediting a 
political opponent. Managing the development of policies and 
legislation to manage what is and is not acceptable, while at 
the same time balancing rights to form an opinion, is a com-
plex endeavor.
Governments should develop or strengthen policies and 
agencies that cover the treatment of online propaganda, 
misinformation, libel, and cybercrimes. Agencies are re-
quired to monitor policy compliance and track, prevent, and 
investigate disinformation to protect its citizens, to enforce 
compliance and sanction lack policy violations. Governments 
20. Stiglitz 2018.
21. Wall Street Journal.
55
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>need to regulate and influence social media Big Tech compa-
nies (such as Facebook, Instagram, and Twitter) to ensure the 
appropriate use of AI tools and to take down content that is 
malicious, hateful, propagandist, and false. 
The government’s use of AI to provide citizens with infor-
mation about access to services also needs to be covered 
by a policy that governs the use and re-use of this infor-
mation. This will mitigate the risk of misuse of this informa-
tion. Handled correctly, AI has enormous potential to ensure 
appropriate targeting of information about, for example how to 
access certain government services, to the groups most likely 
to be beneficiaries of such programs.
AI can also be used as a tool that can be used to track 
and surveil people, something that may be very helpful, 
for example in managing public health outcomes or re-
ducing traffic congestion, but which also has risks of ex-
cessive government surveillance that could infringe on 
human rights. The opacity around governmental use of AI as 
a surveillance tool makes it very difficult to assess the magni -
tude of the problem. According to Feldstein 2019, at least 75 
out of 176 countries were using AI technologies for surveil -
lance. Typical platforms for surveillance include smart cam-
eras under the smart city initiatives, smart police projects, and 
facial recognition systems for contact tracing to quarantine 
COVID-19 carriers. AI can be used to track the movement of 
employees to monitor performance in the public sector (police 
rounds), the private sector (pizza delivery). Therefore, policies 
governing the privacy and rights of employees need to be de-
veloped to avoid misuse of AI. 
Data privacy laws, transparency, and citizen’s voice 
should be strengthened to manage risks that AI used for 
surveillance is in the public interest. Europe has adopted 
the General Data Protection Regulations and many govern -
ments have legislation covering personal rights to privacy, 
personal data protection, and civil liberties but compliance and 
enforcement remain challenges. Promoting full disclosure of 
information being tracked by AI and robots through existing transparency frameworks can be strengthened, and manag -
ing the risks of misuse of such measures will pave the way for 
the productive use of AI in this domain for the public good, for 
example, to trace and identify those at risk from contact with a 
contagious disease. 
Weaponized AI systems have the potential to increase 
the use of autonomous weapons in conflicts, requiring 
a specific policy to address the ethical use of AI in war-
fare. The control and use of autonomous weapons systems 
may in turn destabilize regions and increase potential conflicts 
as human costs may be reduced. Global military spending on 
autonomous weapons systems and AI is projected to reach 
$16 billion and $18 billion respectively by 2025.22 The cost of 
drones that can be advanced enough to defeat a U.S Air Force 
fighter pilot in combat simulations is as little as $35.23 AI prin-
ciples of adoption emphasize human control and AI use for 
human benefit. The application of these principles to the use 
of autonomous weapons is an issue of global importance and 
coordination. Global governance through multilateral forums 
and international cooperation is needed to address these is-
sues. The role of civil society to influence the debate is also 
important.
Concentration
AI can also lead to increased concentration of wealth in the 
hands of a few individuals controlling the big firms. These big 
firms can finance expensive research and attract top talent 
through better financial incentives. These big firms not only 
control the AI research and talent but also the associated data 
center infrastructure through cloud computing. This concen -
tration would provide even more resources at the disposal of 
these individuals to influence public policy through campaign 
financing, lobbying, corruption, and influence peddling. This 
will also lead to a net outflow of resources from the develop -
ing to the developed countries, as most of these big firms are 
based in the high-income countries. 
22. Sander and Meldon, 2014; Research and Markets, 2018.
23. Cuthbertson, 2016.
56
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>6.Government’s AI Building Blocks>>>
A whole-of-government, data fabric AI architecture is central to the technology vision 
of the government and forms the building blocks for the use of AI. The government’s ap-
proach needs to encompass interoperability and security and the importance of continuity of ar-
chitecture among AI systems designed for use in a whole-of-government architecture. By under -
standing the components and building blocks of AI systems at a high level, common knowledge 
becomes a tool for exploring relevant entry points with technologists to guide the broad direction 
of possible solutions. Three key concepts that constitute the building blocks are (a) a whole-of-
government architecture; (b) interoperability; and (c) data standardization. 
Whole-of-Government Architecture
Most World Bank client countries are managing stand-alone legacy systems, often re-
ferred to as “silos.”  These systems are not interoperable or have problems with interoperabil -
ity. Since AI models need large amounts of data to work well, the “ideal” architecture needs the 
silo systems to feed data into a large distributed data storage repository—often referred to as a 
data lake. The data lake is then made accessible to various AI applications. A government aspir-
ing to greater digital transformation should adopt a whole-of-government architecture, which is 
the de-facto industry standard.
Siloed systems can be “stitched together” through a common data platform. A govern -
ment has many ministries, departments, and divisions. Each one typically operates autono-
mously, but often reports to a central government agency. A data fabric is a similar concept (see 
Box 4 below). It has several data centers with many departmental computing resources. Each 
one operates autonomously, but each one reports to a central computing administration system 
using a standard set of rules or protocols for data storage, security, and processing. They are all 
“stitched together” using a common software platform that spans the whole-of-government. The 
data though remain separate and independent. This is a simple description of how the kind of 
system that can lead to incredibly powerful capabilities in AI and data processing. 
57
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>> > >
BOX 4 - Data Fabric in Brief
The term “data fabric” refers to the very large-scale continuous Big Data architectures used by some of the largest or-
ganizations in the world. A data fabric provides storage, computation, and security for organizations with exceptionally 
large data pools, such as governments and multinational corporations. A data fabric also supports distributed computing 
between multiple data centers spanning entire countries. In 2019, Gartner identified the data fabric among the top 10 
trends in data and analytics technology (Gartner 2019).
The difference between Big Data and data fabric. Big Data systems are more uniform and monolithic while data fabrics 
offer a common computing layer across a variety of systems that include these characteristics:
• One or more databases containing data from various sources (Big Data). The database and file system layers com-
prise the data lake as explained later
• Application Program Interfaces (APIs) to connect with external government systems such as financial management 
information systems, payroll, integrated tax administration systems, and e-procurement.
• Data and cluster management tools, including:
 »Storage APIs for real-time (or batch) data ingestion, updates, creation, and deletion.
 »Data tools such as streaming, machine learning, and preprocessing systems.
 »Administrative tools for data access control, monitoring, and provisioning. 
The general purpose of data fabric architecture is to unify data storage and AI computation across many independent gov-
ernment departments while keeping data safe from loss and protected from unauthorized access. A data fabric does not replace an 
existing architecture in one iteration. A government can roll out a data fabric over time and incorporate all the existing data systems 
into the fabric architecture, slowly replacing “old” walled-off legacy systems with “new” interoperable systems at their discretion. 
Figure 14 presents architecture built atop a data fabric.
58
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Streams
LayerTransaction
Event T opicCleared
Event T opicBlockchain
Event T opickafka> > >
FIGURE 13 - General Data Fabric Architecture for Whole-of-Government Use
Source: The World Bank.
Firewall[... ]FirewallFirewallTAX
Audit
APPX-Action
APPBlockchain
APPBudget
APPGovernment
Systems
Application
LayerData Fabric 
ClusterProcurement eHealth
Event
APIEvent
APIEvent
APIDatabase
LayerRaw
X-actionsCleared
X-actionsEphemeral
ReportsGeneral
DatabaseNoSQLFilesystem
LayerML
ModelsCSV
ReportsTransaction
BlockchainSystem
LogsFilesystemFile
APIDB
APIData DataPAYROLL FMIS
Data Data Data Data Data
A data fabric architecture has two high-level layers: gov-
ernment systems and the data fabric cluster. Government 
systems are shown in the gray box at the top of Figure 14. 
Inside this layer are all the government’s applications, which 
belong to the various departments. Each white box represents 
departments or divisions. The two applications on the right, 
Procurement and eHealth, represent commercial-off-the-shelf 
(COTS) solutions. All applications send and some receive 
data from the data fabric cluster. Existing applications, such 
as tax, FMIS, and payroll, share data with an AI application 
layer inside the data fabric cluster, which is at the top of the 
data fabric cluster portion of Figure 14.
The data fabric cluster layer is subdivided into four lay-
ers: application, streams, database (DB), and filesystem. 
The first layer, the AI application layer, holds all the custom AI 
applications inside the data fabric cluster. Underneath that, a 
stream layer ensures that data flows from one place to anoth-er inside the cluster in real-time. Beneath the streams layer, 
a database layer gives departmental AI applications a place 
to store their rapid-access data. Lastly, beneath the database 
layer, the filesystem layer stores archival data and even larger 
data structures for long-term storage in blockchains and flat 
file systems similar to the hard drive on a personal computer, 
but scaled to handle the data needs of an entire country and 
all its citizens. Appendix A provides a discussion of the poten-
tial role of blockchain technology for government systems.
The Standardized Application Programming Interfaces 
(APIs) are the threads that stitch the fabric together. Fig-
ure 14 represents these connections with bold arrows. They 
are labeled DB API, File API, and Event API. They are the core 
of interoperability for this architecture. The most successful 
large-scale operations, including India’s system for issuing a 
unique digital ID (Aadhaar), use this design.
59
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Streams
Layerkafka> > >
FIGURE 14 - High-Level Data Fabric Architecture
Source: The World Bank.High Level Data Fabric Architecture 
Government
Systems
Application
LayerData Fabric 
ClusterProcurement Policy eHealth IFMIS
Data
Flow
Event
APIDatabase
LayerNoSQLFilesystem
LayerFilesystemFile
APIDB
APILow Storage
High Storage
Mid Storage
Low Storage
High Compute
Mid Memory
High Storage
Mid Compute
Hogh Memory
High Storage
Low Compute
Low Memory
Figure 15 illustrates the resource utilization and performance 
requirements in a mature whole-of-government architecture. 
Resource requirements and utilization are major factors in 
determining the total costs of ownership (TCO). Here, the AI 
application layer and the streams layer comprise the whole of 
the primary AI layer. The database and file system layers com-
prise the data lake. The general takeaway from Figure 15 is 
the distinction between the AI layer and the data lake within a 
data fabric cluster. Any external AI solutions can leverage data 
APIs to access data within the cluster from anywhere within 
the dominion of the whole-of-government. Also, the relation -
ship between resource consumption and broader layers of 
architecture is not uniform, which allows for lower TCO. More-
over, the data fabric cluster remains independent of top-level 
government systems. Each department may have its services 
built into silos. All the computers within the data fabric cluster 
can have different capabilities and distributed locations.Sometimes referred to as a data lake, a data fabric – though 
it is more than a data lake as mentioned in Box 4 – is made 
up of commodity hardware systems at up to exabyte scale 
(1018 bytes) throughout a wide variety of architectural pat-
terns; some cloud-based, some on-premise, and some in a 
hybrid configuration of both.
Data storage for AI is broken into three tiers: ephemeral, 
persistent, and archival. Each tier favors a particular subset 
of structured data, accessible through a standardized inter-
face and abstracted into an accessible format through pro-
grammatic and algorithmic convention, which may be open 
source or proprietary. Storage in 2020 can be localized to one 
machine, one drive, or spread across geographies in sophisti -
cated and redundant data topologies that distribute exabytes 
across global geographies while offering access-control lay-
ers (ACLs) for strict management. The storage tiers are con-
sidered part of the storage layer in the AI technology “stack.”AI Layer Data Lake
60
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>> > >
BOX 5 - Blockchain: Distributed Ledger T echnology
> > >
BOX 6 - Actionable Insight: Data Fabrics Can Overcome SilosThe data in the persistent file system tier may be grouped into blocks of information and hashed with an identifier linked 
to a previous block: a blockchain. This blockchain formulates an archival decentralized ledger. To further the utility of 
blockchain technology, also known as distributed ledger technology (DLT), applications relying on the use of a distributed 
ledger may prevent the completion of a transaction or asset transfer until enough computing nodes in the infrastructure 
reach consensus through Application Program Interfaces in real-time.
By distributing and requiring consensus among participating compute nodes, DLT essentially offers an immutable solution 
to asset tracking and transactional audit. Modern DLT solutions even offer safeguards against Byzantine attacks in which 
malicious agents attempt to gain majority influence within a network of computing nodes that are running DLT applica -
tions. Appendix A has more detailed information about how the blockchain plays an integral role in the long-term use of 
whole-of-government data architecture.
A data fabric architecture prevents and solves problems arising from data silos. A central agency, responsible for govern-
ment-wide digitalization, could deploy data fabric architecture to overcome silos. It deserves much-needed consideration 
for governments wanting to harness the power of data by streamlining operations with a large-scale AI-ready infrastructure.Besides the core AI architecture layers, a few other consid -
erations play an important part in the technical design of the 
data fabric AI stack.
Interoperability Patterns
Data silos are the opposite of scalable, interchangeable, and 
interconnected computing systems. They are rigid, limited, 
and isolated from other systems. Imagine a government in 
which various departments, ministries, or divisions did not 
speak a native or common language and could not commu-
nicate. These are silos. Successful large-scale deployments 
rely on the following patterns to compensate for a lack of in-
teroperability between silos:
• Data exchange standards and schemas.
• Secure APIs.
• Cohesively interconnected layers of services using IPC 
best practices.
• Geographically distributed data centers within the data 
dominion.
• Architectural redundancy and replication.
• ACLs.A data silo is an architecture that is isolated due to the ab-
sence of a common API for IPC. Data silos can emerge from 
vendor lock-in, proprietary systems design, or poor planning. 
A system of silos lacks a common denominator to effectively 
allow for interoperability. Data are trapped in the silo. Over 
time, the silo will bloat and stagnate with information that could 
otherwise be utilized by AI systems.
Various agencies and departments tend to pursue entirely in-
dependent solutions to solve narrow problem statements spe-
cific to their short-term needs. This common practice creates 
complex pervasive fragmentation. As a result, interdependent 
organizational units end up with entirely independent systems 
that are isolated from one another in the long run.
Siloed systems can potentially become bottlenecks for 
data sharing that prevent useful implementations of AI. As 
a result, to discover trends and patterns with AI, departments 
must export enormous volumes of data to a centralized stor-
age location, which is extremely time-consuming and costly.
Data silos stifle whole-of-government AI development, al-
though they are preventable. This pattern is consequential 
to siloed systems, reflective of turf sensitivities, and lack of 
interagency coordination mechanisms. Luckily, there are solu-
tions to address the issue.
61
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>> > >
BOX 7 - Actionable Insight: Governments Should Standardize Data
The central agency may develop standards for data formats and interoperability through engagement with line ministries. 
The creation of data governance councils and nomination of ministry- and agency-wise data stewards help ensure standard 
quality in data sharing. Also, engagement with stakeholders to develop consensus using Requests for Comments is suc-
cessful among international standards organizations.Data are the lifeblood of any AI architecture—it is the 
“gold.” As a form of untold wealth, data are worth sharing 
among stakeholders within an organization. To overcome en-
trapment in silos, interoperability plays a crucial role in suc-
cessful AI systems development. Although enterprise comput-
ing solutions—such as enterprise resource planning (ERP), 
data lakes, and databases—often use compressed binary 
streams internally, there is a high probability their data storage 
systems open a pathway to external applications through an 
API. Standards make this possible.
Instituting data governance arrangements promote stan-
dardization of data necessary for interoperability. Modern 
governments, like Estonia, create data governance councils 
and appoint data stewards in each agency to coordinate data 
standardization and interoperability. These arrangements are 
part of the data governance strategy that defines the authority 
and control over the data assets and includes policies, process-
es, standards, definitions, and data exchange arrangements.
Data standardization across agencies could also follow 
good practices of standardization internationally. These 
are more common in the private sector, though some models 
also exist in the public sector. In the private sector, several 
standards evolved using these practices. Programming, inter-net, and network protocols rely intensively on standards es-
tablished by the Internet Engineering Task Force, International 
Standards Organization (ISO), and the Institute of Electrical 
and Electronics Engineers (IEEE). The processes and sub-
structures within these organizations are oriented toward de-
veloping a very uniform agreement between ground-level engi -
neers responsible for implementing the technologies that drive 
the internet’s evolution. When a fundamental technological 
agreement must be reached, a consensus is reached through 
a Request for Comment (RFC), which contains guidelines for 
the implementation of and use of the technology needing stan-
dardization through peer review. A complete RFC must con-
tain core tenets explaining and enumerating every behavior 
and function in technical detail and depth. RFC practice was 
used in several global standards: World Wide Web, JavaScript 
Object Notation (JSON),24 and the Portable Operating System 
Interface, a family of standards developed by IEEE that pro-
vides a standardized protocol for communications within and 
between computing file system layers worldwide.25 
Good models also exist in the public sector at the inter-
national level. The Open Contracting Data Standard targets 
contracts in general and enables disclosure of data and docu-
ments at all stages of the contracting process by defining a 
common data model.
24. See Request for Comments (8259), “The JavaScript Object Notation (JSON) Data Interchange Format,” Internet Engineering Task Force, at https://tools.ietf.org/html/rfc8259.
25. The Open Group 2018.Data Standards
62
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Open, consistent standards and methodologies are the 
ground-level blueprints for a successful whole-of-government 
implementation of AI technologies. Prospects are that a global 
governance standard for data will likely emerge over time. 
This notion of a data standard can extend further to include 
suggested best practices for developing interoperable data 
fabrics within and across governments. Their practices may 
differ substantially from one another, though the technical pro-
cesses for accountability and integrity share common stan-
dardized infrastructural patterns.
By enforcing standards, the international community of policy -
makers can achieve an intergovernmental vision of AI interop -
erability. By leveraging standards for document data storage, 
and APIs common to the databases supporting various ex-
isting silos, governments can deploy system integrations that 
evolve continuously with the trends and advancements in AI at 
a national and international scale.
Access to data is the key to managing governments at all 
levels of AI deployment. The software platforms and solu-
tions that do the actual computation often provide APIs that 
access standardized databases. Developers and data scien-
tists alike may be able to access a data fabric over network 
interfaces and conduct experimental research that helps de-
termine the proper course in developing permanent AI solu-
tions to common problems in government. This will also pro-
vide avenues for more effective data collection, aggregation, 
experimentation, policy management, and access control.
Data are more accessible than governments may realize. 
Leveraging data stored in existing silos should be the essen-
tial tenet of any digital transformation strategy. The majority 
of ERP, custom-developed, or open-source solutions these 
days provide some type of data access control through direct 
communication with the database layers that these systems 
utilize. Therefore, siloed solutions do not require forced ob-
solescence either. Governments may continue to utilize them 
while they transition to newer data fabric oriented architec-
tures. Limitations do exist among mainframe systems devel -
oped before the turn of the millennium, which require custom 
programming to extract data from COBOL (or common busi-
ness-oriented language) and other flat file systems.
An application within a data fabric can query existing data-
bases for new records and feed the data to an ingestion layer, 
which routes information to the appropriate hardware resource controllers within the data fabric. While it is true that granular 
data access can lead to a “spaghetti” dependency structure, 
entirely independent distributed services can be tuned to exe-
cute any arbitrary set of applications, especially whole-of-gov -
ernment AI models over the long run. More granular informa-
tion about advanced connectivity is available in Appendix A.
In conclusion, a data fabric offers an intrinsically resil-
ient, adaptive, and decentralized architecture that has no 
single point of failure. Trends are moving toward AI as an 
operating system among developed governments. The intro-
duction of the FedRAMP marketplace established by the Unit-
ed States, which provides vendors with stipulated standards, 
requirements, and guidelines for being authorized to provide 
cloud services to federal agencies, provides an indicative di-
rection of the emerging trends. 
Within whole-of-government systems, standardized access to 
data enables many types of practitioners to experiment and 
design all kinds of use cases for AI. In reality, the AI application 
layer can contain tens of thousands of AI models for all types 
of purposes. Each application can easily leverage all types 
of information simultaneously stored within the architecture—
data such as text, audio, video, and biometrics. This allows 
for better solutions over the long run by enabling a fail-fast 
approach using data access as a baseline. Ultimately, govern -
ments can develop long-term strategies in AI innovation that 
count on standards. There is little doubt that a tidal shift is un-
folding for governments that are serious about improving their 
long-term strategic advantages in AI.
To proceed and formulate a more in-depth view of AI systems, 
see Appendix A. It dives into the core concepts of AI in practi-
cal applications. The concepts are meant to inform the reader 
of the basic, advanced, and real-world AI applications. Again, 
understanding these foundational concepts demystifies much 
of the jargon and hype orbiting the topic of AI. The key topics 
that Appendix A depicts in greater detail include:
• Project development patterns.
• Cloud, hybrid, and on-premise architectures.
• AI connectivity.
• Microservices.
• AI models.
• AI workflows.
• Distributed ledger technology (DLT) in AI architectures.
63
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>7.Conclusions>>>
AI is still a new area even for many of the advanced digital economies, but its rapid diffusion in 
every facet of private and public life is increasingly more visible. The enormity of development 
challenges requires exploring modern approaches, tools, and techniques. AI offers immense 
opportunities to address some of these challenges. However, it has inherent risks that can have 
profound consequences for society. Governments have to lead the efforts to manage these risks 
while promoting the use of AI in the private and the public sectors. This paper distills existing 
knowledge on these aspects for client governments. Conclusions as well as priorities going for-
ward are highlighted here. 
Human-centric AI design is a key principle to guide the development and deployment of 
AI. AI will not eliminate human oversight in decision-making. Also, entirely externalizing deci-
sion-making using AI is unrealistic due to bias, which is impossible to eliminate but reasonably 
controllable. Public sector AI technology must remain under the guidance of humans because 
it has the potential to affect trust, human health, safety, and overall well-being. Fortunately, the 
state of the art in AI demands it and all mission-critical AI deployments keep humans “in the loop” 
to varying degrees.
Governance and government practices benefit from transparency and evidence-based 
decision-making.  AI systems must operate with transparency, human oversight, and neutrality 
while attempting to manage and disclose bias, which humans will never fully eradicate from AI 
solutions. However, well-managed AI solutions yield a repeatable model that may provide fun-
damental services through an open-source consortium of international collaborators. Therefore, 
while this paper encourages collaboration, any general government AI solution must take secu-
rity, privacy, and data protection into full account to protect the sanctity and privacy of people 
and their governments. Currently, close to 135 governments are implementing privacy and data 
protection in their legislation, which applies to AI for the benefit of stakeholders.
64
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>The process of AI implementation is a journey. It starts with 
the most critical basic foundation: the acquisition, aggregation, 
management, and storage of reliable data. With quality data in 
hand, policymakers, data scientists, and AI engineers can per-
form introspective and comprehensive iterative deployments 
to expose the possibilities for full-scale AI systems. The jour-
ney requires coordination and collaboration between teams 
of stakeholders at all levels of government. It also demands 
that the outcomes earn the citizens’ trust through disclosure, 
explainability, and transparency wherever bias is a concern. 
Where necessary, administrators may provision AI algorithm 
audits, especially in cases requiring forensic investigations.
Governments need to adopt a large-scale data fabric ar-
chitecture to serve as the common denominator for stan-
dardized data interchange among a fully digital whole-
government infrastructure. This approach enables robust AI 
solutions to grow and evolve with changing needs. The funda-
mental shift in the mindset of developing countries involves an 
emphasis on interoperability and IPC through standardization 
and API enablement.
The promise of AI is riddled with commercial marketing 
hype, but the fundamental value of the introspection can-
not be overstated. AI systems offer a mechanism for quali -
tative predictions using quantitative measures of information. 
The various patterns of AI analysis provide tools for attack-
ing a multitude of problems that are emerging in the face of 
increasingly intricate governance systems. Regardless of the 
flavor of governance employed, one thing remains clear: AI 
has the potential to revolutionize human intelligence in un-
precedented ways. Despite the hype associated with being at 
the forefront of innovation by being the first to deploy one or 
more cleverly marketed solutions, the real focus should be on 
solving problems for internal governance and citizens. Also, 
government agencies must be willing to adopt standards and 
practices that enable fast and agile delivery, with an accept-
able degree of failure risk.
A myopic view of AI is counterproductive. Immediate prob-
lems are like individual fires in a forest ablaze. Governments 
must avoid this tendency and commit to building a whole-of-
government infrastructure that allows line agencies to operate 
interdependently. Systems at this scale require the collective 
efforts of nearly everyone in the scope of government influ-
ence to learn, trust, and invest. By creating fabrics of informa-
tion, governments can promote their missions of better gover-
nance, transparency, accountability, and efficiency.Priorities Going Forward
Based on the issues highlighted in the paper, several priorities 
could be considered by policymakers.
• Governments must adopt policies and governance 
frameworks that promote human-centric AI while 
maximizing opportunities. A few aspects of the policy 
framework are mentioned below:
 »Ethical AI requires the adoption of an AI policy 
and strategy. It could be tailored to specific settings 
but should be approved at the policy level to provide 
the authorizing environment. Governments in many 
settings have issued AI strategies approved by the 
parliament, president, prime minister, or cabinet. 
These policies should be based on ethical principles. 
Governance and operational framework are essential 
to specify broad guidelines and institutional arrange -
ments. An innovation hub could be established to pool 
talent, establish partnerships with academia and the 
private sector, promote research, and facilitate ex-
perimentation by line ministries. The innovation hub 
should source the best talent through adequate in-
centives. Innovative procurement approaches should 
be adopted to leverage private sector skills with agili -
ty to allow iterative, problem-driven approaches to the 
RFP. The implementation teams should also manage 
the risks associated with AI, including bias, security, 
and unintended consequences, among others.
 »Promote transparency and accountability through 
inclusion and multi-stakeholder engagement at 
every step of the AI policy design and implemen-
tation. Affected communities and populations should 
be informed and provided with avenues for contesting 
AI logic without delays and hurdles.
 »Adverse ethical implications of AI could be man-
aged through broader economic policies. These 
could include industrial policy, tax policy, competition 
policy, human capital policy, among others. 
 »These policies should also promote digital skills, 
education, and redeployment efforts to support 
people as they adjust to the shifting nature of work 
in the coming decades. Unskilled people and disad-
vantaged groups should be given special attention.
65
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>> »A policy framework to fight online propaganda, 
misinformation, libel, and cybercrimes should be 
given priority. Also, governments could establish 
agency mandates to monitor policy compliance and 
track, prevent, and investigate disinformation to pro-
tect their citizens. Engagement with social media Big 
Tech—Facebook, Instagram, and Twitter—should aim 
at encouraging the deployment of AI tools and profes-
sional fact-check partnerships to take down content 
that is malicious, hateful, propagandist, and false.
 »Strengthen privacy, data protection, and civil 
liberties and monitor compliance, which is typical-
ly weak in most settings. Promoting full disclosure  
of information being tracked by AI and robots  
through transparency frameworks should also 
be strengthened.
• Investments should be made in human capital and 
digital infrastructure. AI research, digital skills, AI entre-
preneurship, and foundational digital technologies could 
be prioritized.
 »Investments should be directed to fund research, 
education, and digital skills development pro-
grams in general and in AI in particular. They could 
include scholarships, apprenticeships, and research 
funding in AI, computer science, STEM education, 
and AI-related disciplines such as data science. 
Special emphasis could be given to disadvantaged 
groups such as women, minorities, and those at risk 
of being left behind.
 »Innovative entrepreneurship could be promoted. 
This could be done through an innovation fund, loan programs through state development banks, income-
contingent loans for students or others, and small 
business loan programs. Variations of these funding 
modalities are already used in China, Brazil, Denmark, 
the European Union, Finland, Germany, Israel, and the 
United States (Mazzucato 2015). AI could be one of 
the areas to be incentivized through these programs.
 »The innovation hub should be staffed with the  
appropriate talent on market-based salaries. 
These skills are in high demand and could easily 
drain overseas. 
 »Data fabric architecture, including interoperabil-
ity, should be considered for investments. This 
will overcome silos, and leverage data assets for de-
cision-making, compliance monitoring, and analytics. 
The initial focus should be on interoperability, open 
data, and data standardization. A hybrid cloud option 
should be explored to leverage the computing power 
at much lesser costs to pilot AI solutions.
 »Proof-of-concept and pilot AI projects could be 
the starting point for exploring opportunities. 
Many governments have deployed AI to solve spe-
cific problems. Key use cases include citizen engage -
ment, service delivery, regulatory compliance, deci-
sion analytics, fraud, and anti-corruption. Hackathons 
promote emerging talents and start-ups as seen in 
Austria, Estonia, India, Pakistan, Poland, and the 
United States.
• Risks should be identified and managed, rather than 
avoided. They could be mitigated through self-assess-
ments, peer reviews, and inclusion.
66
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Appendix A. AI T echnical Primer>>>
Appendix A explains a large subset of technical and operational details about AI project manage -
ment, architecture, types, methods, and models, as a self-contained primer. This is based on the 
best industry advice from practitioners. The consolidated information herein intends to benefit 
the reader. For even more information, practical guidance is available in many books, blogs, 
articles, and other technical resources.
Project Development Patterns
Agile Development
Iterative, agile development is the key to the steady adoption of any technology. The ag-
ile methodology offers an adaptive model for destructuring complex projects into manageable 
stages with discreet goals, short term development intervals, and continuous delivery. Agile 
teams convene regularly, often daily, in scrum meetings to disclose incremental progress and 
dependencies. Agile methodology is a longstanding backbone among organizations of all sizes.
Agile offers a method for execution that complements a goal-setting methodology consisting of 
objectives and key results (OKRs). They keep all levels of organization, especially individuals, 
holistically accountable to the project. Because OKRs are typically disclosed publicly to all stake-
holders, the whole organization may audit the development process for measurable progress. 
Of course, process management is not a panacea for projects attempting to reinvent the wheel, 
or parts therein. To that end, there are ample turn-key solutions that ship with a unique set of 
caveats to consider.
Iteration times have been steadily decreasing in the decades since the 1980s. Early wa-
terfall-based methodologies “iterated” through projects over months up to a year. The 1990s 
brought the adoption of the Rational Unified Process, an early precursor to Agile Development 
and eXtreme Programming (XP). These advances in management timelines reduced develop -
ment iterations down to two to three weeks. The unit of code development has also decreased 
significantly since the 1980s with the advent of Service-oriented architectures and microservices. 
Today, Continuous Deployment techniques allow high performance organizations to release mi-
croservice applications to production several times a day. Figure A.1 illustrates the changes in 
iteration time and code volumes. Figure A.2 illustrates the change in unit of code over the previ-
ous 15 years prior to 2020.
67
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>> > >
FIGURE A.1. - Iteration Times and Code Volumes versus Time
> > >
FIGURE A.2. - Unit of Code Scale ChangeSource: Reproduced with permission from ©Paul Clarke; further permission required for reuse.
Note: Paul Clark, “Computer Science Lecture Notes,” Dublin City University and Lero, the Irish Software Research Center.
Source: Reproduced with permission from ©Paul Clarke; further permission required for reuse.1970
2005 2010 2015 20201980 1990 2000
Scrum RIPP
(Dupont)Monolithic 
architectures 
are not 
well suited 
to rapid 
deployment
WahterfallMany Months
#T ools
#Models / MethodsMultiple YearsVery Large Systems
Microservices
<1 day
ContinuousDeployable Increment Size
Iteration Duration
Integration Duration
V-ModelSpiral
RAD
RUP XP Lean
[---- Other Agile Methods---->DevOps CSE2010
Client
Application Server
OSApplication
Runtime
HardwareClient
Application Server
OSApplication
Runtime
Virtual ServersClient
Runtime
Cloud ManagedMicroservices
Containers
Cloud ManagedClient
Cloud Managed
Cloud ManagedCode (Function)
Cloud Managed
Cloud Managed
68
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Wherever possible, problems will require a consistent process 
for reducing complexity and establishing a manageable scope 
of execution. Project management plays a critical role in estab-
lishing a track record of success for software developments at 
the national level. The temptation to boil the ocean and attach 
a blanketed solution to a broad scope of operating require -
ments is often irresistible within novel undertakings. Project 
managers help mitigate risk and counteract scope creep by 
coordinating and elucidating the requirements and steps nec-
essary for projects during the planning phase. This practice 
does not necessarily assume end-to-end project development 
of every possible use of a fundamentally valuable data infra-
structure. Rather, project managers (PMs) solidify first-level 
operational requirements for a data infrastructure, the applica -
tion layer, and critical external dependencies. These require -
ments capture the general requirements necessary for the 
future development of siloed executable applications, each 
assigned a dedicated PM that coordinates with the central 
development team to ensure consistent standards capable 
of supporting the various permutations of core and second -
ary systems. With proper coordination, standardization, and 
management among government stakeholders, government 
efforts will secure a development process that ensures that 
projects reach completion for timelines, which span changes 
in elected officials, and survive the varied political landscape.
Prior stepping into the domain of applied solutions in other 
sections, this appendix to the paper will step through some 
processes for reducing and scoping problems into solutions. 
These are by no means a comprehensive list of project man-
agement best practices, but they facilitate understanding of 
the intricacies of software development planning and execu-
tion. These processes are also not intended to replace the 
acumen of an experienced project management professional. 
Each project has a specific set of requirements that accom-
pany project-specific nuances. Furthermore, the number of 
variables involved during execution necessitate careful co-
ordination, investigation, and execution by professional plan -
ners and managers. There is no one-size-fits-all solution in 
AI project management, just processes based on the type, 
scope, and timeline required for execution.
Project Management
Avoid solutions looking for problems. All too often, tech-
nologists invent a groundbreaking solution in a theoretical 
environment and apply the solution to problems that simply 
do not exist. As unimaginable as this may seem, the promise 
of technology may outweigh the actual benefit when practical 
solutions fail to emerge from concrete and battle-tested best 
practices, even if those practices are manual or fragmented in nature. Governments tend to silo operations within the scope 
of an agency or department due to budgetary firewalls. Solu-
tions developed by localized operations successfully focus on 
the problem at hand, incorporating turnkey solutions and con-
sulting opportunities for highly specialized services deemed 
vital to deliverables needed within a budgetary window of op-
portunity. This practice leads to fragmentation and a lack of 
interoperability, often solving problems many steps ahead of 
the current set of requirements.
Conversely, as experimental technologies emerge from aca-
demic organizations and professional firms alike, their applied 
implementations may be directed toward advanced problems 
that ignore the scope of current operation. This leads to the 
presentation of a technology that is inconsistent with the im-
mediate requirements of any organization, much less those of 
the entire group of organizations comprising government. In 
brief, no one has engineered anything resembling a “govern -
ment-in-a-box,” but many consultancies come close to sell-
ing solutions as a panacea for the most mission-critical prob-
lems. They do so in a manner that opens a dialogue that often 
demands full-scale adoption of a technological product that 
requires immense customization or otherwise a total replace -
ment of the existing infrastructure that is incompatible with the 
fragmented, siloed solutions described previously.
AI technologies and automations offer myriad possibilities for 
enhancing the decision-making process used within manual 
systems. Replacing a manual system from day zero is not 
necessarily the best solution because of the lack of introspec -
tion. By establishing proper ground-level data infrastructure, 
solution architects can coordinate with data scientists to 
study the quality of information produced by the government 
and carefully scrutinize prospective product solutions with a 
knowledge of the internal workings of data. Thus, teams can 
subsequently derive quick wins before the need for advanced 
analytics or sophisticated 
software emerges in ac-
tuality. Therefore, it is a 
wise strategy to organize 
data early on through the 
implementation of poli -
cies that standardize data 
within a very large distrib-
uted data fabric that sup-
ports further development 
and the integration of pro-
prietary software tools by 
providing APIs for filesys-
tem data access.Entities 
should not 
be multiplied 
without 
necessity.
William of Ockham
69
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Resource Management
Among human resources, create a stakeholder hierarchy that 
is largely decentralized across areas impacted by the infra-
structure. This means that organizational decision-makers 
need representation from inception. Eligible representatives 
include IT directors, executives, and project leaders: people 
who are central to planning, policy-making, standards, and 
execution. Choose only the key members required to com-
municate and address project requirements. These members 
will take responsibility for communication with supporting 
members of their respective teams. Only including the nec-
essary individuals occupying central roles prevents paralysis 
by analysis. Supporting team members will have the power 
to comment on policies and standards that emerge by issu-
ing documents in RFC format. Teams review comments as 
they funnel into the project and conduct discourse to evaluate 
and settle on a final specification for project requirements. Ul-
timately the top-level representatives ensure that the needs of 
their organizations are met. Standards may evolve over time 
to reflect changing architectural requirements.
Concerning AI systems infrastructure, prepare to manage tens, 
hundreds, even thousands of experimental projects of varying 
scale. There is no one-size-fits-all infrastructure that picks all 
the locks and opens all the doors. There is no panacea, no 
completely turn-key solution. AI requires work in layers of ap-
plication infrastructure built on large volumes of data. There 
are no fewer than hundreds of tools available for AI engineer -ing across a variety of free and open source and paid licensed 
solutions from private software firms. Whatever the path taken 
to develop a formal production model, supporting members 
of the AI engineering team will analyze data many ways dur-
ing the process. Once in production, the compounding effect 
of various deep learning applications will continue to extend 
the infrastructure. By using commodity hardware systems 
on-premises and considering a hybrid cloud infrastructure for 
experimentation, administrators can attain considerable flex-
ibility and adapt to the changing demands of uncertain futures. 
Capacity with headroom will ensure that new models and new 
data are able to proliferate. A safe general guideline is to main-
tain a minimum 20 percent of additional capacity, whether on-
premise or in-cloud, in order to have burst capabilities for new 
initiatives in machine learning and artificial intelligence.
Continuous Deployment and Automation
The final key concept in understanding the rapid develop -
ment of any technology, especially those in the space of AI 
microservices development within a data fabric is continuous 
development and service deployment automation. The actual 
programming portion of large-scale systems development and 
deployment is a fraction of the entire delivery pipeline. This 
is important to note in light of the possible solutions that ex-
ist. Even COTS systems require continual development and 
releases to address bug fixes and the deployment of new fea-
tures. It is therefore quite useful to understand the continuous 
deployment pipeline.
> > >
FIGURE A.3. - Unit of Code Scale Change
Source: The World Bank
70
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Essentially, any application within a larger architecture 
contains source code. Engineers specify criteria for the prop-
er function of that source code in the form of various tests. Engi-
neers store the code in source control systems. They then edit 
the code and commit changes to the central repository for their 
preferred source control system. The tests must run to validate 
the functionality of the application before new changes propa-
gate to production. Every change requires that all tests pass in 
order for the change to be validated for release into the wild. 
Breaking changes prevent release. Passing changes merge 
into the master branch of the source code tree, and the product 
evolves. This is the continuous development pipeline.
Continuous integration (CI) systems automate this process so 
that engineers can work on the core of the product and con-
tinuously deploy code to production. The early days of manual 
testing are long gone for all modern software enterprises. 
Thus, the expectation today is that developers make new fea-
tures and fixes in real time and deploy these to production 
without hesitation, sometimes several times a day among vari-
ous teams managing various projects tied to the many appli -
cations supporting a microservices architecture.To further the example, application containers offer en-
gineers dependency management at previously unprec-
edented scale. Applications with dependencies for particular 
software modules with specific versions get packaged into 
small images called containers that are encapsulated to run in 
an isolated instance alongside many other application contain -
ers without overlapping dependencies. Thus, a small number 
of computing nodes can run a large number of application in-
stances independently. This further reduces the complexity of 
dependency management among commodity infrastructures 
and lowers TCO over time. The initial investment of setting 
up this application deployment environment pays handsome -
ly to teams with limited resources such as those of govern -
ment agencies, their respective contracting consultants, and 
in-house technical management teams. There is little to deny 
the virtue of pursuing this course for any government wishing 
to develop a long-term plan for successful AI infrastructure. 
The following diagram illustrates the continuous integration 
and continuous deployment of an application using containers 
within Kubernetes, which is the gold standard in application 
container management among world-class software engi -
neers and architects.
> > >
FIGURE A.4. - Continuous Integration and Continuous Deployment Pipeline Workflow with Kubernetes
Source: ReactiveOpsCommit code, 
push to gitBuild new
Docker imageCreate
new podRestart
new pod
Push new 
Docker imageDOCKER
REPOSITORYNew pod is
healthyNew pod is
not healthy
Update 
Kubernetes 
deploymentDelete
old podRun testsCheck pod
healthLet old pod
continue runningGIT RepoCI/CD Pipeline Workflow with Kubernetes
DEVELOPER CI SERVER KUBERNETES
CI Server 
notices new 
code in Git 
repo & starts 
running 
through its 
pipelineKubernetes 
receives 
request to use 
new imagePull new 
Docker 
image
71
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Brief History of Computing Architecture
Computing history is a history of the levels of logical abstrac-
tion. Early concerns with physical disk sectors gave rise to 
operating systems. Low-level languages gave rise to dynamic 
modern interpreted languages, thanks to layers of abstraction. 
The same applies to architectures leading to the boon in arti-
ficial intelligence. Before diving into types of architecture, it is 
worth rewinding through brief history to better understand how 
technology evolved.
As ancient computers became logical computing languages 
that gave rise to operating systems, a key principle of com-
puter system architecture emerged; it was called the Single 
Responsibility Principle (SRP). SRP dictates that programs 
do one thing and do it well: work together, and handle text 
streams as a universal interface. In short, single-focus pro-
grams, when strung together, may perform a varied and com-
plex assortment of tasks. A more detailed explanation may be 
found in The Unix Programming Environment, the book by 
Brian Kernighan and Rob Pike. 
The engineering community largely forgot the SRP pattern in 
favor of the object oriented paradigm during the late 1980s and 
early 1990s, as the languages C++ and Java gained popularity. 
The promised vision of object orientation and code reuse was 
never fully realized due to ironic problems with polymorphism. 
This period in history also gave rise to monolithic systems that 
often had millions of lines of code buried in one executable.
The 2000s brought the introduction of network-aware ap-
plications and the application server model, which was 
characterized by large monolithic code bases, massive rela-
tional databases —with stored procedures for query optimi-
zation—and Common Object Request Broker Architecture 
(CORBA) and Common Object Model (COM) for distributed 
communication and application interoperability.26 
The revolutionary 2000s also gave rise to XML27 as a 
means to configure and communicate. The open stan-
dards community developed the Simple Object Access Pro-
tocol (SOAP) as a “superior” alternative to CORBA and COM. Because SOAP is text-based, it had better interoperability al-
though it was still cumbersome compared to modern, gRPC,28 
JSON29 and RESTful APIs.30 Hence, the Software-as-a-Ser -
vice (SaaS) gained traction among Web companies, and the 
industry began its shift toward the Web as the primary applica -
tion service delivery mechanism.
Increasing pressure to deliver rapid iterations of both custom-
er and internal-facing software systems led to the shift toward 
open-source software systems, led by organizations such as 
Apache and GNU/Linux.31 This shift was an irreversible bi-
furcation that took power away from enterprise leaders and 
democratized innovation at the hands of hobbyists, start-ups, 
and academics at a previously unprecedented rate. Where 
once everyone waited on centralized policy-making, now the 
community offered unparalleled power and agility that led to 
the advent of cloud computing.
Amazon Web Services launched its Elastic Computing Cloud 
(EC2) in 2006, Google Compute Engine followed suit in 2008, 
and Microsoft Azure in 2010. In 2019, Amazon Web Services 
(AWS) reported revenues of $35 billion, indicating the extent 
of the seismic shift in the software industry, which sought out 
the most innovative software tooling developers for leader -
ship and not the enterprise software vendors. As cost mod-
els shifted away from large up-front capital expenditures to 
lower ongoing operating costs, scaling and resources could 
be used and paid for on-demand, and the entire deployment 
stack transformed into a DevOps32 infrastructure as code with 
the advent of CI and continuous deployment services.
Open source software and operational expenditure fueled a 
resurgence of the Unix Philosophy and gave rise to the mi-
croservices architecture: many small, fine-grained services 
that perform a single function all trying to achieve the goal of 
distributed networked components. Microservices gave rise to 
an engineering culture that embraces automated testing and 
deployment and embraces failure with unprecedented levels 
of fault tolerance. Microservices teams have the power to work 
on independent, deployable units of application code that are Computing Architectures
26. The Common Object Request Broker Architecture (CORBA) is a legacy binary communication protocol that was popularized in the early 2000s. The Common Object Model was 
a Microsoft specification and alternative to CORBA. RESTful APIs and gRPC replaced both technologies.
27. XML – Extensible Markup Language
28. gRPC is a modern, open source remote procedure call (RPC) framework that can run anywhere. It enables client and server applications to communicate transparently and 
makes it easier to build connected systems: https://grpc.io/.
29. JSON (JavaScript Object Notation) is a lightweight data-interchange format. It is easy for humans to read and write. It is easy for machines to parse and generate: https://www.
json.org/.
30. RESTful API design (Representational State Transfer) is designed to take advantage of existing protocols: https://restfulapi.net/.
31. The GNU/Linux operating system is free software that is an alternative to Microsoft Windows and macOS: https://www.gnu.org/.
32. DevOps is a set of practices that combines software development (Dev) and IT operations (Ops). It aims to shorten the systems development life cycle and provide continuous 
delivery with high software quality. DevOps is complementary with Agile software development; several DevOps aspects came from Agile methodology.
72
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>elastic, resilient, minimal, and complete. These applications 
scale individually and horizontally.
The underlying idea of microservices existed since the 
1970s. Distributed systems are a permanent and enduring real-
ization of the power of decentralized, democratized computing 
as a process and superset of products. Modern cloud infrastruc-
tures are built on microservices. Rapid, continuous integration 
and deployment pipelines are the reason for the overwhelming 
success of cloud computing platforms, where new features can 
move directly to production without human intervention after 
passing a stringent series of automated tests.
To conclude this brief excursion through computer architec-
ture and summarize the experience, it is evident that the main 
driver for the evolution of computing architecture is speed of 
deployment. The demand to get code into production is unre-
lenting. Early results lead to rapid innovations regardless of 
the product. Cloud-native computing services open the clear-
est path to achieving the goal of rapid engineering and deploy -
ment at blistering rates.
Basic Components of AI Architecture
A nation’s production of AI infrastructure requires that 
data centers be built on commoditized goods and ser-
vices. The key to infrastructure cost savings and operating 
strategy is a commodity of goods and services. By defini -
tion, commoditized hardware is limited to components that 
are readily available at economies of scale for general pur-
poses in computing—drives, racks, switches, routers, cool-
ers, power supplies, etc. Highly successful data centers use 
commoditized hardware to maximize the procurement of parts 
for repairing equipment failures and performing system main-
tenance. Much effort goes into minimizing failure rates across 
large-scale computing infrastructure. Failure is unavoidable. 
Generally, data centers frown upon specialized computing in-
frastructure. Research shows that the type and utilization of 
commodity hardware can reduce failure by orders of magni -
tude. Commoditized hardware minimizes TCO, a key metric 
for financial viability of any data center. Analysts often use 
TCO for cost-benefit analysis when deciding whether to pur-
sue cloud software systems such as AWS, Microsoft Azure, 
Google Compute Platform, or others.
Consider using cloud services wherever possible. On the 
upside, cloud services minimize TCO by multiples for opera-
tions of all sizes, especially during early phases of develop-
ment. Cloud systems offer a variety of commoditized hardware 
and services. Cloud offerings range in complexity and com-
putational power; customers may purchase bare, dedicated 
systems and turn-key AI solutions alike through an all-in-one interface. The downside, however, is the lack of ownership and 
geographical disadvantage that the location of data centers 
presents. A lack of data centers is not uncommon in nations 
with nascent computing industries, and likewise limited domes-
tic control of cloud-based data infrastructure. Nations with data 
infrastructure located in foreign nations face the real potential 
for disruptions to corporate agreements due to unanticipated 
geopolitical tensions spurred by sanctions during periods of 
conflict. Although this is uncommon, strategic vulnerability re-
mains a key reason for the slow adoption of cloud computing in 
government infrastructure among developing nations.
Consider a hybrid-native cloud services approach in order 
to maximize redundancy and protect data. Cloud services 
offer undeniable benefits and minimize TCO by offering com-
prehensive lists of commoditized services on demand. One 
particularly valuable benefit is the ability to extend on-premise 
infrastructure with dedicated cloud infrastructure. The reasons 
for developing the hybrid infrastructure model are primarily cen-
tered around redundancy and specialization. Data redundancy 
is essential to successful operations. Systems fail in all envi-
ronments, without exception, and data are always at risk for 
total loss. The operating cost associated with archival storage 
may not be equitable in the long run as data volume increases. 
Similarly, databases and data processing systems often require 
redundant nodes to guarantee serviceability and failover and 
eliminate any SPOF. Scalability relies on the mitigation of these 
factors. A hybrid model offers effective strategic reserves for 
growing infrastructural demands by providing burst capabilities 
for adjusting to unanticipated demand during phases of growth. 
Furthermore, growth requires investment in innovation, and in-
novation requires specialization of novel services. Rather than 
develop new service infrastructure in an on-premise environ-
ment, cloud service providers offer a wide gamut of specialized 
artificial intelligence infrastructure suitable for experimentation 
using on-demand billing agreements. The incurred expense is 
limited to only what is used by the organization. In either redun-
dant or specialized use cases, on-premise infrastructure gets 
extended over the network and organizations have the power 
to control the security and topology entirely.
General AI Architectures
Cloud computing originated with the need to run virtual ma-
chines on standardized hardware inside remote data cen-
ters—what is commonly known as Infrastructure-as-a-Service 
(IaaS). In the present day, cloud computing services span 
a vast array of on-demand services that address all sizes 
of computing tasks. Three major competitors dominate the 
global market for cloud-native services: Amazon, Google, and 
Microsoft. These provide customers with similar service offer-
ings, which are listed in Table A.1. 
73
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Service type AWS Google Azure
Compute 14 8 17
Data and storage 13 12 12
Network 6 8 13
Developer 9 13 9
AI and Machine Learning 11 15 35
Other (e.g. Io T) 56 33 24
T otal 109 89 110> > >
TABLE A.1 - Cloud Service Counts Services 
Source: World Bank.
Source: The World Bank.The service landscape continues to transform as new technol -
ogies enter the market, and ground-breaking work from pub-
licly traded technology companies continues to evolve in the 
open source communities, especially the work in AI and ML. 
Therefore, the goal for executive leadership is to understand 
architectural principles and compose services into systems 
designed to achieve specific business goals. Systems may 
target general solutions, such as storage for AI experimenta -
tion, or specific siloed solutions, that detect a specific form 
of fraudulent activity within data streaming from a discrete 
source. By maintaining a general inventory of service types, 
practitioners can zero in on desirable results.Overall, achieving the most effective AI IaaS model relies 
on understanding four pillars: Architecture, Development, 
Operations, and AI. To understand is to pursue the following 
respective inquiries with the goal of depth and breadth, un-
derscoring a clear strategy of experimentation and execution.
• ARCHITECTURE : What are the architectural patterns for 
adopting AI computing infrastructure?
• DEVELOPMENT: What are the best development tools, 
frameworks, and best practices?
• OPERATIONS: What are the best practices to deploy and 
manage services in production?
• AI: What are the available ML/Data Services? How can 
problems be best solved with these tools?
> > >
FIGURE A.5. - Pillars of Effective AI Architecture
Architecture Operations AI Development
Microservices
RPA
Protocols
Messaging
Queueing
Events
Data Models
Cloud
AI Services
Computation
Networking
ERPs
COTSCI/CD
Logging
Monitoring
Performance
Analytics
Databases
SecurityData Science
Deep Learning
Chat Bots
GANs
NLP
T ext-to-Speech
Speech-to-T ext
Machine Learning
Preprocessing
PredictionFrameworks
T ooling
Debudding
IDEs
T echnologies
Cloud Service
APIs
Engineering
74
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Function-as-a-Service Architecture
Serverless software is the state of the art in software ser-
vice development today. There is no “official” definition of 
the term “serverless,” therefore the following is adopted as a 
working definition:
“Serverless computing is a form of cloud utility computing 
where the cloud provider dynamically manages the underly -
ing resources for the user of the service. It provides a level 
of abstraction over the underlying infrastructure, removing the 
burden of management from the end user.”
Serverless software avoids the explicit creation and manage -
ment of infrastructure, such as servers and containers. In-
stead, functions that are managed and run by the cloud ser-
vice provider replace these traditional computer resources. 
This category of cloud computing services is called Function-
as-a-Service (FaaS), the overarching pattern serverless archi-tecture. It is important not to be misled by the term serverless. 
Data stores and persistent layers are available to FaaS appli -
cations. The point is that practitioners no longer concentrate 
on the infrastructure because the cloud vendor provides a 
layer of abstraction over all underlying infrastructure.
The principles of FaaS (serverless) computing architecture 
can be summarized as follows:
• On-Demand cloud functions replace all servers and con-
tainers.
• Preference goes to managed services and third-party 
APIs over custom-built resources.
• Architectures are event driven and distributed.
• Engineers focus on developing the core product, not the 
low-level infrastructure.
Source: van Eyk et al. (2018).> > >
FIGURE A.6. - Evolution of Architectures: A History of Computing Concepts Leading to Serverless Computing
Serverless2010s 1990s 1970s 2000s 1980s 1960sEvent-Driven Workflows
Event-Driven Arch.
Workflow orchestration
REST
SOA
(Pasik, 1994)URI
(RFC 1630, 1994)Microservices
LDAP
(RFC 1487, 1993)
DNSRPC Implementation
IBM CICS TransactionsRemote Procedure Call
(RFC707, 1976)
Virtualization Requirements
(Popek, 1974)
IMB VMs
(CP-40/CMS, 1968)Grid ComputingOGSAGoogle App Engine
AWS Cloud (2006)IaaS, PaaS, SaaS
(NIST Cloud Ref. Archi, 2010)Containers
(Docker, 2013)Container orchestration
Stored ProceduresCGI
(NCSA, 1993)Virtual Private 
ServerVMWare ESXFreeBSD JailscgroupsLinux Containers
CORBA, 
DCOM, OSF
Workflow, Bo Ts
Events
(When to
execute?)Execution flows
(How to model 
the program?)Functions
(What to 
execute?)Naming/registry
(How to find the 
executable?)Code functions
(How to
execute?)Resources
(Where to 
execute?)Actor Model
(Hewitt et al., 1973)
Concurrency
(Djikstra, 
Hoare, 1960s)Func. Programming
(McCarthy, 
1960)Event Sourcing
(McCarthy,
1963)Function-as-a-Service
75
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Infrastructure-as-a-Service Architecture
Also known as cloud-native solutions, infrastructure as 
a Service (IaaS) is common to AWS, Azure, and Google 
Compute Engine. AWS offers a dedicated government infra-
structure for qualifying customers. IaaS is based on the premise 
that infrastructure costs are significantly reduced when shared 
multiple tenants maximize rack resources in a data center. Ten-
ancy does not affect security. Individual systems operate on vir-
tualized machines referred to as Virtual Private Clouds (VPC). 
VPCs are isolated from one another; the end user “sees” them 
as physical machines when really the resources are constrained 
according to a virtualization policy specific to the customer’s in-
dividual requirements and cost selections.
COTS AI Architecture
Most commercially available AI toolkits abstract the learning 
process with models developed for specific uses in a siloed 
environment. These pre-trained models require a specific da-
taset with custom features. Data inputs must have the pre-
scribed features in order to realize accurate predictions. The 
end user simply needs to make known data available to a 
specific COTS product, and activities take off. This absolves 
the end user from possessing an in-depth knowledge of the underlying AI models, but still requires the end user to col-
lect, clean, and provide as much data with relevant features 
as possible to the COTS solution.
Training is an important part of the AI process. Intelligence is 
the result of an emergence of outcomes that are trained and 
tested repeatedly over countless cycles of iteration depending 
on the model and methods employed. Most of the computa-
tional cost—and the biggest barrier to entry overall—lies in the 
fact that training requires a large volume of data processing 
on compute-intensive resources. Therefore, it is beneficial to 
approach new problems with an understanding of pre-trained 
AI models available from COTS and cloud service providers.
Cloud service providers discussed here have several ap-
plications and services available to attack common AI 
problems. These span a wide gamut of topics including docu-
ment analysis, speech recognition, sentiment analysis, object 
detection, recommendation, and forecasting. Table A.2 lists 
common cloud AI services. This section of the Appendix dis-
cusses how to employ several of the services listed in Table 
A.2 to address notable problems in the final chapters, which 
contain practical examples of AI systems.
Application Use Service
Natural 
Language 
ProcessingMachine Translation AWS Translate
Document Analysis AWS T extract
Key Phrases
AWS ComprehendSentiment Analysis
Topic Modelling
Document Classification
Entity Extraction
Conversational Interfaces Chatbots AWS Lex
SpeechSpeech-T o-T ext AWS Transcribe
T ext-T o-Speech AWS Polly
Machine 
VisionObject, scene, and activity detection
AWS RekognitionFacial recognition
Facial analysis
T ext in images
OthersTime Series Forescasting AWS Forecast
Real-time personalization and recommendation AWS Personalize> > >
TABLE A.2 - AI Applications and Services
Source: Amazon Web Services
76
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Cloud-Native Hybrid Architecture
Redundancy, persistence, and service access underscore the 
value proposition when considering whether to extend a native 
architecture into the cloud. All the major cloud services provid-
ers offer an assortment of general ML and AI services using an 
on-demand commodity, software-as-a-service (SaaS) model. 
The services include both software and hardware solutions for 
common problems in ML and AI such as image object detec-
tion, natural language processing, computer vision, and speech 
recognition. Although the on-demand price of services may ap-
pear to be a significant expense for any “24/7” solution, custom-
ers may pay only for the time and computation they utilize for a 
specific task, experiment, or stage within the project lifecycle.
A cloud-native hybrid solution allows discrete access con-
trol and infrastructure integration. Overall, the physical data 
center can extend its topology with a virtual topology in the cloud 
called a virtual private cloud (VPC) that is nearly identical to a 
Virtual Private Network, governed by access and security poli-
cies that the government’s Development Operations (DevOps) 
Manager has control over. The extension behaves as though it 
is on-premises. Whitelisted infrastructure communicates inter-
nally, using private DNS network addresses. DevOps adminis-
trators may enforce one or more firewall proxies to grant access 
to vetted external components and services.
Implementation occurs in several phases depending on 
the desired objectives and key results. First, the team deter-
mines the purpose of external cloud services. If there is a need 
for bespoke AI compute services, then managers may surmise 
estimates by using existing local development processes that 
may address computational shortcomings. Per the information 
in the previous section on cloud architecture costs, from these 
shortcomings, DevOps Administrators may estimate the hourly 
TCO of cloud services based on the anticipated service require-
ments. Similarly, if there is a need for storage redundancy, then 
managers will estimate storage durability (level of redundancy) 
and availability (time-to-access), based on the current footprint 
and operating requirements. With anticipated estimates in hand, 
budgets may be appropriated, and resources deployed accord-
ing to the prescribed needs of the project. DevOps will man-
age the deployed resources and ensure that operating require-
ments are adequately resourced, which admins may choose to 
automate using cloud management tools for the long run.
By using cloud management tools, the size of cloud-native 
services hybrid architectures can expand and contract auto-
matically, on-demand. Although it requires an investment of 
time to develop a formalized topology of services and storage nodes, the benefits of investment pay off in spades over the 
long run. Once services are operating as planned, depending 
upon their intended use among citizens (public) or agencies 
(private), a configuration and template system will generally 
manage the scale of the infrastructure operation. Typically, a 
YAML32 (YAML Ain’t Markup Language) document will contain 
the topology requirements for the entire system and individu -
ally within containers that make up the constituent services.
Several containerization and instance management solutions 
are available through the major cloud service providers, but 
many of the best solutions are Free and Open-Source Software 
(FOSS). In particular, Kubernetes (K8s) paired with docker con-
tainers is among the most well-regarded solutions for full-scale 
application infrastructure management. Docker Containers are 
lightweight disk images containing an application—and all soft-
ware dependencies—configured in a fully operational state. 
Containers will deploy on any computer (node) running docker 
software. Docker reduces deployment time, eliminates system 
dependency management, and allows nodes with different 
operating systems to run “dockerized” applications stored in 
docker containers. Capital allocated to DevOps and DevOps 
stretches much farther with a managed cloud application clus-
ter. K8s deployments are clusters of nodes running dockerized 
service applications that employ easy-to-use configurations to 
scale with minimal human intervention.
Cloud-Native COTS Hybrid Architecture
Starting small is adequate for long-term proliferation of suc-
cessful solutions, yet there are hybrid alternatives with com-
mercial off-the-shelf (COTS) solutions that may extend AI ca-
pabilities. There are several commercial large-scale systems 
for transactional accounting and financial audit available. 
Many rely on proprietary cloud infrastructures in foreign data 
centers. This places significant barriers to entry for govern -
ment teams facing long-term goals of developing a conver-
gent data infrastructure on-premises. Governments that con-
sider making an investment in small-scale development once 
a broad data infrastructure strategy is in place may have a 
higher likelihood of long-term success. By edifying a formal 
iterative agile process, small-scale projects can spiral upward 
through versions. The key to iterative development is failing 
fast and often: projects that invest long time-spans to realize 
products at any scale become burdensome and fail to garner 
enough momentum to endure or provide value in the face of 
changing economic and political landscape. Thus, it is impor-
tant to start small and scale with experimentation through it-
eration in order to prove the effectiveness of novel solutions, 
especially those in artificial intelligence.
32. YAML (“YAML Ain’t Markup Language”) is a human-readable data-serialization language. It is commonly used for configuration files and in applications where data is being 
stored or transmitted.
77
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Turn-key solutions may present quick and easy wins, but need 
careful vetting to prevent unnecessary technical debt from 
sneaking up. More nuanced than mere costs associated with 
cutting corners, technical debt is a hidden intrinsic cost of tech-
nological development, which emerges as bugs, unfinished 
tasks, improvements, features, and upgrades that accompany 
the process of engineering any software or hardware product. 
Technical debt is inherently unavoidable and impossible to 
eliminate. Yet, there are process management best practices 
for mitigating the risk of extensive technical debt. During initial 
stages, careful planning and scoping are the best measures 
for maximizing productivity without incurring debt. But some 
things cannot be anticipated, so it is commonly acceptable to 
commence in a small scope that addresses the key concepts 
that underpin a full-scale long-term solution.Measure the costs associated with a cloud computing archi-
tecture in terms of TCO, which takes into account more than 
the cost of engineering and implementation. TCO factors in 
the cost of personnel, heating, ventilation, and air conditioning 
(HVAC), maintenance, monitoring, hardware, software, land, 
facilities, electricity, and innovation. By leveraging on-demand 
Infrastructure-as-a-Service (IaaS), projects forgo the cost of 
brick-and-mortar data center underutilization. Planning calls for 
expected server capacities that are guaranteed to fluctuate due 
to regular cycles of use on a daily basis. Moreover, although early 
projects demand fewer resources, planning for lateral growth to 
accommodate new deployments places a burden on resources 
that may naturally underwhelm the overall server infrastructure, 
leading to idle systems that demand step-wise investments for 
anticipated future demands. The overall efficiency of cloud-native 
architecture exceeds on-premise systems significantly, as illus-
trated by the graphs and statistics below, provided by AWS.
> > >
FIGURE A.7. - Overall Efficiency of Cloud-Native Architecture
> > >
FIGURE A.8. - Optimizing Cost of Providing IT Services and AWS ValueIncrease in VM 
managed per adminDecrease in 
downtimeReduction in overall 
spend per userDecrease in time 
to market for new 
features/ applications
Source: Amazon Web Services
Source: Amazon Web Services27.4% 57.9% 56.7% 37.1%Before
AWSAfter
AWSBefore
AWSAfter
AWSBefore
AWSAfter
AWSBefore
AWSAfter
AWS
Nearly 3x
More new features 
delivered25%
More productive application 
development teams
94%
Less time lost to 
unplanned downtime$36.5M
Additional revenue per 
year per organization62%
More efficient IT 
infrastructure staff51%
Lower 5-year cost
of operations6 monts
To
payback
90%
Less staff time to deploy 
new storage
14%
Increase in business 
user productivityImproved IT and Business Agility
Business Operations Impact
78
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Cloud-native costs fall into one of four contract catego-
ries: reserved, partially-reserved, spot, and on-demand.  
Reserved instances require up-front payment for a period of 
one to three years with no additional monthly costs for cen-
tral processing unit (CPU) and memory utilization. Partially-
reserved instances require a partial payment for one to three 
years but require a reduced monthly service cost for CPU 
and memory. Both reserved options are contract-based solu-
tions. Contracts can be sold at a rate prorated to the remain -
ing duration of the contract period if a customer should deem 
the contract unusable. Contract options are also specific to 
machine types, which often ship with immutable memory and 
CPU configurations. Spot instances offer significant savings 
similar to reserved instances, but their availability is not guar-
anteed. Spot rates allow the customer to specify the maximum 
allowable cost per hour of VPC use for a given VPC configura -
tion. Customers pay a reduced variable cost for the instance, 
but should the market cost exceed the customer’s maximum, 
the instance can terminate, causing potential loss of data. The 
key is to set a high cost threshold and the VPC remains pro-
tected. Additional configuration can allow for persistent disk 
mounts that protect volumes of information from loss in the 
event of an unexpected termination. Spot instances are best for experimental development and skunkworks usage in which 
there are no risks to the general public. Lastly, on-demand in-
stances offer no savings compared to reserved instances, but 
still remain very competitive with TCO of on-premise deploy -
ments. The market determines the on-demand rate.
Also, important to note, disk drives (volumes), network input-
output (I/O), monitoring, and dedicated VPCs are additional 
costs on top of the VPC cost in a cloud-native infrastructure. 
These are metered in fractions of a unit of computational pay-
load (in bytes) and sold as add-ons, which still offer significant 
savings and added efficiency over the on-premise model.
The overall reduction in costs is compounded by the increase 
in efficiency of AI algorithms, which are outpacing predictions 
made by Moore’s Law, which states that the number of transis-
tors on a microchip doubles about every two years, though the 
cost of computers is halved. This leads to exponential increas-
es in computational power. Coupled with the fact that research 
in AI algorithms is increasing their efficiency, AI is outpacing 
Moore’s Law faster than expected. Figures A.9 and A.10 below 
illustrate the fact; the first is efficiency, and the second is com-
pute according to a study conducted by researchers at OpenAI.
> > >
FIGURE A.9. - Less Compute Required to Get to AlexNet Performance 7 Y ears Later – Efficiency Level
Source: OpenAI201350
45
40
35
30
25
20
15
10
5
0
2015 2017 2019 2014 2016 2018 2020
T otal amount of compute in teraflops/s-days used to train to AlexNet level performance. Lowest compute points at any 
given time shown in blue, all points measured shown in gray. 2,5,6,7,8,9,10,11,12,13,14,15,1644x less compute required to get to AlexNet performance 7 years laterTraining Efficiency FactorEfficientNet-b0
ShuffleNet_v2_1x
ShuffleNet_v2_1_5x
MobileNet_v2
DenseNet121Squeezenet_v1_1
Resnet-18GoogLeNet
AlexNet
VGG-11ShuffleNet_v1_1xCompute
(log scale)Efficiency 
(linear)
79
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>> > >
FIGURE A.10. - 44x Less Compute Required to Get to AlexNet Performance 7 Y ears Later – Compute (log scale)
Source: OpenAI201310
5
1
0.5
0.1
0.05
2015 2017 2019 2014 2016 2018 2020
T otal amount of compute in teraflops/s-days used to train to AlexNet level performance. Lowest compute points at any 
given time shown in blue, all points measured shown in gray. 2,5,6,7,8,9,10,11,12,13,14,15,1644x less compute required to get to AlexNet performance 7 years later
EfficientNet-b0ShuffleNet_v2_1xShuffleNet_v1_1xShuffleNet_v2_1_5xMobileNet_v1MobileNet_v2DenseNet121
Squeezenet_v1_1Resnet-50Wide_ResNet_50
ResNext_50
GoogLeNetAlexNetVGG-11Compute
(log scale)Efficiency 
(linear)T eraflops/s-days
Overall, the TCO for cloud-native and hybrid infrastructure makes a strong case for consideration in government systems, 
if only during planning and research phases of new initiatives, even after a government deploys the core on-premise infrastructure.
80
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>The necessity of a common data interchange standard as well 
as compact and accessible interfaces in the design of an AI 
capable infrastructure cannot be understated when pursuing 
the task of access to clean data processing pipelines.
Global Data Interchange
Plan globally for data interchange the long run and act 
locally for data processing in the short term. The coor-
dination and management of a digital infrastructure requires 
conscious effort to identify existing requirements and plan the 
digitization of the most mission-critical systems. These sys-
tems may not have a national scope. They may be localized 
operations devoted to managing reporting under an integrated 
financial management information system, procurement, or 
asset management and exchange. Whatever the entry point, 
stakeholders must carefully consider the design of the under -
lying architecture early during the planning phase with a focus 
on accommodating long-term use of data to satisfy secondary 
operational requirements. Health, education, transportation, 
and public safety systems are examples of secondary opera -
tional requirements in a digital government environment.
Efficient Communication 
with Compact Data
In the early days following the shift from mainframe to distrib-
uted systems of computing, engineers began to address IPC 
between applications, computers, and data centers with often 
creative solutions. At the smallest scale, data communication 
between applications using XML and SOAP protocols allowed 
for independently specialized applications to share informa-
tion somewhat effectively. At a very large scale, immense data 
transfers required the physical transport of magnetic reels 
and hard drives over land to mitigate the total cost of net-
work transfer. Today’s standards may require the occasional 
physical transport, but among emerging data technologies at 
massive scale, IPC is managed efficiently in real time using 
compact data standards and communication protocols. Two 
standards in structured data stand out above others: JSON 
and protocol buffers - also called protobuffers or protobufs.
JavaScript Object Notation
JSON is an object definition “language” standard that gives 
the practitioner the ability to define key-value relationships be-
tween any number of values, which may be primitive types 
such as strings, numbers, and Boolean values or complex 
types such as arrays and nested JSON objects. Engineers 
and data scientists refer to one of these comprehensive and 
completely self-contained units of information as a document. The attributes and values within a document are iterable—
programs can “walk” the document to retrieve values—and 
mutable—programs can alter the values of the attributes.
JSON documents are the primary structure of document stor-
age in several of the most successful databases and big-data 
storage solutions on the market. JSON is also the preferred 
format for data exchange among web services architectures 
throughout the world of software development. There are in-
ternational standards for the structure and definition of JSON 
documents. More information about JSON is available at www.
json.org, and many other resources exist that cover the sub-
ject matter exhaustively. The discussion of JSON in AI Archi-
tecture continues in the section on Leveraging Microservices.
It will suffice to write that JSON provides a very efficient IPC 
standard for virtually any application that will ever be engi -
neered. It is fast, compact, semantically endowed for human 
consumption, and provides a low barrier to entry for practitio-
ners in need of rapid interchange of data between specialized 
applications. In some instances, internal applications require 
even more performance, less readability of payload while 
maintaining semantic interoperability. This leads engineers to 
consider protobuffers.
Protocol Buffers
When speed of interchange and consistent structure is cru-
cial for mission-critical applications—such as those in finance 
or infrastructure management—protobuffers provide a valu-
able alternative to JSON. Protobufs are platform-independent, 
language-independent extensible mechanisms for serializing 
structured data. Once in a document schema, developers struc-
ture the data, and any applications wishing to communicate 
with that data can simply implement an API that is automatically 
generated in any programming language on any platform.
A specialized remote procedure call framework further extends 
the power of protobuf’s compact data interchange format with 
structured programmatic function definitions called gRPC. With 
gRPC in place, IPC occurs over any network topology by le-
veraging exposed functions capable of ingesting and output-
ting protobufs. This means that highly specialized and compact 
application services can be built to communicate in real time 
and process large volumes of information for large scale imple-
mentations of AI services. This is the technology at the heart 
of Google’s global infrastructure. The standards and software 
supporting this technological breakthrough are FOSS.Advanced AI Connectivity
81
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Other Data Formats
Other standards in data do exist. Many are proprietary to the 
systems that leverage that data. One notable data format is 
called Parquet—a FOSS construct of the Apache Software 
Foundation. The purpose of Parquet was to provide a colum-
nar data access format that interoperates well with Hadoop 
software systems. Hadoop came to prominence in the 2010s 
after Google Published a MapReduce white paper describ -
ing in detail the design and development of the original archi-
tecture that powered PageRank algorithms to notoriety. The 
report was reverse-engineered, and an open source MapRe -
duce solution hit the market leading to a trend in Big Data 
technology, which never fully panned out for Hadoop and its 
consortium of supporters.
As a modern relic, Hadoop (and Parquet) technology proves 
primarily that industry hype can mislead practitioners in search 
of problems looking for a solution. In contrast, the more sim-
ple, streamlined, and effective long-tail solutions and patterns 
of application and data architectures continue to satisfy the 
requirements for modern Big Data best practices. 
Leveraging Microservices
As mentioned, monolithic systems have critical faults that 
lead to eventual collapse, for reasons of obsolescence stem-
ming from stifling complexity. Microservices, conversely, are 
a methodology of designing, architecting, and developing a 
wildly scalable infrastructure of highly specialized applica -
tions. Engineering teams focus on each application indepen -
dently, while inter-process communication, especially when 
leveraging the power of gRPC, remains versioned through API 
standards. Thus, project management dependencies are lim-ited to the scope of each independent component within the 
microservices application infrastructure. Teams can and often 
do work independently to achieve incredibly rapid results for 
very large scale systems. Thus, the future of AI systems engi -
neering and data fabric infrastructure rests on this fundamen -
tally advanced pattern of application architecture development 
regardless of the course of physical deployment, be it in the 
cloud, on-premise, or a hybrid of both.
A large volume of information exists on the subject of microser-
vices development. This appendix to the paper does not delve 
into the subject further, but rather uses this mechanism as a talk-
ing point to illustrate the necessary jargon that is essential in un-
derstanding the factors allowing for the development of solutions.
Advanced AI Models
Artificial Neural Networks
Neural networks are at the heart of advanced concepts in AI. 
Neural networks perform computations that derive potentially 
vast sets of self-selected features. Deep Learning relies on 
artificial neural networks (ANNs). First studied in the 1950s, 
ANNs have emerged today through several cycles of dorman -
cy due in large part to the copious amount of raw computing 
power available in the cloud. At their core, ANNs are orga-
nized layers of decision nodes called perceptrons. Numbers 
enter an input layer and exit through an output layer. Hidden 
layers exist between the two. The goal of ANNs is to iteratively 
learn weights for each perceptron layer and produce an ap-
proximation of the desired result in the output layer. “Deep” 
refers to the number of hidden layers in an ANN, which may 
be as few as seven to eight but most often hundreds. Figure 
A.11 represents the basic ANN structure.
> > >
FIGURE A.11. - Basic Deep Neural Network Structure
Source: The World Bank.Hidden Perceptron Output Perceptron
Neuron
NeuronInput Perceptron Hidden Perceptron
Neuron
NeuronNeuron
Neuron
NeuronNeuron
Neuron
Neuron
82
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Language is the most powerful and potent human mecha-
nism. With great access to language data comes great re-
sponsibility. Today’s commercial email, word processing, and 
voice communication tools are constantly scanning and inter-
preting human language with a goal of suggesting grammati-
cal corrections, advertisements, and translating our conver-
sations into written language. Smartphones and smarthomes 
alike respond to words, sometimes when 
a passive conversation is “overheard.” 
The annals of news reporting are at the 
mercy of suggestions catered to individ -
ual indulgences. At the heart of all this  
is NLP.
Since around 2013, NLP and chatbots 
have gained presence nearly everywhere 
in society at large. Google search became 
smarter and more capable of interpreting 
more human-like inquiries. Smartphone 
auto-correct and auto-complete followed 
suit, and the emergence of personalized 
phone assistants began to gain traction. 
In government, NLP began to emerge as a tool for combating corruption and giving a voice to citi-
zens. One project called Hack Oregon used natural language 
campaign finance data to find connections between political 
donors because it seemed that politicians were hiding their 
donors’ identities behind obfuscating language in their cam-
paign finance filings.
Basic NLP systems track term fre-
quency relative to inverse document 
frequency (TF-IDF). These evolved to 
“chain” clusters of word frequencies in 
order so that predictions could be made 
about the best “next” word, also called 
Markov chains. These conditional, proba -
bilistic distributions have evolved since 
into very sophisticated systems of inter-
preting, “understanding,” and formulating 
language into topics with semantic mean-
ing using math alone. Fascinating barely 
begins to describe the power of NLP.
In government and beyond, the necessity 
of beneficial machines with prosocial be-Natural Language Processing> > >
FIGURE A.12. - AI and Machine Learning Algorithms and Applications
Source: Peter Elger and Eoin Shanaghy, AI as a Service, Manning 2020.Artificial neural networks
Association rule learning
Bayesian networks
Clustering
Decision tree learning
Genetic algorithms
Inductive logic programming
Reinforcement learning
Representation learning
Rule-Based machine learning
Similarity and metric learning
Sparse dictionary learning
Support vector machinesProblem solving
Search
Constraint satisfaction
Knowledge, reasoning and 
planning
Logic agents
First-order logic
Planning and acting
Knowlege representation
Probabilistic reasoning
Decision making
Learning
Learning from examples
Knowledge in learning
Learning probabilistic models
Reinforcement learning
Communication, 
perceiving and acting
Natural language processing
Perception
RoboticsReinforcement
learning
Machine
learningSupervised
learningUnsupervised
learning
Artificial
IntelligenceDeep
reinforcement
learning
Deep
learning
Language is 
the foundation 
upon which 
we build our 
shared sense 
of humanity.
Dr. Arwen Griffioen, Senior Data 
Scientist - Research, Zendesk
83
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>> > >
FIGURE A.13. - Chat Box Recirculating (Recurrent) Pipeline
Source: Lane, Howard, and Hapke (2019).havior that leads to greater cooperation among actors remains a key focus of ongoing NLP research. Governments are able to 
leverage NLP for interfacing with citizens for many purposes, the least of which is gathering information about the quality of service 
within the government. As news sources become increasingly aligned with the indulgences and personal preferences detected 
among patrons of various internet service providers, the quality of information revealed to citizens in relation to the government 
also falls into question. A simplified process of NLP operations in the AI algorithm is depicted below in Figure A.13.
To highlight the many use cases, NLP makes it possible to 
review contract submissions, resumes, proposals, campaign 
advertisements, published documents, and financial trans-
actions for authenticity with minimal bias. The mathematical 
models that enable these technologies to perform such im-
portant tasks fall outside the scope of this paper. Instead a quick exploration of how an NLP architecture operates pre-
cedes later sections that enumerate the examples of NLP in 
action within government. NLP is among the most interesting 
topics in AI that will make a lasting impact on the way in which 
human beings interact with computers, organizations, the en-
vironment, and each other for decades to come.Tex t
Database
Satements
responses
scroes
user profileScored
Statements
Possible
responsesScored
StatementResponse
string
Scored
responses
Structured
data
(feature vector)
Response
feature
vector4. Execute
Generalize & classify
update models
update objective
update dialog plan
select respond
1. PARSE
T okenizers
regular expressions
tag
NER
extract information
reduce dimensions2. Analyze
Check spelling
check grammar
analyze sentiment
analyze humanness
analyze style
CNN
GAN3. Generate
Search
templates
FSM
MCMC
RBM
RNN
84
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Generative adversarial networks (GANs), introduced to the AI 
ecosystem in 2014 by Ian Goodfellow, enable computers to 
generate realistic data by using two separate neural networks. 
Although these were not the first computer programs used to 
generate data, their results and versatility set them apart from 
all the rest. GANs achieve remarkable and often alarmingly 
convincing results that were previously considered virtually impossible for artificial systems, such as the ability to generate 
fake images (and videos) with real-world quality. GANs can 
turn a scribble into a photographic image or turn video foot-
age of a horse into a zebra—all without the need for incredibly 
large painstakingly labeled data. A staggering example of how 
far machine data generation is able to advance because of 
GANs is the synthesis of human faces—see Figure A.14. 
By 2017, GANs enabled computers to synthesize fake faces rivaling high-resolution photographs. Most notably, GANs produced 
fake videos of notable celebrities and political figures whose speech and countenance are virtually indistinguishable from real life 
recordings simply by “mutating” the face of any recorded individual to appear as the synthesized individual, as shown in Figure 
A.15. This is of particular interest to government policymakers due to the fact that fake-news videos can be produced and prolifer -
ated by anyone with access to GAN modeling toolkits in order to misinform and manipulate the public with practically any video 
content imaginable.Generative Adversarial Networks
> > >
FIGURE A.14. - Progress in Synthetic Human Face Generation, 2014–2017
> > >
FIGURE A. 15. - GAN Transformation of One Politician into Another33 
2014 2015 2016 2017
INPUT
 CYCLE-GAN
 RECYCLE-GANGAN Human Face Generation
GAN TransformationSource: Brundage et al. (2018).
Source: Bansal (2018).
33. Watch the video: https://www.youtube.com/watch?v=F51RCdDIuUw.
85
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>GANs are a class of machine learning techniques that consist 
of two simultaneously trained models competing with one an-
other as adversaries: one (the Generator) trained to generate 
fake data, and the other (the Discriminator) trained to discern 
the fake data from real-world examples.
The term “generative” refers to the overall purpose of the mod-
el: to create “new” data. The data that GANs learn to gener -
ate depends on the choice of the training set. In the example 
mentioned above, if a practitioner wants a GAN to generate 
images that look like the president of any country, they will use 
a training dataset of the president’s face.
The term “adversarial” refers to the game-like competi-
tive dynamic between the two models that constitute the 
GAN framework. The Generator creates examples that are 
indistinguishable from the real-world data in the training set: 
fake images of the public figure. The Discriminator verifies the 
authenticity of the images believed to be the president. The 
two networks are continually trying to outwit each other: the 
better the output of the Generator, the better the Discrimina -
tor needs to be at distinguishing real examples from the fake 
ones. The term “network” indicates the class of machine learn -
ing models most commonly used to represent the Generator 
and Discriminator: deep neural networks. The complexity of 
the artificial neural networks employed varies from simple to 
extreme and the results are unimaginably concerning for poli -
cymakers interested in preserving public trust and ensuring 
the safety of representatives of government charged with pro-
tecting national security.
GANs is explored further in the section about AI in policy. 
Before progressing to the topic of general artificial intelligence, 
it is worth noting that technological advancements in AI also 
enable concerns with voice synthesis in addition to image 
synthesis. Present day AI technologies allow the mimicry of 
human speech with relative ease. Thus, with moderate effort, 
AI models can produce human speech samples that are prac-
tically indistinguishable from actual human voice, furthering the concern over influence due to intentional disinformation 
spread through social media and the news. All hope is not lost, 
however, with the introduction of authentication mechanisms 
that practitioners can implement to prevent the loss of integ-
rity for state-sponsored messaging using asset encryption and 
cryptographic signing, which produces a digital watermark us-
ing state-sponsored media. Despite this possible solution, as 
AI methods continue to improve, the need for more robust 
authentication and prevention mechanisms will accelerate in 
order to keep pace with more advanced methods of image 
and audio forgery.
General Artificial Intelligence
The ultimate goal of artificial intelligence is to emulate the in-
telligence of humans and animals by modeling the behavior of 
neural pathways and the brain. General artificial intelligence 
takes that goal one step further by pursuing the ability to learn 
how to learn. The mention of General AI conjures visions of 
a singularity and the domination of mankind by sentient ma-
chines. This is fodder for science fiction and cinema. Learning 
to learn—sentience—is beyond AI’s current capabilities. Pres-
ently, all AI practitioners operate within the confines of artifi-
cial methods of guided learning and modeling based mostly 
on advanced statistical models built to process vast amounts 
of information in order to assist with specific decision-making 
goals. They cultivate interpretive data models that train com-
puters to provide sound decision-making similarly to humans, 
by emulating the physiological design of the brain. General 
AI is a proverbial mecca on the AI horizon that aims to elimi -
nate the need for human influence over the learning process. 
There are no known instances of this phenomenon in current 
employment among AI practitioners, although researchers 
have made significant contributions to reach this ultimate goal. 
There is no doubt that current AI resources will be instrumen -
tal in the emergence of General AI, however, the timeline for 
the realization of the singularity is uncertain. Therefore, it is 
outside the scope of the paper to explore this topic any further, 
however many resources exist for those interested in learning 
more about General AI.
86
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>The real world AI workflow has five main components: data 
preparation, model building, evaluation, optimization, and pre-
dictions on new data. Although applying these steps has an in-
herent order, most real-world applications revisit each step mul-
tiple times using an iterative process. Practitioners first build a 
model using historical input data from a particular ML algorithm. 
Next, they iteratively evaluate model performance and optimize 
for accuracy and scalability to fit the project requirements. Last, 
they use the final model to make predictions on new data in-
puts to the system. Historic data helps build the model, and new 
data flows into the resulting AI model to create predicted data. 
Predicted data flows into data streams that may be useful in ap-
plications for additional computational workloads and eventual 
archival storage in distributed ledger technologies (DLT). This 
appendix touches upon DLT—a useful tool in combating long-
term data tampering—in later sections.Human-Out-of-the-Loop Workflow
The basic AI workflow, absent of human oversight, is also 
referred to as an Out-of-the-Loop workflow. This simply 
refers to the fact that humans do not evaluate the predicted 
outcomes before applications take additional action. It is worth 
noting that this is a simplified representation of an AI system, 
and the following human-out-of-the-loop table does not ac-
count for the steps one must take to optimize the AI model 
building process, such as feature engineering and model tun-
ing. Overall, an Out-of-the-Loop approach is a useful way to 
approach non-critical decision-making systems such as rec-
ommendation engines and general classification engines—
see Figure A.16. For mission-critical applications that result 
in consequential collateral actions—the detection of fraud and 
other criminal acts—practitioners must employ advanced AI 
workflows with human intervention built into the AI loop.Real World AI Workflows
> > >
FIGURE A.16. - Basic Out-of-the-Loop AI Workflow
Source: The World Bank.Basic AI Workflow
Archival
AlgorithmBlockchain
AlgorithmResult
Event Stream
Model
OptimizationModel
EvaluationArchival
Event StreamNew
Data
Historical
DataModel
BuildingPredicted
DataAI Model
87
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Human-in-the-Loop Workflows
Of the many forms of advanced AI workflows, the simplified—yet advanced—flow depicted in Figure A.17 has the same compo-
nents of the basic workflow, but with an additional human element that improves model performance and prevents unintended 
consequences in mission-critical systems. Workflows with human intervention loops are referred to as human-in-the-loop and 
human-over-the-loop workflows.
> > >
FIGURE A.17. - Basic Out-of-the-Loop AI Workflow
Source: The World Bank.Advanced AI Workflow
Archival
AlgorithmBlockchain
AlgorithmResult
Event Stream
Model
OptimizationModel
EvaluationArchival
Event StreamNew
Data
Model
BuildingFeature
EngeneeringHistorical
Data
YES
YES
NO
Human 
InterventionPredicted
DataAI Model
~100%
Match
Vetted
Data
88
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>The Human-in-the-Loop workflow incorporates an additional logi-
cal step in the overall workflow that presents all predicted data to 
a human for further intervention. This logical gate may enrich the 
result data, complete with additional information related to the 
outcome. There, a human being can review the information and 
select the proper classification. This is useful when there must be 
no doubt in the accuracy of predicted results. The Human-over-
the-Loop workflow selectively gates results that demonstrate a 
high probability of accuracy for additional human intervention. 
This allows humans to concentrate on other mission-critical tasks 
when the certainty of predicted results is uncompromising, and it 
allows for intervention when the quality of predicted results falls 
below a threshold of desired probability.
Both variants of the advanced AI workflow may feed human-vet-
ted predicted results back to the historical data stream for model 
retraining before continuing into the data stream that captures 
and distributes results to subsequent applications. Vetted data 
are particularly useful for improving the quality of decision-mak-
ing over time, because human intervention lessens the gap of 
uncertainty and provides the model with increasing accuracy.
Feature engineering is also present in the advanced AI 
workflow. All problem domains require specific knowledge 
when deciding what data to collect. This valuable domain 
knowledge can also be used to extract value from collected 
data. Creating new data from existing data are called feature 
engineering. This phase occurs prior to model building. Once 
the AI loop is functioning adequately, practitioners often find the 
majority of their time going into this part of the optimization pro-
cess. This is the more creative part of developing AI solutions 
since it requires imagination and knowledge to invent ways to 
improve the model by extracting hidden value from standard 
data. Common examples of feature extraction include convert-
ing dates and times to times of day/week/year, location-wran-
gling, in conjunction with census data, and object detection in 
land use imaging data that is useful in classification.
The mention of data streams deserves some attention 
when discussing real-world AI. Data streams are an impor-
tant component in hybrid AI architectures. During develop -
ment, data scientists may load data from comma-separated 
or tab-delimited data files for cleaning and processing. In 
practice, data flows from input to output in a constant stream. 
Thus, the term “data stream” is applicable. It is fair to won-
der “what” exactly streams the data. Data streams are usually 
event-driven applications that “listen” for specific events within 
the system architecture. These are powered by open source 
technologies, particularly Kafka, that consume data from vari-
ous sources through a standardized API, such as those of-
fered by a relational database or ERP system. Data streams 
are especially useful because they offer data to one or more authorized applications that are capable of “listening” to the 
data stream producers using APIs over the network.
Distributed Ledger  T echnology
Any mention of DLT among the general public stirs the topic 
of cryptocurrency and alternative currency markets, particu-
larly Bitcoin. However, while Bitcoin is a particularly popular 
example of DLT, when put to novel use, the subject of DLT 
spans a much broader variety of topics. At the core, DLT is a 
distributed network of computing nodes that manage identical 
ledgers containing blocks of data. Each ledger contains blocks 
linked together in a manner that prevents tampering by enforc-
ing consensus requirements across the distributed network. 
At a minimum, each block contains an index that references 
the block order, a timestamp that references creation time, a 
cryptographic hash that is a signature of the data contents 
“salted” with the previous hash, a hash referencing the pre-
ceding block, and the rows of data, which may be individually 
encrypted for added security.
DLT  Architecture
A block is defined simply as a collection of batched data. Block 
size is determined by the fault tolerance of the blockchain net-
work due to distributed denial-of-service attacks and other fac-
tors related to network capacity. The typical block size is 1MB 
but that can be tuned to the needs of a particular application. 
The data stored in blocks is typically metadata on the order of 
kilobytes in scale. Storing large files in blocks is contraindicated 
to the functionality of the standard blockchain. Typically, large 
files are stored in a filesystem while the information describing 
their contents such as location, author, and perhaps a hashed 
checksum that serves as a signature for the integrity of the file, 
is written to the batched data buffer that eventually becomes 
part of the block. Data stored in a block can be encrypted and 
later deciphered upon retrieval. When batched data reaches 
the block size limit, the block is hashed using a cryptographic 
algorithm, and the blockchain algorithm places a request to add 
the block to the distributed ledgers throughout the network. Mul-
tiple requests may be placed from different nodes in the net-
work; these are handled in sequential order, and the consensus 
mechanism aids in orderly propagation of data.
Because a block hash is unique to data contained within the 
block and the blockchain links blocks using hashes generated 
from the previous hash and current, any mutation to the data 
within a prior block will change the reference in subsequent 
blocks, thereby breaking the blockchain. Figure A.18 illustrates 
the design of a single computing node within the DLT network.
89
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>> > >
FIGURE A.18. - Basic Blockchain Node
Source: The World Bank.Basic Blockchain
Blockchain Node
Blockchain Ledger
Block 0 [Genesis Block]
Index
Hash
dfd265ca281e96659f09954e
Previous Hash
null
Data
ec805923 8c9b5c82
a3c96d6a a43ab504Data
TimestampBlock 1
Index
Hash
eae7274960de395b72a87959
Previous Hash
dfd265ca281e96659f09954e
Data
c3e83f08 b3121395
ae2be25c 15d86fdeTimestampBlock 2
Index
Hash
426017c20ea19a03689c8e40
Previous Hash
eae7274960de395b72a87959
Data
22ea779b a8e7d68f
10dfdb40 8ab65a28TimestampBlockchain
Application
Blockchain security is compelling for archival data storage in 
government systems. Yet, a single node is insufficient for es-
tablishing a proper DLT network. The owner of one central-
ized node can simply alter any block arbitrarily and rewrite 
the hashes for all the subsequent blocks! Thus, the power 
lies in decentralization. DLT architectures distribute the en-
tire ledger to nodes qualified to participate in the DLT network 
and require consensus before new blocks may append to the 
blockchain. DLT requires consensus to prevent the Byzantine 
Generals Problem, which arises when actors attempt conflict-
ing actions such as overwriting or altering the blockchain with 
nefarious intent.
Consensus
There are several mechanisms to achieve consensus: proof-
of-work, proof-of-stake, proof-of-bid, and the list goes on. The goal of each consensus algorithm is to validate the blockchain 
integrity before the block append operation is distributed to all 
the remaining nodes in the network. Energy consumption is 
the reason so many forms of consensus exist. A network con-
sensus generally consumes a tremendous amount of compu-
tational power—thus, electricity—and utilizes a large amount 
of network resources. Therefore, it is imperative for the DLT 
network architecture to implement an efficient consensus 
mechanism. Figure A.19 illustrates the architecture of a DLT 
network. When one of the nodes in the network captures 
enough data to write a block to its local blockchain, it issues 
a consensus request to other nodes in the network accord-
ing to the rules of the consensus mechanism. When the net-
work reaches a consensus, the source node writes the block, 
and the block propagates throughout the remaining nodes in 
the network.
90
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>> > >
FIGURE A.18. - Basic Blockchain Node
Source: The World Bank.DLT Network Architecture
Government information systems can benefit significantly 
from the use of DLT for maintaining the integrity of mission-
critical data. Data for procurement, FMIS, and other systems 
generating transactions are the primary benefactors. When a 
transaction is generated, AI systems can send data to archival 
DLT nodes for archiving. Archived data stored in a DLT archi-
tecture helps maintain integrity throughout the network of par-
ticipating nodes for reasons that should be obvious, given the 
context of preceding sections. Overall, a network of govern -
ment agencies, or even departments within an agency, may 
become a stand-alone DLT network that is capable of main-
taining, authenticating, and honoring long-term commitments 
to data integrity within the government. Should any participat -
ing node attempt to sabotage the integrity of the transactional 
blockchain archive, a mechanism can be established to alert 
overseers of the transgression and preventative action can be 
taken to investigate the problem and take appropriate action 
to prevent any fraud or corruption.The most prolific government projects leveraging DLT for the 
purposes of distributed trust are those of central banks. Banks 
leverage DLT for Treasury Single Accounts that are host to for-
eign exchange transactions between central banks to speed 
the settlement of international exchange. Several experimen -
tal models are under consideration by the Monetary Authority 
of Singapore in ongoing research conducted through Project 
Ubin (https://www.mas.gov.sg/schemes-and-initiatives/Proj -
ect-Ubin).
Additionally, governments can benefit significantly from imple -
menting DLT along with AI processes in procurement and lo-
gistics. By tracing the procurement process with a distributed 
ledger, equipment, raw materials, and various critical resourc-
es can be transferred between parties with granular control.Blockchain
NodeBlockchain
NodeBlockchain
NodeBlockchain
Node
Blockchain
NodeBlockchain
NodeBlockchain
Node
Blockchain
NodeBlockchain
Node
91
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Appendix B. AI and the Sectors>>>
AI is gaining traction as an invaluable tool in urban planning, resource utilization, energy 
management, and climate change. Several practical applications are in development due in 
part to successful academic research funded by private enterprise. This trend will continue as 
humans occupy more densely populated urban areas that make use of natural resources in all 
manners. The scope of development and land use is enormous considering that most of human 
resource management touches on nearly every aspect of society in some form. Appendix B at-
tempts to highlight many solutions that rely on AI for improvements in efficiency, scientific analy -
sis, and prediction within the disciplines mentioned above. Figure B.1 illustrates the timeline of 
AI innovation in the environment and potential impact over the next 20 years.
92
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>> > >
FIGURE B.1. - Timeline of AI innovation in the Environment and Impact Over the Next 20 Y ears
Source: http://www3.weforum.org/docs/Harnessing_Artificial_Intelligence_for_the_Earth_report_2018.pdf.Smart AgricultureAI in weather & climate forecasts Ocean data platform Decentralized water systems Deep reinforcement learning for newsuper conductor discoveriesQuantum computers power Earth & materials science breakthroughsFully automated & connected urbantransport systems in major citiesCommunity disaster response platform Earth Bank of Codes On-demand EV-AVs in major cities Hybrid reinforcement learning for climate/weather scienceTrial decentralized energy, water grids & trading
AI-designed smart citiesAutonomous farmin
End-to-end optimized food Widespread decentralizedenergy grids & trading Real-time & public digitaldashboard for the Earth
Home supercomputer amplify R&D2020 2030 2040
The list of sectors covers energy, agriculture, materials 
science, transportation, climate management, and urban 
planning. The overall effort is toward a more effective feed-
back loop that mitigates risks brought on by overpopulation 
and resource scarcity in all of these sectors.
Agriculture
Agricultural innovators are currently using AI to model several 
interdependent factors in an effort to maximize food production 
yields. By consuming vast amounts of weather conditions, 
satellite and drone imaging, temperature, water use, soil 
conditions, crop rotation, and annual yields, AI systems are 
able to suggest optimal planting patterns that guide heavy 
equipment using geospatial precision. AI monitoring assists 
with managing water distribution during the growing season. 
As harvest approaches, AI leverages hundreds of thousands 
of data points on the ground from the Internet of Things (IoT) 
devices combined with satellite or drone imagery to determine 
optimal harvest quality and accuracy, which minimizes food spoilage in the post-harvest supply chain. In parts of Africa and 
Asia, AI helps maximize food production given the increasing 
dearth of annual rainfall, which forces farmers to become more 
precise in their forecasting and planning. The use of computer 
vision in combination with deep learning methods can detect 
potential fluctuations in pests, disease, water shortage, and 
harvestability. This is all a part of an emerging discipline called 
precision agriculture.
More specifically, a project called Ag-Analytics is collecting 
farmland data in the cloud and making it available to farmers 
for precision agriculture. Ag-Analytics uses sensors to collect 
soil, tillage, and yield-data for specific plots of farmland (https://
analytics.ag/Home/HowItWorks). Microsoft Azure stores the 
data and shares the information with farmers through user-
friendly APIs to lower costs, improve yields, and minimize the 
environmental cost of agriculture. 
AI is also assisting with labor shortages in agriculture. 
As society becomes more urbanized, the supply of labor con-
tinues to move toward urban centers. Seasonal agricultural 
demand is faced with consistent shortages. Companies like 
93
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Root.AI are developing robotic harvest systems to bridge the 
gap between supply and demand of labor during harvest. Ad-
vanced methods in autonomous robotics, computer vision, 
botany, and biotechnology form the basis for the production 
of large scale operations capable of detecting ripeness and 
continuous harvesting at the peak of efficiency.
Chatbots also enable farmers to share information and 
resolve problems in the supply chain. The proliferation of 
chatbots in AI is made possible through the use of advanced 
NLP frameworks. Farmers can turn to chatbots for difficulties 
in production planning and resource management that are 
common to agriculture.
Agricultural monitoring by whole-of-government systems, us-
ing a data fabric, can leverage resource production and pre-
vent state capture events from occurring in underrepresented 
regions. Many of the methods in agriculture are also relevant 
to mineral resources and energy production, so investment in 
these technologies is worth considering for the advancement 
of digital government systems.Ecology, Climate, and Conservation
Deforestation and land degradation are major problems 
for ecosystems. Governments and NGOs are using AI to 
monitor the steady decline of forests worldwide. By using 
multi-agent AI systems (MAS), resource utilization scenarios 
can better understand the impact that agricultural expansion 
has on forest decline. MAS has the ability to manage complex 
systems with several stakeholders to allow the exploration of 
alternative forest and land management systems. Moreover, 
MAS serves as a tool for learning and understanding, rather 
than predictive analysis. Reinforcement learning (RL) meth-
ods using computer vision and transfer learning are most suit-
able for forest management and conservation.
Climate change stemming from deforestation also requires 
a comprehensive understanding of additional factors in the 
overall health of both local and global ecosystems. Several 
AI subdomains are necessary for the comprehensive analysis 
of such a monumental topic. Table B.1 illustrates the various 
subdomains relating to AI that are currently employed for cli-
mate impact mitigation.
> > >
TABLE B.1 - AI for climate impact mitigation
Source: https://miro.medium.com/max/1400/0*7_Ilv_JRbf85ClQj.
Computer
Vision
NLP
Time-series 
analysis
Unsurpevised
learning
RL &
Control
Casual
inference
Uncertainty
qualification
Transfer
learning
Interpretable
ML
Other
Electricity Systems1 1.1 1.1
1.21 1.1 1.1
1.21.3 1.1 1.1
Transportation2.1
2.2
2.42 2.1
2.42 2.1
2.42 2.1
2.42
Building & Cities 3.2 3.3 3 3 3.1 3.1 3.3 3
Industry4.1
4.34.3 4.3 4 4.2
4.34.2
4.34.3
Farms & Forests5.1
5.3
5.45.2 5.4
CO2 Removal 6.3 6.3 6.3 6.2
Climate Prediction 7.1 7 7.3 7
Societal Impacts8.1
8.48.4 8.2
8.38.2 8.3 8.2 8.1 8.3
Solar Geoengineering9.3 9.4 9.3
9.49.2
T ools for Individuals 10.1 10.1 10.2 10.3 10.2 10.1 10.2 10.2
T ools for Society11.1 11.2
11.111.3 11.2
11.111.1
11.311.1 11 11.1 11.1
11.3
Education 12.2 12.1
Finance 13.2 13 13.2
94
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>The world’s oceans are under increasing threat due to 
human overpopulation.  A project called OceanMind is using 
satellites and AI to preserve biodiversity, protect the livelihoods 
of fishermen, and prevent slavery in the fishing industry. It col-
laborates with governments to prevent illegal, unreported, and 
unregulated fishing by analyzing vessel movements in real 
time. AI algorithms detect anomalous behavior that Ocean-
Mind shares with regulatory agencies to direct ocean patrols 
more efficiently.
In forest management and conservation, SilviaTerra is trans-
forming how conservationists and landowners measure and 
monitor forests (https://www.silviaterra.com). The system 
tracks an inventory of forest resources for the protection and 
management of ecological, social, and economic health. Sil-
viaTerra uses AI frameworks on Microsoft Azure to study the 
effects of climate change and improve habitats using high-res -
olution satellite imagery, U.S. Forest Service inventory, and 
field data to train AI models to measure forest values.
In species conservation to fight extinction, Wild Me is leverag-
ing computer vision, citizen science, and deep learning algo-
rithms to power Wildbook (http://www.wildbook.org/doku.php). 
Wildbook scans and identifies individual animals and species. 
Wildbook is notably an open source platform. It provides scal-
able and collaborative wildlife data storage and management, 
extensible easy-to-use software tools, API support, data expo-
sure to external biodiversity resources, and animal biometrics 
that support easy data access. This robust design for data in-
terchange using APIs makes it a stellar example of a system 
that will integrate well with a whole-government data fabric ar-
chitecture. (Wildbook, Software to Combat Extinction) Another 
project in the same domain is Protection Assistant for Wildlife Security (PAWS). PAWS uses AI to aid conservationists in the 
fight against poaching by utilizing AI for learning, planning, 
and behavior modeling. PAWS collects information from previ-
ous poaching activities and then generates predictions about 
poaching locations and optimal patrol routes, resulting in more 
effective patrols and better use of resources in the fight against 
poaching endangered animal species (Fang 2013). 
More technical information about the goal of tackling climate 
change with AI is available from a technical report published 
by a consortium of researchers from many prominent universi -
ties worldwide (Rolnick et. al. 2019). 
Urban Planning
In one prominent example, researchers leverage advanced 
methods in predictive analysis using AI for urban planning. 
By using cellular automata in conjunction with evolutionary al-
gorithms and AI, a mathematical model for predicting evolving 
spatial patterns examines the impact of policy and geography 
on the outcomes of various urban planning scenarios (Yang et. 
al. 2019). In plain English, this means they are using math to 
model the evolution of any urban environment over time. This 
framework optimizes Urban Development Demand by leverag-
ing a model to synthesize changes in urban growth boundar-
ies (UGB). The model uses historical observations of different 
time intervals and per-capita land requirements. Next, a patch-
based cellular automata (CA) model simulates urban growth by 
estimating urban development probability using a random for-
est machine learning algorithm (Figure B.2). 
Source: Yang et. al. 2019.> > >
FIGURE A.18. - Basic Blockchain Node
VotingRandom subset 3 Random subset 1
Tree 1
Class 2Random subset 2
Tree 1
Class 2Tree 3
Class 1Random subset 4
Tree 4
Class 1
Outcome: Class 1Datasets
95
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>The “patches” represent plots of land. Then, genetic algorithms optimize key model parameters, and finally the system aggregates 
land maps from multiple model runs to generate UGB alternatives. The random forest (RF) algorithm models a classification hier-
archy using a strategy that creates a “forest” of individual decision trees. RF is hardly the only model in AI, but it is the most useful 
in this case. Each “tree” in the RF model makes independent decisions based on the feature variables and a random selection of 
observations derived from training data. Final outputs are the resulting averages of the decisions of the individual trees, which is 
considered a “voting strategy” that generates the resulting outcome. The RF method is insensitive to outliers, noise, and overfitting. 
Figure B.3 illustrates the workflow of modules within this predictive UGB framework.
Source: Yang et. al. 2019.> > >
FIGURE B.3. - Workflow of Modules within Predictive UGB Framework
Trend extrapolations
Projections 
of urban demandProbability of urban
developmentEcological constraints
for urban developmentRandom Forest
PATCH-BASED CELLULAR AUTOMATA MODEL
DELINEATION OF URBAN GROWTH BOUNDARIOESOptimized using Genetic Algorithm
Organic Proportion
Isometry
Morphology
OperatorsUGBs under
different scenariosSimulated urban
land mapsNormal distributions
of patch sizeOrganic urban
growth procedureSpontaneous urban
growth procedurePopulation Protected
areasDriving 
factorsLand use/
cover
96
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Figure B.4 shows the CA patch generation function with a size of three cells. Think of the cells as patches of land with land use 
probabilities. Note again that the cells represent plots of land area.
Source: Yang et. al. 2019.> > >
FIGURE B.4. - Cellular Automata Patch Generation Function with a Size of Three Cells
Cells that are 
excluded from the 
seeding procedureSeed that survives the 
test to initialize an 
urban patch
Candidate cells whose 
probability is scaled by 
the ismetry parameterSelected central cell of the 
scanning window to take 
part in the survival test
Candidate cells that don’t 
survive the test during the 
self-growing procedureCells that don’t survive 
the test during the self 
growing procedure
Cells that survive the 
test and is added to 
the patchCandidate cells 
for self-growing 
of new patches
The self-growing procedureThe seeding procedure
The urban development
probability
Scaling to probability of 
overlapped neighbors using the 
isometry parameter
Randomly select another cell
from the candidate pool to take 
part in the surival testThe selected cell survives and 
is added to the patch, and its 
neighbors is added to the pool
The selected cell don’t survive 
the test and is excluded from the 
patch-growing procedureAdd the seed to the patch and 
randomly select another cell to 
take part in the survival test
Randomly select another cell 
from the candidate pool to take 
part in the survival testCentrally placing a scanning 
window on the seed and add its 
neighbors to the candidate pool
The selected cell survives and is 
added to the patch. The patch 
generation process ends.The pruning operation to 
eclude impossible cellsSurvival test to find seed 
from the candicate cellsA cell survives to be the 
seed to initialize a patch0.2 0.2 0.3 0.4 0.2 0.2
0.3 0.7 0.5 0.2 0.4 0.6
0.2 0.8 0.7 0.3 0.6 0.6
0.3 0.8 0.6 0.1 0.3 0.8
0.7 0.6 0.7 0.8 0.6 0.2
0.6 0.7 0.4 0.5 0.3 0.5
97
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Lastly, Figure B.5 illustrates the simulated and observed land map in 2009 and 2016. Map “a” is the observed land map, while map 
“b” is the simulated solution with the highest fitness score, meaning the best fitting results of the genetic algorithm. Note the ac-
curacy of the predictions. In the real world, this model was put to use in an undisclosed rapidly growing city in China and revealed 
high reliability in the simulation of urban growth and the delineation of UGBs.
The patch-based CA model, which represents urban growth 
as an organic and spontaneous process can simulate more 
realistic urban landscapes by coupling the spatial process with 
the pattern of urban development. The RF model can suc-
cessfully show the relationship between driving policy factors 
and the urban development probability. Key model parameter 
calibration is achieved through genetic algorithms that cap-
ture the landscape characteristics of historical urban changes 
quite well and can therefore be used for future projections. The results also suggest that empirical (observed) knowledge 
from historical observations can assist the genetic algorithm 
with avoiding overfitting, to some extent. Although this model 
leverages simple population projection methods, the factors 
that drive future urban development can be further enhanced, 
and government planners can derive a deeper understand -
ing and analysis of the resulting planning scenarios with more 
comprehensive data.Note: Map “a” is the observed land map; map “b” is the simulated solution with the highest fitness score.
Source: Yang et. al. 2019.> > >
FIGURE B.5. - Simulated and Observed Land Maps in 2009 and 2016: 
Part 2. 2016Part 1. 2009 
98
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>The introduction of IoT in cities around the world enables 
the use of AI in the management and planning of elec-
tricity. IoT smart meters transmit information over Wi-Fi using 
passive communication between a grid of meters, distributed 
in high and moderate density urban environments. The data 
allows energy companies to adjust electricity production to 
nearly real-time accuracy. Prior to the advent of smart grid 
technology, power companies needed to predict demand 
based on a combination of environmental predictions using 
weather and temperature forecasting and almanac predic -
tions. This led to massive inefficiencies and wasteful produc -
tion of electricity.
The invention of large scale data processing systems and 
introduction of data fabric infrastructure allowed power com-
panies to transition to consuming massive pipelines of infor-
mation about electricity use, thereby reducing the impact of 
electricity production on the environment and improving the 
overall efficiency of the electricity marketplace. The utilization 
of electricity in modern cities is now burgeoning as a result of widespread transition to connected electric vehicles, which 
serve as distributed reserves of electricity.
One notable example of AI in a data fabric stems from the 
inception of SmartGrid AI systems using a large scale data 
layer that transformed power grid utilization in Ontario, Cana -
da. The project serves over 70 regional distribution companies 
handling reads from over four million meters and processing 
over 100 million transactions per day (KX Systems 2014). A 
similar project is called FinGrid, which is run by the primary 
transmission provider for Finland (KX Systems 2018). FinGrid 
will process data from 3.7 million locations to deliver 15-min -
ute imbalance settlements between electricity suppliers and 
consumers, an EU regulatory requirement, by December 
2020. This architecture is called DataHub. The data migra-
tion and go-live planning are important examples of how exist-
ing systems can transition to entirely new data architectures 
with minimal disruptions to existing mission-critical services. 
Figure B.6 illustrates the proposed transition plan for FinGrid 
(Fingrid Datahub 2019). Energy and Smart Cities
> > >
FIGURE B.6. - Proposed Transition Plan for FinGrid
Source: Fingrid Datahub 2019.Q1
2020Q3
2020Q1
2021Q3
2021Q2
2020Q4
2020Q2
2021Q4
202121.2.2022Certification 
testing started: 
1. class test 
cases ran & 
accepted
1.1.2020
PHASE III:
Consistency check III 
(new IDs)
29.5.2020PHASE IV:
Consistency check and 
donwloading data to 
Datahub (T4) 30.3.2021PHASE V:
Second trial operation 
Consistency check (T5)
7.9.2021Market party 
has completed
Datahub 
certification
7.5.2021Market 
party’s trial 
operation II
30.9.2021Market party 
has pass the 
go-live dree 
rehearsal
7.21.2021
Market 
party trial 
operation I
30.6.2021Market party 
is ready 
to start 
go-live dress 
rehearsal 
7.11.2021Market party has 
been proven to be 
prepared to enter 
into production 
environment
21.12.2021PHASE V:
(First trial operation):
Consistency check
7.6.2021PHASE III:
Consistency check V (T3)
25.11.2020Go-live dress rehearsal:
Consistency check
30.11.2021Data Migration Phase 3 1/2020 - 12/2020 Phase 4 1/2021-5/2021 Phase 5 5 6/2021 - 2/20222019
99
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Figure B.7 illustrates the workflow of the data migration project for FinGrid using kdb+, which is defined here as a column-based 
relational time series database (TSDB) with in-memory database (IMDB) abilities. It is commonly used in high-frequency data sets 
needing storage and retrieval of large data sets at high speed.
For more information about Datahub and Fingrid, visit the Fingrid website at 
https://www.ediel.fi/en/datahub/business-processes/business-process-other-datahub-instructions.> > >
FIGURE B.7. - The Workflow of the Data Migration Project for FinGrid Using kdb+
Source: Fingrid Datahub (2019).Reference
dataFollow-upCheck reports,
migration reports
Checked 
data
Migration
reportsSource dataMarket parties
FingridSupport
DatahubMarket party portal
External data 
sourcesReporting Data checks Transfer
FINGRID
Service provider Solteq OyjDatahub TittaVerification
100
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Glossary>>>
Big DataOne or more databases containing extremely large data from various 
sources.
Data 
DominionThe scope of a government’s ownership and use of data, applications, 
and infrastructure defined by geographic, political, and national 
boundaries.
Data Fabric, 
Data LakeAn interconnected data storage infrastructure that provides a common set 
of interfaces and access control layers for “Big Data” operations spread 
across thousands of servers that may be geographically distributed.
Data 
SiloAn architecture that is isolated due to the absence of a common 
application programming interface (API) for inter-process communication.
DocumentA self-contained JSON object specifying the attributes and values in 
a comprehensive unit of information that is iterable, transactable, and 
mutable.
Dummy 
VariableA binary feature that indicates that an observation is (or is not) a member 
of a category.
FeaturesThe input attributes that are used to predict the target, which may be 
numerical or categorical.
Feature 
EngineeringA form of machine learning optimization that leverages collected data to 
extract features.
Generative 
Adversarial 
NetworkGANs are a class of machine learning techniques that consist of two 
simultaneously trained models competing as adversaries with one another: 
one (the Generator) trained to generate fake data, and the other (the 
Discriminator) trained to discern the fake data from real examples.
Ground Truth The value of a known target variable or label for a training or test set.
Instance 
(Or Example)A single object, observation, transaction, or record.
ModelA mathematical object describing the relationship between features and 
the target.
Online Machine 
LearningA form of machine learning in which predictions are made, and the model 
is updated, for each example.
PreprocessingThe process of cleaning and correcting errors and inconsistencies in 
collected data. Also referred to as data munging or data wrangling.
Protocol Buf -
fers, Protobuf-
fers, ProtobufA language-neutral, platform-neutral, extensible mechanism for serializing 
structured data.
Recall Using a model to predict a target or label.
Supervised 
Machine 
LearningMachine learning in which, given examples for which the output value is 
known, the training process infers a function that relates input values to 
the output.
Target 
(Or Label)The numerical or categorical (label) attribute of interest. This is the 
variable to be predicted for each new instance.
Training Data The set of instances with a known target to be used to fit a ML model.
Unsupervised 
Machine 
LearningMachine learning techniques that do not rely on labeled examples, but 
rather attempt to find hidden structure in unlabeled data.
101
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>References>>>
ACT-IAC (American Council for Technology and Industry Advisory Council). 2020. Artificial 
Intelligence (AI) Playbook for the U.S. Federal Government . Fairfax, VA: ACT-IAC.  
https://www.actiac.org/system/files/AI%20Playbook_1.pdf.
ANI (Asian News International). 2019. “Bahrain and UK First in the World to Pilot New 
Artificial Intelligence Procurement Guidelines Across Government.” Business Standard,  
July 4, 2019. https://www.business-standard.com/article/news-ani/bahrain-and-uk-
first-in-the-world-to-pilot-new-artificial-intelligence-procurement-guidelines-across-
government-119070401389_1.html.
Bansal, Aayush. 2018. “Donald Trump to Barack Obama.” August 11, 2018. YouTube 
video, 0:06. https://www.youtube.com/watch?v=F51RCdDIuUw.
Berryhill, Jamie, Kévin Kok Heang, Rob Clogher, and Keegan McBride. 2019. Hello, 
World: Artificial Intelligence and its Use in the Public Sector. Paris: OECD Publishing.  
https://oecd-opsi.org/wp-content/uploads/2019/11/AI-report-Online.pdf.
Brundage, Miles, Shahar Avin, Jack Clark, Helen Toner, Peter Eckersley, Ben Garfinkel, 
Allan Dafoe, Paul Scharre, Thomas Zeitzoff, Bobby Filar, Hyrum Anderson, Heather Roff, 
Gregory C. Allen, Jacob Steinhardt, Carrick Flynn, Seán Ó hÉigeartaigh, Simon Beard, 
Haydn Belfield, Sebastian Farquhar, Clare Lyle, Rebecca Crootof, Owain Evans, Michael 
Page, Joanna Bryson, Roman Yampolskiy, and Dario Amodei. 2018. The Malicious Use 
of Artificial Intelligence: Forecasting, Prevention, and Mitigation . Oxford, UK: Future of 
Humanity Institute. https://maliciousaireport.com/.
Buchanan, Ben. 2020. 2020. “A National Security Research Agenda for Cybersecurity and 
Artificial Intelligence.” CSET Issue Brief, Center for Security and Emerging Technology, 
Washington, DC.  
https://cset.georgetown.edu/wp-content/uploads/CSET-A-National-Security-Research-
Agenda-for-Cybersecurity-and-Artificial-Intelligence.pdf.
Bughin, Jacques, Jeongmin Seong, James Manyika, Michael Chui, and Raoul Joshi. 
2018. “Notes from the AI Frontier: Modeling the Impact of AI on the World Economy.” 
report, McKinsey & Company, Washington, DC.  
https://www.mckinsey.com/featured-insights/artificial-intelligence/notes-from-the-ai-
frontier-modeling-the-impact-of-ai-on-the-world-economy#.
Chen, Stephen. 2019. “Is Fraud-Busting AI Systems Being Turned Off for Being Too 
Efficient?” South China Morning Post,  February 4, 2019. https://www.scmp.com/news/
china/science/article/2184857/chinas-corruption-busting-ai-system-zero-trust-being-
turned-being.
102
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Chilamkurthy, Kowshik. 2020. “Reinforcement Learning for Covid-19: Simulation and 
Optimal Policy.” Towards Data Science  (blog), March 31, 2020.  
https://towardsdatascience.com/reinforcement-learning-for-covid-19-simulation-and-
optimal-policy-b90719820a7f.
Coursera. 2019.
Craddock, M. 2019. “UN Global Platform.” Retrieved June 27, 2020, from https://unstats.
un.org/unsd/bigdata/conferences/2019/presentations/seminar/day1/5th%20Big%20
Data%20External%20Workshop%20Slides%20-%20UN%20Global%20Platform.pdf.
Dandekar, Raj, and George Barbastathis. 2020. “Quantifying the Effect of 
Quarantine Control in Covid-19 Infectious Spread Using Machine Learning.” 
medRxiv; 2020. DOI: 10.1101/2020.04.03.20052084. https://www.medrxiv.org/
content/10.1101/2020.04.03.20052084v1.
Data Center Map. 2020.
Dignan, Larry. 2017. “IBM’s Rometty Lays Out AI Considerations, Ethical Principles.” 
Between the Lines  (blog), June 17, 2017. https://www.zdnet.com/article/ibms-rometty-
lays-out-ai-considerations-ethical-principles/.
Dutton, Tim. 2018. Building an AI World Report On National and Regional AI Strategies. 
Toronto, Canada: CIFAR. https://www.cifar.ca/docs/default-source/ai-society/
buildinganaiworld_eng.pdf.
EC (European Commission). 2019. Ethics Guidelines for Trustworthy AI. An independent 
Report by the High-Level Expert Group on Artificial Intelligence. Brussels: European 
Commission.
Eykholt, Kevin, Ivan Evtimov, Earlence Fernandes, Bo Li, Amir Rahmati, Chaowei Xiao, 
Atul Prakash, Tadayoshi Kohno, and Dawn Song. 2018. “Robust Physical-World Attacks 
on Deep Learning Models.” arXiv:1707.08945v5. https://arxiv.org/abs/1707.08945. 
Fang, Fei. 2013. “Protection Assistant for Wildlife Security.” Societal Computing (Applied 
Systems and Infrastructure), November 13, 2013. https://sc.cs.cmu.edu/research-
detail/102-protection-assistant-for-wildlife-security.
Federico, C., and T. Thompson. 2019. “Do IRS Computers Dream About Tax Cheats? 
Artificial Intelligence and Big Data in Tax Enforcement and Compliance .” Journal of Tax 
Practice & Procedure  February–March 2019: 43–47. https://www.crowell.com/files/2019-
Feb-March-Do-IRS-Computers-Dream-About-Tax-Cheats-Federico.pdf.
Feldstein, Steven. 2019. “The Global Expansion of AI Surveillance.” Working 
Paper, Carnegie Endowment for International Peace, Washington, DC. https://
carnegieendowment.org/files/WP-Feldstein-AISurveillance_final1.pdf.
Fingrid Datahub. 2019. Go-Live Plan for Centralized Information Exchange Services 
(Datahub) for Electricity Market . Helsinki, Finland: Fingrid Datahub Oy. (May 28, 2019). 
https://www.ediel.fi/sites/default/files/Go-Live%20plan%20for%20centralised%20
information%20exchange%20services%20%28Datahub%29%20for%20electricity%20
market.pdf.
103
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Fjeld, Jessica, Nele Achten, Hannah Hilligoss, Adam Nagy, and Madhulika Srikumar. 
2020. Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based 
Approaches to Principles for AI. Cambridge, MA: Berkman Klein Center for Internet and 
Society. https://dash.harvard.edu/handle/1/42160420.
Floridi, Luciano, Josh Cowls, Monica Beltrametti, Raja Chatila, Patrice Chazerand, 
Virginia Dignum, Christoph Luetge , Robert Madelin, Ugo Pagallo, Francesca Rossi, 
Burkhard Schafer, Peggy Valcke, and Effy Vayena. 2018. AI4People’s Ethical Framework 
for a Good AI Society: Opportunities, Risks, Principles, and Recommendations . Brussels: 
Atomium–European Institute for Science, Media and Democracy Atomium. 
https://www.eismd.eu/wp-content/uploads/2019/11/AI4People%E2%80%99s-Ethical-
Framework-for-a-Good-AI-Society_compressed.pdf.
Gartner. 2019. “Gartner Identifies Top 10 Data and Analytics Technology Trends for 2019.” 
Press Release, February 18, 2019. 
https://www.gartner.com/en/newsroom/press-releases/2019-02-18-gartner-identifies-top-
10-data-and-analytics-technolo.
GDS (Government Digital Service) and OAI (Office for Artificial Intelligence). 2019. “A 
Guide to Using Artificial Intelligence in the Public Sector.” GOV.UK, June 10.
 https://www.gov.uk/government/collections/a-guide-to-using-artificial-intelligence-in-the-
public-sector#contents.
Government of Canada. 2019a. “Directive on Automated Decision-Making.” Government 
of Canada, modified February 5, 2019.
 https://www.tbs-sct.gc.ca/pol/doc-eng.aspx?id=32592.
IEEE (Institute of Electrical and Electronics Engineers). 2019. Ethically Aligned Design: A 
Vision for Prioritizing Human Well-being with Autonomous and Intelligent Systems,  First 
Edition. Piscataway, NJ: IEEE.
https://standards.ieee.org/content/dam/ieee-standards/standards/web/documents/other/
ead1e.pdf .
IOTA (Intra-European Organisation of Tax Administrators). 2018. Impact of Digitalisation 
on the Transformation of Tax Administrations . Budapest, Hungary: IOTA. 
https://www.iota-tax.org/sites/default/files/publications/public_files/impact-of-digitalisation-
online-final.pdf.
ITU (International Telecommunication Union). 2019. Measuring Digital Development: 
Facts and Figures 2019 . Geneva: ITU. 
https://www.itu.int/en/ITU-D/Statistics/Documents/facts/FactsFigures2019.pdf.
Kernighan, Brian and Rob Pike. 1984. The Unix Programming Environment . New Jersey: 
Prentice Hall.
KX Systems. 2014. “KX Systems’ kdb+ Chosen by Ontario Electric Grid Operator.” Press 
Release, June 22, 2014. 
https://kx.com/news/kdb-technology-chosen-for-retrieval-and-querying-of-smart-meter-
data-processed-by-ontario-smart-metering-system/.
KX Systems. 2018. “European Energy Market Contract Win with FinGrid.” Press Release, 
104
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>July 16, 2018. https://kx.com/news/european-energy-market-contract-win/.
Lane, Hobson, Hannes Hapke, and Cole Howard. 2019. Natural Language Processing in 
Action. Shelter Island, NY: Manning Publications.
McKinsey Global Institute. 2017. Harnessing Automation for a Future that Works. 
Washington, DC: McKinsey & Company.
Mazzucato, Mariana. 2015. “Re-Igniting Public and Private Investments in Innovation.” 
Report presented at the U.S. Senate Forum of the Middle Class Prosperity Project 
“Building the Economy of the Future: Why Federal Investments in Science and 
Innovation Matter.” Washington, DC, July 27. https://marianamazzucato.com/wp-content/
uploads/2015/07/Mazzucato-Statement-Middle-Class-Prosperity-Project-.pdf.
Mozur, Paul, and Lin Qiqing. 2019. “Hong Kong Takes Symbolic Stand Against 
China’s High-Tech Controls.” New York Times, October 3, 2019. https://www.nytimes.
com/2019/10/03/technology/hong-kong-china-tech-surveillance.html.
Nakasone, Keith. “Game Changers: Artificial Intelligence Part II; Artificial Intelligence and 
the Federal Government.” Statement of Keith Nakasone, Deputy Assistant Commissioner, 
Acquisition Operations, Office of Information Technology Category (ITC), U.S. General 
Services Administration, before the Subcommittee on Information Technology of the 
Committee on Oversight and Government Reform, Washington, DC, March 7, 2018. 
https://republicans-oversight.house.gov/wp-content/uploads/2018/03/Nakasone-GSA-
Statement-AI-II-3-7.pdf.
2154 Rayburn House Office Building
Ntoutsi, Eirini., Pavlos Fafalios, Ujwal Gadiraju, Vasileios Iosifidis, Wolfgang Nejdl, Maria-
Esther Vidal, Salvatore Ruggieri, Franco Turini, Symeon Papadopoulos, Emmanouil 
Krasanakis, Ioannis Kompatsiaris, Katharina Kinder-Kurlanda, Claudia Wagner, Fariba 
Karimi, Miriam Fernandez, Harith Alani, Bettina Berendt, Tina Kruegel, Christian Heinze, 
Klaus Broelemann, Gjergji Kasneci, Thanassis Tiropanis, and Steffen Staab. 2020. “Bias 
in Data-Driven Artificial Intelligence Systems—An introductory Survey.” WIREs Data 
Mining and Knowledge Discovery  10 (3).
O’Brien, Tim, Steve Sweetman, Natasha Crampton, and Venky Veeraraghavan. 2020. 
“How Global Tech Companies Can Champion Ethical AI.” World Economic Forum Annual 
Meeting, Davos-Klosters, Switzerland, January 21-24, 2020. 
https://www.weforum.org/agenda/2020/01/tech-companies-ethics-responsible-ai-
microsoft/.
OECD (Organisation for Economic Co-operation and Development). 2016. Preventing 
Corruption in Public Procurement . Paris: OECD Publishing. http://www.oecd.org/gov/
ethics/Corruption-Public-Procurement-Brochure.pdf.
OECD (Organisation for Economic Co-operation and Development). 2019. 
“Recommendation of the Council on Artificial Intelligence.” OECD Legal Instruments, May 
21, 2019. https://legalinstruments.oecd.org/en/instruments/OECD-LEGAL-0449.
105
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>PDPC (Personal Data Protection Commission). 2020. Model Artificial Intelligence 
Governance Framework  (Second Edition). 2020. Mapletree Business City, Singapore: 
Infocomm Media Development Authority and PDPC. https://www.pdpc.gov.sg/-/media/
files/pdpc/pdf-files/resource-for-organisation/ai/sgmodelaigovframework2.pdf.
Perrault, Raymond, Yoav Shoham, Erik Brynjolfsson, Jack Clark, John Etchemendy, 
Barbara Grosz, Terah Lyons, James Manyika, Saurabh Mishra, and Juan Carlos Niebles. 
2019. The AI Index 2019 Annual Report . Stanford, CA: AI Index Steering Committee, 
Human-Centered AI Institute, Stanford University.
Public-Private Analytic Exchange Program. 2018. AI: Using Standards to Mitigate Risks . 
Washington, DC: U.S. Department of Homeland Security. 
https://www.dhs.gov/sites/default/files/publications/2018_AEP_Artificial_Intelligence.pdf.
Rolnick, David, Priya L. Donti, Lynn H. Kaack, Kelly Kochanski , Alexandre Lacoste , 
Kris Sankaran , Andrew Slavin Ross , Nikola Milojevic-Dupont , Natasha Jaques , Anna 
Waldman-Brown , Alexandra Luccioni , Tegan Maharaj , Evan D. Sherwin , S. Karthik 
Mukkavilli , Konrad P. Kording ,Carla Gomes, Andrew Y. Ng, Demis Hassabis , John C. Platt, 
Felix Creutzig , Jennifer Chayes , and Yoshua Bengio . 2019. “Tackling Climate Change with 
Machine Learning.” arXiv: arXiv:1906.05433v2. https://arxiv.org/pdf/1906.05433v2.pdf .
Rossi, Francesca. 2019. “Building Trust In Artificial Intelligence.” Journal of International 
Affairs 72 (1). https://jia.sipa.columbia.edu/building-trust-artificial-intelligence.
Stiglitz. 2018. https://royalsociety.org/science-events-and-lectures/2018/09/you-and-ai/
The Open Group. 2018. The Open Group Base Specifications Issue 7. San Francisco: 
The Open Group. https://pubs.opengroup.org/onlinepubs/9699919799/ .
UN (United Nations). 2004. United Nations Convention Against Corruption . New York: 
United Nations.
https://www.unodc.org/documents/treaties/UNCAC/Publications/Convention/08-50026_E.pdf.
UNESCO (United Nations Educational, Scientific, and Cultural Organization). 2020. 
“UNESCO Appoints International Expert Group to Draft Global Recommendation on 
the Ethics of AI.” Press Release, March 11, 2020. https://en.unesco.org/news/unesco-
appoints-international-expert-group-draft-global-recommendation-ethics-ai .
U.S. Department of the Treasury. 2017. 2017 Annual Privacy, Data Mining, and Section 
803 Reports . Washington, DC: Department of the Treasury. https://home.treasury.gov/
system/files/236/annual-privacy-data-mining-Report-and-section-803-Report-final-2.pdf.
van Eyk, E., L. Toader, S. Talluri, L. Versluis, A. Uta, and A. Iosup. 2018. “Serverless Is 
More: From PaaS to Present Cloud Computing.” IEEE Internet Computing 22 (5): 8–17.
Venkateswaran, T.V. 2020. “AI Isn’t Unbiased because Humans are Biased.” The Eighth 
Column  (blog), February 19, 2020. https://thefederal.com/the-eighth-column/artificial-
intelligence-algorithms-unbiased-humans-biased/.
WEF (World Economic Forum). 2018. Harnessing Artificial Intelligence for the Earth
Cologny, Switzerland: World Economic Forum. 
http://www3.weforum.org/docs/Harnessing_Artificial_Intelligence_for_the_Earth_
Report_2018.pdf.
106
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>WEF (World Economic Forum). 2020. “AI Procurement in a Box: AI Government 
Procurement Guidelines.” Toolkit June 2020, World Economic Forum, Cologny, 
Switzerland. http://www3.weforum.org/docs/WEF_AI_Procurement_in_a_Box_AI_
Government_Procurement_Guidelines_2020.pdf.
West, Darrell. 2018. “Will Robots and AI Take Your Job? The Economic and Political 
Consequences of Automation.” TechTank (blog), April 18, 2018. https://www.brookings.
edu/blog/techtank/2018/04/18/will-robots-and-ai-take-your-job-the-economic-and-political-
consequences-of-automation/.
World Bank. 2016. World Development Report 2016: Digital Dividends. Washington, DC: 
World Bank. https://www.worldbank.org/en/publication/wdr2016.
Yang, J., J. Gong, W. Tang, Y. Shen, C. Liu, and J. Gao. 2019. “Delineation of Urban 
Growth Boundaries Using a Patch-Based Cellular Automata Model under Multiple Spatial 
and Socio-Economic Scenarios.” Sustainability 11 (21): 6159. 
https://www.mdpi.com/2071-1050/11/21/6159.
Zheng, Stephan, Alex Trott, Sunil Srinivasa , Nikhil Naik , Melvin Gruesbeck , David 
Parkes, and Richard Socher . 2020. “The AI Economist: Improving Equality and 
Productivity with AI-Driven Tax Policies.” Salesforce Research  (blog), April 28, 2020. 
https://blog.einstein.ai/the-ai-economist/ .
107
EQUITABLE GROWTH, FINANCE & INSTITUTIONS INSIGHT | GOVTECH LAUNCH REPORT AND SHORT-TERM ACTION PLAN >>>Supported by the GovT ech Global Partnership: www.worldbank.org/govtech
Republic of Korea