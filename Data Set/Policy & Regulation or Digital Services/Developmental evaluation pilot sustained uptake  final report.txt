 
DISCLAIMER: The views expressed in this publication do not necessar ily reflect the views of the United States Agency for International 
Development or the United States Government.  
 DEVELOPMENTAL EVALUATI ON PILOT    
SUSTAINED UPTAKE  
Final Report March 2019 
  
i 
 CONTENTS  
ACRONYM LIST  II 
EXECUTIVE SUMMARY  V 
PART 1: WHAT WAS IT?  1 
PART 2: WHY DID WE D O IT?  3 
PART 3: WHAT DID IT LOOK LIKE?  4 
PART 4: WHAT WERE TH E RESULTS?  8 
PART 5: WHAT HAPPENE D WHEN?  14 
ANNEXES  17 
ANNEX 1: UPTAKE PILO T INFOGRAPHIC  18 
ANNEX 2: RESEARCH QU ESTION MEMOS  19 
ANNEX 3: M AJOR DELIVERABLES  33 
ANNEX  4: SUSTAINED UPTAKE  PILOT CODING HIERAR CHY AND FREQUENCIES  34 
ANNEX 5: METHODOLOGI CAL DETAILS OF EVALU ATIVE A CTIVITIES  42 
ANNEX 6: WORKS CITED  84 
 
   
ii 
 ACRONYM LIST  
BFS   Bureau for Food Security  
BTGx    Beyond the Grid x  
CDD    Center for Digital Development  
D2FTF    Digital Development for Feed the Future  
DDI   Development, Democracy, and Innovations Bureau  
DE   Developmental Evaluation  
DEPA -MERL   Developmental Evaluation Pilot Activity  
DFS   Digital Financial Services  
DI   Digital Inclusion  
DIV   Development Innova tion Vent ures 
EIA   Office of Evaluation, Impact , and Assessment  
Evaluator   Embedded Evaluator  
FCR    Findings, Conclusions, and Recommendations  
FTF   Feed the Future  
HQ   Headquarters  
iDesign    Innovation Design and Advisory team   
JPP   Joint Partnership Plan  
Lab   Global Development Lab  
LOE   Level of Effort  
LWP   Lab-Wide Priority  
MERLIN   Monitoring, Evaluation, Research, and Learning Innovations  
OAA    Office of Acquisition and Assistance  
OU   Operating Unit  
PPL   Office of Policy, Planning, and Learning  
PRP   Program, Resource, and Policy Burea u  
PSP   Program and Strategic Planning Office  
RQ   Research Question  
Search    Search for Common Ground  
SI   Social Impact, Inc.  
SOGE    Scaling Off -Grid Energy  
SOW    Scope of Work   
iii 
 T3   Transformation Task Team  
USAID    United States Agency for International Development  
WDI   William Davidson Institute at the University of Michigan  
   
v 
 EXECUTIVE SUMMARY  
Over the course of its history, the U.S. Global Development Lab ( hereinafter , “the Lab”)  of the United 
States Agency fo r International Development (USAID)  has evolved its programming related to scaling, 
adoption, acceleration, and uptake. This ev olution  occurred in response to the Lab’s charter to “source, 
test, and scale” development solutions, and was also informed by ad  hoc learnings from previous efforts. 
Following the conception of the Lab  Wide Priorities (LWPs), the Lab agreed to undertake active lear ning 
to enable the m to better understand and implement  different approaches to scale/sustained uptake. Over 
the course of nearly two years, the Development al Evaluation Pilot Activity  (DEPA -MERL ) supported Lab 
teams using a developmental evaluation ( DE) approach. The DE approach helped several Lab teams  and 
offices  – including Digital Development for Feed the Future (D2FTF ), Scaling Off -Grid Energy (SOGE), 
Digital Financial Services (DFS), Digital Inclusion (DI), and the Office of Evaluation , Impact , and Assessment 
(EIA) – to rigorously collect, analyze, and disseminate learnings regarding the sustained uptake of 
innovation s these teams seek to promote within and beyond USAID. The DE appealed to the teams given 
its innovative  and rigorous nature , and most  importantly , its emphasis on providing timely , on-demand,  
and use-focused deliverables.  
Over the course of its engagement , the Sustained Uptake DE worked with seven teams in the Lab over 
22 months . In doing so, teams engaged in capacity building around su stainability planning and answered 
the following question s.  
Sustained Uptake DE Evaluation Questions  
1. What are the conditi ons and working relationships  necessary in the LWPs, the Lab, and its 
partners to achieve sustained uptake internally (Missions and Bu reaus) and externally?  
2. How do we determine which current Lab approaches are most effective at sustained uptake ? 
What has b een the perceived and real value add of the approaches? What can we learn from Lab 
uptake models?  
3. What are the replicable principles/e lements  from the different sustained uptake models and 
how should others apply them to a different context?  
4. How does the L ab balance sustained uptake initiatives that are  internal versus external ? What 
impact (internal or external) does the Lab value more?  Where can the Lab have the most impact?  
In order to answer these questions, the Developmental Evaluator (here inafter, “ Evaluator ”) employed 
appreciative inquiry, positive deviance case studies, process tracing, outcome harvesting, and various 
facilitated work with the DE teams. These evaluative efforts contributed to an iterative database used 
throughout the DE, resulting in  evidence informed by 474 sources and 1 ,675 unique data points. The 
findings, conclusions, recommendations, and adaptive work with the  DE teams resulted in the following 
key outcomes from the Sustained Uptake DE.  
Key Outcomes : 
• The DE identified effective and efficient models to achieve sustained uptake with both internal and 
external audiences.  
• The DE helped six teams develop and initia te implementation of Sustainability Plans and exit 
strategies, thereby improving sustainability of programming and increas ing the understanding of 
pathways to scale for the teams ’ respective innovations.   
vi 
 • The DE created and disseminated the Mission Engagement Playbook  – a how -to manual built on 
DE evidence  of how to work with USAID Missions effectively . This helped to improve  the 
efficiency and effectiveness of Mission -Headquarters (HQ)  relationships for teams who 
implemented the guidance.  
• The DE imp roved  working  relationships  between  Bureaus  and with private  sector  partners.  
• The DE helped team s’ design  pathways  to scale,  including the ability  to assess  ecosystem -
level impact.  
• The DE improved team culture for five teams focusing on developing action -oriented, adaptive 
decision -making.  
Overall, the Sustained Uptake DE provided extensive evaluative and adaptive management suppor t to the 
Lab, providing the m with evidence on effective and efficient models for both internal and external sustained 
uptake . The DE further  improved teams ’ capab ilities of achieving ecosystem -level outcomes, and provided 
tools to continue this work moving  forward.    
1 
 PART 1 : WHAT WAS IT? 
OVERVIEW  
This section provides a high -level overview of what t he Sustained Uptake DE pilot entailed, including its  
timeline, participants, and focal areas . It also provides an overview of DE, including the aspects of the 
approach which  made it suitable for the learning needs of the pilot’s various stakeholders. For a n 
infographic depiction of the overview, see Annex 1.  
BACKGROUND  
THE DEVELOPMENTAL EV ALUATION PILOT ACTIV ITY 
Programs in complex settings or with untested the ories of change often face a challenge when trying to 
use traditional mid -term or end-term evaluations to assess their impact. In such programs, traditional 
evaluations may fail to provide useful information in a timely fashion or capture important outcome s not 
defined at the outset. To help address this issue, EIA at USAID funded DEPA -MERL —a mechanism to 
pilot the use of  DE and assess its feasibility and effectiveness in t he USAID  context. DEPA -MERL is an 
initiative under the Lab’s Monitoring, Evaluation, Research and Learning Innovations (MERLIN) program  
and is  implemented by Social Im pact, Inc. (SI) with partners  Search for Common Ground ( hereinafter, 
“Search ”), and the William Davidson Institute at the University of Michigan (WDI).  
DE is an evaluative a pproach aimed at facilitating continuous adaptation of interventions. It includes having 
one or more evaluators integrated into the implementation team , usually on a full -time basis. These 
Developmental Evaluators  work embedded  with teams to contribute to modifications in program design 
and targeted outcomes throughout implementation. They participate in team meetings, document 
decisions , processes, and dynamics, and collect and analyze data – feeding it back to the teams on a regular 
basis. DEs are methodo logically agnostic and utilization -focused. They adjust research questions and 
methodological and analytic techniques as the project changes , and deliver contextualized and emergent 
findings on an ongoing basis.  
THE SUSTAINED UPTAKE PILOT  
Over the course  of its history,  the Lab has evolved its 
programming related to scaling, adoption, 
acceleration, and uptake . This ev olution  occurred  in 
response its charter to “source, test, and scale” 
development solutions, and was also informed by  ad 
hoc learnings from previous efforts. Following the 
conceptio n of the LWPs, the Lab agreed to undertake 
active learning to enable th em to get smarter about 
the viability of different approaches to scale/sustain 
uptake . Over  the course of nearly two  years , DEPA -
MERL supported  Lab teams  using the DE approach. 
The DE a pproached helped several Lab teams –
including  D2FTF, SOGE,  DFS, DI, and EIA – to The Global Development Lab focuses on 
integr ating their innovations, tools, and 
approaches across top Agency priorities an d 
Presidential initiatives. They do this in program 
design, as well as in building specialized teams 
to tackle complex development issues, such as 
the Ebola crisis, digital devel opment in 
agriculture, and scaling the off -grid energy 
market. Lab Wide Priori ty Teams are formed 
by bringing together staff with diverse skillsets 
to tackle these complex issues on a limited 
basis, generally a three -year engagement.  
(USAID, 2018)   
2 
 rigorously collect, analyze, disseminate, and disseminate  learnings regarding the  sustained uptake  of 
innovations that these teams see k to promote within and beyond USAID . The DE appealed to the teams 
given its innovative  and rigorous nature , and most importantly , its emphasis on providing timely and easy -
to-use deliverables.  
TIMING OF UPTAKE PILOT  
DEPA -MERL and the Lab teams anticipat ed a start date of October 2016 and a  pilot duration of 12 months. 
However, delays in the hiring of the Evaluator (see “Hiring of Evaluator” section) postponed the start of 
evaluative work until March 2017. The teams ultimately agreed to two , extensions  of three months each , 
which kept the DE  running through December 2018 for a total of 22 months.  
  
OVERVIEW OF KEY PILOT  STAKEHOLDERS  
The DE primarily  examined the work of four teams at USAID’s Global Development Lab  in the first two 
phases :  
• The work of th e D2FTF seeks to promote the use of technology to accelerate the outcomes of 
the Feed the Future (FTF) program, which is spearheaded by the Bureau of Food Security (BFS).  
• A part of the Power Africa Initiative, SOGE  works with internal USAID and external s takeholders 
to increase the use of off -grid energy solutions throughout the continent.  
• DI facilitates the expansion of internet access in USAID presence countries to accelerate the 
Agency’s  development objectives.  
• The DFS team works to create inclusive, p ro-poor financial sectors that serve the needs of 
governments  and underserved populations . 
 
3 
 EIA also played a significant role , helping to guide the technical direction of the pilot, using and promoting 
the DE resul ts, and —in the final phase —directly benefi ting from some of the implementation of key 
recommendations.  
In the DE’s final phase , two additional  Lab teams engaged in sustainability planning , implementation , and 
training:  
• The Innovation Design and Advisory team   (iDesign) team sources, tests, and i ntegrates 
innovative design practice at  USAID  through training, development of knowledge products, and 
collaboration with Office of Policy, Planning, and Learning  (PPL) ; Office of Acquisition and 
Assistance  (OAA) ; and others  to improve overall program desi gn guidance, policy, and 
opportunities for USAID.  
• The Program and Strategic Planning Office ( PSP) provide s support to teams across the Lab, 
providing guidance on  monitoring and semi -annual portfolio reviews, and serving as the program 
and budget office fo r the Lab  
PART 2: WHY DID WE DO IT? 
PILOT PROBLEM STATEM ENT  
As noted above, the Lab has evolved its programming related to scaling, adoption, acceleration, and uptake, 
in response to both its charter, and from ad hoc learnings from previous efforts to sc ale. Following the 
conception of the LWPs, the Lab agreed to undertake active learning to enable the m to get smarter about 
the viability of different approaches t o scale/sustained uptake. At the time of the pilot’s inception however, 
that learning had not been approached systematically. The premium that DE places on learning from and 
about processes meant that the DE was well -suited to address this challenge, enabl ing the various 
programs and entities to learn from one another effectively and respond quickl y to the rapidly evolving 
environment. This was of particular importance once USAID began a major organizational restructuring 
(hereinafter , “Transformation”). Through the Transformation, USAID is aligning  “its framework and 
foundation to remain dynamic, i mpactful, and capable of operationalizing Administrator Green’s vision to end the 
need for foreign assistance .” (USAID , 2018) 
ORIGINAL MOTIVATION FOR PILOT  
In the summer of  2016, the D2FTF team approached EIA about conducting a DE of their work on 
integra ting technology into FTF programming through the DEPA -MERL mechanism. Following preliminary 
discussions on the scope of work (SOW) however, D2FTF felt that it w as important to leverage learnings 
from other teams within the Lab and thus put forward partial funding for the pilot under the condition 
that another team contribute d to and participate d in the DE. EIA recruited the SOGE  team (formerly  
known as “Beyond th e Grid x [BTGx]”) as D2FTF’s counterpart . EIA also agreed to subsidize the pilot to 
make it more  financially accessible to the LWP teams. EIA was also  a natural home for any cross -team 
learnings , and thus  had a stake in documenting findings about both scaling across teams and also  the 
innovative evaluation approaches used to obtain them.  
Though thei r work is quite different, for the purposes of the initial SOW development, the D2FTF and 
SOGE teams agreed to focus on their efforts to “scale” their respectiv e innovations. This was agreed upon  
4 
 at the outset given the Lab’s mandate to house expertise reg arding how to scale, though stakeholders 
within the Lab felt that it had not dedicated sufficient resources to learn about how to scale , nor to  share 
those lear nings with the broader Agency. DEPA -MERL worked with these two teams to develop a revised 
SOW co dified in a Joint Partnership Plan (JPP). At the time of JPP development, the teams envisioned a 
12-month pilot with a focus on the following:  
“The DE will help identify the different types and levels of scaling attempted and achieved by Lab 
programs, par ticularly D2FTF and BTGx  [SOGE] . The DE will help the Lab evaluate and learn in 
real time and capture that learning into a more comprehensive report about th e effectiveness of its 
scaling efforts. In addition, the DE will explore how scaling works in diffe rent USAID sub -contexts. 
This will guide both current and future Lab scaling objectives and priorities, including identification 
of future [Lab Wide Prioriti es] and similar efforts.”  (Griswold, et. al ., 2016) 
Given the very different modalities of the D2FT F and SOGE teams, the focus of the DE evolved through 
joint discussions held early in the planning process. Namely, the focus on “scaling” shifted to “sustai ned 
uptake”  attempted and achieved by Lab programs. The term “sustained uptake” refers to the adopt ion of 
innovations promoted by Lab teams by USAID Missions (or external stakeholders) beyond the direct 
period of engagement with those Lab teams.  
The DE ai med to help the Lab evaluate and learn about this subject in real time and capture that learning 
into a more comprehensive report about the effectiveness of its sustained uptake efforts. In addition, the 
DE explored how sustained uptake works in different USAID sub -contexts. Buyers of the DE envisioned 
that this would guide both current and future Lab s caling objectives and priorities, including identification 
of future LWPs and similar efforts.  
As noted above, t he SOW and timeline of the pilot expanded twi ce over the course of the DE. The details 
of these expansions —including the motivations for each —are detailed in Part 3 . 
PART 3: WHAT DID IT LOOK LIKE? 
OVERVIEW  
The focus and participants of the DE varied over its 2 2-month duration. This section details the three 
major phases of the pilot, including the motivation, focus, and stakeholders involved in ea ch phase.  
Phase 1 and 2 offered an opportunity to conduct both individual and comparative analysis across the 
engage d teams to better understand influencing factors, operations, and defined success. This then enabled 
more comprehensive analysis across the  DE dataset late in Phase 2, which provided broader lessons 
learned on internal and external uptake for the Agency. T hese findings led to work on systems theories 
of change and capacity building in this approach for support staff teams, building the abilit y to implement 
improved uptake strategies across the Agency.  
 
  
5 
 Focus  Research Question 1  Research 
Question s       2  
& 3 Sustainability 
Planning  
Timing  March -August 2017  September 
2017-September 
2018 October -
December 
2018 
Teams Involved  • D2FTF  
• SOGE  
• (EIA)* 
 • D2FTF  
• SOGE  
• (EIA)* 
 • EIA 
• PSP 
• iDesign  
 
*Where noted in parentheses, EIA was involved as financial supporters and technical managers of the 
pilot. However, the DE did not examine the team’s work until the third phase of the DE.  
PHASE 1  
HIRING OF EVALUAT OR  
Concurrent with JPP development, DEPA -MERL launched a rigorous recruitment effort for the 
Developmental Evaluator position. The team vetted and proposed a strong candidate in October 2016 
shortly after the finalization of the JPP. Members from the D2FT F, SOGE, and EIA provided their approval 
for the candidate . However, a fter extensive negotiations, the candidate accepted another position. A 
similar process occurred in early 2017 with an alternate candidate. In March 2017, the consortium 
proposed a third  potential Evaluator who was approved by  the three teams  and accepted the position. 
She was hired  later that month and began work on the DE launch.  
DE FOCUS 
As noted above, the initial focus of this DE was on the D2FTF and SOGE teams’ efforts to promote a nd 
“scale” various innovations within and beyond USAID. However, though initial discussions, particularly 
during the May 2017 Acculturation Workshop, it becam e evident that “scaling” was not the most 
appropriate term to characterize both teams’ work. In pa rticular, D2FTF objected to this characterization. 
Thus, the teams jointly elected to rename the pilot “Sustained Uptake” (shortened to “Uptake”) in order 
to reflect the commonalities between the teams, i.e., their endeavors to help other stakeholders to a dopt 
certain innovations (e.g., digital technologies in agriculture and acceleration of the off -grid energy market ) 
and sustain their use beyond a period of d irect engagement.  
During the previously mentioned May 2017 Acculturation Workshop, the teams refi ned the focus of the 
DE as stated in the JPP. In particular, they honed in on four research questions (RQs) to guide the 
Evaluator’s work:  
1. What are the condi tions and working relationships  necessary in the LWPs, the Lab, and its 
partners to achieve sustai ned uptake internally (Missions and Bureaus) and externally?   
6 
 2. How do we determine which current Lab approaches are most effective at sustained uptake ? 
What has  been the perceived and real value add of the approaches? What can we learn from Lab 
uptake models ? 
3. What are the replicable principles/elements  from the different sustained uptake models and 
how should others apply them to a different context?  
4. How does the  Lab balance sustained uptake initiatives that are  internal versus external ? What 
impact (internal  or external) does the Lab value more? Where can the Lab have the most impact?  
Following the Workshop, the Evaluator collected and analyzed data primarily on the first RQ. The 
Evaluator developed findings, conclusions, and recommendations  (FCR)  matrices fo r the D2FTF and SOGE 
teams that provided LWP -specific responses to this question. The Evaluator helped each of the teams 
prioritize and implement selected rec ommendations primarily through “strategic learning debriefs.” The 
recommendations focused on increasing efficiencies and core partner relationships.  The results of RQ1 
are summarized in a memo (see Annex 2).  
PHASE 2  
FIRST  EXTENSION AND EXPANSION  
The response from the LWP teams to the DE and the Evaluator ’s work was overwhelmingly positive. 
During  the six months  that the Evaluator took to answer the first RQ, other teams took notice and 
interest in the work given its broad applicability around the Lab. Given cost  savings , DEPA -MERL  – in 
consultation with D2FTF, SOGE, and EIA  – agreed to expand the focus  of the DE to include two additional 
teams from the Lab’s Center for Digital Development (CDD) : DI an d DFS. The teams agreed to the 
selection of DI a nd DFS based on several key criteria:  
• Unique model for driving sustained uptake (i.e., non -duplicative wit h the LWP models) ; 
• Interest and alignment with the current DE scope and two remaining research questions ; 
• Opportunities for shared learning across al l stakeholders (i.e., enough similarity with  the LWPs 
for all parties to benefit from each other’s  learning ); 
• Opportunities for adaptation ( i.e., newly included teams must be able and willing to adapt 
programming based on DE findings) ; 
• Value -add to both the DE overall and to each individual team (i.e., how much will the individual 
teams stand to benefit from in clusion, as well as the overall findings of the DE) . 
At th e time, DE stakeholders agreed to extend the length of the pilot by thr ee months to accommodate  
the expanded scope. As the Evaluator ha d already concluded work on the first RQ at the time of the 
extension, the Evaluator only collected and analyzed data from the two new teams regarding the remaining 
three RQs.  
The Evaluator l aunched the second phase with a second Acculturation Workshop to onboard the DI and 
DFS teams. During the Workshop, the Evaluat or had each of the teams map out their initial uptake models 
onto “canvases”, which she then helped refine in the weeks following  the Workshop. These canvases 
served as the basis for RQ2. However, the majority of work on this RQ entailed carrying out three  
evaluative efforts :  
7 
 Evaluative Effort  Description  
Positive Deviance Case 
Studies (“Bright Spots”)  The Evaluator worked with the DE teams to identify their most successful 
cases of uptake with their primary stakeholders, employing a positive 
deviance appr oach. Positive deviance case studies looked at the enabling 
environment, engagement process, and identified crucial elements f or 
replication of these successful uptake approaches.  
Uganda Process Tracing 
Study  This study sought to compare and contrast enga gement approaches 
employed by Lab teams in a particular context. The Evaluator worked with 
another DEPA -MERL team member to co nduct process tracing across the 
four teams’ engagements with USAID/Uganda, which is the only Mission 
that had ongoing work stream s and demonstrated uptake across all four 
teams. The study tested ten hypotheses identified by the Evaluator as 
relevant aspec ts of the Lab teams’ models to initiate, solidify, and achieve 
sustained uptake with that Mission.  
Ecosystem Outcome 
Harvesting  This study sought to examine the various ways in which Lab teams 
conduct “enabling environments” work, defined as influenc ing the 
market, policies, external practices, and/or leverages private sector 
engagement. However, a fter harvesting a significantly low er number of 
identified outcomes than anticipated, the Evaluator —in consultation with 
colleagues from DEPA -MERL a nd EIA —decided that the teams would 
benefit from a series of Theory of Change workshops and determination 
of outcome -level indicators and miles tones would help teams track their 
progress towards changing ecosystems.  
 
Upon completion of these activities, the Evaluator prepared a memo s ummarizing  RQ2 results  (see Annex 
2). She presented these results to each of the teams as well as Lab Senior Mana gement in light of the 
upcoming Agency Transformation.  
ides 
The Evaluator then  turned to RQ3, which synthesizes the findings of RQs 1,2, and 4 (a cross -cutting 
question answered th roughout  the course of the other questions). The first major activity under  RQ3 was 
the development of a “Mission Engagement Playbook,” which draws on wor k conducted under RQs1 and 
2 regarding effective engagement strategies with Missions  (see Annex 3). This document provided guidance  
to help USAID headquarters  staff better under stand the needs of USAID Missions, develop productive 
relationships, and achiev e sustained uptake in programming with Mission counterparts. The response to 
the Playbook was overwhelmingly positiv e. For details, see “Research Question 3” in Part 4 of this r eport .   
In addition to the Playbook, the Evaluator developed a series of four one-pagers for each of the findings 
prioritized by the DE stakeholder teams. These include, “Achieving Effective Uptake”, “10 Steps to Better 
Mission Engagement,” “Creating an A daptive, Action -Oriented Team,” and “Strengthening Ecosystem 
Initiatives” (Anne x 3). The one -pagers were disseminated across the Global Development Lab through a 
presentation at the June  2018 All Hands Meeting, through emails, and in -person in order to sha re the 
findings and recommendations from the Uptake DE more broadly.   
8 
 PHASE 3  
SECOND  AND FINAL EXTENSION  
Once the Evaluator completed work on the four RQs, EIA requested another three -month extension to 
the DE to help solidify some of the work initiated un der the pilot. Specifically, the team agreed to focus 
on building the capacit y of EIA and other Lab staff to conduct sustainability planning —a key 
recommendation under RQ2 and a particularly important subject as USAID is undergoing its 
Transformation . The final activities under this pilot sought to help EIA and the Lab Front Office  help teams 
with potential  changes to organizational structures, specifically  the anticipated Program, Resource, and 
Policy Bureau (PRP) and Development, Democracy, and Innovation s (DDI) Bureau to achieve sustained 
uptake and sustainable program design.  
Throughout the final phase , the Evaluator held workshops with the teams o n the following subjects1: 
• DFS – operational planning with their new systems theory of change  
• EIA – sustaina bility planning towards the Agency Transformation  
• iDesign – sustainability planning, including building a systems theory of change  
• EIA/PSP – trainin g on sustainability planning  
In addition to holding workshops with these groups, the Evaluator supported the m in the development of 
complementary deliverables. These deliverables include process case studies to serve as practical examples 
of sustainability planning, and a matrix to ou tline resources and decision factors for future application of 
sustainability p lanning. The Evaluator also worked with EIA and Lab leadership to promote the utilization 
of the Mission Engagement Playbook internally at USAID, as part of the Transformation a nd design of new 
Bureaus, as well as externally through presentations and the r elease of external versions of deliverables.  
PART 4: WHAT WERE THE RESULT S?  
OVERVIEW  
The Uptake DE generated wide -ranging findings from the examination of the three primary RQs and the 
ongoing data collection with the DE teams and their work. Initial wor k on the pilot primarily yielded 
findings related to operations and organizational cult ure. Using these findings, the Evaluator worked with 
the teams to both adapt the way they work and also engage partners. Subsequent data collection efforts 
provided insi ght into efficient and effective approaches to working with USAID Missions and external  
partners to achieve sustained uptake of the focal approach and/or innovation. This section provides an 
                                                
 
 
1 Unfortunately, the DI team de layed the timeline of these workshops, pushing them to December 2018 when the 
Embedded  Evaluator was no longer available  due to commitments with other teams in the Lab. However , she met 
with the DI team and provided them with resources on how to move forward with the adaptations.   
9 
 overview of key findings from each RQ, as well as from the sustai nability planning activities from Phase 3 
of the Uptake DE. More in -depth FCRs  can be f ound in the Major Deliverables in Annex 3.  
RESEARCH QUESTION 1  
What are the conditions and working relationships necessary in the LWPs, the Lab, and its partners to ach ieve 
sustained uptake internally (Missions and Bureaus) and externally?  
Teams involved  
• D2FTF  
• SOGE  
Data sources  
• 18 interviews, 2 focus groups, and 324 media sources  
• 766 excerpts, 1,945 code applications  
• Partners maps, timeline data collection (D2FTF)  
 
From  this, the key conclusions were:  
• Being an “adaptable, action -oriented team”  and levera ging learning and individual 
skill set s increases efficacy and efficiency in achieving objectives . Examples from the 
D2FTF team’s approach of creating a space to reflect  on every activity, cull learnings, and 
immediately apply adaptations towards improvement ha ve helped accelerate the team’s work 
towards their results framework, enabled them to overcome unforeseen b arriers, and should be 
replicated across teams within the  Lab and the Agency.  
• Proactive, persistent, evidence -supported, and varied engagement builds and 
maintains strong Intra -Agency partnerships.  This includes creative engagements with 
partner Operating Units (OUs); including, but not limited to in -person mee tings, sharing 
publications, invitations to sectoral meetings, contributing to specific products, and assisting with 
other in -house work stre ams, priorities , and portfolio reviews.  
• Financial commitment is necessary for participatory partnerships. SOGE’s p rivate 
sector and other partner relationships where the other party has made a significant 
financial investment are much more active than ext ernal partnerships where there 
has been no financial contribution.  More active partnerships contribute to the accel eration 
of sustained uptake of off -grid energy. The DE did not identify a certain percentage of contribution 
or amount, etc. that qualifies a s sufficient financial commitment.  
• Private sector partnerships require a frank  onboarding to partnering with USAID and 
regular, clearly communicated expectations around approval processes and 
timelines.  Misunderstandings in partnerships with private sector  actors and LWPs regarding how 
USAID operates  – and in particular the decision -making and approvals processes  – contribute to 
frustrations and inefficiencies in interactions.  
• The inaccurate capture of relationship management level of effort ( LOE ) leads to 
overburdening of workstreams that impact efficacy. When external partners feel USAID 
teams are responsive and acces sible, there is stronger buy -in and commitment. However, the LOE 
required to engage with external partners in this way is significant, and often unaccounted for in 
 
10 
 team work planning. This relationship management LOE burden needs to be captured and planned  
for to increase efficacy across all workstreams.  
• Technical expertise is a contributing factor in Lab team success and is non -
transferable b etween individuals.  The distinct technical expertise of individual staff members 
of the Lab teams allows  them to be  leaders in their respective sectors, establish influential working 
relationships with the private sector, and effectively provide unique gui dance and support to 
Missions. This technical expertise is not transferable to other staff and not readily availabl e from 
other staff or candidates. As such, attrition and any changes to staffing structures will have 
detrimental effects on their efficacy t owards achieving sustained uptake of their respective 
innovations.  
RESEARCH  QUESTION 2  
How do we determine which c urrent Lab approaches are most effective at sustained uptake? What has been the 
perceived and real value add of the approaches? What can the Agency learn from the Lab models?  
Teams involved  
• D2FTF  
• SOGE  
• DFS 
• DI  
 
Data sources  
• 1,625 unique data points from 474 sources  
• Process tracing, positive deviance case studies, and outcome harvesting  
 
The DE utilized three different evaluative efforts to answer this question including process tracing; positive 
deviance; and outcome harvesting, which was later pivoted to  include outcome -oriented theory of change 
exercises. Overall, the FCRs  below come fr om 1,625 unique data points from 474 sources, including 
interviews with private sector leaders, USAID Mission staff, implementing partners, and more.  
Findings and conclusi ons can be broken down into four categories : effective models, Mission  engagement 
for sustained uptake, achieving internal uptake, and achieving external uptake. Key findings and conclusions 
from each category are below:   
EFFECTIVE MODELS  
• For internal acc eleration of innovative practices —where responsibility for scale can 
be managed by a more traditional OU—the LWP  model is a particularly effective 
model and staffing structure . Bringing together a dedicated team with both USAID  familiarity 
and unique techn ical expertise in the relevant innovative practice enables an accelerated time 
frame for achieving initial internal uptake and proof of concept. Ensuring this type of team has an 
adaptive and action -oriented team culture  with a  strong learning component fu rther ensures the 
efficacy of the model.  
 
11 
 • The LWP  model is less effective for long er-term initiatives that are focused on 
outcome -level changes or ecosystem initiatives , as well as those that require extensive 
work with externa l USAID actors.  This type of work is more successful without the time pressure 
of a limited engagement. A longer engagement period is needed to build trust and sustainable 
relationships with external parties and achieve longer -term outcomes for these type s of initiatives. 
This does no t mean that ecosystem work should have no end date, but rather that it should set 
outcome targets and establish an exit strategy based on those targets.  
• When considering more traditional models (long -term teams), the current DF S 
model is a productive, yet adaptable structure to achieve uptake both internally and 
externally.  A healthy balance between internal uptake and ecosystems activities requires an 
effective Mission engagement strategy . It also requires teams to  maint ain a network of champions 
and dedic ate sufficient expertise and time t o technical assistance and relationship maintenance. 
This, coupled with efforts to build market intelligence and active participation in sectoral 
partnerships, supports effective internal and external uptake. The more rob ust the network of 
champions and dedicated team resources, the quicker this model can achieve scale. This model 
also benefits from a dedicated learning function that leverages data towards quick adaptations and 
pivots that resp ond to operational efficiency , contextual factors, emergent lessons learned, and  
sectoral  growth.  
MISSION ENGAGEMENT  
Robust evidence across all models showed that  the following components are collectively essential to 
achieve effective and efficient buy -in with USAID Missions:    
• The Lab staff must have unique technical expertise to support a Mission in their offerings. Sourcing 
to implementing partners alone is insufficient.  
• Offerings should be sourced from a sector and context -specific assessment that id entifies gaps in 
the sector.  
• Offerings must align with Mission priorities, at a minimum including the Country Development 
Cooperation Strategy , relevant high -level strategies (such as the Global Food Security Strategy), 
and Office -level interests.  
• Lab te ams should utilize pre -existi ng entry points and/or relationships to initiate the engagement.  
For more detailed findings and how -to guidance on applying effective strategies for Mission engagement, 
see the Mission Engagement Playbook in Annex 3.  
INTERNAL UPTAKE  
Evidence regarding in ternal efforts to scale, integrate, and accelerate innovations clearly demonstrates that 
the Lab has an effective niche in changing USAID ’s development enterprise to be more efficient, effective, 
and innovative internal ly. Data from more than six Mission e ngagements demonstrated considerable 
operational changes from business as usual, programmatic improvements, as well as ancillary benefits of 
more robust marketplaces and stronger, more collaborative networks of developm ent and private sector 
actors. Custom er-service oriented teams with intentional uptake strategies have been able to greatly 
enhance USAID development initiatives in a handful of countries , and could scale their efforts further with  
more  dedicated resources .  
12 
 EXTERNAL UPTAKE  
In order to speak t o the efficacy of these initiatives in the future, more data is needed on both the 
milestones towards ecosystem -level outcomes and the distinct USAID contribution to the change 
pathway. Milestone metrics are particularl y important given the amount of time often required to affect 
ecosystem change compared to traditional project cycle lengths. The DE found that external efforts to 
accelerate markets, build infrastructure, and influence ecosystems have not consistently yie lded substantial 
outcomes to date , and there is insufficient evidence of a distinct USAID contribution to many of the 
outputs that exist. Only two ecosystem -level outcomes wer e substantiated through outcome harvesting 
efforts (which assessed  six workstream s with multiple enabling environment -oriented activities tracing 
back to 2013 ).  
RESEARCH  QUESTION 3  
What are the replicable principles/elements from the different sustained uptake models and how should others 
apply them to a different context?  
Teams involved  
• EIA 
• Lab Leadership  
• BFS 
• Transform ation Task Team ( T3) 
 
RQ3 did not require additional data collection efforts. Discussions with EIA staff members helped identify 
priority targets for dissemination. The DE produced the Mission Engagement Playbook, a s well as four 
one-pagers to disseminate key findings and recommendations for USAID  in a utilization -focused manner. 
Outreach efforts involved:  
• Presentation at the June and November Lab All Hands meetings ; 
• Presentation at the June Senior Leadership Team a nd Managers Meeting ; 
• Discussions with Lab Leadership  on how best to integrate and further disseminate the Mission 
Engagement Playbook ; 
• Discussions with BFS personnel (a D2FTF partner bureau) about presentations and trainings on 
the Mission Engagement Playb ook, as well as dissemination of hard cop ies to their Country 
Support Officers ; 
• Implementation of the Mission Engagement Playbook by DFS, iDesign, some staff within BFS, and 
request for implementation assistance by DI;  
• Mission Engagement Playbook was soci alized with T3 staff to support workstrea ms that are 
working on design of Agency Field Support Services ; and 
• External presentations on DE as an adaptive management and evaluative approach: 2018 USAID 
Evaluation Summit 2018, 2018 American Evaluation Association conference , and LEARN  (a 
mechanism that supports strategic learning and knowledge management at USAID ).  
 
13 
 SUSTAINABILITY PLANN ING 
Teams involved  
• EIA 
• iDesign  
• PSP 
 
Phase 3 of the Uptake DE focused on  the application of an Uptake DE recommendation from RQ2 
concerning stronger development of sys tems theories of change and sustainability planning across the Lab. 
The Evaluator worked with the D2FTF, DFS, and SOGE team s under Phase 2 of the DE to develop an 
exit strategy for D2FTF (which closed in September 20 18); a systems theory of change for DFS ; and a 
roadmap to a sustainable transition of the work for SOGE. This echoed a recommendation from the Lab’s 
Evaluation, Research, and Learning Plan, which was an independent but mutually reinforcing effort to the 
DE: Intentionally experiment with activity  design process/ requirements, to include sustainability analyses, 
plans, and exit strategies in new or existing Lab awards / activities . To take  this recommendation a step 
further , Phase 3 of the Uptak e DE focused o n building capacity for sustainability pl anning with teams that 
are anticipated to become  PRP (EIA) and DDI (PSP) in the Transformation. The training approach taken 
leveraged a “See One, Do One, Teach One” approach, enabling two more teams to undergo sust ainability 
planning at the Lab so that tra ining could be hands -on. The EIA and iDesign teams underwent sustainability 
planning from August to December 2018. The finding  from which  Phase 3 was designed and emphasized 
is:  
Early, thorough, and systematic sus tainability planning is crucial to achievi ng ecosystem -
level outcomes and long -term success of USAID ventures . There are different types of 
sustainability planning, including planning for exit, planning for a transition, and planning for sustainability 
of a model’s outcomes. All types of sustainab ility planning require comprehensive design of the 
model/strategy, program, project, or activity that leverages robust systems understanding.  
KEY OUTCOMES  
• The DE identified effective and efficient models  to achiev e sustained uptake with both 
internal and external audiences.   
• The DE helped six teams develop and implement  Sustainability Plans and exit strategies , 
improving sustainability of programming and increasing the understanding of pathways to scale for 
the tea ms’ respective innovations.  
• The DE create d and disseminated the Mission Engagement Playbook  – a how -to manual built 
on evidence from the DE on how to work with USAID Missions effectively  – that improved the 
efficiency and effectiveness of Mission -HQ relat ionships for teams who implemented the gui dance.  
• The DE improved working relationships  between Bureaus and with private sector partners . 
• The DE helped teams’ design pathways to scale , including their ability to assess ecosystem -
level impact.  
• The DE improved team culture  for five teams focusing on developing action -oriented, adaptive 
decision -making.  
  
 
14 
 PART 5: WHAT HAPP ENED WHEN?  
Below is a timeline of key events  in the DE.  
Date 
(m/yr)  Title  Description  Engaged Parties  
April - June 
2016 Initial 
Procurement 
Outreach and 
Scoping D2FTF team reached o ut to DEPA -MERL 
COR  about a collaborative Lab DE, and 
outreach for additional Lab teams and 
securing a DE began.  EIA, D2FTF, SOGE, 
DEPA -MERL 
24 October 
2016 JPP Signed DEPA -MERL signed a JPP with the D2FTF 
and SOGE teams, with EIA as a 
supportive backer f or a one-year DE. EIA, D2FTF, SOGE, 
DEPA -MERL 
August 
2016- 
February 
2017 Recruitment of 
Evaluator Recruitment of the Evaluator began amid 
finalization of the JPP. Two candidates 
were found and then later declined the 
position . A member  from the DEPA -
MERL conso rtium was ultimately hired .  DEPA -MERL 
March - April 
2017 Onboarding of 
Evaluator to DE 
and Participating 
Teams The Evaluator started March 20th, 2017 , 
first onboarding with SI for two weeks 
and then integrating with the DE teams 
and doing preliminary  data collection 
before the Acculturation Workshop.  EIA, D2FTF, SOGE, 
DEPA -MERL, 
Evaluator 
1-2 May 
2017 First 
Acculturation 
Workshop  The first Acculturation Workshop was 
held on May 1st and 2nd, 2017. The 
Workshop was a kickoff event with all DE 
stakehold ers to cover an introduction to 
DE, working with an Evaluator, and refine 
the RQs. EIA, D2FTF, SOGE, 
DEPA -MERL, 
Evaluator 
August 2017  RQ1 Answered  Data collection on RQ1 concerning 
conditions and working relationships that 
support sustained uptak e was com pleted 
in July 2017, with findings, conclusions, 
and recommendations memos shared 
with the teams in August.  D2FTF, SOGE, 
Evaluator 
August 2017  Decision to 
Expand Inclusion 
of Teams in 
Uptake DE  The warm reception to the RQ1 findings 
and adaptati ons made in response led to 
the decision to expand the number of 
teams included in the DE. Multiple teams 
were interviewed and offered the EIA, DEPA -MERL, 
Evaluator, Di gital 
Inclusion, DFS, 
iDesign, CD D, 
Development  
15 
 potential to join, with the decision coming 
down to DFS and DI (both teams within 
CDD ). Innovation Ventures 
(DIV) 
5 October 
2017 Second 
Acculturation 
Workshop with 
New Teams  A second Acculturation Workshop was 
held to bring the new teams on board and 
manage expectations and alignment on the 
remaining RQs. EIA, D2FTF, SOGE, 
DEPA -MERL, 
Evaluator , DI, DFS 
October 
2017 Initial Uptake 
Models 
Developed  In order to answer which models for 
sustained uptake were the most efficient 
and effective, the DE helped teams outline 
each team’s model for achieving uptake as 
part of and following the second 
Acculturation Workshop.  D2FTF, SOGE, DI, 
DFS, Evaluator 
March-April 
2018 Evaluation 
Question #2 
Answered  Analyzing data from a process tracing 
evaluation, positive deviance case studies, 
and an outcome harvesting asses sment, 
the Evaluator was able to answer RQ2 
and provide team recommendations to 
improve Mission e ngagement, internal and 
external uptake.  EIA, D2FTF, SOGE, 
DEPA -MERL, 
Evaluator , DI, DFS 
May 2018 Phase 3 
Extension 
Decision (for 
July-December 
2018) During e arly sharing of findings on RQ2, 
conversations began between DEPA -
MERL and EIA about a cost -exten sion for 
the DE to help EIA through the 
Transformation.  EIA, DEPA -MERL, 
Evaluator 
1 July 2018  Mission 
Engagement 
Playbook 
Developed  The Evaluator, with suppor t from SI 
colleagues converted all existing findings 
and recommendations on Mission 
engagement from the DE into a user -
friendly, how -to playbook to improve 
sustained uptake through Mission 
engagement across the Agency.  Evaluator, DEPA -
MERL 
April-
December 
2018 Broader 
Engagement on 
Uptake DE 
Findings ( RQ3) As part of RQ3, the Evaluator worked 
with EIA to expand engagement with the 
DE findings across the Agency through 
presentations at the Lab All Hands, with 
Managers and the Senior Leadership 
Team, Lab Lead ership, T3, and 
conversations with BFS.  EIA, BFS, L ab Front 
Office, T3, Evaluator  
16 
 August -
September 
2018 Phase 3 Scope 
Finalized DEPA -MERL worked with EIA in August 
and September 2018 to finalize the Phase 
3 scope, focusing on implementing and 
training on  sustainability planning.  EIA, DEPA -MERL, 
Evaluator 
August -
December 
2018 Sustainability 
Planning with EIA  The Evaluator worked with the EIA team 
on scenario planning and adaptive 
management recommendations 
concerning the evolution of their work as 
they mo ve into PRP.  EIA, Evaluator 
September - 
December 
2018 Sustainability 
Planning with 
iDesign The Evaluator worked with the iDesign 
team to develop their systems theory of 
change, scenario planning, and other 
sustainability planning exercises to ensure 
a rob ust transition and growth towards 
objectives beyond the Transformation.  iDesign, Evaluator 
October - 
December 
2018 Sustainability 
Planning Training  The Evaluator trained staff from EIA and 
PSP in sustainability planning to ensure the 
capacity would be main tained beyond the 
life of the DE.  EIA, PSP, Evaluator 
31 
December 
2018 End of DE  The DE ended December 2018, finishing 
with delivery of a sustainability planning 
resource package and final report.  EIA, DEPA -MERL, 
Evaluator 
   
17 
 ANNEXES  
   
18 
 ANNEX 1: UPTAKE PI LOT INFOGRAPHIC  
Uptake DE Data 
and Methods.pdf
 
The authors  could not insert the Uptake Pilot InfoGraphic into this  report without compromising the 
structural and formatting integrity and coherency of both the infographic and the report as a whole. The 
authors will share  the link to the InfoGraphic when it becomes available to the public .    
19 
 ANNEX 2: RESEARCH QU ESTION MEMOS  
RESEARCH QUESTION 1  
 
Uptake Developmental Evaluation  
Interim Findings  
August 2017  
 
What is the Uptake Developmental Evaluation? The Up take Developmental Evaluation (DE) 
is a one -year evaluative effort of the Global Development Lab, lead by the Evaluation and Impact 
Assessment Office, to look at successful approaches, models, and replicab le principles for achieving 
sustained uptake of inn ovations at USAID. It currently encompasses Lab Wide Priority 2 (D2FTF) and Lab 
Wide Priority 3 (SOGE) in its scope, and has recently concluded work on Research Question #1:  
What are the conditions and wor king relationships necessary in the LWPs, the Lab, and its partners to achieve 
sustained uptake internally (Missions and Bureaus) and externally?  
Findings:  
1. Being an “adaptable, action -oriented team” that leverages learning and individual skillsets 
increases efficacy and efficiency in achieving objectives. D2FTF’s example of creating a 
space to reflect on every activity, cull learnings, and immediately apply adaptations towards 
improvement has helped a ccelerate the team’s work towards their results framework, enabled 
them to overcome unforeseen barriers, and  should be replicated across teams within the Lab 
and the Agency. Identifying replicable strategies for establishing  adaptable,  action -oriented  
teams  is part of the DE scope  for the next two research  questions.  
2. Proactive, persistent, evidence -supported, an d varied engagement builds and 
maintains strong Intra -Agency partnerships. E.g., creative engagements with a partner 
OU; includi ng but not limited to in -person meetings, sharing publication, invitations to sectoral 
meetings, contributing to specific produc ts, and assisting with other ‘in -house’ workstreams, as 
well as regular ‘portfolio reviews’, and pre -empting provision  of inform ation pertinent  to the 
partner  OUs current  workstream  and priorities.   
20 
  
 
3. There are insufficient staff to maximize the impact of the LWP teams. Both teams 
have made marked progress towards their results framework and in the uptake of their 
innovations, however the teams are carrying significant, increasing workloads. This increa se in 
work without sufficient capacity is starting to i mpact the efficacy  and  efficiency with which 
they are able to implement their existing    workstreams.  
 
Without additional staff, the team’s will need to learn when to say “no” to maintain maximum 
effectiveness within their given portfolios, and limit th e number of activities they are able to 
take on.  
4. Financial commitment is necessary for participatory partnerships. SOGE’s private 
sector and other donor relationships where the other party has made a s ignificant financial 
investment are much more active th an their other external partnerships, and contribute to the 
acceleration of sustained uptake of off -grid energy. The DE has yet to identify a certain 
percentage of contribution or amount, etc. that qua lifies as sufficient  financial  commitment,  but 
can dig deeper  on this finding  if it is prioritized.  
5. Misunderstandings between partnerships with private sector actors and LWPs regarding how 
USAID operates, in particular the decision -making and approvals pro cesses, contribute to 
frustrations in the partnership a nd inefficiencies in interactions. Private sector partnerships 
will benefit from a frank “on -boarding” to partnering with USAID and regular, 
clearly communicated expectations around approvals processes  and timelines.  
6. Responsiveness and perceptions of acces sibility by the USAID team can lead to stronger buy -in 
and commitment from external partners, but has a significant LOE burden. For participatory 
partnerships, this accessibility is advantageous, but teams need to hon estly plan for LOE 
burdens so as not to  overburden workstreams and impact their  efficacy.  
 
Recommendations to leverage continuation of the Uptake DE:  D2FTF’s  Improved Mission Engagement Example:  
The existing ties and relationships D2FTF had in Ghana and Uganda focused their strategy for engagement and preemptively 
labelled the work  under the umbrella of the existing ties and specific digital technologies. Thei r approach to Nepal and 
Cambodia was less directive and narrow, in part due to the lack of previous working relationships with those Missions. 
The joint selection of opportunit ies by D2FTF and the Missions, identified through in -country assessments, and th e 
additional capacity offered by D2FTF is in sharp contrast with how the Lab is perceived based on previous interactions 
with Missions. This engagement strategy was proactive, provided learnings (through the needs assessment), and the 
activities were devel oped in a co -creative, collaborative manner with Mission staff. D2FTF has also actively taken on more 
of the administrative burden in establishing partnerships than other teams  do, according to the Mission, which has led to 
more successful and truly collab orative partnerships with Missions for them. D2FTF’s engagement approach in Nepal 
is the most significant contributing factor to their ability to accelerate their work and main tain buy -in, as 
well as dedicated staff resources, at the Mission . 
 
“Main thing,  [...] if all projects follow the route, the process itself, the level of engagement, the quality of work that [D2FTF] have 
put, the paperwork itself, how supportive they are a t each and every step of the process, our life is very much easier. [...] Things  need 
to be simplified for us when we buy -in to other activities. That process, the process itself, and then of course the people who are 
engaged [from D2FTF], they were very w ell organized and that needs to be replicated. If other mechanisms had this leve l of 
engagement it would be better across the  Agency. ” 
-Nepal Mission   
21 
 As the Uptake DE moves toward examining Research Question #2 and #3, the Lab and the Agency 
would benefit from including other approaches to achi eving sustained uptake in the DE. The subsequent 
Research Questions are:  
2. How do we determine  which  current  LWP approaches  are most effective  at sustained  uptake?  
What  has been  the perceived  and real value add of the approaches?  What  can we learn from the 
LWP model?  
3. What  are the replicable  principles/elements  from the different  sustained  uptake  models  and how 
should others apply them to a different  context?,  
 
The inclusion of additional Operating Units, programs, and activities with provide a more robust dat a 
set, and allow for the evaluation of a larger variety of models to determine which is the most effectiv e at 
sustained uptake. Currently, the next stage of the DE will look at whether or not the LWPs are effective 
models, and how they can improve. Includi ng other units that focus on innovation, acceleration, and 
sustained uptake will enable a comparison to t he LWPs and a more beneficial analysis of replicable 
factors for such models for the Lab and the Agency more broadly. Priority programming includes:  
● CDI: Discover & Test / Applied & Acceleration/  iDesign  
● CDD: DFS/ Digital Inclusion/  GeoCenter  
● CDR:  Evidence -to-Action  
 
Expanding the Uptake DE to include any of the above will require additional resources, but 
will yield substantial and timely evidence for d ecision -making as the Lab considers revising 
programming due to resource constraints. 
22 
 APPENDIX 1: UPTAKE D E DATA SOURCES FOR R ESEARCH QUESTION #1 FINDINGS  
 
Relevant 
Finding  Data 
Source(s)  
Overall 
07.19.2017  18 Interviews; 2 Focus Groups; 324 Media sources  (emails, weekly meeting notes, supportive evidence, 
etc.); 766 excerpts; 1945 code applications; Partners maps (for each team); Timeline data collection 
(D2FTF)  
1 D2FTF example: the team had 15 instances, from the data set, of adaptations that enhanced t he success 
of their work towards their results framew ork, 39 demonstrated instances of supportive, action -
oriented team culture, and additional data trends around capacity, strong personnel with necessary 
expertise, and learning, action -centric leadership  
2 This finding has been triangulated between interviews, focus groups, and team documentation, and 
further validated by the evaluator’s observational data  
3 The data clearly supports the known, perennial problem of too much work to do and not enough staf f. 
Specifically, the issue of insufficient sta ff has been mentioned across multiple data sources by the LWPs 
themselves, but also their partner OUs, and is further support by observational data from the evaluator.  
4 This finding has been identified in the  data around the various types of engagement, participation, and 
sectoral support across SOGE’s private sector and other donor relationships. Active participation was 
analyzed as part of the sampling methodology for interview selection. This was later comp ared to size 
of financial contribu tion. 
5 The on -boarding recommendation was a specific recommendation made by one of the Founding 
Partners interviewed. The frustrations and their impact leading to circular conversations and 
inefficiencies has been mentio ned by both partners and LWP staff alike, as well as regularly captured in 
weekly documentation.  
6 Responsiveness and accessibility was a common theme across interviews conducted with SOGE 
founding partners, as well as invested companies. The ensuing issu e regarding the LOE burden wa s 
mentioned in every interview with LWP staff, and further validated through weekly documentation 
and observational data from the evaluator.  
   
23 
 RESEARCH QUESTION 2  – OVERALL   
EFFECTIVE MODELS FOR SUSTAINED 
UPTAKE  
RESULTS  FROM  THE  UPTAKE  DEVELOPMENTAL EVALUATION  
Key Conclusions  
1. The Lab-Wide Priority Model is an effective model and staffing structure to achieve 
internal uptake of innovative practices and improve the Agency’s development enterprise, 
especially where long -term management  can be absorbed  by another  Operating  Unit. 
2. The Digital Finance Team’s model is an effective and efficient model to achieve 
both internal (Agency -facing) and external (enabling environments) uptake that is 
best housed within a center focused  on innovation,  techn ology,  and emerging  market  trends.  
3. An effective Mission engagement strategy must include a) hands -on involvement of USAID HQ 
staff with technical expertise, b) alignment with a Missions CDCS, high -level priorities, and 
Office -level interests , c) initiation of Mission engagement through pre -existing relationships, 
and d) utilization of assessments that identify  market  gaps and opportunities  that helps refine  
service offerings.  
4. The Lab has been most effective in improving the efficacy, efficien cy, and innovation of 
internal Agency practices and must make more concerted efforts to identify and 
substantiate the outcomes of ecosystem -level efforts.  
 
Introduction  
Over the course of its history, the Lab has evolved its programming related to scaling,  adoption, 
acceleration, and uptake, in response to both its charter to “source, test, and scale” development 
solutions, and from ad hoc learnings from previous efforts.  Following the conception of the Lab -wide 
priorities (LWPs), the Lab agreed to undertak e active learning to enable the Bureau to get smarter about 
the viability of different approaches to scale/sustained uptake. Since that time, D2FTF, SOGE, DFS, Digital 
Inclusion, and EIA have bought into the use of a  Developmental  Evaluation  (DE) to share  and consolidate  
learnings  on sustained  uptake.  
Evaluation Questions and Approach  
This Memo  addresses  the answers  and recommended  next steps  related  to the second  research  
question  of the DE: How do we determine which current Lab approaches are most effecti ve at sustained 
uptake? What has been  the perceived and real value add of the approaches? What can we learn from the 
Lab-Wide Priority model? Three different  evaluative  efforts  were  undertaken  to answer  this question,  
including  process  tracing,  positive  deviance, and outcome harvesting, which was later pivoted to 
include outcome -oriented theory of change exercises. Overall,  the findings,  conclusions,  and 
recommenda tions below  come  from 1625 unique  data points  from 474 sources, including interviews 
with priv ate sector leaders, USAID Mission staff, implementing partners, prize finalists, and  more.  
Models Tested   
24 
 Lab-Wide Priority Model (D2FTF):  
Digital Development for Feed the Future (D2FTF) is a three -year collaboration between USAID’s Global 
Development Lab a nd the Bureau for Food Security, focusing on integrating digital tools and technologies 
into Feed the Future activities. D2FTF developed t echnical guidance on digital technology for the GFSS 
country planning process and made a number of services available to USAID Missions to further support 
the integration of digital approaches into their respective programming. The team can assess opportun ities, 
recommend actions, and deploy resources to support solutions. D2FTF has established mechanisms that 
can be depl oyed or bought -into to provide tailored support on digital extension, connectivity, digital 
financial services, geospatial analysis. The t eam leads fostered an adaptive, action -oriented culture that 
enabled them to learn from four core pilots in   their fi rst two years and then scale to nine additional 
Missions in their last year and develop a strategic exit strategy to secure sustainability  of their efforts. For 
this portfolio, a dedicated team of six full -time equivalent (FTEs) positions, with $6 Million/ year in program 
funds enabled them to solidify Mission engagement, run innovative programs, and secure sustained uptake.  
Lab-Wide Priority  Model and Grand Challenge (SOGE):  
Scaling Off -Grid Energy (SOGE) is a platform for leading donors and investors to de velop Africa’s off -grid 
energy sector and coordinate investments to connect more households and businesses to electricity, 
faster. SOGE in centivizes technological innovation, funds early -stage companies, and supports critical 
elements of the off -grid ecosy stem. This platform was established through a three -year Lab -Wide Priority 
team that then transformed into a Grand Challenge. The USAID/SO GE team serves as secretariat for the 
platform, innovative financier, and advocate for off -grid energy market accelera tion. The team has done 
this with three to four core FTEs and $6 Million per year in programming funds provided by Power Africa 
plus an av erage of $4 million per year from the US Global Development Lab, in addition to the 
coordination of funds across the p artnership. The nature of the ecosystems work being conducted by this 
team makes a time -limited Lab -Wide Priority model a incongruent fit with achieving ecosystem outcomes, 
but a good fit in terms of leveraging resources across the Lab in order to conduct more complex work. 
Accelerating country -level ecosystems as well as innovation in the off -grid energy sector required nearly 
two years of experimentation, which is just now solidifying into more robust collaboration and moving 
from outputs to outcomes. As such, the team pursued more time to continue their work, which was 
supported by both the USAID Global Development Lab and Power Africa. Th e team is now working 
towards a sustainable exit strategy for 2021 with capture of ecosystem -level outcomes.  
Tradition al Team Models:  
Digital Financial Services (DFS) Team  
USAID's Digital Finance Team identifies and advances market level opportunities that  support the 
acceleration of development objectives across sectors such as agriculture, resilience and food security, 
health, and energy. The team meets these objectives through USAID Mission engagement, partnership and 
alliance building, and training and capacity -building of USAID staff and implementing partners. This team 
works both on internal and external uptake, demo nstrating success in both, but acknowledges their 
internal uptake model is more readily scalable and has seen marked outcomes in improving  USAID 
operations and programming. Since their start in 2013, the team has evolved and refined their model 
towards lea rning-oriented team leadership and long -term Mission buy -in. There has been rapidly increasing 
demand for DFS support through the team’s c reation of a robust Agency network of DFS champions,  
25 
 including indirect scale to new Missions as existing Mission staf f champions enter a new rotation. With six 
FTEs and a budget of $3.5 million per year for programming, the DFS team can maintain engagemen t with 
six to eight countries per year, as well as research and other partner collaborations. With $5 -10 million 
per y ear in programming funds the DFS team could sustain at least one large scale flagship program that 
would serve as a way to accelerate ecos ystem change to the point of sustainability in a viable market. The 
team’s ability to scale internal uptake is directl y related to the number of dedicated, technically proficient 
staff on the team, with ecosystem change more directly linked to the amount o f programming funds 
available to invest in a dedicated enabling environments effort.  
 
 
Digital Inclusion Team  
The Digi tal Inclusion team at the Lab facilitates the expansion of internet access to accelerate the Agency’s 
development objectives, while ensuri ng the most marginalized have the skills and resources to be active 
participants in the digital economy. Active since 2013, the team’s model has evolved from primarily 
external focused, to strategically more Mission -driven, to their current status as a tea m that both supports 
Missions, as well as undertaking more ecosystem -level initiatives. This team, and subsequently th eir model, 
has faced the most Agency pressures amongst the four tested models in terms of how their strategy, 
focus, and activities should  evolve. Past ecosystem level efforts, such as the Alliance for Affordable Internet 
and mWomen have influenced the  conversations  in the connectivity  space,  but it has been difficult  to 
provide  substantial  evidence  of USAID’s distinct contributions to outco mes from these initiatives thus far. 
More recent work that combines infrastructure activities alongside Mission engage ment, like the work 
with USAID/Liberia and USAID/Uganda, that also engages the private sector has had more attributable 
success in both im pacting the USAID development enterprise as well as country -level ecosystems. This 
team supports four Missions, alongs ide larger alliance and challenge initiatives, research, and enabling 
environment work with four FTEs and a budget of $2 -3 million in prog ramming funds per year.  
Most Effective Model Structure  
For internal acceleration of innovative practices, where respon sibility for scale can be managed by a more 
traditional operating unit, the Lab -Wide Priority model is a particularly effective model and 
staffing structure . Bringing together a dedicated team with both Agency familiarity and unique technical 
expertise in the relevant innovative practice enables an accelerated time frame for achieving initial internal 
uptake and proof of concept. Ensuring this type of team has an adaptive and action -oriented tea m culture, 
with a strong learning component,  further  ensures  the efficacy  of the model.  The ability  of a well-equipped,  
dedicated  team to implement this work is also partially dependent on their ability to leverage a diverse 
range of resources from other t eams, like  D2FTF  did in their engagements  with DFS, iDesign,  PEER, and a 
multitude  of other  Lab teams.  
The Lab -Wide Priority model is less effective for more long -term initiatives that are focused 
on outcome -level changes or ecosystem initiatives , as well as those that require extensive work 
with external USAID actors . This type of work is more successful without the time pressure of a limited 
engagement. A longer engagement period is needed to build trust and sustainable relationships with 
external parties  and achieve longer -term outcomes for these type of initiatives . This does not mean that  
26 
 ecosystem work should have no end date, rather that it should set outcome targets and establish an exit 
strategy based on those targets (as the SOGE team did). For thi s type of work, there is insufficient evidence 
around what type  of model is best, only which model provide a less conducive structure.  
When considering more traditional models (long -term teams), the current DFS model is a 
productive, yet adaptable structur e to achieve uptake both internally and externally . A 
healthy b alance between internal uptake and ecosystems activities requires an effective Mission 
engagement strategy, maintenance of a network of champions, and sufficient team expertise and time to 
dedicate towards technical assistance and relationship maintenance.  This, coupled with efforts to build 
market intelligence and active participation in sectoral partnerships, supports effective internal and 
external uptake. The more robust the network of champ ions and dedicated team resources, the quicker 
this model can a chieve scale. This model also benefits from a dedicated learning function that leverages 
data towards quick adaptations and pivots that respond to operational efficiency, contextual  factors,  
emergent  lessons  learned,  and growth  in the sector.  
The Mission E ngagement Strategy  
Robust evidence across all models demonstrates the following components are essential to achieve 
effective and efficient buy -in with USAID Missions:  
1. The Lab staff must have unique technical expertise to support a Mission in their 
offerin gs. Sourcing to implementing partners alone is  insufficient.  
2. Offerings should be sourced from a sectorally, and context -specific assessment that 
identifies gaps in the  sector.  
3. Offeri ngs must align with Mission priorities, at a minimum including the followi ng three: 
the CDCS, relevant  high-level strategies  (such  as the Global  Food Security  Strategy),  and 
Office -level interests.  
4. Lab teams  should  utilize  pre-existing  entry  points  and/or  relationships  to initiate  the 
engagement.  
The implication of these conclus ions becomes clear when considering the current attrition of staff and 
difficulties in hiring facing the Lab (and the Agency more broadly). As staff with key technical 
expertise leav e and are  unable to be replaced, Lab teams lose the unique value they 
prov ide in supporting Missions and advancing their use of innovative development solutions. This 
expertise and capacity is not being replaced at the Mission -level. This will not only imp act the efficacy 
of team’s work through the transition,  but should also be  a key consideration in staffing strategy 
for the new Bureau.  
The evidence from the Developmental Evaluation also highlighted additional factors that support effective 
and efficient Mission engagement related to relationship management, effective service o fferings, and 
additional engagement best practices that go beyond the existing Mission Engagement Protocol and Best 
Practices documents. These are available through more de tailed deliverables on the Uptake DE internal 
Lab page.  
Internal Uptake  
Evidence reg arding internal efforts to scale, integrate, and accelerate innovations clearly demonstrates 
that the Lab has an effective niche is changing the Agency’s development enterp rise to be more efficient,  
27 
 effective, and innovative internally. Data from more than  six Mission engagements demonstrated 
considerable operational changes from business as usual, programmatic improvements, as well as 
ancillary benefits of more robust marke tplaces and stronger, more collaborative networks of 
development and private sector actors.  
Customer -service oriented teams with intentional uptake strategies have been able to greatly enhance 
USAID development initiatives in a handful of countries, and co uld scale their efforts further with 
dedicated resources.  
Ecosystem Initiatives  
In order to speak to the efficacy of these initiatives in the future, more data is needed on both the 
milestones towards ecosystem -level outcomes and the distinct USAID contrib ution to the change 
pathway. Milestone metrics are especially important given the am ount of time often required to effect 
ecosystem change compared to traditional project cycle lengths. The DE found that external efforts to 
accelerate markets, build infras tructure,  and influence ecosystems have not consistently yielded 
substantial outcom es to date and there is insufficient evidence of a distinct USAID contribution to many 
of the outputs that exist. Only two ecosystem -level outcomes were able to be substant iated through the 
outcome harvesting efforts assessing six workstreams with multiple  enabling environment oriented 
activities tracing back to  2013.  
Next Steps  
In moving towards broader, intentional adaptations to implement the above -mentioned best practice s, 
the following next steps should be considered:  
1. The data -driven best practices on Mission engagement should be shared and 
implemented across all Lab teams , and to relevant teams in other Operating Units as 
soon as possible. The DE is currently working on a more expansive, evidence -based Mission 
Engagement Playbook that could be part of that dissemination  strategy.  
2. Any new initiatives focusing on scale, integration, acceleration, and/or uptake should 
determine their internal versus external uptak e objectives before consolidating a 
team, and choose one of the DE -identified effective models  for uptake as their implementation 
structure. This would be the Lab -Wide Priority model for internal uptake of innovative 
practices, and the DFS structural model  for dual uptake objectives  (internal  and external)  that 
requires  more  long-term implementatio n to see scale or outcomes.  
3. Any ecosystems level or external uptake work must have stronger outcome -level 
indicators with dedicated monitoring, evaluation, and le arning efforts to build 
learnings and evidence around the USAID specific contribution to this type of  work.  
4. The evidence from the DE concerning Mission engagement and uptake are not exclusively 
applicable to teams under the Global Development Lab, and as s uch should be intentionally 
disseminated to other relevant  Operational  Units  with an eye towar ds implementation.    
28 
 RESEARCH QUESTION 2 – ECOSYSTEMS SPECIFI C  
Outcome Harvesting Efforts and Findings from the Uptake Developmental  
Evaluation  
May 2018  
This document provides an overview of the purpose, methodology, findings, conclusions, and 
recommendati ons from an evaluative effort that examined what ecosystem initiatives from the 
developmental evaluation (DE) stakeholder teams have achieved to date. This ef fort was conducted 
under the Uptake DE’s Research Question  #2: How do we determine which current L ab approaches are 
most effective at sustained uptake? What has been the perceived and real value add of the approaches? 
What can we learn from Lab  models?  
Purpose of the Study  
As part of Research Question #2, the teams asked the embedded evaluator to exami ne the various DE 
stakeholder teams’ approaches to achieving uptake under enabling environment work or ecosystem 
initiatives.12 The goal of this effort was no t to provide a performance -based judgement on one approach 
or another. Rather, the DE intended to understand 1) how this work interplays with the more internal 
(Agency -focused) uptake work the teams are doing, 2) outcomes of this work to date given that ef ficacy 
markers have largely been grounded in anecdotes and/or output -level data, and 3) share less ons learned 
regarding what has or has not worked within the teams’ models and USAID’s particular role to play in 
ecosystem initiatives.  
Methodology  
This evalu ative effort used an outcome harvesting approach. Outcome harvesting was chosen due to the 
lack of  consistent and documented data on the impact of ecosystems initiatives to date. Establishing what 
sustained uptake -related outcomes have come from this work will help teams to 1) better articulate their 
distinct value -add and 2) replicate effective pathwa ys towards ecosystem change. Further, understanding 
the emergence of those outcomes and their relative significance was anticipated to support the teams in 
adapting their ecosystems initiatives, drawing on lessons learned about strategies that best leverag e 
USAID’s particular contributions to these spaces.  
Definitions : For the purpose of this evaluative effort, “ecosystem -level outcome” was defined as: an 
outco me from a market acceleration, infrastructure, or other enabling environment effort that has 
achieved sustained uptake. Sustained uptake includes, but is not limited to; implementation/rollout of a 
policy, demonstrated proof of private sector partners acti ng upon a commitment, scale at the desired unit 
of analysis, or improvement of a market barrier. T he key determination is centered around ownership 
and action by the target audience.  
Outcomes can be at both the levels of a country market and/or the global market, as both are considered 
ecosystem work by the DE stakeholder teams. This definition is in l ine with the Objective Level indicators 
and language of the Strategy Refresh CDD Results Framework, approved by all CDD teams in early May 
2018.   
29 
 Outcome Harvesting Process: The DE first identified which ecosystem initiatives each team would 
like the focal  area for the outcome  harvesting.  Given  the number  of team and ecosystem  workstreams,  
the DE focused  on two workstreams per team as a self-selected sample of ecosystem initiatives. Team 
Leads gave final approval for workstream selection. After protocol deve lopment, the embedded evaluator 
conducted a series of interviews with key stakeholders from the Lab teams, their core collaborative 
partners, and primary ‘beneficiaries’ as they relate to this work in order to identify a list of possible 
outcomes from each  team’s ecosystem initiatives. Once a preliminary  list of outcomes  were established  
for each team,  the evaluation  team intended  to conduct  a participatory prioritization exercise to identify 
the key outcomes that teams wanted to further substantiate. Given  the lack of outcomes identified, all 
identified outcomes were analyzed to test substantiation.  
Sampling:  This evaluative  effort  employed purposive  sampling.  The initial  purposive  sample  sought  to 
capture  the Lab staff that are best informed and discuss th e ecosystems initiatives that have been done by 
the four DE stakeholder teams. From there, the evaluators reviewed the lists of possible interviewees 
provided and delineated a second and third grouping of interviewees. The first group consisted of core 
actors best able to identify outcomes from the ecosystems initiatives identified. The second group 
comprised others involved in act ivity implementation and anyone deemed a primary beneficiary (Missions, 
government partners, awardees) that can help substantiat e the prioritized outcomes. A final grouping was 
composed of anyone that could contribute to substantiating  outcomes,  but was more tangentially  involved  
according  to teams  and other  interviewees.  
Outcomes Substantiated  
1. The DFS Team’s work digitizing salary  payments with the Ministry of Education and Ministry of 
Health in Liberia  led to a decrease  in time and resources  required  for teachers  and health  
workers  in select  districts to retrieve their salary  payments.  
2. USAID’s  engagement  and investment  in the Alliance for Affordable  Internet  created  an 
unbiased  voice  on connectivity  issues,  raising  awareness  and collaborative  efforts  around  
internet  affordability.  
3. mWomen  raised  awareness  around  women’s  digital  access  and participation  issues  
towards  a global conver sation,  which  encouraged  long-term funding  around  the digital  
gender  gap. 
 
Findings, Conclusions, and Recommendations  
The outcome harvesting effort was unable to substantiate other ecosystem outcomes due to lack of 
evidence. Of the three outcomes above, th e connection to development impact is tenuous for the more 
global initiatives. The scale of the work in Liberia has not of yet reached ecosystem -level impact. The lack 
of ecosystem -level outcomes more broadly across the six workstreams highlighted strategi c-level 
problems the teams are facing. One of these strategic problems is the time it takes to r ealize ecosystem -
level outcomes versus the relatively short reporting deadlines and leadership’s expectations for results. 
Another strategic -level problem is th at teams did not have robust theories of change, i.e. those supported 
by evidence of the causal linkages in those theories, or  outcome -level milestone indicators available to 
track progress towards those changes. As such, the DE is unable to make any concl usions as to the efficacy 
of these initiatives. In order to speak to the efficacy of these initi atives in the future, more data is needed  
30 
 on both the milestones towards ecosystem -level outcomes and the distinct USAID contribution to the 
change pathway. Mil estone metrics are especially important given their ability to provide supplemental 
reporting da ta of progress towards the objective -level goal. Any ecosystems level or external uptake 
work must have stronger outcome -level indicators with dedicated monitor ing, evaluation,  
and learning  efforts  to build learnings  and evidence  around  the USAID  specific  contribution  to this type 
of work . 
31 
 APPENDI X 2: SUBSTANTIATED OUTC OME DESCRIPTIONS  
1. The DFS Team’s work digitizing salary payments with the Ministry of Education and 
Ministry of Health in Liberia led to a decrease in time and resources required for 
teachers and  health  workers  in select  districts  to retrieve  their  salary  payments.  
(Clear  and detailed but  emergent)  
The DFS team’s efforts with the Liberian government to test the impact of converting teacher and 
health worker salary payments to digital payments has achieved clear results over the past year. 
Twenty percent of public school teachers were enrolled as of March 2018 and this translated to 
a 96.4% reduction i n time and a 55.4% reduction in cost to collect salary.  Likewise, 1180 Minist ry 
of Health employees were enrolled, leading to a 62.9% reduction in cost and a 79.2% reduction 
in time to receive salary payments. Beyond these direct benefits, this means teac hers are spending 
more time in the classroom teaching and health workers more time in clinics, an average of an 
additional 10.6 hours on duty for those enrolled in digital salary payments. More comprehensive 
enrollment may eventually lead to improved educa tion outcomes over the long -term. However, 
with only 20% of staff enrolled and  the Liberian government not yet  signed  onto broader  
application  across  their ministry  departments,  this is an emerging  ecosystem -level outcome. This 
outcome had the most third p arty data available, and was the most directly tied to USAID efforts 
from the outcome harvesting effort. Recommendations were made to the DFS team to continue 
an additional year of programming (the program ended in March 2018 after only one year of 
impleme ntation) in order to solidify government buy -in, capacity support coming from the DFS 
Working Group, and scale the program past the tipping  point.  
2. USAID’s  engagement  and investment  in the Alliance  for Affordable  Internet  (A4AI)  
created an unbiased voice on  connectivity issues, raising awareness and 
collaborative efforts around inter net affordability. (Ecosystem -level outcome, but 
unable to connect to development outcomes. Contribution is  clear.)  
USAID as a seed funder, convener, and neutralizing force among st industry stakeholders enabled 
A4AI to gain credibility and significance in the global affordable internet dialogue, according to 
data collected under the developmental evaluation. As such they’ve been able to influence tax and 
internet laws in Ghana and  Liberia respectively, and change UN Broadband Commission targ ets. 
Those policy efforts have not yet seen implementation and therefore there is no data yet on their 
ecosystem -level impact. The affordability conversation and market expansion efforts were al ready 
underway by industry leaders, but data demonstrates that  USAID had a clear neutralizing effect 
on how A4AI was/is perceived.  
3. mWomen  raised  awareness  around  women’s  digital  access  and participation  issues  
towards a global conversation, which encourage d long -term funding around the 
digital gender gap. (Ecosystem -level outcome, but unable to connect to 
development outcomes and validate MNO actual numbers versus commitment 
projections. Unique contribution is tenuous - evaluation team is unable to determine  
if this would have been a natural progression of the informat ion age with or without 
USAID highlighting this  issue.)   
32 
 mWomen contributions include facilitating acceleration of the industry to serve more resource -
poor women, as seen through continued market ing of mobile phones and devices to women 
inresource -constrain ed communities and continued commitments from MNOs, as well as through 
the follow -on projects of Connect Women by GSMA and Women Connect by USAID. USAID’s 
direct contribution to this is in both the role as funder to the mWomen initiative, but also as a 
convener of stakeholders in this space, legitimizing the importance of the issue and disseminating 
pertinent information related to the breadth and depth of the issue as it was available.    
33 
 ANNEX 3:  MAJOR DELIVERABLES  
MISSION ENGAGEMENT P LAYBOOK  
The Mission E ngagement Playbook is available  to the public and can be accessed online here.   
ONE -PAGER ON ACHI EVING EFFECTIVE UPTAKE  
The o ne-pager on Achieving Effective Uptake  is available to the public and can be accessed online  here.  
ONE -PAGER ON 10 STEPS TO BETTER M ISSION ENGAGEMENT  
The one -pager on 10 Steps to Better Mission Engagement  is available  to the public and can be accessed 
online here. 
ONE -PAGER ON STRENGT HENING ECOSYSTEM INI TIATIVES  
The one-pager on Strengthening Ecosystem Initiatives is availabl e to the public and can be accessed 
online here.  
ONE -PAGER  ON CREATING AN ADAPTIVE , ACTION -ORIENTED TEAM  
The one -pager on Creating an Adaptive, Action -Oriented Team  is available to the public and can be 
accessed online here.   
34 
 ANNEX 4: SUSTAINED UPTAKE PIL OT COD ING HIERARCHY  AND 
FREQUENCIES  
 
CODE  FREQUENCY  
DECISION -MAKING  13 
 DECISION MAKING PRO CESS 203 
 DECISION MOTIVATING  FACTOR  32 
 DECISION POINT  57 
 DECISIONS MADE W/O IMPLEMENTATION  10 
 IMPLEMENTATI ON PROCESSES  219 
 INDECISION POINT  1 
 NEGATIVE DECISION  7 
 POSITIVE DECISION  11 
DEFINITIONAL  1 
 ACCELERATION  0 
 ADOPTION  2 
 ENABLING ENVIRONMEN T 1 
 INTEGRATION  3 
 OTHER  5 
 SCALE/SCALING  16 
 SUSTAINABILITY  2 
DEVELOPMENTAL EVALUA TION  0 
 ACCULTUR ATION  3 
  BARRIER  13 
  ENABLER  16 
  LESSON LEARNED  0  
35 
   OTHER  1 
 DE CROSSOVER  0 
 DE OUTCOMES  8 
 DE RECOMMENDATION  2 
 ENGAGEMENT WITH EVAL UATOR  17 
 EXPRESSED CONCERNS  0 
 IMPLEMENTATION LESS ON LEARNED  2 
 POTENTIAL RESEARCH QUESTIONS  10 
 PROGRESS/S HIFT OF DE  14 
 STAKEHOLDER EXPRESS ED DESIRED OUTCOMES  6 
ENABLING ENVIRONMENT S WORK  98 
 DFS- B 0 
 DFS- SIERRA LEONE  44 
  OUTCOME 1  32 
  OUTCOME 2  4 
  OUTCOME 3  8 
 DIGITAL INCLUSION - A4AI 26 
  OUTCOME 1  5 
  OUTCOME 2  29 
  OUTCOME 3  4 
  OUTCOME 4  4 
  OUTCOME 5_EMERGING  2 
 DIGITAL INCLUSION - MWOMEN  10 
 SOGE -NIGERA  18 
  OUTCOME 1  18  
36 
   OUTCOME 2  5 
 SOGE -UGANDA  0 
 TYPES OF EVALUATOR W ORK 7 
FUTURE OF LWPS  66 
INFLUENCING FACTORS  77 
 AGENCY ENABLING ENV IRONMENT  107 
  CHAMPION  160 
  CURRENT POL ITICAL CLIMATE  37 
  EXISTING PRACTICES  46 
  LEADERSHIP  172 
  ORGANIZATIONAL CUL TURE  238 
  ORGANIZATIONAL STR UCTURE  103 
  OTHER  91 
 EVIDENCE/HISTORICAL  LESSONS LEARNED  498 
 M&E 242 
 STAFF CAPACITY  262 
  + 191 
  - 84 
  RECOMMENDATIONS  276 
  ROLES  AND RESPONSIBILITIE S 211 
  TURNOVER/ATTRITION  33 
NPE RELEVANT  0 
PROCESS TRACING  1,514 
 HYPOTHESIS 1  271 
  FAILING  0  
37 
   PASSING  0 
  RIVAL HYPOTHESIS  0 
   SUPPORT FOR RIVAL  HYPOTHESIS  0 
 HYPOTHESIS 10  208 
  FAILING  28 
  PASSING  161 
  RIVAL HYPOTH ESIS 40 
   SUPPORT FOR RIVAL  HYPOTHESIS  0 
 HYPOTHESIS 2  162 
  FAILING  0 
  PASSING  0 
  RIVAL HYPOTHESIS  0 
   SUPPORT FOR RIVAL  HYPOTHESIS  0 
 HYPOTHESIS 3  270 
  FAILING  0 
  PASSING  0 
  RIVAL HYPOTHESIS  0 
   SUPPORT FOR RIVAL  HYPOTHESIS  0 
 HYPOTHE SIS 4 252 
  FAILING  0 
  PASSING  0 
  RIVAL HYPOTHESIS  0 
   SUPPORT FOR RIVAL  HYPOTHESIS  0 
 HYPOTHESIS 5  248 
  FAILING  38  
38 
   PASSING  95 
  RIVAL HYPOTHESIS  147 
   SUPPORT FOR RIVAL  HYPOTHESIS  4 
 HYPOTHESIS 6  247 
  FAILING  47 
  PASSING  134 
  RIVAL HYPOTHESIS  33 
   SUPPORT FOR RIVAL  HYPOTHESIS  0 
 HYPOTHESIS 7  207 
  FAILING  41 
  PASSING  132 
  RIVAL HYPOTHESIS  27 
   SUPPORT FOR RIVAL  HYPOTHESIS  0 
 HYPOTHESIS 8  204 
  FAILING  131 
  PASSING  48 
  RIVAL HYPOTHESIS  19 
   SUPPORT FOR RIVAL  HYPOTHES IS 0 
 HYPOTHESIS 9  214 
  FAILING  0 
  PASSING  60 
  RIVAL HYPOTHESIS  163 
   SUPPORT FOR RIVAL  HYPOTHESIS  0 
QUESTIONS  0 
 NOTES  0  
39 
 RELEVANT ACTOR  155 
 BFS 281 
 CDD 36 
 D2FTF  2,070 
 DFS 628 
 DIGITAL INCLUSION  395 
 EIA 10 
 IP/INVESTOR/PARTNER  1,233 
 LAB LEADERSHIP  204 
 MISSION  418 
  NEPAL  1,276 
  NIGERIA  0 
  PERU  0 
  PHILIPPINES  0 
  RWANDA  100 
  SIERRA LEONE  6 
  UGANDA  1,379 
 OTHER (AGENCY)  394 
 OTHER (EXTERNAL)  791 
 POWER AFRICA  286 
 SOGE  608 
SPECIFIC RECOMMENDAT ION 142 
 IDENTIFIED B ROADER OPPORTUNITIES  44 
SUPPORTIVE EVIDENCE  40 
TO WATCH  121  
40 
 WHAT DOESN'T WORK  90 
 CAPACITY/SKILL  85 
 CONTEXTUAL FACTORS  82 
 ENGAGEMENT  208 
 EVENTS/ SPECIFIC ACTIVITIES  127 
 EXAMPLES  56 
 EXTERNAL COMMUNICAT IONS  12 
 FUNDING/FINANCES  77 
 LOGISTICS  74 
 PEOPLE/RELATIONSHIP S 101 
  COORDINATION  48 
  STAKEHOLDER BUY -IN 15 
 PROCESS  100 
  EVALUATION CRITERI A/PROCESSES  25 
 SUPPORTIVE EVIDENCE  7 
 TOOL  10 
  M&E, DATA  16 
WHAT WORKS  156 
 CAPACITY/SKILL  293 
  DELIVERABLES  12 
  SUPPORT ON THE GRO UND 65 
 CONTEXTUAL FACTORS  307 
 ENGAGEMENT  568 
  KNOWLEDGE OF EXIST ING PROGRAMS/PROJECT S 51 
  LEVERAGING EXISTIN G STRATEGIES  105  
41 
   PARTNERSHIP APPROACH  94 
  SPECIFICITY OF REC OMMENDATIONS  23 
 EVENTS/SPECIFIC ACT IVITIES  503 
 EXAMPLES  119 
 EXTERNAL COMMU NICATIONS  69 
 FUNDING/FINANCES  251 
 LOGISTICS  93 
 PEOPLE/RELATIONSHIP S 486 
  COORDINATION  456 
  STAKEHOLDER BUY -IN 74 
 PROCESS  277 
  EVALUATION CRITERI A/PROCESSES  129 
 TOOL  128 
  M&E, DATA  254 
WORKING RELATIONSHIP S STRUCTURE  65 
TOTALS  25,356 
   
42 
 ANNEX 5: METHODOLOGICAL DETAILS OF EVALUATIVE ACTIVITIES  
PRE-WORKSHOP RESEARC H 
The Evaluator developed the design report as a plan for initial DE research prior to delivery of the 
Acculturation Workshop — namely though key informant interviews (KIIs) with k ey D2FTF and SOGE 
stakeholders.  
FOCUS OF THE STUDY  
PURPOSE OF THE RESEARCH  
The purpose of these interviews is to collect sufficient data regarding rhetoric around scaling and the 
definitions used by USAID Global Development Lab staff, understand the desire d outcomes and 
expectations around the implementation of the Developmental Evaluation, identify existing ideas for 
research questions among Lab Staff, and collect any other priority current and historical Lab -related 
information to inform the development o f the Acculturation Workshop to be held on May 1st and 2nd. 
Given the speci ficity of the information needed and the desire to ensure the Workshop is tailored for the 
desired focus and level of acculturation of Lab staff, key informant interviews were deter mined to be the 
best way to go about collecting this data, in addition to o ngoing document review of requested 
documentation from EIA, LWP 2, and LWP 3, as well as more informal conversations that are occurring 
as the evaluator is embedding into the Lab.  
KEY DELIVERABLES  
1. A refined Workshop Agenda that incorporates existing draft ed workshop ideas and builds on the 
focus and structure of the Workshop based on feedback and incorporating content from the 
interviews.  
a) A list of inclusive and refined research qu estions with an identified priority research 
question based on interview da ta to suggest at the Workshop.  
b) Definitions sourced from Lab staff concerning scaling, integration, adoption, sustainability, 
and any additional related rhetoric to inform discussio ns around agreed upon working 
definitions and renaming of the DE.  
c) An unders tanding of the various key stakeholders’ level of acculturation to developmental 
evaluation to determine what acculturation exercises are best suited for the audience.  
d) Identified b arriers, enablers, and expectations at this early stage of what the DE will be 
able to accomplish and how it will be able to integrate into the various Lab teams.  
2. Data will also be coded for more ongoing analysis towards the priority research question(s).  
INCLUSION/EXCLUSION CRITERIA OF PARTICIP ANTS  
Participants/Interviewees  were selected based on the need to speak with key decision -making and leading 
staff of each LWP, EIA, and the Global Development more broadly. These key decision -makers are also 
some o f the existing and necessary champions to ensuring the successful imple mentation of the DE, as 
well as crucial to building buy -in and ownership in a priority research question(s). The expectations of 
these stakeholders will influence the perceptions of suc cess, value add, and responsiveness of the DE. 
Knowing that more KIIs w ill be necessary in order to collect the necessary data for the research 
question(s), once decided upon, we also wanted to minimize the burdens to the teams on the number of 
interviews/ time required by the Evaluator at this stage, as well as ensure the int erviews were right -sized 
to ensure sufficient but not superfluous input into the Acculturation Workshop.   
43 
 DATA MANAGEMENT  
HOW DATA (AND RECORDS) WILL BE COLLECTED, STORED , AND SECURED  
All interview notes will be collected either via transcribed Word files  or via written documentation that 
can then be uploaded to the appropriate destinations. The notes will then be stored on Google Drive 
within the private DEPA MERL Consortium Working fo lder that currently has restricted access for just 
the active members o f the DEPA MERL Consortium from the implementing partners who have all signed 
an NDA associated with their contracts, as well as select members of the EIA staff at the Global 
Developmen t Lab, primarily the COR on the DEPA MERL contract. Notes containing an y information that 
the interviewee identifies as SBU or otherwise sensitive will be further restricted to access by just the 
Evaluator. Notes will also be uploaded and coded into Dedoos e, utilizing the secure Dedoose Cloud drive. 
Quotes will be anonymized in any reports or other outputs from these KIIs, and any sharing of data 
otherwise will have all sensitive files scrubbed from the shared data set.  
INFORMED CONSENT  
DESCRIPTION OF CONSENT/ ASSENT PROCESS  
Each participant will be briefed on the purpose of the interview and given an overview of the content and 
focus of the interview questions. Participants will then be told about the process of information collection, 
analysis, and what t hey should expect to hear and/or receive back from the research process  and within 
what timeline, mostly focusing on seeing their input directly in the content and in shaping the Acculturation 
Workshop Agenda without any personal identifiable information. The Evaluator will then explain the 
intended data usage and storage pro cedures and verbally ask for informed consent. If consent is given, the 
Evaluator will transcribe consent. If consent is not given, the interview will be terminated and any early 
notes taken deleted.  
STUDY METHODS AND PR OCEDURES  
STUDY DESIGN  
This is not a  full study, rather a small set of KIIs being utilized to inform the design and content of the 
Acculturation Workshop to kickoff the Developmental Evaluation for the USAID Global Develo pment 
Lab. In order to keep the data collection burden small and have a  lean intervention impact, identification 
of key informants was minimized to those we felt would provide sufficiently detailed information for 
development of a robust and informed Accul turation Workshop. Other necessary information and 
validation, when nec essary, will be pulled from document review. Additional interviews with a larger batch 
of stakeholders is anticipated post -Workshop targeting data needs to inform DE research based on t he 
prioritized research question.  
The KIIs themselves will be semi -structured interviews focusing on the rhetoric around scaling and the 
definitions used by USAID Global Development Lab staff, understand the desired outcomes and 
expectations around the imp lementation of the Developmental Evaluation, identify existing ideas fo r 
research questions among Lab Staff, and collect any other priority current and historical Lab -related 
information. In total 12 interviews are currently being scheduled with 4 EIA staf f, the Acting Executive 
Director of the Lab, three LWP 2 staff, three L WP 3 staff, and Kristin Cronin who drafted the PMP for 
the Lab. These interviews will be paired with data from ongoing document review from shared Lab 
documentation according to the fol lowing coding hierarchy  (which is up for adaptation itself).   
44 
 LIMITATIONS  
Given the timing of these KIIs, there are implications of shifts in rhetoric and opinions based on th e more 
recent budget and other discussions happening at USAID that may not hold true for more long term 
perspectives and desires for the DE. If politically possible, this will be specifically touched upon in the 
interviews.  
As mentioned, this is a lean ap proach to informing the Acculturation Workshop and the Evaluator 
acknow ledges that not all Lab voices, opinions, and DE ideas will be captured through this process. Given 
the intentional participatory process and already scoped sessions of the Acculturatio n Workshop, there 
will be opportunities for all relevant stakeholders t o contribute to the formation of the research 
question(s), working definitions, and other preliminary DE deliverables even if these stakeholders are not 
involved in this first round of KIIs.  
ANALYSIS PLAN  
The KIIs, along with currently shared Lab documentation, will be inputted into Dedoose and coded 
according to the above referenced coding hierarchy (which is inclusive of anticipated future coding nodes 
in addition to nodes specific to  the Acculturation Workshop). Data will collectively be analyze d to 
determine which teams utilize which “scaling”  rhetoric, whether there are gender implications for how 
‘scaling’ is discussed, and the influence of external and/or historical evidence to th ese conversations and 
use of “scaling” rhetoric and implicit de finitions. Analysis will also look at patterns and themes emerging 
from all the suggested research questions, using graphic representation of references to particular 
questions if relevant, and  leading to the selection of one or two priority research quest ions to be suggested 
at the Workshop. The data will further be analyzed to identify any barriers and enablers that might indicate 
current levels of acculturation and any significant possible ‘s ticky’ aspects of acculturating the stakeholders. 
This will inf luence the structure of the Workshop Agenda, as well as the sessions content. The data is also 
being coded into the broader DE project file and will potentially contribute to analysis further d own the 
line regarding the prioritized research question(s).  
INTERVIEW QUESTIONS  
***Not all questions are relevant for all interviewees. A question matrix to identify which questions will 
be asked of what interviewees has been added to the KII list.  
DE FAMILIARITY/PRELIMINA RY READINESS:  
1. If an opportunity to adapt in order to improve programming comes up, what is the decision making 
process on your team?  
2. Can you give me a recent example of how your team has adapted or changed an approach based 
on lessons learned?  
a. How is implementation of  the decision going?  
3. Have you heard of developmental evaluation before? If so, how?  
[If affirmative answer for #3, ask the following]  
4. Do you think developmental evaluation is an appropriate fit for the Global Developmen t Lab? If 
so, why?  
5. Do you think d evelopmental evaluation is an appropriate fit for your particular team? If so, why?  
6. How would you describe the role of the Evaluator?   
45 
 a. How do you see the Evaluator integrate into the work of your team?  
DEFINITIONS OF SCA LE, INTEGRATION, AND ADOPTION : 
7. What are some key terms you use regarding your efforts to increase uptake of your team’s 
activities?  
8. How do you define [ insert key term(s) mentioned in #1 ]? 
9. What are terms or definitions of terms that you see used regularly that are worrisome to you? 
Why do they concern you?  
10. What would your preferred name be for the DE that encompasses what you think the focus 
should be?  
DE RESEARCH QUESTION S, DESIRED OUTCOMES,  AND CONCERNS  
11. What would you like to learn from the DE?  
12. Do you have any questions in mind that yo u’d like the DE specifically to tackle and hopefully 
answer?  
13. If time and resources only allowed for one outcome from the DE, what would your preferred 
outcome be? Would that satisfy you?  
14. What concerns do you have about t he implementation of the DE?  
a. External implications on its implementation?  
b. Any barriers to successful implementation that you see?    
46 
 RQ #1 INCEPTION REPO RT 
The Evaluator developed the design report to lay out a plan to respond to RQ1.  
Focus of the Study  
DE Research Question #1: What are the conditions and working relationships necessary in the LWPs, the Lab, and 
its partners to achiev e sustained uptake inter nally (Missions and Bureaus) and externally ? 
Purpose of the research  
The purpose of these interviews and focus groups is to contribute to data collection to answer the DE 
Research Question #1, cited above. The focus of these convers ations will particularly focus on the working 
relationships of the LWP teams, attempting to u nderstand the preferred and perceived ‘strong’ 
relationships by the LWP teams, the ‘weakest’ relationships and ones that cause frustration, and their 
conceptualiza tion of what a strong or weak relationship is as it relates to achieving sustained uptake. 
Further data collection will take place directly with selected partners that exemplify perceived strongest 
and weakest relationships to the LWP team to determine if it is a mutual perception and better identify 
any replicable conditions, processes, SOPs, etc . to foster more positive and productive working 
relationships to achieve sustained uptake.  
Key Deliverables  
1. A mini report out on identified conditions and workin g relationship components from team 
interviews and focus groups.  
2. Immediate feedback to teams  as emerges related to enhancing and/or adapting existing 
relationships.  
3. Requests to teams for strategic follow on interviews with key partner POCs.  
4. Data will al so be coded for more ongoing analysis towards the priority research question(s).  
Interviewee s 
First Round:  
1. SOGE:  
a. [name]  
b. [name]  
c. Full team Focus Group on Power Africa relationship  
2. D2FTF  
a. [names] around Uganda Mission and IPs  
b. **Determine if Nepal or Cambodia  is stronger relationship wise and then set up an 
interview with either [name] or [name]  
c. Full team Focus Group on BFS relationship  
Second Round:  
TBD based on extent on information collected in first round and gaps to fill before contacting partners 
outrig ht.  
47 
 Update: 06.21.2017  
1. SOGE:  
a. Sarah Bieber - Shell Foundation  
b. Shell Foundation: Pra[names]deep and Gareth  
c. Vitalite  
d. GLP 
e. Power Africa: MHR and [name]  
2. D2FTF:  
a. Nepal Mission POCs - conference call focus group possible  
b. BFS: 
c. [name]  
d. [name]  
e. Data Working Group  
Inclu sion/Exclusion Criteria of Participants  
Participants/Interviewees were selected  based on the need to speak with key LWP partner POCs, or 
those holding/managing relationships with core partners, the team -identified ‘strongest’ relationships, and 
the team -identified ‘weakest’ relationships. Focus groups were done with all core members of the LWPs 
who are fully situated within the LWP portfolio and regularly work with or communicate with the partner 
Bureau.  
Knowing that more KIIs will be necessary in order to  collect the necessary data for the research 
question(s), including a second ro und needed for this question both with LWP staff, as well as a round 
with select partners themselves, we wanted to minimize the burdens to the teams on the number of 
interviews/ time required by the Evaluator, as well as ensure the interviews were right -sized to ensure 
sufficient but not superfluous data collection to answer the research question(s).  
Data Management  
How will data (and records) be collected, stored and secured  
All interview notes will be collected either via transcribed Word files, Google D ocs, or via written 
documentation that can then be uploaded to limited access folders on the DEPA MERL Google Drive. For 
SBU content or otherwise sensitive material, it will eit her remain offline until approval is received or 
uploaded to a separate folder with access limited to the Evaluator (and persons sharing the file if relevant). 
Most notes will be stored on Google Drive within the private DEPA MERL Consortium Working folder  
that currently has restricted access for just the active members of the DEPA M ERL Consortium from the 
implementing partners who have all signed an NDA associated with their contracts, as well as select 
members of the EIA staff at the Global Development La b, primarily the COR on the DEPA MERL contract. 
Notes will also be uploaded and  coded into Dedoose, utilizing the secure Dedoose Cloud drive. Quotes 
will be anonymized in any reports or other outputs from these KIIs, and any sharing of data otherwise will 
have all sensitive files scrubbed from the shared data set .  
Informed Consent   
48 
 Description of consent/assent process  
Each participant will be briefed on the purpose of the interview and given an overview of the content and 
focus of the interview questions.  Participants will then be told about the process of information collection, 
analysis, and what they should expect to hear and/or receive back from the research process and within 
what timeline, mostly focusing on seeing their input directly in the content  for identified, and yet to be 
identified,  points of input for the DE, without  any personal identifiable information. The Evaluator will 
then explain the intended data usage and storage procedures and verbally ask for informed consent. If 
consent is given , the Evaluator will transcribe consent. If consent is not given, the interview  will be 
terminated and any early notes taken deleted.  
Study Methods and Procedures  
Study design  
This is a small set of KIIs and a handful of focus groups (one for each LWP aro und the partner Bureau 
relation, with a potential third with Mission POCs withi n the Lab) being utilized to answer the DE 
prioritized Research Question #1 listed above. In order to keep the data collection burden small and have 
a lean intervention impact, identification of key informants was minimized to those the Evaluator felt 
handled and knew the outliers, both the bright spot relationships that were perceived as the most effective 
at achieving or contributing to sustained uptake, as well as those that h ave proved the most frustrating or 
have not been as productive as anticipated. This selection was made in order to help formulate a theory 
and to identify conditions for working relationships (the identification, implementation, and management) 
that should  be avoided and those most effective at contributing to sustained uptake.  
Data collection started with light -touch stakeholder mapping conducted with both LWPs. It was 
completed in two rounds including a brainstorming session and then a request for input into a partially 
completed database built from the brainstorming session. Both rounds were supported with visualization 
of the stakeholder map given to the individual teams to confirm and refine the relationships. As is 
appropriate, helpful to the teams, a nd relevant to answering the DE research questions, the stakeholder 
maps will b e refined and built up with additional partner -related data. First round KIIs and the need to do 
partner Bureau focus groups for each LWP came from identification of strategic a nd particularly influential 
or troublesome partnerships. Sampling was done base d on this information, paired with ongoing findings 
from document review and data from the pre -Acculturation Workshop round of KIIs.   
Sampling over the course of the DE and in response to the research questions is theoretical and therefore 
iterative in na ture, also reasoning why second round interview candidates from the partners have yet to 
be selected (see above), and with theories developing throughout data collection and ana lysis regarding 
each prioritized research question.  
Other necessary informatio n and validation, when necessary, will be pulled from document review. 
Additional interviews with a larger batch of stakeholders, including select partners, is anticipated.  
The KIIs and Focus Groups themselves will be semi -structured focusing on Partner Setup, Roles and 
Responsibilities, Communication, Looking Forward, and Comparative questioning. Additional document 
review will include targeted analysis of selection processes an d criteria. The total number of interviews 
and focus groups to answer  Question #1 has yet to be determined, but will occur in rounds and progress  
49 
 as necessary to achieve reasonable saturation. These interviews will be paired with data from ongoing 
document  review from shared Lab documentation according to the following  coding hierarchy  (further 
adaptation and refinement of which will continue during coding of these interviews as t hemes and patterns 
emerge around Research Question #1).  
Limitations  
As with the pre -Acculturation Workshop KIIs, there are still implications of shifts in rhetoric and opinions 
based on the continued  budgeting and restructuring discussions happening at U SAID that may not hold 
true for more long term perspectiv es and desires for the DE. This is specific reviewed, coded for, and 
analyzed from ongoing meetings and in interviews where and when appropriate.   
Stakeholder mapping and partner commentary has been  subjectively provided by LWP team members 
thus far. The effectiveness and productivity of the ‘strongest’ identified relationships will be compared to 
their reporting to date, outputs and outcomes related to sustained uptake to verify perceived levels of 
efficacy. A second round of data collection will also be done on the partner side, as mentioned, to see if 
perceptions are mutual related to the strengths and/or weaknesses as applicable of the partnership(s).  
Analysis Plan  
The KIIs and Focus Groups, alon g with currently shared Lab documentation, requested comp any profiles, 
any reporting obtained, and relevant notes from other meetings that highlight partner interactions and 
working relationships, will be input into Dedoose and coded according to the above  referenced coding 
hierarchy (which is inclusive of antic ipated future coding nodes in addition to nodes specific to the 
Research Question #1). Data will collectively be analyzed to determine both individual and comparative 
elements of effective and ‘frust rating’ working relationships as it relates to achieving goals around sustained 
uptake. Analysis will also look at patterns and themes emerging from all the suggested research questions, 
using graphic representation of references to particular questions if  relevant. The data will further be 
analyzed to identify any barriers and enablers that might indicate current avenues for adaptations of 
existing relationships, and any possible input into upcoming partnering decisions. The data is also being 
coded into t he broader DE project file and will potentially contribut e to analysis further down the line 
regarding the prioritized research question(s).  
Focus Group  
1. Focus groups are focused on the partnered Bureau relationship for each LWP (D2FTF with BFS; 
SOGE with Power Africa) and how those partnerships have enabled or created barriers to 
achieving sustained uptake for the LWPs and evolved over time.  
a. Can you describe the process of setting up your relationship with BFS/Power Africa?  
b. Was there anything unique about [names’]  engagement with Power Africa/BFS that 
affected t he partnership setup or progress?  
2. People have spoken about the influence of having Judy Payne as a key influencer or Brian King’s 
networking on the BFS side as critical to ensuring a working relatio nship with BFS. What about 
those relationships was unique  (D2FTF only)? Are there any other pivotal people (both teams)?  
3. Do you have an integrated work plan with your partner Bureau? How did it come about? Has it 
been successful? If so, in what ways?    
50 
 4. What is the role of BFS’s/Power Africa’s ecosystem (i.e. bu dget, development policy, resources, 
leadership etc.) in relation to the LWP ecosystem? Has it enabled or inhibited the LWP work in 
any way?  
5. How regular/frequent is communication with your partner B ureau? How involved are they in the 
day to day happenings  of the LWP and how does that affect your work?  
6. What are specific, identifiable barriers that prevent LWP work from happening as planned caused 
by BFS/Power Africa, if any?  
7. What are frustrations in  working with BFS/Power Africa, but perhaps are not barri ers to the work 
itself?  
8. What are bright spots, specific instances when things have gone smoothly or worked well in terms 
of achieving objectives towards sustained uptake? How was BFS/Power Africa en gaged in that 
process?  
Interview Questions  
Partner Setup : 
1. How did you identify this partner pre -evaluation through selection criteria? If there are selection 
criteria, can you share them?  
2. How did you set up a relationship with this partner?  
a. Is it a formal  partnership? What was necessary to make it formal?  
b. Did y ou engage or work together informally before it was official? If so, how?  
Roles and Responsibilities:  
1. How have you delineated the roles and responsibilities of this partnership?  
a. Is it formalized in any way?  
b. How have the roles and responsibilities changed over time?  
2. How do you think you've held up to what you set forth that you or your team would do (LWP)?  
3. How do you think the partner has done? Please give specific instances of when they have met 
and not met the set or perceived roles and responsibilities of this partnership.  
Communication:  
1. Are you able to get in touch with this partner when you need to?  
a. Are they responsive?  
2. How effective do you feel the SOPs for communication are? Are they formalized , why or why 
not? 
3. What has worked well in your communication with this partner?  
a. What has been the most difficult?  
Looking Forward:  
1. What is a bright spot event with this partner, something that went well that really stood out?  
2. What has been this partner’s m ost signi ficant impact on progress towards sustained uptake 
within your LWP efforts?  
3. What do you wish would improve with this partner, why?   
51 
 Comparative Questions:  
Shell Foundation (Maurice):  
1. Why is this partnership a valued partnership?  
2. Are there other p artnerships that could have this level of significance for SOGE’s work? If so, is 
there a strategy for engagement?  
Pillar I (Maurice):  
D.Light  
Fenix  
GLP 
ORB Energy  
PEG 
Shinbone Labs  
Village Energy  
Vitalite  
 
1. You labelled all the Pillar I partnersh ips as strong. What makes a strong partnership for you?  
2. With 8 partnerships, there has to be some variance between them. Are there any standouts 
amongst the 8 grantees on Pillar I? Why are they a standout?  
a. Do all these grantees report about the same level s of expected outputs/outcomes?  
3. Have all of these partnerships always been strong? What relationship was the easiest to setup and 
why? What relationship took the longest to setup and wh y?  
4. D.Light seems to be a bit of a showcase partnership. Why is that?  
a. Is it a partnership model you’d like to replicate or do you have a better/stronger 
partnership in mind?  
USAID PCM vs. USAID DIV relationships (Molly):  
*Notes from SOGE Partner Map Data  from Molly  
USAID - 
Connectivity  Medium  Early stages of exploring how C onnectivity/SOGE teams can collaborate   
52 
 USAID - DIV Strong  DIV provided significant support in last year's SOGE awards and regularly 
sends pipeline proposals to SOGE for review  
USAID - PCM Weak  Multiple attempts to engage PCM but capacity doesn't seem like it's there  
 
1. You listed each partnership along a spectrum from weak to strong, what constitutes a strong 
partnership for you?  
2. What do the different OU partnerships contribute or provide to SOGE (USAID Connectivity, 
DIV, PCM)?  
3. What does SOGE provide to eac h of these OU partners?  
4. How would you describe the capacity differences between DIV and PCM? How is the structure 
of those relationships different (level of POCs, h ow they interact with other OUs, etc.)?  
5. How did you get DIV ‘bought -into’ the SOGE vision?  Do you think that is why they send you 
pipeline proposals?  
6. Was your DIV relationship always strong? How has it changed over time?  
7. What would constitute a strong r elationship with PCM for you? What are the specific barriers, 
even specificities around cap acity that have prevented that type of relationship from developing 
thus far?  
Norfund vs. rest of investor portfolio (Molly):  
1. Is there a model for investor partners hips that you are aiming for? What are the unique 
implications of the energy sector and off -grid energy in particular in establishing and maintaining 
these partnerships?  
2. How do you identify investor partners in this sector? Have you developed formal selec tion 
criteria/processes? Do you think they’re effective?  
3. What do you expect from an invest or partnership? How are you capturing output or progress in 
these partnerships?  
4. What does SOGE provide to these partnerships? How are you aligning with investor int erests?  
5. Even though you mentioned that there has been minimal engagement with each investo r in the 
portfolio you maintain and mentioned these should be strong relationships in the future, you 
placed all relationships except Norfund at Medium. Why does Nor fund standout? How has that 
partnership engagement differed from selection to implementatio n to ongoing management from 
others in the investor portfolio thus far?  
6. You mentioned difficulties with Sunfunder, tensions related to identifying ideal funding cha nnels. 
Did you go through a similar process with other investors? If so, why do you think i t was more 
difficult with Sunfunder? How are you working towards resolving this?  
Interviews/Focus Groups with SOGE/D2FTF partners:  
7. (SOGE -Shell) Is there a model fo r investor partnerships that you are aiming for? What are the 
unique implications of the en ergy sector and off -grid energy in particular in establishing and 
maintaining these partnerships?   
53 
 (SOGE - Shell) What do you expect from an investor partnership? How are you capturing output 
or progress in these partnerships?  
8. Can you describe the process of setting up your relationship with SOGE/D2FTF?  
a. What was the most appealing aspect of the partners hip?  
b. What were you most concerned about at the beginning, before starting the partnership?  
c. Were there any key people who were crucial to the process of secu ring the 
partnership? Why were they so important?  
9. How have you delineated the roles and responsibi lities of this partnership internally? How does 
that impact the partnership?  
10. How regular/frequent is communication with SOGE/D2FTF? What information do you w ant 
from SOGE/D2FTF? Is there any way you would improve your communications?  
11. What are specific, id entifiable frustration or barriers in your opinion to effective work with 
D2FTF/SOGE?  
12. What is one thing you would improve in the partnership?  
13. In your opini on, what is the best outcome thus far from the partnership? What outcome are 
you most anticipating?  
14. What is one thing you would want to replicate from the partnership (activity, process, person, 
etc.)?  
15. How, if in any way, does this partnership influence yo ur work?  
a. Your broader objectives in this sector?  
b. Thinking about this sector?  
c. Your fu ture planning?  
d. Has engagement in this partnership spurred any additional activities, projects, or 
processes that are indirectly related, or that the Lab team is not a par t of?  
16. Are there any learnings from the developmental evaluation that you are particul arly keen to 
hear about?    
54 
 PROCESS TRACING  
The Evaluator developed the following design report to guide work on the primary evaluati ve effort 
under RQ —a case study of the D2FTF, SOGE, DI, and DFS workstreams with SUAID/Uganda. This 
study employed process tracing to assess factors that contribute to su ccessful or unsuccessful Mission 
engagements.  
Purpose of the Study  
A significant part  of achieving sustained uptake of a particular intervention or initiative at USAID is 
dependent upon engagement with and uptake at Missions. Understanding the enabling fa ctors for 
engagement at a Mission, such as interest, bandwidth, and resources, as well  as efficacy of different 
processes of engaging the Mission, can help improve uptake efforts within the Lab and the Agency more 
broadly. USAID/Uganda presents a unique op portunity to examine these dynamics, as all four Lab teams : 
have ongoing work with thi s Mission; perceive the Mission to have a strong enabling environment; and 
currently have successful workstreams with the Mission, IPs, and even private sector actors in this space. 
Thus, the objective of the Uganda case study is to facilitate learning sur rounding strategies to promote 
sustained uptake. This will complement the DE’s wider efforts, as well as highlight contributing enabling 
environment factors to be used fo r Mission identification criteria in the future.  
Evaluation Questions  
The underpinni ng evaluation question for this study comes from the three main questions from the Uptake 
Developmental Evaluation. This Uganda Case Study is one evaluative effort among four data collection 
processes designed to answer DE Research Question #2:  
How do we determine which current Lab approaches are most effective at sustained uptake? What has been the 
perceived and real value add of the approaches? What can we learn from La b models?  
In addition to the overarching DE Research Question that led to this study, the evaluation questions this 
study seeks to answer are as follows:  
1. How do Lab teams select Missions to approach? Are there some criteria that indicate more 
potential for  uptake than others?  
2. For the teams’ work dependent on Mission uptake, how do they sec ure initial 
activities/partnerships with the Mission?  
3. What are the turning points in engagement with the Missions? When does a Mission -Lab initiative 
solidify?  
4. How effec tive are the various teams at achieving uptake with their audiences (available data to  
date)?  
5. Are there some Mission engagement strategies that are more effective than others?  
6. What are the barriers and enablers to engaging with a USAID Mission?  
7. What are t he contextual factors at the Uganda Mission that contribute to successful uptake of 
Lab-based initiatives?  
Study Methods and Procedures  
Study design   
55 
 As noted above, the Lab’s engagements in Uganda present a unique opportunity to compare and contrast 
the various teams’ engagement strategies and enabling environment factors in the same context. The case 
study will further test the conclusions drawn in response to Research Question 1 about the teams’ uptake 
strategies.   
The team conducting the case study wil l employ a series of minimally invasive qualitative data collection 
and analysis methods. First, the team will develop a coding hierarchy, which it will use as it conducts an 
extensive document review (see section IX below for a preliminary list of documen ts). From this review, 
the team will develop a series of draft hypotheses in resp onse to the guiding lines of inquiry for this case 
study. The hypotheses will suggest what types of approaches are or are not successful in engaging Missions. 
The team will tr iangulate these hypotheses by conducting a handful of Washington -based interviews  with 
key stakeholders knowledgeable about the Lab’s work streams in Uganda.  
Once the team refines the hypotheses to the extent necessary, it will test them through a second round 
of data collection with Uganda -based stakeholders. The team will conduct KI Is and possibly a limited 
number of focus group discussions with USAID/Uganda staff, as well as local partners and counterparts. 
The DE team understands the need to minimize b urdens on USAID/Uganda staff and thus will avoid 
planning for in -country data col lection if at all possible; rather, it will plan to conduct KIIs remotely. 
However, the team may find through the course of the document review that it is essential to conduct  a 
short TDY to Kampala to collect data essential to properly respond to the line s of inquiry.  
The purpose of this second round of data collection will be to test the hypotheses generated in accordance 
with a Process Tracing methodology. Once all the data  are coded and analyzed, the team will subject the 
hypotheses to four tests:  
• 'straw in the wind', which lends support for an explanation without definitively ruling it in or out,  
• 'hoop', failed when examination of a case shows the presence of a necessary causal condition, 
when the outcome of interest is not present. Common hoop condit ions are more persuasive than 
uncommon ones  
• 'smoking gun', passed when examination of a case shows the presence of a sufficient causal 
condition. Uncommon smoking gun conditio ns are more persuasive than common ones  
• 'doubly definitive' passed when examinati on of a case shows that a condition is both necessary 
and sufficient support for the explanation. These tend to be rare.  
The team will analyze the data through this framework and at the end of this case study, will be able to 
conclude with relative certain ty that certain approaches have been more successful than others.  
Sampling  
This study will employ theoretical sampling both in the development of hypotheses to be used for pr ocess 
tracing, as well as for the purpose of testing those hypotheses. The initia l purposive sample is based on 
those best informed from the Lab perspective to articulate the intended process of Uganda Mission 
engagement for each team. The hypotheses will then be tested in an iterative manner through engaging 
with three categories of p articipants: Mission staff working with the Lab Teams, other Lab staff working 
with the Uganda Mission, and Lab team external partners connected to workstreams with the Uganda  
Mission (ex: implementing partners that are delivery on services in -country in p lace of Lab team staff). 
Additional input may be solicited from each Lab team’s team lead (current or active during initial Uganda  
56 
 Mission engagement) in order to clarify and provide contexts to the hypotheses. Additional participants 
may be selected over the course of the iterative sampling process as they relate to the team’s work or 
understanding USAID HQ to USAID Uganda Mission engagement processes. This sampling approach w as 
selected as it best supports the chosen study design.  
Inclusion/Exclusion Cri teria of Participants  
Interviewees were selected based on the need to speak with those managing, interacting, or participating 
in implementation of the Lab teams’ workstreams with the Uganda Mission. There are no unique gender 
or other demographic criteria  for inclusion or exclusion of participants, especially given the small sampling 
pool available when considering those who meet the first criteria. Team members that have work ed on 
one discrete activity or as part of covering for another person for a short  period of time (less than one 
month) are not anticipated to be included in the sampling. The iterative and selective nature of this 
sampling methodology will help right -size the case study, while also ensuring sufficient but not superfluous 
data collectio n to answer the research question.  
Informed Consent  
Each participant will be briefed on the purpose of the interview and given an overview of the content and 
focus of the in terview questions. Participants will then be told about the process of informatio n collection, 
analysis, and what they should expect to hear and/or receive back from the research process and within 
what timeline, mostly focusing on seeing their input direc tly in the content for identified, and yet to be 
identified,  points of input for  the DE, without any personal identifiable information. The evaluators will 
then explain the intended data usage and storage procedures and ask the interviewee to sign an info rmed 
consent statement (unless it is a remote interview, in which case the evalua tors will solicit verbal consent). 
If consent is given (through the form or verbally), the evaluators will transcribe consent. If consent is not 
given, the interview will be t erminated and any early notes taken deleted. For the verbal consent 
statement, se e this document.   
Interviewees  
First Round: Lab Team Uganda POCs  
• D2FTF: [name]  
• DFS: [name] 
• SOGE: [name]  (gone); [name]  (Power Africa)  
• Digital Inclusion: [name]  
Second Round: Mission staff working with Lab Teams  
• To be mapped out in the First Round Interviews, for most recent Mission staff contacts, historical 
contacts, and connections t hrough Lab Team POCs  
Third Round: Other connected parties (Lab staff working on Uganda, other Lab team external partners 
and IPs working on their initiatives)  
• Lab, CAI: [name]   
57 
 • NetHope/SIA: [name]  
• mSTAR: TBD  
• ? : [name]  (works with SOGE on the ground)  
• More t o be mapped out in the First Round Interviews with Lab Team POCs  
Limitations 
Due to the attention and amount of Lab interactions with the USAID Uganda Mission, there is likely survey 
fatigue among some members of the Mission staff. More importantly, there is already push back against 
any additional field work in Uganda by L ab staff due to the assumed perception that another evaluation 
effort seen to benefit the Lab, and which burdens Uganda Mission staff, would be unfavorable to Mission 
relations. The evalu ation team is taking every precaution and necessary coordination step  in both raising 
awareness of the possible need for a week or two of fieldwork in Uganda for this case study, as well as 
assessing the gaps in available data sources to determine the need  and make a strong case for field work 
if necessary. KIIs with Missio n staff will be necessary with or without field work, and each one will be 
purposefully selected and seek to minimize the time requests and any repetitive questioning on Mission 
staff. Co nnecting this effort to the Concept Note and development of a Uganda Innovation Center (also 
known as the mini -Lab effort) may be an additional way to frame this study as value add to Mission staff 
versus another evaluative effort seen only to benefit the USAID HQ Lab.  
Also, due to the limited number of staff in each teams ’ interactions, as well as the complexity of other 
ongoing data collection efforts, it is not anticipated that any hypotheses will demonstrate ‘double 
definitiveness’ through the process tracing efforts. This methodology was selected acknowledging this 
limitation. Process tracing was chosen as an appropriate methodology to test the perceptions and 
processes of Mission engagement that have led to successful workstreams for each DE stakehold er team 
with the Uganda Mission, as well as a rigorous and well -known  approach that would lend legitimacy to the 
findings at the Lab and more broadly at the Agency, which is an expressed desire of the DE stakeholders.  
Analysis Plan  
The KIIs, along with al l documentation from the document review, will be input into Dedoose and coded 
according to the above referenced coding hierarchy. Documentation sourced through the document 
review, as well as the first round of interviews will be analyzed first to develop  hypotheses around how 
Lab teams’ engaged with the Uganda Mission (bo th those seen as successful and those identified as 
ineffective). Subsequent rounds of KII data will be analyzed on an ongoing basis to support the theoretical 
sampling and test each hypo thesis until a reasonable conclusion is drawn based on sufficient dat a. Data 
and analysis of each hypothesis will also be compared between Lab teams in order to identify any 
similarities, divergent process paths, or contradictory aspects of engaging the Ug anda Mission from Lab 
HQ. The data will further be analyzed to identi fy any barriers and enablers that might indicate current 
avenues for adaptations of ongoing workstreams with the Mission, and any possible input into upcoming 
partnering decisions. The da ta is also being coded into the broader DE project file and will cont ribute to 
analysis further down the line through the collective data analysis effort for Research Question #2 and in 
the refinement of principles to share under Research Question #3.  
Data Management   
58 
 All interview notes will be collected either via transcr ibed Word files, Google Docs, or via written 
documentation that can then be uploaded to limited access folders on the DEPA MERL Google Drive.  
SBU content or otherwise sensitive material will either remain offline until approval is received or 
uploaded to a separate folder with access limited to the Evaluator, the Project Manager, and the Project 
Director. Most notes will be stored on Google Drive within the private DEPA MERL Consortium Wo rking 
folder that currently has restricted access for just the active  members of the DEPA MERL Consortium 
(whose contracts include non -disclosure provisions), as well as select members of the EIA staff at the 
Global Development Lab —the DEPA MERL COR. Notes  will also be uploaded and coded into Dedoose, 
utilizing the secure D edoose Cloud drive. Quotes will be anonymized in any reports or other outputs 
from these KIIs, and any sharing of data otherwise will have all sensitive files scrubbed from the shared 
data set.  
Key Deliverable  
The team will produce a 10 -12 case study report that contains the following components:  
• Overview of each entity’s implemented model in Uganda;  
• Review of Uganda Mission enabling environment;  
• Demonstration of sustainability factors o bserved in model impleme ntation; and,  
• Findings and conclusions around effective (and ineffective) components of the various models for 
sustained uptake.  
Tools  
First Round Interviews (rival hypothesis generating)  
• First Round: Interview with each Team’s Uga nda POC 
• [name]  (Digital Inclusion)  
• [nme]  (Power Africa/SOGE)  
• [name]  (ex-SOGE)  
• [name]  (DFS/D2FTF)  
*See Informed Consent Above  
Eval Q: How do Lab teams select Missions to approach? Are there some criteria that indicate more potential for 
uptake than others?  
1. How did you identify Uganda as a Mission you wanted to engage with?  
a. What was the rationale for approaching Uganda in partic ular? 
b. What criteria (if any) were used?  
c. Were there any other internal or external factors that influenced the final selection?  
Eval Q: For the teams’ work dependent on Mission uptake, how do they secure initial activities/partnerships with 
the Mission?  
2. What were your initial ideas for activities/engagements with this Mission?   
59 
 a. How do those compare to the activity(s) that were initiated  at first (evolution of those 
ideas, selection from those ideas, something new, etc.)?  
3. Can you describe the process from firs t outreach to securing the first activity with the Mission? 
Prompts: Who reached out? To whom? When? How so?  
4. Is there anything uniq ue about this initiative process compared to the other Missions you work 
with?  
5. What steps (if any) did your team take to faci litate/secure buy -in by Mission staff?  
a. To support the Mission in setting up the first activity?  
Eval Q: What are the turning point s in engagement with the Missions? When does a Mission -Lab initiative 
solidify?  
6. What signs did you use or identify as having  secured buy -in more broadly to your team’s 
mission/values/objectives?  
7. What would you say are the contributing factors to solidifyi ng your team’s engagement with the 
Mission?  
Eval Q: How effective are the various teams at achieving uptake with their audien ces (available data to date)?  
8. What data do you have that demonstrates uptake with the Uganda Mission?  
a. Have there been extensions or  follow -on activities? If so, why did they get 
approved/funded? To what extent are they aligned with the current or previous CDCS?  
b. In particular, is there data or evidence of uptake captured somewhere outside of reporting 
against your results framework?  
c. What would you say are the undocumented or observed successes around uptake?  
Eval Q: Are there some Mission engagement strategies that are more effective than others?  
9. Taking a look at your Uptake Model Canvas, was this the approach you used to engage with 
Uganda?  
a. If not, how was it different, and why?  
Eval Q: What are the barriers and enablers to engaging with a  USAID Mission?  
10. What have been the barriers and enablers to engaging with the Uganda Mission?  
a. Have any of the barriers been dealt with? If so, how?  
b. What are the persistent barriers that you are still dealing with? What strategies have you 
tried to addres s them?  
c. Have you leveraged any of the enablers in your engagement strategy? If so, how?  
Eval Q: What are the contextual factors at the Uganda Mission that contribute to successful uptake of Lab -based 
initiatives?  
11. What are the contextual or enabling factors  at the Uganda Mission and in Uganda more broadly 
that you would say have contributed to the success of your engagement in Uganda?  
  
60 
 12. Do you agree with  the following statements? If you disagree, or would make alterations, please 
explain.  
a. D2FTF's model for Mission engagement was effective and efficient in achieving buy -in with 
the Uganda Mission.  
b. Uganda Mission Leadership is the most important enabling f actor that supports 
implementation of Lab team activities.  
c. Uganda's available digital infrastructure (mob ile phone use, connectivity, etc.) and market 
has allowed for Lab teams to have more advanced work or larger portfolios in Uganda.  
d. The Lab teams sele cted the Uganda Mission for their activities because of pre -existing 
entry points/relationships instead of  leveraging a criteria for identifying a receptive 
environment for uptake.  
e. Lab teams depended on existing entry points to initiate engagement with th e Uganda 
Mission. Their activities with the Mission were limited to the sector/niche/focus of that 
entry p oint. 
f. Aligning with Mission needs and going above and beyond to take care of administrative 
logistics jointly contribute to securing activities with t he Uganda Mission.  
g. Alignment with the Mission's CDCS enables expansion of a team's workstream and more 
likelihood for sustained uptake.  
13. Who have Uganda Mission staff POCs been? Can you introduce us?  
14. Who else has helped implement your workstream in Uganda  (IPs, other Mission folks, other 
people at the Lab or other Bureaus)?  
Document Review  
Document Review Process  
Documents will be reviewed in three rounds  
Document Request:  
• Uganda STIP Integration Workplan (access)  
• AADs  
• PADs  
• MOUs  
• Historical emails from DFS  and Digital Inclusion detailing or setting up their Uganda Mission 
engagement (have some from the LWPs, need to process those first before request to them)  
• D2FTF Programming and Assessment toolkits (have access)  
• Results Frameworks (have access)  
• Uganda Mis sion CDCS (have)  
• Uptake Model Canvas (have)  
• CAI PE data (have)  
Partner Interviews  
1. What was  the first interaction with the Lab that you remember?  
a. What did you feel like the Lab was offering in Uganda?   
61 
 b. Were any of the offers more attractive than others to Mi ssion colleagues that you know 
of? 
2. How do the ideas that were originally proposed compare to the activity(s) that were initiated?  
3. Can you describe the process from the Lab team’s first outreach to securing the first activity?  
4. Is there anything unique about  this initiative process compared to the other engagements and 
activities coming from Wash ington?  
5. Did you have to secure additional buy -in with other parts of the Mission or other stakeholders 
before the activities could be implemented? How did you secure that buy -in and with whom?   
6. When would you say the relationships with the Lab solidified?  Were there any specific events or 
actions they took that you felt strengthened the relationship?  
7. What are the opportunities for more long -term engagements and for th e Lab team to support 
the Uganda Mission better in the future?  
8. Have you felt any barriers at the Mission or more broadly in your engagement with the Lab teams? 
How have you dealt with those barriers?  
9. Was there any existing support and entry points that you  leveraged to bring on the work of the 
Lab teams into the Missions work?   
If they’ve work ed with multiple Lab teams:  
1. Have you observed differences in way teams at the Lab interact with Missions?  
a. Do you think any team is more effective than another? Why?  
b. Do you think any team is more efficient than another? Why?  
2. What differences do you observe between the teams you work with in the Lab?  
3. Have there been any complications in working with multiple teams on similar activities?  
4. Have you seen any compounded bene fits from the similar work implemented by the teams in the 
Lab in Uganda?  
Mission S taff Interviews  
1. What was the first interaction with the Lab that you remember?  
a. What did you feel like the Lab was offering you?  
b. Were any of the offers more attractive than  others?  
c. How did the offers align with existing priorities and interests at the Mi ssion?  
2. How do the ideas that were originally proposed compare to the activity(s) that were initiated?  
3. Can you describe the process from the Lab team’s first outreach to sec uring the first activity?  
4. Is there anything unique about this initiative process co mpared to the other engagements and 
activities coming from Washington?  
5. Did you have to secure additional buy -in with other parts of the Mission before the activities 
could b e implemented? How did you secure that buy -in and with whom?  
6. When would you say the  relationships with the Lab solidified? Were there any specific events or 
actions they took that you felt strengthened the relationship?  
7. Has the work of the Lab teams expand ed beyond your office and partnership with them at the 
Mission?  
8. What are the oppor tunities for more long term engagements and for the Lab team to support 
the Uganda Mission better in the future?   
62 
 9. Have you felt any barriers at the Mission or more broadly i n your engagement with the Lab 
teams? How have you dealt with those barriers?  
10. Was t here any particular existing support and entry points that you leveraged to bring on the 
work of the Lab teams into the Missions work?  
11. Is there anyone else we should speak with in particular about the Lab teams work in Uganda?    
63 
 BRIGHT SPOT CASE STU DIES  
This design report was developed for another evaluative effor t under RQ2 —the bright spot case studies. 
These studies employed a positive devianc e methodology discussed below.  
PURPOSE OF THE STUDY  
Under Research Question #2, concerning effective approaches for achieving sustained uptake, it will be 
beneficial to not  just know which approaches or parts of approaches are effective, but also which efforts 
or particular applications have been most effec tive at achieving sustained uptake. It is surmised that these 
cases may offer evidence of how teams have overcome partic ular barriers, leveraged enablers in an 
innovative manner, or tweaked the application of their model advantageously. By compiling mini c ase 
studies on these outliers, utilizing a positive deviance approach, we should be able to better understand 
each team’ s ideal application of their model, what contextual factors were at play, and which aspects would 
most ideally be replicated. The positi ve deviant case will be jointly identified with each team, and each 
team will receive an individualized case study. The studies will then be analyzed collectively to highlight 
any similarities, contradictory findings, and where teams might be able to adapt  from each others’ learnings.  
EVALUATION QUESTIONS   
The underpinning evaluation question for this study comes from the three main questions from the Uptake 
Developmental Evaluation. This Bright Spots Case Studies is one evaluative effort among four data 
collection processes designed to answer DE Research Question #2:  
How do we determine which current Lab approaches are mo st effective at sustained uptake? What has been the 
perceived and real value add of the approaches? What can we learn from Lab models?  
In addition to the overarching DE Research Question that led to this study, the evaluation questions this 
study seeks to answer are as follows:  
1. Which engagements by the Lab teams demonstrate the most success, or highest potential for 
uptake?  
a. Which of these  engagements demonstrate indications for sustainability of that uptake?  
2. How was each team's model applied in that best case?  
a. What was unique about the application of the team's model?  
b. How did it diverge from other applications/engagements?  
3. What contextu al factors influenced the uniqueness of each team's best engagement?  
4. What factors are contributing to the likelihood of  sustainability of uptake for each team's best 
engagement?  
5. What additional efforts are needed to enhance the sustainability of the upta ke achieved?  
6. Is there any prioritization of factors contributing to uptake in these cases as compared to others?  
7. What a re the similarities and dissimilarities comparatively between the team's best engagements?  
8. Have any of the factors leading to success i n the best engagements led to negative effects in other 
engagements? If so, why?  
9. How dependent are the best engagements  on external or contextual factors?  
STUDY METHODS AND PR OCEDURES   
64 
 STUDY DESIGN  
The first component of this study is to establish a definition for what would constitute a positive deviant 
case. Understanding that the issue at hand is the struggle to get des ired stakeholders to utilize and/or 
implement an identified and beneficial inno vation. It is not just a singular use case that teams are looking 
for, but rather a pattern of behavior that would indicate continued use, growth, and proactive sharing on 
the b enefits of the innovation by the desired stakeholder group.  In the case of the  Uptake DE, a positive 
deviant then would be any unique application of a team’s model that demonstrates through observational 
and monitoring data (and/or other indicators for up take identified by that team) successful initial uptake, 
as well as indications  for sustained use/growth. The case(s) chosen should be those demonstrating the 
strongest data available to date on their sustainability.  
With that definition in mind, positive  deviants will be jointly selected by the teams through facilitation by 
the Eva luator. Each team will select one case to be evaluated. This self -selection process leverages the 
amount knowledge the individual teams have of their past and present model appl ications and also ensures 
that the case study aligns with other interests they have in terms of data collection, sharing their success, 
and the current political climate. The Evaluator will ensure these decisions are evidence -driven and have 
final approval  of the case study selection.  
Once cases are selected, data collection will co mmence focusing on comparing the identified positive 
deviant case to the team’s established model and selection criteria, making note of any evolutions in the 
current models bei ng compared to, as well as model adaptations made based on the positive deviant  case 
itself. Models are captured in the Uptake Model Canvases, but may need to be supported with older 
documentation of previous iterations of the team’s model. Data collection  will include a document review 
of any formal documents detailing the engagemen t, with particular requests for email exchanges 
documenting the initiation of the relationship with the key stakeholder, decision making around the 
engagement, and initial imple mentation. Requested documentation should also include any data that has 
been c ollected regarding demonstration of uptake and its sustainability.  
Document review will be compared against key informant interviews, completed in three rounds. The 
first round  interviews will be conducted with each team’s main point of contact or activit y manager for 
the identified positive deviant engagement. This interview will provide the most nuanced and complete 
picture of the engagement, to be validated through the docume nt review, and further refined with 
subsequent interviews. A second round of in terviews with relevant Mission/OU/primary stakeholder 
informants will provide an external perspective on the engagement and allow for further validation of 
sustained uptake by t hese key actors, as well as any ‘exceptional’ aspects of the identified positiv e deviant 
engagement from their perspective. Finally, a final round of interviews with implementing partners and 
other engaged parties should expound upon details from the groun d, throughout implementation, and 
allow for testing of some preliminary data tr ends by another external party.  
Analysis should look to A) tell the story of the engagement and highlight anything that was particularly 
different or divergent from other model  applications (from the perspective of the team, the 
Mission/primary stakeholde r, and the primary IPs where applicable), and B) identify trends, patterns, 
confirmed levers, contextual factors, selection criteria, or engagement protocols that contributed mo st 
effectively to achieving uptake. This analysis will be presented in 3 -5 page  case studies for each individual 
identified positive deviant case. Identification of current and future applications within case studies and  
65 
 across with then be conducted for a  larger comparative case study geared towards possible adaptations 
teams can ta ke in their future engagements from cross -study learnings.  
SAMPLING  
This study will employ purposive sampling. The initial purposive sample is based on those best informed 
from the Lab perspective to articulate the process of engagement and current status of uptake for each 
identified positive deviant case. Following interviewees will be identified from these initial points of contact, 
who will also be necessary to make introduc tions and identify best available time slots for some of the 
more sensiti ve interviewees, such as Mission staff. All key relevant personnel, who have been directly 
engaged in the initiation, solidification of engagement, and implementation of activities fo r each positive 
deviants case study should be interviewed. This may inclu de people who have changed positions, but can 
still be accessed through existing contacts. The total sample for each case study should be sufficient to 
validate the engagement story a nd triangulate key findings, with minimal overlap in perspectives. This 
sampling approach was selected as it best supports the chosen study design.   
INCLUSION/EXCLUSION CRITERIA OF PARTICIP ANTS  
Interviewees were selected based on the need to speak with th ose managing, interacting, or participating 
in implementation of the Lab teams’ workstreams with the identified positive deviant cases. There are no 
unique gender or other demographic criteria for inclusion or exclusion of participants, especially given th e 
small sampling pool available when considering those who meet the first  criteria. Team members that have 
worked on one discrete activity or as part of covering for another person for a short period of time (less 
than one month) are not anticipated to be included in the sampling. The iterative and selective nature of 
this samp ling methodology will help right -size the case study, while also ensuring sufficient but not 
superfluous data collection to answer the research question.  
INFORMED CONSENT  
Each parti cipant will be briefed on the purpose of the interview and given an overv iew of the content and 
focus of the interview questions. Participants will then be told about the process of information collection, 
analysis, and what they should expect to hear and/ or receive back from the research process and within 
what timeline, mostl y focusing on seeing their input directly in the content for identified, and yet to be 
identified,  points of input for the DE, without any personal identifiable information. The eval uators will 
then explain the intended data usage and storage procedures a nd ask the interviewee to sign an informed 
consent statement (unless it is a remote interview, in which case the evaluators will solicit verbal consent). 
If consent is given (through the form or verbally), the evaluators will transcribe consent. If consent  is not 
given, the interview will be terminated and any early notes taken deleted. For the verbal consent 
statement, see this document.   
INTERVIEWEES  
The Identifi ed Positive Deviant Case Studies are:  
• D2FTF - Nepal  
• DFS- Rwanda  
• SOGE - Funding financial intermediaries [Delayed until later in DE after consultation with the 
team due to early status of implementation of awards]  
• Digital Inclusion - Peru [Pending approval fr om [name]  (activity manager)]   
66 
 The responsibility for completion of each case study is as follows:  
• Nepal - [name]  
• Rwanda - Search  
• Funding financial intermediaries - DELAYED  
• Digital Inclu sion- Search  
• Comparative - [name]  
First Round: Lab Team Positive Deviant P OCs 
• D2FTF : [name]  (past); [name]  (current)  
• DFS: [name]  
• SOGE: [name]  
• Digital Inclusion: [name]  
Second Round: Mission staff working with Lab Teams  
• To be mapped out in the First Round I nterviews, for most recent Mission staff contacts, 
historical contacts, and connections through Lab Team POCs  
Third Round: Other connected parties (Lab staff working on a particular positive deviant engagement, 
other Lab team external partners and IPs work ing on their initiatives)  
• To be mapped out in the First Round Interviews with Lab Team POCs  
LIMITATIONS  
Typically a positive deviance approach is a much more participatory and iterative process than what is 
planned for these case studies. This is indeed a light touch application of positive deviance, done to identify  
actionable ‘best’ practices for achieving sustained uptake at the Lab, leveraging existing data and analysis 
completed under Uptake DE Research Question #1, as well as the Uganda Process Tracin g case studies, 
the Enabling Environments Outcome Harvesting s tudy, as well as ongoing day -to-day data collection 
completed under the DE. Given the other robust evaluative efforts, as well as existing knowledge of what 
is working under each team’s uptake m odels, the decision was made to make these positive deviant ca se 
studies more lean. The minimized participation is not seen to be an issue given the fact that the 
stakeholders needed for adaptations to be employed in model application are only the Lab team ’s 
themselves, as this is a higher level, strategic developmen tal evaluation, versus one with more 
programmatic implications and on -the-ground beneficiaries that could be involved in the adaptation 
processes, and the fact that the application of adaptation s process will be highly participatory.  
The lean nature of the  case studies is seen both in the light touch participatory nature of these case 
studies, as well as in the lack of fieldwork (given the placement and number of interviews need it was 
deemed unn ecessary) and shorter nature of the anticipated deliverables. This light touch approach will be 
mitigated through the overarching, robust evaluation efforts that are going into answering Research 
Question #2.  
The cases also do not work through the applica tion of the possible recommendations as with a more 
traditiona l positive deviance approach as part of the study design, as recommendations will be given  
67 
 collectively and workshopped in a participatory manner that builds on all the different ongoing evaluat ive 
efforts being utilized to answer Research Question #2.  
ANALYSIS PLAN  
The KIIs, along with all documentation from the document review, will be input into Dedoose and coded 
according to the above referenced coding hierarchy. The data from the document r eview, Lab POC 
interviews, and Mission/primary stakeholder int erviews should go through a round of light touch analysis 
to identify preliminary trends and patterns in order to inform the final round of interviews to ask more 
targeted questions for the purp ose of triangulation or further validation based on identifyin g a clear history 
of the engagement, data that demonstrates uptake has been achieved, and any model, implementation, 
contextual or other factors that contributed to effective uptake. After all t he data is collected a final round 
of analysis should be compl eted to uncover the above -mentioned details for each identified positive 
deviant case. Data and analysis at this point should be kept strictly to each individual case. All 
documentation, coding,  and analysis should be done within the overarching Uptake DE Dedoose project 
file.  
The data from all four individual case studies will be further analyzed to identify any barriers and enablers, 
as well as consistent best practices or contradictory findin gs that might indicate current avenues for 
adaptations of ongo ing workstreams with relevant stakeholders, and any possible input into upcoming 
partnering decisions. The data is also being coded into the broader DE project file and will contribute to 
analys is further down the line through the collective data analysis effort for Research Question #2 and in 
the refinement of principles to share under Research Question #3.  
DATA MANAGEMENT  
All interview notes will be collected either via transcribed Word files,  Google Docs, or via written 
documentation that can then be up loaded to limited access folders on the DEPA MERL Google Drive.  
SBU content or otherwise sensitive material will either remain offline until approval is received or 
uploaded to a separate folde r with access limited to the Evaluator, the Project Manager, a nd the Project 
Director. Most notes will be stored on Google Drive within the private DEPA MERL Consortium Working 
folder that currently has restricted access for just the active members of the DEPA MERL Consortium 
(whose contracts include non -disclosure p rovisions), as well as select members of the EIA staff at the 
Global Development Lab —namely the DEPA MERL COR. Notes will also be uploaded and coded into 
Dedoose, utilizing the secure Dedoose Cl oud drive. Quotes will be anonymized in any reports or other 
outputs from these KIIs, and any sharing of data otherwise will have all sensitive files scrubbed from the 
shared data set.  
KEY DELIVERABLE  
There will be two sets of deliverables from this study :  
1. Identified Positive Deviant Case Studies:  
a. 4 case studies; o ne for each identified positive deviant case or one per team;  
b. Each case study will be 3 -5 pages in length; and,  
c. The case study content will consist of: Brief summary of methodology, review of hi story 
of engagement highlighting unique or divergent aspects o f the model application,  
68 
 discussion of identified trends and patterns for effective uptake and achieving sustainability, 
conclusion and recommendations to specific team.  
2. Comparative Case Study:  
a. A single case study comparing the four team’s approaches to a chieving sustained uptake;  
b. The case study is anticipated to be 8 -10 pages in length;  
c. The case study is be accompanied by a 1 -2 page summarization of findings geared towards 
Lab leadership and other Agency leadership; and,  
d. The larger comparative case study content will consist of an introduction to the Uptake 
DE’s Research Question #2 approach, a brief summarization of the methodology, a 1 page 
review of each team’s uptake model identified posit ive deviant application; a rev iew of the 
key findings from each individual case study, findings, conclusions, and recommendations 
from the comparative data.  
TOOLS  
PRE-KII QUESTIONS  
1. Which engagements by the Lab teams demonstrate the most success, or highes t potential for uptake?  
a. Positive Deviants selection process with Team Leads.  
2. Which of these engagements demonstrate indications for sustainability of that uptake?  
a. Defining exercis e to narrow Positive Deviants selection with Team Leads. Indications for 
sustainability of uptake assessed mainly through observational data from Lab Teams at this 
stage.  
FIRST ROUND INTERVIE WS (PD LAB POC)  
*Italicized text are the evaluation questions, n ot the interview questions. Notes for Interviews should only 
contain the i nterview questions.   
1. How was each team's model applied in that best case?  
a. Walk me through the initiation and implementation of work with identified positive 
deviant primary stakehol der (IPD).  
b. How was IPD selected? What criteria was used?  
c. Follow up if not  covered under first question: What was the progression of 
implementation of existing activities with this IPD?  
2. What are the similarities and dissimilarities comparatively between t he team's best 
engagements?  
a. Now look at this engagement compared to the e ngagement where you’ve struggled the 
most or failed to achieve uptake. Is there anything specific that stands in stark contrast 
between the two processes of engaging, activities impl emented, or other contributing 
factors?  
3. What was unique about the applica tion of the team's model? How did it diverge from other 
applications/engagements?  
a. What was unique about the application of the team's model? How did it diverge from 
other applicatio ns/engagements?  
4. What contextual factors influenced the uniqueness of each team's best engagement?  
a. What were the contextual factors at play that you observed that influenced your 
engagement with IPD?   
69 
 b. How have you seen these contextual factors influence yo ur work with any other 
engagement?  
c. What are the additional factors outsid e of your own team’s actions and contextual 
factors that you believe positively influenced the success of this engagement?  
5. Have any of the factors leading to success in the best engagements led to negative effects in 
other engagements? If so, why?  
a. Have a ny of the factors leading to success in the best engagements led to negative 
effects in other engagements? If so, why do you think so?  
6. Is there any prioritization of factors contributing to uptake in these cases as com pared to 
others?  
a. In your opinion, how would you prioritize the factors contributing to uptake in this case 
as compared to others?  
7. What factors are contributing to the likelihood of sustainability of uptake for each team's best 
engagement?  
a. How do you defin e the likelihood of sustainability of uptake with this IPD?  
b. How are you tracking the sustainability of uptake?  
8. What additional efforts are needed to enhance the sustainability of the uptake achieved?  
9. What additional efforts are needed to enhance the lik elihood of sustainability of uptake in  this 
case?  
10. Are there any such initiatives underway?  
11. How are you tracking their contributions to the success of this engagement?  
DOCUMENT REVIEW  
DOCUMENT REVIEW PROC ESS 
Documents should be reviewed and coded on a rol ling basis, with the exception of the Uptake Models, 
which should be familiar pre -interviews. Documentation of activities, interactions with the IPD, or 
otherwise should be viewed as a separate data source to be compared against the interview data, versus 
foundational data on which the intervi ews are based.  
DOCUMENT REQUEST:  
1. AADs  
2. MOUs  
3. Historical emails from DFS and Digital Inclusion detailing or setting up their PD engagement 
(have some from the LWPs, need to process those first before request to them)  
4. D2FTF Programming and Assessment toolkits  (have access)  
5. Results Frameworks (have access)  
6. Uptake Model Canvases (have)  
MISSION STAFF INTERV IEWS  
1. Walk me through the initiation and implementation of work of how the team from the Lab 
engaged with you on X activity.   
a. When and on what criteria did you decide that the proposed initiative or partnership was 
worthwhile?  
b. What was the identified value add?    
70 
 2. Looking at how this activity or work with this Lab team has impacted the Mission, is there anything 
specific that stands in stark contrast between the t his process of engaging, activities implemented, 
or other contributing factors versus similar activiti es that the Lab has initiated?  
3. What has been the best part of this engagement, in your opinion?  
4. What has been the best outcome, in your opinion?  
5. What w ere the contextual factors at play that you observed that influenced your engagement with 
IPD?  
6. How ha s the work expanded since it was first initiated?  
a. Are there any additional Mission staff engaged in this workstream?  
b. How has this work been embedded in the Mission’s work or with Mission partners?  
7. How do you define the likelihood of sustainability of th is type of work?  
a. What additional efforts are needed to enhance the likelihood of sustainability of uptake in 
this case?  
i. Are there any such initiatives u nderway?  
ii. How are you tracking their contributions to the success of this engagement?  
PARTNER INTERVIE WS 
1. Walk me through the initiation and implementation of work with [Lab team] on [activity relevant 
to partner].  
a. How did they first reach out to you?  
b. What was the process of setting up the SOW?  
2. Have you worked with [Lab team] or any other team at the Glob al Development Lab in any other 
engagement? Was there anything unique about the work with this team on this project?  
3. What were the contextual factors at p lay that you observed that influenced your engagement in 
[country] or with the [country’s Mission]?  
a. How have you seen these contextual factors influence your work with any other 
engagement?  
b. What are the additional factors outside of your own team’s actions  and contextual factors 
that you believe positively influenced the success of this engagement?  
c. Have any of the factors leading to success in the implementation of the prize led to 
negative effects in other engagements? If so, why do you think so?  
4. In your o pinion, how would you prioritize the factors contributing to success of [relevant activity]?  
5. How do you define the likelihood of sustainability of uptake with this IPD?  
a. What additional efforts are needed to enhance the likelihood of sustainability of uptak e in 
this case?    
71 
 OUTCOME HARVESTING   
PURPOSE OF THE STUDY  
As part of answering Research Question #2 under the Uptake DE regarding effective mo dels for sustained 
uptake, it is necessary to look at the various USAID Global Development Lab teams’ approaches to 
achieving uptake under what they call an Enabling Environment approach. This work includes uptake of 
innovations external to the Agency, tha t facilitates either immediate acceleration or builds the 
infrastructure necessary for the inno vation(s) to scale. This work is complex and very context or market 
specific, however the goal of this study is not to provide a performance -based judgement on o ne approach 
that is better than others for this uptake pathway. Instead, it is necessary to und erstand 1) how this work 
interplays with the more Agency -focused uptake work the teams’ are doing, 2) what the outcomes of this 
work have been given that most as suredness of its effectiveness is grounded in anecdotes and/or output -
level data, and 3) share lessons learned on what has and has not worked regarding various components 
of the team’s models and USAID’s particular role to play in enabling environments wor k. For example, 
this case study will strive to see what outcomes have come from building high -level, collaborative, and 
external partnerships to leverage increased investments and awareness in a sector, and why certain Lab 
team approaches in this have been  more successful than others. This study will also look at how USAID 
strives to influence polic y and emerging markets, not to say one strategy is best or preferred, but to 
provide data on what has worked to a broader audience, and identify what has not wor ked as well and 
can be improved.  
EVALUATION QUESTIONS   
The underpinning evaluation question fo r this study comes from the three main questions from the Uptake 
Developmental Evaluation. This Enabling Environments Study is one evaluative effort among four d ata 
collection processes designed to answer DE Research Question #2:  
1. How do we determine which  current Lab approaches are most effective at sustained uptake? What has 
been the perceived and real value add of the approaches? What can we learn from Lab mode ls? 
2. In addition to the overarching DE Research Question that led to this study, the evaluation 
questions this study seeks to answer are as follows:  
3. What have been the outcomes of the Lab team's enabling environment activities?  
4. How do those outcomes compar e to expected outcomes and what is captured in the team's 
results framework (and older versions  of the results frameworks)?  
5. What are the perceptions of outcomes from the Lab team's versus their partners?  
6. What is the frequency/consistency of particular out comes across engagements?  
7. What outcomes best contribute to the team's higher level objective(s )? And, how does that relate 
to the achievement of sustained uptake with the teams’ primary stakeholders?  
8. How sustainable are the outcomes?  
What lessons learned can be derived from the teams’ enabling environment approaches and the outcomes 
of that work in  different sectors?  
STUDY METHODS AND PR OCEDURES  
STUDY DESIGN  
This study is using an outcome harvesting approach to contribute to findings around the Uptake DE 
Research Question #2. Outcome harvesting was chosen as an appropriate methodological component to  
72 
 the evaluative efforts under this Research Question because of the proportion of each Lab teams’ portfolio 
that is focused on Enabling Environment work as an approach to achieving sustained uptake, and the lack 
of data conc erning the impact of that work . Establishing what outcomes related to sustained uptake have 
come from this work will help teams to better articulate the value add of their enabling environmen t 
work. And, understanding ho w those outcomes came about and their relative significance will s upport the 
teams in adapting their enabling environment work, leveraging past lessons learned and incorporating 
more successful strategies that best leverage USA ID’s particular contributions to these spaces.  
Identifying Outcomes : The first component of this study, past protocol development of useable q uestions, 
is to conduct a series of interviews with key stakeholders both from the Lab teams, their core collaborative 
partners, and primary ‘beneficiaries’ as they relate to this wor k in order to identify a list of possible 
outcomes from each team’s enabl ing environment portfolio. This initial list is not meant to be a 
comprehensive harvesting of all possible outcomes, and will still be focused on outcomes related to the 
teams achievi ng uptake. Outcome descriptions at this phase should be preliminary draft s.  
Prioritizing Outcomes to be Substantiated:  Once a preliminary list is established for each team, a list 
overview with shortened outcome descriptions will be shared with the teams and each team will undergo 
a prioritization exercise to identify the key outcomes they would like further substantiated. This will be a 
participatory outcome selection process, and the evaluation team will also request input from Lab 
leadership and/or othe r identified OU leadership as requested by the teams to identify any addi tional 
outcomes that would be of interest to a broader audience. The prioritization exercise will also involve a 
smaller, secondary scoping exercise to dig deeper into those outcomes,  relevant documents to request, 
and any additional possible interviewees to add to the list.  
Substantiating Outcomes:  A third round on interviews, possibly a fourth, and additional document review 
will then be leverage to substantiate the prioritized outcomes, including KIIs with the Lab team POCs that 
focus in on the prioriti zed outcomes. Incoming data will be monitored  against the prioritized outcomes 
to track how much data is supporting the various prioritized outcomes, and more importantly the 
outcome descriptions will be further flushed out and refined with incoming data.  
Testing Outcomes:  The last step of the study  will be to analyze the data for each outcome to test which 
can be validated and where data can be triangulated. From analysis of the outcomes and data collected 
through this process the evaluation team will com pose findings, conclusions, and recommendatio ns 
matrices for each team to share what can be learned from the outcomes and the teams’ strategies to their 
enabling environments work.  
SAMPLING  
This study will employ purposive sampling. The initial purposive sample is based on those best informed 
from t he Lab perspective to articulate the enabling environment work that has been done by the four DE 
stakeholder teams, share documentation of that work and it’s outputs/outcomes, and provide further 
identification of interviewees. From there, the evaluators w ill review the lists of possible interviewees 
provided and delineate a second, third, and possible fourth grouping of interviewees. The first group 
should be core actors, no more than one from each major stakeho lder group, that may be able to best 
identify  outcomes from the enabling environments work that was done. The second group should be 
others involved in activity implementation and anyone deemed as a primary beneficiary (Missions, 
government partners, award ees) that can help substantiate the prioritiz ed outcomes. The final grouping  
73 
 should be backup actors, anyone that may be able to contribute to substantiating outcomes, but was 
identified by the teams or subsequent interviewees as more tangentially involved . This group will only be 
interviewed is part icular outcomes need additional testing past interviews with the third group and 
document review. This sampling approach was selected as it best supports the chosen study design and 
the scope of possible respond ents for the type of work the Lab teams’ are doing.   
INCLUSION/EXCLUSION CRITERIA OF PARTICIP ANTS  
Interviewees were selected based on the need to speak with those managing and receiving services from 
implementation of the Lab teams’ enabling environment w orkstreams. There are no unique gender or 
other demographic criteria for inclusion or exclusion of participants, especially given the small sampling 
pool available when considering those who meet the first criteria. Team members that have worked on 
one dis crete activity or as part of covering for ano ther person for a short period of time (less than one 
month) are not anticipated to be included in the sampling, and if so will be in the fourth grouping. The 
iterative and selective nature of this sampling meth odology will help right -size the case study, while also 
ensuring sufficient, but not superfluous, data collection to answer the research question.  
INFORMED CONSENT  
Each participant (external to Lab team members who have already be briefed on this many ti mes) will be 
briefed on the purpose of the in terview and given an overview of the content and focus of the interview 
questions. Participants will then be told about the process of information collection, analysis, and what 
they should expect to hear and/or  receive back from the research process and w ithin what timeline, 
mostly focusing on seeing their input directly in the content for identified, and yet to be identified,  points 
of input for the DE, without any personal identifiable information. The evalua tors will then explain the 
intended data usag e and storage procedures and ask the interviewee to sign an informed consent 
statement (unless it is a remote interview, in which case the evaluators will solicit verbal consent). If 
consent is given (through th e form or verbally), the evaluators will tran scribe consent. If consent is not 
given, the interview will be terminated and any early notes taken deleted. For the verbal consent 
statement, see this document.   
INTERVIEWEES  
FIRST ROUND:  
• DFS: [name] (Enabling Environments POC) and [name]  
• Digital Inclusion: [name] and [name]  
• SOGE: The whole team - [name] ; [name] ; [name] ; [name] ; and [name] depending on team input 
about her participation in this  first scoping conversation  
• D2FTF: [name]  *this conversation will need to determine the extent to which D2FTF is even 
covered by this study depending on if they qualify any of their work as enabling environment 
work.  
SECOND ROUND:  
To be determined based o n input from Lab team Enabling Environment POCs and scoping conversations  
THIRD ROUND:  
To be determined based on input from Lab team Enabling Environment POCs and scoping conversations   
74 
 FOURTH ROUND:  
To be dete rmined based on input from Lab team Enabling Environment POCs and scoping conversations  
Interview tracker can be found  here.  
LIMITATIONS  
The lengths of each teams’ enabling environment activities does raise concern about the strength of 
outcomes that can be sourced. The lack of evidence for some facets and evolution of the teams’ theory 
of change(s) related to their enabling environment wor k may also lead to a very small list of possible 
outcomes. Given the lack of evidence the team’s currently possess about the outcome -level impact of this 
aspect of their work, any outcomes that can be identified through this study will be of value to the t eams.  
Given the small number of stakeholders engaged in this work and the non -traditional beneficiary profile, 
there is some concern that the number of interviewees may limit possible substantiation of some 
outcomes. The evaluation team will strive to con duct the maximum number of purposeful interviews 
possible for this study to help mitigate this concern.  
The contextual nature and sectoral differences in enabling environments work may limit the usefulness of 
findings outside the originating DE Stakeholder  Lab teams. The core focus of the DE is how USAID (the 
DE stakeholder teams in particular) achieve uptak e, what strategies are more effective given USAID's 
positioning/resources/bureaucracy, and what it takes for those strategies to be effective. The goal of this 
study is not to say this is the ideal enabling environment outcomes or holistic model for all en abling 
environments work. But, through gathering  currently non -existent data on what is effective at the 
outcome -level and how the enabling environment  work was rolled out by the various teams, this study 
aims to see both operational and strategic approac hes that best leverage what USAID has to offer and 
have led to positive outcomes in the past.  
ANALYSIS PLAN  
The KIIs, along with all documentation from  the document review, will be input into Dedoose and coded 
according to the existing and evolving Uptake  DE coding hierarchy, with additions made to best capture 
patterns and trends from the enabling environment data. The data from scoping conversations wi th DE 
stakeholder team POCs should go through a round of lighttouch analysis to identify the preliminary  list of 
possible outcomes in order for subsequent rounds of interviews to ask more targeted questions for the 
purpose of prioritization of outcomes, an d later triangulation or further substantiation of those outcomes. 
All documentation, coding, and analys is should be done within the overarching Uptake DE Dedoose 
project file.  
Once all data collection has been completed and coded, analysis should be done  for each team’s prioritized 
outcomes to test the strength and validity of the outcomes. Other individua l team trends, patterns, and 
lessons learned from implementation of their enabling environments strategies should be analyzed at this 
time too.  
The dat a from all four teams will be further analyzed to identify any barriers and enablers, as well as 
consist ent best practices or contradictory findings that might indicate current avenues for adaptations of 
ongoing workstreams with relevant stakeholders, and any possible input into upcoming partnering 
decisions. The outcomes from individual teams’ work should a lso be comparatively assessed to check for 
any similarities or common lessons learned. The data is also being coded into the broader DE project file  
75 
 and will contribute to analysis further down the line through the collective data analysis effort for Resea rch 
Question #2 and in the refinement of principles to share under Research Question #3.  
All data and documentation should be coded to the relevant ena bling environments codes within the 
codebook on Dedoose, as well as any other relevant codes related to the broader Uptake DE codebook, 
especially those related to Influencing Factors, What Works, and What Doesn’t Work. Relevant Actors 
must be coded for ea ch excerpt.  
DATA MANAGEMENT  
All interview notes will be collected either via transcribed Word files, Go ogle Docs, or via written 
documentation that can then be uploaded to limited access folders on the DEPA MERL Google Drive.  
SBU content or otherwise sen sitive material will either remain offline until approval is received or 
uploaded to a separate folder w ith access limited to the Evaluator, the Project Manager, and the Project 
Director only. Most notes will be stored on Google Drive within the private DE PA MERL Consortium 
Working folder that currently has restricted access for just the active members of th e DEPA MERL 
Consortium (whose contracts include non -disclosure provisions), as well as select members of the EIA 
staff at the Global Development Lab —namely the DEPA MERL COR. Notes will also be uploaded and 
coded into Dedoose, utilizing the secure Dedoose Cloud drive. Quotes will be anonymized in any reports 
or other outputs from these KIIs, and any sharing of data otherwise will have all sensitive files scrubbed 
from the shared data set and approval from DE stakeholders secured before sharing.  
KEY DELIVER ABLE S 
1. Findings, Conclusions, and Recommendations (FCR) Matrices  
a. Audience: Each DE Stakeholder team  
b. Purpose: A utilization -focused sharing of the finding s and recommendations for the team’s 
to deal with the meat of the study and focus on possible adaptation s. This may be good 
to accompany with an all -of-DE stakeholder presentation to go over the findings 
as a high -level on the day for disseminating the FCR s and Study Memo mentioned 
below.   
c. Each team will receive a utilization -focused FCR Matrix upon completi on of the Enabling 
Environments study.  
d. The FCR Matrices will contain individualized content for each team, as well as comparative 
content based on comp arative analysis of outcomes across the teams’ enabling 
environment work.  
e. The Recommendations listed wi ll be prioritized by validation of findings and anticipated 
impact to adapting the teams’ work.  
f. The FCR Matrices will be accompanied by a brief, facili tated Strategic Learning Debrief 
for each team to talk through the findings, identify internal prioritiz ation of 
recommendations, and make an action plan for any adaptations the teams’ would like to 
make at the time the study is delivered.  
2. Enabling Enviro nments Study Memo  
a. Audience: DE Stakeholder teams, EIA, Lab Leadership, possibly T3  
b. Purpose: A more polis hed and concise presentation of the findings, conclusions, and 
recommendations that is geared more for dissemination of any findings, etc. that would 
be useful outside of the DE stakeholder teams.   
76 
 c. A single memo comparing the four team’s approaches to achi eving sustained uptake 
outcomes with their enabling environments workstreams;  
d. The case study is anticipated to be 5 pages in length, the shorter the be tter. If longer than 
5 pages, it should be accompanied with a one -page briefer as well;  
e. The memo conten t will consist of an introduction to the Uptake DE’s Research Question 
#2 approach, a brief summarization of the methodology, a paragraph review of each  
team’s enabling environment work and how it integrates into their uptake model; a review 
of the key fin dings from each team, and findings, conclusions, and recommendations from 
the comparative data.  
TOOLS  
DOCUMENT REVIEW  
Documents should be reviewed and coded on a rolling basis, with the exception of the Uptake Models, 
which should be familiar pre-interviews. Documentation of activities should be viewed as a separate data 
source to be compared against the interview data, versus foundational data on which the interviews are 
based. Document review is anticipated to provide substantial background i nformation of the Lab teams’ 
enabling environments work, as well as a clearer understanding of the evolution of the teams’ enabling 
environments strategies and upta ke model. Some documentation may be available to help with the 
substantiation of outcomes, b ut it is an acknowledged limitation that most reported data on this work is 
output -level .  
DOCUMENT REQUEST:  
1. AADs  
2. MOUs  
3. Historical emails from Lab teams detailing or  setting up their enabling environments work  
4. Enabling Environments initiative program docume nts and reports (ex: from A4AI, mWomen, 
etc.) 
5. Teams’ Results Frameworks (have access)  
6. Uptake Model Canvases (have)  
7. Additional documentation for collection and revie w will be identified through the scoping 
conversations  
SCOPING CONVERSATION  PROTOCOL (LAB TE AMS ENABLING ENVIRON MENT POCS)  
OBJECTIVES:  
1. Elicit a concise definition of what qualifies as enabling environments work for each team, for 
comparison between teams and as a quality assurance measure against their activity selection.  
2. Identify which discrete  past and current activities each DE stakeholder team considers to be a 
part of their Enabling Environment portfolio.  
3. Identify core and tangential stakeholders that  either participated directly in implementing this 
work for the Lab team, collaborated in th e implementation of the work, are perceived to have 
benefited from the work, or are perceived to have astute observations on the work, in order to 
develop an interv iewee list.  
4. Identify and request supporting documentation that covers the initiation, solidi fication, 
implementation, and any documented results of the enabling environments activities the teams 
highlight.   
77 
 RELEVANT QUESTIONS:  
1. Define Work  
a. How do you define Enabling Environments work on your team?  
b. What aspects of this type of work does your team e xplicitly focus on?  
c. How does Enabling Environments work tie into your Results Framework, in your own 
words?  
2. Identify Activities  
a. What activities, past/current, that  your team has implemented do you consider to be a 
part of your Enabling Environments portfo lio? [ Ensure the list is composed of discrete 
activities ] 
b. What are the Activity Names? [ from procurement mechanisms, PADs, etc .] 
c. If there are activities you qualify  as discrete, but are not procured through a mechanism, 
what is/are the:  
i. Name of activity?  
ii. Objective of activity?  
iii. What are the actions/support/work done under this activity?  
iv. Where has this been done? [ Distinct implementation examples ] 
v. How does activity t his tie in with your model or results framework?  
3. Identify Stakeholders [ Going thr ough each named activity one -by-one]  
a. What stakeholders/partners were directly involved in the implementation of this Enabling 
Environment activity?  
i. Who from those stakeholder /partners was involved?   
ii. What stakeholders collaborated on the implementation of  this activity?  
iii. Who from those stakeholder/partners was involved?  
iv. Who benefited from this activity?  
v. Who from those beneficiary groups could best speak to the benefits?  
vi. Who e lse would have thoughts, opinions, perspectives on the outcomes of this 
activity?  [Get specific names ] 
vii. [Secure contact details for all mentioned ] 
viii. Who from the list you’ve provided will we need to be introduced to through you? 
Do you have any other requests  in terms of protocol for reaching out and 
interviewing these stakeholders?  
b. Identify Documentation [ Going through each named activity one -by-one]  
i. What types of documentation do you have from this activity?  
ii. Where can I find this documentation?  
iii. Is there any documentation that might exist of the results of this activity that you 
do no t currently have? Are there other stakeholders with whom we should check 
for documentation of the outcomes or progress of this activity? [ Secure names 
and specifics on what ty pe of documentation they might possess ] 
iv. [Secure access through follow -up on speci fic document names ] 
IF INSUFFICIENT DATA IS COLLECTED:  
1. Follow up with direct questions via email the day after. Wait up to three days for a response. 
Ensure Team Lead is CC’d .   
78 
 2. Follow up with an interview/scoping conversation request with the Team Lead, stating you need 
additional information. Refine questions for that conversation based on gaps in data collection 
from initial scoping conversation with POC.  
3. Collect more infor mation from first round of implementing stakeholder to expand the scoping 
data.  
4. Document any limitations due to lack of information received.  
NEXT STEPS:  
1. Compare Enabling Environments work definitions from all teams once secured for any 
identifiable dis crepancies. Take note for analysis.  
2. Secure access to all documentation identified. Review for relevancy. Ensure upload to 
corresponding GDrive -Uptake DE folders. Upload relevant documents to Dedoose. Code.  
3. Fill in Interview Tracker with stakeholder/inter viewee details. Review interviewee list and 
prioritized based on proximity to activities, scheduling with direct implementers/recipients first. 
Compose Interview Request email and identify which interviewees will require introductions 
through Lab team POCs . Send first round on interview request and schedule interviews.  
 
FIRST ROUND INTERVIE W PROTOCOL (CORE STA KEHOLDERS TO E WORK)  
*Italicized text are the evaluation questions, not the interview questions. Notes for Interviews should only contain 
the intervi ew questions.   
OBJECTIVES : 
1. [Ideally completed during Scoping Conversations and follow up] Confirm activities to be included 
in the Enabling Environment case study with Lab Team POCs, per Lab Team.  
2. Describe the activity/activities and confirm the status of the work. This will confirm and add clarity 
to document review data and initial activity details provided during the scoping conversations with 
core stakeholders.  
3. Identify a list of emerging or actualized outcomes related to the activity’s enabling environ ment 
work. This preliminary list will be prioritized and substantiated in the next phases of the case 
study with other stakeholders.  
4. Refine/add to the list of tangential (second and third round) stakeholders who collaborated in the 
implementation of the wo rk, are perceived to have benefited from the work, or are perceived to 
have astute observations on the work (and enabling environment outcomes in particular).  
5. Identify and request any additional supporting documentation that verifies emerging or actualized  
enabling environment outcomes of the activity.  
RELEVANT QUESTIONS : 
Opening: [Activity/activities]  were identified by [Team/Scoping Conversation respondent]  as activities that, in 
part, seek to change/effect the Enabling Environment in some way in [referen ce a sector, specifics of the 
activity and country] . [Team/Scoping Conversation respondent]  recommended we speak to you considering 
your role as [note why the individual was selected as a key stakeholder/respondent] . We would like to ask you 
some questions  about these activities, and in particular talk about outcomes (those emerging or actualized) 
from these activities that you have observed or are currently observing.   
79 
   
DESCRIBE ENABLING EN VIRONMENT WORK OF EA CH ACTIVITY  
 [Repeat following questions for ea ch identified discrete activity for inclusion in this case study for each Lab Team. 
Also use this section to follow up on remaining questions from the scoping conversation.]  
1. To prepare for this conversation, I reviewed several documents about this activity  and spoke with 
[Team/Scoping Conversation respondent] . Based on this review, I first want to confirm the details of 
this enabling environment activity.  
a. What is the name and objective of this discrete activity?  
b. What is the status of this activity [ongoing or completed] ? 
c. What are the key outputs the activity has achieved/seeks to achieve? What does/did the 
activity do toward removing/addressing barriers in the enabling environm ent? [ Output are 
the products, goods and services which result from an interventio n.] 
d. Who benefited/is benefitting from this activity [the enabling environment work in particular] ? 
e. Please describe [Lab team] ’s engagement with this activity. How has [Lab te am] supported 
the enabling environment work you just described?  
 IDENTIFY OUTCOMES  (EMERGING OR ACTUALIZED)  
Outcomes are a result or effect that is caused by or attributable to a project, program or 
policy.  Outcomes can be intermediate or intended effects , such as changes in behavior, relationships, 
actions, activities, policies, or pr actices of an individual, group, community, organization or institution. For 
example, drafting and integrating new procurement language may be an output of an activity with a  Mission. 
The outcomes of that activity are how the new procurement language impac ts which types of organizations 
apply and why, how proposals evolve to meet the new criteria, and most importantly how that new criteria 
influences the types of programs that  are implemented.  Now that we have discussed the progress and 
outputs of the acti vity as they relate to enabling environments, let’s discuss the outcomes that are either 
emerging or already achieved (observed).  
1. [Outcome brainstorm with the respondent i f outcomes are not immediately apparent to the 
respondent/stakeholder, or they are  unclear about the distinction between outputs and outcomes. Use 
some or all of the questions below – and pull from initial list of outcomes developed during document 
review – and make a list of outcomes mentioned.]  
a. You mentioned [output] resulting from th e activity’s enabling environments work. What 
has been the impact/effect of this output?  
b. Have this activity’s outputs influenced/changed [positively or negatively]…  
i. Policies/ laws/regulations/decrees?  
ii. Markets/market standards?  
iii. Budget allocations?  
iv. Financing strategies?  
v. International agreements/conventions/treaties?  
vi. Public infrastructure?  
vii. Norm or customs?  
viii. Expectations?  
ix. Availability of information?  
x. (one of the above not changed, in-action)?   
80 
 xi. Other changes?  
c. To confirm, [outcome] has helped/hindered the movement o f a product or service along 
its value chain. Correct? [Confirm how value chain actors behaved, and how they behave 
now in response to the enabling environment output/outcom e achievement.]  
 [For each identified outcome for which evidence exists, dive deepe r with the respondent. Focus on 
outcomes that the respondent believes can be verified/substantiated.]  
a. [Define] Please describe this outcome. What changed and when? At what level is this 
outcome [international, national, local]? Where did this outcome occur /take place?  
b. [Contribution] Who/what caused the change? How did the change agent contribute to 
this outcome? Did [Lab team] have any influence on this outcome?  
c. [Significance] Why is this ou tcome significant/why does it matter? [Why is the outcome 
important ? How did it change the enabling environment?]  
d. [Alternate Explanations] What other factors may have influenced/contributed to this 
outcome?  
DOCUMENT/DATA REQUES TS 
[Interviewer can also use this time to follow up on scoping conversation questions still unan swered 
regarding documentation.]  
a. What type of documentation do you have regarding the outcomes that we discussed? 
What evidence is there that substantiates the outcomes we have discussed?  
b. How can I access this documentation?  
c. Is there any documentation that  might exist of the outcomes of this activity that you do 
not currently have? Are there other stakeholders with whom we should check for 
documentation of the outcomes or progress of this ac tivity? [Secure names and  
specifics on what type of documentation t hey might possess]  
STAKEHOLDER IDENTIFICATION  
 [Interviewer can also use this time to follow up on scoping conversation questions still unanswered 
regarding stakeholder identification.]  
We want to speak with additional stakeholders that can talk about this  activity and the impact it is 
having/has had.  
a. [Team/Scoping Conversation respondent] mentioned [name of stakeholder] as someone 
that would be helpful to speak with about activity outcomes.  Do you agree? Can you 
provide/confirm contact information for [name of stakeholder] ? 
b. Who else do you recommend we speak with to learn more about the outcomes of this 
activity? Who from the beneficiaries you described would best speak about the benefits 
of the activity? Who else would have thoughts, opinions, perspectives  on the outcomes 
of this activity? [Get specific names]  
c. Who from the list you’ve provided will we need to be introduced to through you? Do 
you have any other requests in terms of protocol f or reaching out and interviewing 
these stakeholders?   
81 
  IF INSUFFICIE NT DATA IS COLLECTED:  
a. Follow up with direct questions via email the day after. Wait up to three days for a 
response.  
b. TBD…  
NEXT STEPS:  
a. Insert identified outcomes into the ‘Harvested Outcome Description_Template’. 
Complete details for each outcome, where pos sible, and revise/tailor interview protocol 
for next round of interviews based on holes in the data, per outcome.  
b. Secure access to all documentation identified. Review fo r relevancy. Ensure upload to 
corresponding Gdrive -Uptake DE folders. Upload relevant documents to Dedoose. 
Code.  
c. Fill in/update Interview Tracker with stakeholder/interviewee details. Review 
interviewee list and prioritize based on proximity to activities , scheduling with direct 
implementers/recipients first.  
d. Compose Interview Protocol and  Request email for next round of interviews, and 
identify which interviewees will require introductions through Lab team POCs. Send 
interview requests and schedule interv iews. 
PRIORITIZATION EXERC ISE PROTOCOL (FOR LA B TEAMS)  
Preliminary outcomes per team/p roject are tracked in the Outcome Tracker. Disaggregate by 
team/project and prepare a handout of outcome basics (outcome name, brief description, contribution). 
This info rmation can also be projected if technology is available. The prioritization exercise will be 
conducted by the Evaluator and include relevant Lab team members. Document discussion with/amongst 
the team throughout the process.  
1. Confirmation of Outcome Harves t Purpose/Goal (for each team)  
 The evaluator should do the following:  
a. Define ‘outcome ’ 
b. Define Enabling Environment work, according to each Lab Team (see result of scoping 
conversations)  
c. Restate purpose of the case study overall (for the DE)  
d. State unique p urpose the case study can serve for each Lab Team  
e. Better document what [Lab Team] has achieved  
f. How outcomes can influence/evolve team’s Enabling Environment strategies (depending 
on the team)  
NOTES:  
  
 
2. Discussion of Preliminary Outcome Harvest Process and Resulting Outcomes   
82 
 Refresh the team on the process used to identify the outcomes on the handout (document 
review, # scoping conversations, # first round interviews, coding)  
a. Briefly describe each preliminary outcome  
b. Make sure to communicate to L ab Teams that no outcomes have been substantiated at 
this stage  
c. Discuss, as necessary (for examp le, Lab Teams may not have seen one or two of the 
outcomes before as they may have come up in first round interviews. The evaluator 
should describe and answer q uestions as necessary so that the team understands the 
outcomes and can shift toward prioritizat ion) 
NOTES:  
  
 
3. Prioritization  
This is a participatory process that teams can do on their own according to their own needs and 
goals within the scope of the DE  (see result of Section I above), but the evaluator should 
recommend the teams consider the foll owing in selecting 2 -3 outcomes for substantiation for 
each activities/workstream under review, and facilitate the process as necessary.  
A. Identifying 2 -3 dissim ilar outcomes could increase learning from the harvest. All prioritized 
outcomes should relate t o ‘uptake’ of the team’s enabling environment work to align and 
contribute to answering Uptake DE Research Question #2. The evaluator should have access to 
a white board to be able to document and note the team’s prioritization progress/decisions, as 
they discuss. Some different outcome categories the evaluator can prompt with, are:  
a. Outcomes direct relation to higher team objective  
b. Approach used to achieve the outcome  
c. Differing aspects of enabling environment work (such as regulatory, policy, private 
sector  engagement, funding/investments, access, etc.)  
d. Expected or unexpected outcome  
e. Negative or positive outcome  
f. Sustainability of the outcome  
g. Potential for use of  substantiated outcome in ongoing or future Lab work  
B. Facilitation Questions  
a. If you could only pick  one potential outcome to substantiate, which would it be?  
b. Which potential outcome would be most relevant in the new Bureau?  
c. Which potential outcome is most  relevant to your current Enabling Environment 
strategy?  
d. Which potential outcome would teach you the most you don’t already know about your 
work?  
e. Which potential outcome could contribute to knowledge about your theory of change?  
f. Which potential outcome is the scariest?   
83 
 g. Which potential outcomes are you most skeptical about, in terms of the possibili ty to 
substantiate them?  
C. Support for Other Situations  
a. The evaluation team only has capacity to substantiate 2 (maybe 3) outcomes per 
activity/workstream. (2 per activity x 2 activities per Lab team x 3 teams = 12 outcomes). 
However, depending on how the o utcomes stack up, teams may want to ‘trade’ the 
number of outcomes substantiated per workstream within their two worksteams (ex: 1 
outcome and 3 outcomes resp ectively or 0 outcomes and 4 outcomes). This is acceptable 
as long as teams can provide sufficient  reasoning for the decision and demonstrate that it 
is utilization focused in nature.  
b. In the case that there is 2 or less outcomes for an activity/workstream , the entire 
prioritization exercise is not necessary. In such cases, the evaluator can either set  aside 
time in the weekly meeting or email the Lab team staff with brief descriptions of the 
potential outcomes and confirm that they want those outcomes subs tantiated, as well as 
request any additional documentation or stakeholder information necessary to  move onto 
second round interviews.  
NOTES:  
  
 
4.  Reflection and Documentation  
The evaluator should document the reasons why outcomes were or were not prioritized by the team. 
When outcome prioritization is complete, the evaluator should describe the reaso ning behind the 
selection as she understands it and seek confirmation from the Lab Teams. Prioritized outcomes should 
be circulated to the teams after the prioritization exercise for final confirmation before second round 
interviews are conducted.  
NOTES:  
  
 
5. Next Steps  
a. Confirm/clarify second round interview stakeholders (as necessary)  
b. Request additional documentation (as necessary)  
c. Update Lab Teams on C ase Study timeline  
d. Start filling out outcome forms for each prioritized outcome and identify gaps (flag f or 
inclusion in second round interview protocol, for substantiation)  
D. Second Round Interview Protocol (Lab POCs, core stakeholders, primary ‘beneficiari es’, etc.)  
This round of interviews is for outcome substantiation.  
E. Third Round Interview Protocol (Follow -up with more removed stakeholders)  
To be developed in case of need    
84 
 ANNEX 6: WORKS CITED  
Griswold, S., Van der Bijl, S., Burns, C., Plotkin, G., Herrington, R., Esper, H., . . . Jurgens, C. (2016, 
October 24).  Joint Partnership Plan for Developmental Eval uation Pilot Activity Monitoring, Evaluation, Research, 
and Learning BETWEEN Office of Evaluation and Impact Assessment (EIA) via the DEPA -MERL co nsortium AND 
USAID/Global Development Lab: Lab -Wide Priority 2  (D2FTF), Lab -Wide Priority 3 (BTGx)  [PDF]. Wash ington, 
D.C.: USAID . 
USAID . (2018, September 10). Cross -Cutting Activities | U.S. Global Development Lab. Retrieved 
February 1, 2019, from https:/ /www.usaid.gov/GlobalDevLab/about/cross -cutting -activities  
USAID . (2018, November 09). Transformation at USAID  | What We Do. Retrieved February 1, 2019, 
from https://www.usaid.gov/what -we-do/transformation -at-usaid 