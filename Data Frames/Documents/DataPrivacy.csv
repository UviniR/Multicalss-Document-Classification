fileclass,filename,content
DataPrivacy,3000.txt,"Original Paper
COVID-19 Mobile Positioning Data Contact Tracing and Patient
Privacy Regulations: Explor atory Search of Global Response
Strategies and the Use of Digital Tools in Niger ia
Iniobong Ekong1*, MBBS, MPH, CGEIT ; Emeka Chukwu2*, BEng, MSc, CGEIT ; Martha Chukwu3*, BSc, MPA
1Department of Health Planning, Research and Statistics, FCT Health and Human Services Secretariat, Abuja, Nigeria
2Department of Computer Information System, Faculty of Information & Communication Technology , University of Malta, Msida, Malta
3Ragnar Nurkse Department of Innovation and Governance, School of Business and Governance, Tallinn University of Technology , Tallinn, Estonia
*all authors contrib uted equally
Corr esponding Author:
Emeka Chukwu, BEng, MSc, CGEIT
Department of Computer Information System
Faculty of Information & Communication Technology
University of Malta
PG room A24, Level 0
Msida, MSD 2080
Malta
Phone: 356 99330888
Email: nnaemeka_ec@hotmail.com
Abstr act
Backgr ound: The corona virus disease (COVID-19) pandemic is the biggest global economic and health challenge of the century .
Its effect and impact are still evolving, with deaths estimated to reach 40 million if uncheck ed. One effective and complementary
strate gy to slow the spread and reduce the impact is to trace the primary and secondary contacts of confirmed COVID-19 cases
using contact tracing technology .
Objecti ve: The objecti ve of this paper is to survey strate gies for digital contact tracing for the COVID-19 pandemic and to
present how using mobile positioning data conforms with Nigeria’ s data privacy regulations.
Methods: We conducted an exploratory review of current measures for COVID-19 contact tracing implemented around the
world. We then analyzed how countries are using mobile positioning data technology to reduce the spread of COVID-19. We
made recommendations on how Nigeria can adopt this approach while adhering to the guidelines provided by the National Data
Protection Regulation (NDPR).
Results: Despite the potential of digital contact tracing, it always conflicts with patient data privacy regulations. We found that
Nigeria’ s response complies with the NDPR, and that it is possible to leverage call detail records to complement current strate gies
within the NDPR.
Conclusions: Our study shows that mobile position data contact tracing is important for epidemic control as long as it conforms
to relevant data privacy regulations. Implementation guidelines will limit data misuse.
(JMIR Mhealth Uhealth 2020;8(4):e19139) doi: 10.2196/19139
KEYW ORDS
COVID-19; contact tracing; Nigeria’ s National Data Protection Regulation; General Data Protection Regulation; GDPR;
corona virus; surveillance; mHealth; eHealth; digital health
Introduction
The corona virus disease 2019 (COVID-19) is caused by severe
acute respiratory syndrome corona virus 2 (SARS-CoV -2) [1].
This infectious respiratory disease was first detected in WuhanCity, China, in December 2019. It was declared a global
pandemic by the World Health Organization (WHO) on March
11, 2020, and has currently infected over two million people
worldwide and has killed over 150,000 people. Globally ,
responses have been swift and in full influenza pandemic control
JMIR Mhealth Uhealth 2020 | vol. 8 | iss. 4 | e19139 | p. 1 http://mhealth.jmir .org/2020/4/e19139/
(page number not for citation purposes)Ekong et al JMIR MHEAL TH AND UHEAL TH
XSL•FO
RenderXmode [2]. Travel and movement restrictions to curtail spread
both within and across cities are in force. Many cities around
the world are in lockdo wn or lock-in mode. Some have issued
dusk-to-da wn curfe ws. In other scenarios, large gatherings have
either been banned or discouraged. Estimates suggest that this
pandemic can claim the lives of as many as 40 million people
globally [3]. The Spanish flu, which lasted between 1918 and
1920 in some places, has been estimated to have cost the lives
of 21-50 million people globally [4]. Evidence suggests that
influenzas can mainly be spread through large clusters [5]. The
WHO global influenza preparedness plan presents guidelines
for the management and control of influenza and other disease
[6]. Nigeria, one of the countries that adopts WHO guidelines,
has over 493 cases of COVID-19 as of April 17, 2020, with 17
mortalities. This is a substantial increase since the index case
was reported on February 27, 2020. To better manage the spread,
Nigeria’ s federal government has declared a lockdo wn in key
affected states (ie, Lagos, Ogun, and the Federal Capital
Territory). The lockdo wn was in addition to several mitig ating
actions by state governments, ranging from a ban on social
gatherings to dusk-to-da wn curfe ws. During the lockdo wn,
schools, mark ets, churches, mosques, banks, offices, parks,
motor parks, and airports remain closed, often for a 14-day
period.
The Nigeria Centre for Disease Control (NCDC) reported that
it is currently conducting contact tracing of over 9000 contacts
of confirmed cases in an attempt to effectively contain the spread
of the disease, in line with the recommended measures for
pandemic response [7,8]. These measures include antiviral,
vaccine, and nonpharmaceutical measures such as case isolation,
household quarantine, school or workplace closure, and travel
restrictions. Given the scale of the COVID-19 pandemic,
nonpharmaceutical actions appear to be the only practical and
logical option in the absence of any known antiviral drug or
vaccine. Resources are stretched even in countries with advanced
health care systems, as seen in Italy, the United Kingdom, China,
and the United States [9,10].
Although the NCDC's approach has been commended for its
compliance with WHO guidelines for large-scale containment
and contact tracing, there remain options that may yet beexplored [11]. Given the inadequac y of testing kits, it is belie ved
that the number of confirmed cases may be far lower than the
actual number of cases in Nigeria and most African countries.
This is fueling speculations of a real, catastrophic-le vel
pandemic if isolation, containment, quarantine, and
contact-tracing mechanisms are not urgently implemented. In
a country with an already weak health care system occasioned
by poor health investment choices, managing such an outbreak
will become impossible.
There is, therefore, a need to develop and adopt new strate gies,
particularly digitally-enabled strate gies, to facilitate a more
extensi ve, accurate, seamless, and timely response in line with
the high frequenc y of new infections among contacts of
confirmed cases (ie, the secondary infection rate) [12]. The
adoption of digital solutions in Nigeria has been focused on
electronic forms for contact data collection and visualization
for follow-up [13]. Digital technologies can do more than be a
tool for field data collection or serve as an outbreak investigation
platform. Data on households and general population movement
patterns can be extracted through digital technologies [14].
Farrahi et al [15] showed that over a 9-month period, 72
participants made 10,992 phone calls and 9432 SMS records
representing communication flow; additionally , these
participants made 1,973,547 Bluetooth interactions representing
physical proximity movements. When extrapolated for three
cases in Abuja, the capital city of Nigeria, their movement can
result in thousands of interactions. In the case of an infection,
these three cases can initiate an exponential number of contacts
through these interactions, as seen in Figure 1A. The registered
quarantine address can be visualized on the map and movement
of quarantine subjects can be monitored with notifications
enabled (Figure 1B).
Our paper reviews global practices in the use of mobile
positioning data to achie ve a more targeted and efficient
approach at contact tracing and disease surveillance, especially
in the conte xt of the COVID-19 pandemic. We discuss how this
approach is possible within regulatory confines. We also
recommend a novel strate gy for coordinating agencies to
leverage mobile positioning data, and how to ensure patient
privacy is preserv ed.
Figur e 1. Visualization of Movement paths of cases and quarantine location.
JMIR Mhealth Uhealth 2020 | vol. 8 | iss. 4 | e19139 | p. 2 http://mhealth.jmir .org/2020/4/e19139/
(page number not for citation purposes)Ekong et al JMIR MHEAL TH AND UHEAL TH
XSL•FO
RenderXMethods
The COVID-19 pandemic is emer ging and only three months
old with little scholarly work to justify a systematic search,
review, and analysis approach. We conducted an exploratory
(nonsystematic) internet search for technology approaches and
responses to COVID-19. Results from global and national
agencies responsible for infection prevention and control were
analyzed to ascertain how they currently use technology . We
also reviewed how these use cases fit within the regulatory
frame work for contact tracing and isolation. A similar internet
search methodology was adopted for Nigeria’ s response and its
use of digital tools for contact tracing.
Results
Our search yielded results based on emer ging trends and the
use of digital technologies by countries around the world to
respond to COVID-19. We first present global perspecti ves and
response strate gies on the use of mobile position data during
previous and current pandemics. We then present Nigeria’ s
approach.
Mobile Position Data: How It Works
The GSM Association puts the total number of mobile
subscribers at 5 billion unique subscribers and 7 billion
connected devices [16]. Nigeria has 184 million active mobilesubscriber lines [17]. Mobile telecommunications subscriber
communication and movement data was used for contact tracing
during the Ebola outbreak [14]. Many countries are currently
using mobile data for a more rapid response to the COVID-19
pandemic [18,19]. There has been a 90% increase in the number
of countries implementing digital tracking measures and a 100%
increase in reports of censorship [20]. These approaches range
from the use of anon ymized aggre gate data to monitor the
general mobility of people and track the mobile phones of
confirmed cases to tracking suspected patients and their contacts.
In some cases, these approaches were individualized and
mandatory while, in others, they were aggre gated and
anon ymized. In all cases, there were collaborations between the
government, mobile netw ork operators (MNOs), and other data
controllers such as technology companies and financial services
providers.
At any time, each mobile subscriber is connected to a segment
of the MNO base station tower. For simplicity , we have
presented a cell tower and a subscriber in Figure 2. We used
letters A and B to illustrate the farthest and shortest distance of
the subscriber from the base station tower based on power
throughput and internal cell tower position triangulation. The
difference between A and B, representing the diameter of a
user’ s device, which is a proxy for the user’ s location, often
ranges between 50 and 300 meters, but depends other factors
[14].
Figur e 2. Location of a subject with respect to a mobile netw ork operator (MNO) cell tower.
Global Strategies
Table 1details some of the strate gies governments around the
world are adopting to track and isolate COVID-19 patients and
their contacts or for lockdo wn/lock-in enforcement. In the
United States, US $500 million of the US $2 trillion economic
stimulus bill recently signed into law has been allocated to the
US Centers for Disease Control and Prevention to launch a new
surveillance and data collection system to monitor the spread
of COVID-19 [21]. This move is a first for the United States
since stringent patient data privacy and security regulationshave hampered the adoption of contact tracing as a
countermeasure for epidemic control in the past [22]. Similarly ,
the state of Massachusetts has announced the launch of what it
calls the “first contact tracing” call center with 1000 virtual
assistants to call and trace contacts of COVID-19-positi ve
persons [23].
The European Union’ s General Data Protection Regulation
(GDPR) is being tested on a large scale. Within the regulation,
a patient can decide not to disclose who they have been in
contact with or legally resist being traced [24]. Evidence has
emer ged that German y, Austria, and Italy are using aggre gated
JMIR Mhealth Uhealth 2020 | vol. 8 | iss. 4 | e19139 | p. 3 http://mhealth.jmir .org/2020/4/e19139/
(page number not for citation purposes)Ekong et al JMIR MHEAL TH AND UHEAL TH
XSL•FO
RenderXcall detail records (CDRs) to enforce lockdo wn and stay-at-home
policies [25]. As this is an evolving challenge and European
countries such as Italy and France are amongst the worst
affected, changes to the GDPR regulations are expected and
anticipated.
In China, the government worked with telecommunications
companies to track and contact people who had traveled through
Hubei province during the early days of the disease outbreak.
Location data was shared with China's National Health
Commission and other agencies, enabling them to
retrospecti vely simulate the location of confirmed cases and
their contacts, who were then issued warnings via social media[26]. Information has also emer ged that the Chinese government
may have leveraged its large netw ork of sensors and surveillance
cameras supported by an artificial intelligence–po wered facial
recognition and recommender system in its response to the
COVID-19 outbreak [27]. This success may not be unconnected
with the often criticized and loose patient data privacy and
security regulation in China.
It was, however, observ ed that the extent of compliance with
international and country-le vel regulations regarding data
privacy considerations in deplo ying this digital technology
varied from country to country .
Table 1. Strate gies planned or adopted by countries for the use of mobile positioning data in response to the COVID-19 pandemic.
Strate gy planned or adopted Country
The state of Massachusetts announced the launch of its first contact tracing call center to be manned by 1000
virtual assistants [23]. The US federal government announced a US $500 million package for COVID-19
surveillance for the CDC [21].United States [21]
A mandatory smartphone app “Health code” that leverages a mesh netw ork for infected persons contact
tracing and notification.China [22,28]
Telecommunications providers allow for the sharing of location data with health authorities to check whether
people are remaining at home. The data is aggre gated and anon ymous, mapping concentrations rather than
individuals to respect Europe’ s privacy laws.Italy, German y, and Austria [25]
The government created a map of cell phone data provided by telecommunications and credit card companies.
The map was made public, so everyone could track their level of exposure.South Korea [29]
The government is using GMS call detail records in addition to patient mobile phone position data to locate
contacts and trace their movement patterns.Israel [19]
Iranian authorities developed a mobile app with government endorsement for COVID-19 self-diagnosis
checks. It, however, also discretely collects user’ s location data.Iran [30]
Singapore is using a mobile app that uses a Bluetooth-based mesh netw ork to detect people's proximity to
those who have been exposed to COVID-19 and warns them to get tested if they come in close contact.Singapore [18,21]
The Nigerian Strategy
Human travel patterns and mobility can be assessed using
available mobile phone data, and its application can be useful
in disease epidemiology [31]. Panigutti et al [31] also revealed
the adequac y of mobile phone data for tracking infectious
disease spread, particularly in heavily populated and highly
interconnected communities.
Border restrictions, internal travel restrictions, and school
closures or total lockdo wn are reasonable but have minimal
impact compared to effective case isolation or quarantine, which
have been shown to have a signif icant impact if properly
conducted [2]. This is particularly important in Nigeria’ s case,
where total compliance to these strate gies cannot be guaranteed.
Therefore, data on case isolation and quarantine should be a
signif icant priority in Nigeria. Moreo ver, data is useful in
modeling disease transmission. Specif ically , collecting and
analyzing data on transmission in different social conte xts is
highly effective in mapping interv ention strate gies since the
impact of case isolation and quarantine depends on reducing
contact between unaffected individuals and the index and other
cases while they are ill [2].
In order for the NCDC to effectively conduct the current
large-scale contact tracing of over 9000 contacts of confirmed
cases, use of digital technology is inevitable. The number ofcontacts may even be more than this number considering the
frequenc y of new infections. Currently , there are several digital
contact data capture solutions, including the Surveillance ,
Outbr eak Response Mana gement and Analysis System
(SORMAS). These solutions require a field epidemiologist or
their representati ve to visit every contact.
Discussion
Principal Findings
Evidence suggests that contact tracing and data protection can
go together [32]. Signif icant progress is being made with current
strate gies. As promising as they may seem, data privacy
concerns remain a major impediment; it is necessary to find a
balance between deplo ying the technology and maintaining data
safety and patient privacy. Existing patient privacy regulations
are currently being tested. Some countries have attempted to
relax existing stringent regulations that protect patient privacy
to allow for greater access; others have worked around them.
According to Woods [21], many of the new digital technology
approaches appear inevitable and legitimate, given the
unprecedented high frequenc y of the COVID-19 infection
spread. Many countries have now also invoked speedy
legislati ve processes to give legitimac y to their workarounds
and deplo yments.
JMIR Mhealth Uhealth 2020 | vol. 8 | iss. 4 | e19139 | p. 4 http://mhealth.jmir .org/2020/4/e19139/
(page number not for citation purposes)Ekong et al JMIR MHEAL TH AND UHEAL TH
XSL•FO
RenderXIn Israel, for instance, the cabinet has passed an emer gency law
to use mobile data for tracking people infected with COVID-19,
trace their contacts, and identify individuals for quarantine [19].
This law was passed overnight, bypassing parliamentary
appro val. In the United States, privacy advocates are proposing
stringent procedures to keep personal information safe, including
deletion, once the data are no longer in use to prevent abuse by
law enforcement agents [21].
The National Data Protection Regulation (NDPR) of Nigeria
was promulg ated in 2019 [33]. Amongst other stipulations, the
regulation outlines the guiding principles for data processing
in Section 5. These principles consider data processing unlawful
if there is no consent by the individual data subjects (in this
case, the confirmed persons), if it is inaccurate with prejudice
to human dignity and not protected against cybercrime, and if
it is stored beyond a reasonable period of time. However, despite
these guiding principles, Section 6, part 2.0, subsection 2.2 (e)
of the document lists the conditions for lawful data processing
and states that:
Processing is necessary for the performance of a task
carried out in the public interest or the exercise of
official public mandate vested in the contr oller.
The data controller in the case of mobile positioning data is the
MNO, the entity that determines the purposes for and the manner
in which netw ork subscriber phone data is processed or is to be
processed. Section 11 of the regulation states that data
processing by a third party (eg, a public authority such as the
Federal Ministry of Health, the NCDC, or anybody engaged in
processing the location data such as a technology compan y)
shall be governed by a written contract with the data controller .
Interestingly , though the NDPR protects the privacy of personal
mobile location data, it also provides exceptions for the use of
such data that override public interest, such as the current
COVID-19 outbreak.
Recommendations
Mobile phone location data can be effectively utilized in Nigeria
for COVID-19 response. The government can leverage existing
mobile technology resources and infrastructure available
in-country by working with MNOs and technology firms to
optimize the ongoing contact tracing and surveillance of over
9000 known contacts of confirmed cases. This collaboration
should be guided by the NDPR in order to protect and safeguard
individuals' data, prevent a breach of data privacy rights as well
as inappropriate use and abuse by law enforcement agencies
beyond the period of contact tracing and surveillance.
In practice, however, the first step should involve anon ymized
mobile subscriber data in line with good data governance polic y.In the spirit of goodwill, informed consent of confirmed cases
should be appropriately obtained once they are diagnosed,
whene ver possible. The use of the public interest exception
should be a last resort. A simplif ied guideline for these processes
for adhering to the NDPR should be written and made
transparently available for data custodians, requesting bodies,
data handlers, and the patient or contact.
A third-party agreement should also be formally signed between
parties interf acing with patient data in any way. A typical use
case sensiti ve to data privacy concerns is the use of information
only regarding visits to public facilities, including public
transportation systems, parks, churches, mosques, or malls, by
COVID-19-positi ve individuals, as described by Ohmukai et
al [34]. The use of CDRs has been proven to be effective in
detecting outbreak clusters, followed by the use of other
frontline data collection tools for mitig ating impact and
containment [14]. A key limitation of using CDRs from MNOs
is that for basic 2G (second generation) phone users, the location
will rely on mobile netw ork phone mast location triangulation
only. This approach alone has a proximity accurac y of 50-300
meters. This accurac y level is not sufficient to identify persons
who have been in contact with a COVID-19 patient since the
WHO contact definition prescribes two meters [7]. The use of
telecommunication CDRs should complement other strate gies
for effective results.
The immediate action after a successful contact trace is
communicating the expected course of action to citizens of an
infected community cluster . A simple, user-friendly interf ace
using Unstructured Supplementary Service Data will help
impro ve information requests and management for low-income
but literate users. Interacti ve voice response technology will be
suitable and appropriate for awareness response for low-literate
users in their local language.
Conclusions
Mobile positioning data can signif icantly impro ve the capacity
and scope of timely outbreak response and will help
governments as well as other responders in Nigeria. When
implemented early [15], there are opportunities to leverage
positioning data to break the chains of disease transmission in
community clusters. It can impro ve the efficienc y of currently
used field data collection and outbreak investig ation platforms
when used in syner gy.
While mobile positioning data can be used within the current
regulation, guidelines for data handlers must include measures
to curtail misuse and unauthorized access. Future research
should design and implement models for mobile position contact
tracing.
Conflicts of Inter est
None declared.
Refer ences
1. World Health Organization. 2020. Naming the corona virus disease (COVID-19) and the virus that causes it URL: https:/
/tinyurl.com/t82w9ka [accessed 2020-04-01]
2. Ferguson NM, Cummings DAT, Fraser C, Cajka JC, Coole y PC, Burk e DS. Strate gies for mitig ating an influenza pandemic.
Nature 2006 Jul 27;442(7101):448-452. [doi: 10.1038/nature04795 ] [Medline: 16642006 ]
JMIR Mhealth Uhealth 2020 | vol. 8 | iss. 4 | e19139 | p. 5 http://mhealth.jmir .org/2020/4/e19139/
(page number not for citation purposes)Ekong et al JMIR MHEAL TH AND UHEAL TH
XSL•FO
RenderX3. van Elsland SL, O'Hare R. Imperial Colle ge London. 2020 Mar 26. Corona virus pandemic could have caused 40 million
deaths if left uncheck ed URL: https://www .imperial.ac.uk/ne ws/196496/corona virus-pandemic-could-ha ve-caused-40/
[accessed 2020-04-03]
4. Johnson NPAS, Mueller J. Updating the accounts: global mortality of the 1918-1920 ""Spanish"" influenza pandemic. Bull
Hist Med 2002;76(1):105-115. [doi: 10.1353/bhm.2002.0022 ] [Medline: 11875246 ]
5. Ferguson NM, Fraser C, Donnelly CA, Ghani AC, Anderson RM. Public health. Public health risk from the avian H5N1
influenza epidemic. Science 2004 May 14;304(5673):968-969. [doi: 10.1126/science.1096898 ] [Medline: 15143265 ]
6. World Health Organization. 2005. WHO Global Influenza Preparedness Plan URL: https://www .who.int/csr/resources/
publications/influenza/WHO_CDS_CSR_GIP_2005_5.pdf [accessed 2020-03-29]
7. World Health Organization. 2020 Apr 03. Corona virus disease 2019 (COVID-19): Situation Report - 74 URL: https://www .
who.int/docs/def ault-source/corona viruse/situation-reports/20200403-sitrep-74-co vid-19-mp.pdf?sfvrsn=4e043d03_12
[accessed 2020-03-01]
8. World Health Organization Writing Group, Bell D, Nicoll A, Fukuda K, Horby P, Monto A, et al. Non-pharmaceutical
interv entions for pandemic influenza, national and community measures. Emer g Infect Dis 2006 Jan;12(1):88-94 [FREE
Full text] [doi: 10.3201/eid1201.051371 ] [Medline: 16494723 ]
9. Cascella M, Rajnik M, Cuomo A, Dulebohn S, Di Napoli R. Features, Evaluation and Treatment Corona virus (COVID-19).
Treasure Island, FL: StatPearls Publishing; 2020.
10. Cao B, Wang Y, Wen D, Liu W, Wang J, Fan G, et al. A Trial of Lopina vir–Ritona vir in Adults Hospitalized with Severe
Covid-19. N Engl J Med 2020 Mar 18. [doi: 10.1056/nejmoa2001282 ]
11. World Health Organization. 2020 Mar 19. Considerations for quarantine of individuals in the conte xt of containment for
corona virus disease (COVID-19) URL: https://tin yurl.com/y8smo4pe [accessed 2020-03-25]
12. World Health Organization. 2020. Household transmission investig ation protocol for corona virus disease 2019
(    CO VID-19)     URL: https://www .who.int/publications-detail/
household-transmission-in vestig ation-protocol-for -2019-no vel-corona virus-(2019-nco v)-infection [accessed 2020-04-01]
13. Dimagi.com. 2020. Digital Solution for COVID-19 Response URL: https://www .dimagi.com/co vid-19/ [accessed 2020-04-01]
14. Shibasaki R. International Telecommunication Union. 2017. Call detail record (CDR) analysis: Sierra Leone URL: https:/
/www .itu.int/en/ITU-D/Emer gency-Telecommunications/Documents/2017/Reports/SL/D012A0000CA3301PDFE.pdf
[accessed 2020-03-20]
15. Farrahi K, Emonet R, Cebrian M. Epidemic contact tracing via communication traces. PLoS One 2014 May 1;9(5):e95133
[FREE Full text] [doi: 10.1371/journal.pone.0095133 ] [Medline: 24787614 ]
16. GSMA Intelligence. 2017. Number of Mobile Subscribers Worldwide Hits 5 Billion URL: https://www .gsma.com/ne wsroom/
press-release/number -mobile-subscribers-w orldwide-hits-5-billion/ [accessed 2020-04-01]
17. Nigerian Communications Commission. 2017. URL: https://www .ncc.go v.ng/[accessed 2017-02-17]
18. Choudhury SR. CNBC. 2020 Mar 25. Singapore says it will make its contact tracing tech freely available to developers
URL: https://www .cnbc.com/2020/03/25/corona virus-sing apore-to-mak e-contact-tracing-tech-open-source.html [accessed
2020-03-04]
19. Lomas N. Tech Crunch. 2020 Mar 18. Israel passes emer gency law to use mobile data for COVID-19 contact tracing URL:
https://techcrunch.com/2020/03/18/israel-passes-emer gency-law-to-use-mobile-data-for -covid-19-contact-tracing/ [accessed
2020-03-18]
20. Cozzens T. GPS World. 2020 Mar 26. 19 countries track mobile location to fight COVID-19 URL: https://tin yurl.com/
y8kpuvcv [accessed 2020-03-29]
21. Woods A. New York Post. 2020 Mar 26. CDC to launch new surveillance system to track corona virus spread URL: https:/
/nypost.com/2020/03/26/cdc-to-launch-ne w-surv eillance-system-to-track-corona virus-spread/ [accessed 2020-04-01]
22. Rafalski EM. Health Insurance Portability and Accountability Act of 1996 (HIPAA). Encyclopedia of Health Services
Research 1996. [doi: 10.4135/9781412971942.n180 ]
23. Carraggi M. Patch. 2020 Apr 03. MA To Launch First Corona virus Contact Tracing Program In US URL: https://patch.
com/massachusetts/boston/ma-launch-f irst-corona virus-contact-tracing-program-u-s [accessed 2020-04-03]
24. Information Commissioner's Office. Guide to the General Data Protection Regulation (GDPR). Information Commissioner's
Office 2018 [FREE Full text] [doi: 10.1211/pj.2017.20203048 ]
25. Pollina E, Busvine D. Reuters. 2020 Mar 18. European mobile operators share data for corona virus fight URL: https:/
/tinyurl.com/yadpu9gm [accessed 2020-04-01]
26. Grind K, McMilan R, Mathe ws AW. The Wall Street Journal. 2020 Mar 17. To Track Virus, Governments Weigh Surveillance
Tools That Push Privacy Limits, Geolocation and facial-recognition systems can locate vectors of infections, but they also
gather highly personal data URL: https://tin yurl.com/v99re x4[accessed 2020-04-02]
27. Yuan S. Aljazeera China. 2020 Mar 01. How China is using AI and big data to fight the corona virus URL: https://www .
aljazeera.com/ne ws/2020/03/china-ai-big-data-combat-corona virus-outbreak-200301063901951.html [accessed 2020-03-20]
28. Zhang O. Associated Press / Time. 2020 Apr 02. Inside China's Smartphone 'Health Code' System Ruling Post-Corona virus
Life URL: https://time.com/5814724/china-health-code-smartphones-corona virus/ [accessed 2020-03-10]
JMIR Mhealth Uhealth 2020 | vol. 8 | iss. 4 | e19139 | p. 6 http://mhealth.jmir .org/2020/4/e19139/
(page number not for citation purposes)Ekong et al JMIR MHEAL TH AND UHEAL TH
XSL•FO
RenderX29. Lyons K. The Verge. 2020 Mar 23. Governments around the world are increasingly using location data to manage the
corona virus URL: https://tin yurl.com/y75xuwyz [accessed 2020-03-29]
30. Gilbert D. Vice. 2020 Mar 14. Iran Launched an App That Claimed to Diagnose Corona virus. Instead, It Collected Location
Data on Millions of People URL: https://tin yurl.com/yddt2p38 [accessed 2020-03-27]
31. Panigutti C, Tizzoni M, Bajardi P, Smoreda Z, Colizza V. Assessing the use of mobile phone data to describe recurrent
mobility patterns in spatial epidemic models. R Soc Open Sci 2017 May 17;4(5):160950 [FREE Full text] [doi:
10.1098/rsos.160950 ] [Medline: 28572990 ]
32. Abeler J, Bäck er M, Buerme yer U, Zillessen H. COVID-19 Contact Tracing and Data Protection Can Go Together . JMIR
Mhealth Uhealth 2020 Apr 20;8(4):e19359 [FREE Full text] [doi: 10.2196/19359 ] [Medline: 32294052 ]
33. National Information Technology Development Agenc y. 2019. Nigeria General Data Protection Regulation URL: https:/
/nitda.go v.ng/[accessed 2020-04-02]
34. Ohmukai I, Yamamoto Y, Ito M, Okumura T. arXiv.org. 2020. Tracing patients' PLOD with mobile phones: Mitig ation of
epidemic risks through patients' locational open data URL: http://arxi v.org/abs/2003.06199 [accessed 2020-03-27]
Abbreviations
2G: second generation
CDC: US Centers for Disease Control and Prevention
CDR: call detail record
COVID-19: corona virus disease
GDPR: General Data Protection Regulation
MNO: mobile netw ork operator
NCDC: Nigeria Centre for Disease Control
NDPR: National Data Protection Regulation
SARS-CoV -2: severe acute respiratory syndrome corona virus 2
SORMAS: Surveillance, Outbreak Response Management and Analysis System
WHO: World Health Organization
Edited by G Eysenbac h; submitted 05.04.20; peer-reviewed by T Yasaka, GE Iyawa, D Pförring er; comments to author 16.04.20;
revised version received 17.04.20; accepted 19.04.20; published 27.04.20
Please cite as:
Ekong I, Chukwu E, Chukwu M
COVID-19 Mobile Positioning Data Contact Tracing and Patient Privacy Regulations: Explor atory Search of Global Response
Strategies and the Use of Digital Tools in Nigeria
JMIR Mhealth Uhealth 2020;8(4):e19139
URL: http://mhealth.jmir .org/2020/4/e19139/
doi: 10.2196/19139
PMID: 32310817
©Iniobong Ekong, Emeka Chukwu, Martha Chukwu. Originally published in JMIR mHealth and uHealth (http://mhealth.jmir .org),
27.04.2020. This is an open-access article distrib uted under the terms of the Creati ve Commons Attrib ution License
(https://creati vecommons.or g/licenses/by/4.0/), which permits unrestricted use, distrib ution, and reproduction in any medium,
provided the original work, first published in JMIR mHealth and uHealth, is properly cited. The complete bibliographic information,
a link to the original publication on http://mhealth.jmir .org/, as well as this copyright and license information must be included.
JMIR Mhealth Uhealth 2020 | vol. 8 | iss. 4 | e19139 | p. 7 http://mhealth.jmir .org/2020/4/e19139/
(page number not for citation purposes)Ekong et al JMIR MHEAL TH AND UHEAL TH
XSL•FO
RenderX"
DataPrivacy,3001.txt,"  
 西 南 交 通 大 学 学 报   
第 56 卷 第 1 期 
2021 年 2 月 JOURNAL OF SOUTHWEST JIAOTONG UNIVERSITY  Vol. 56 No. 1 
Feb. 2021  
 
 ISSN: 0258 -2724                                                             DOI：10.35741/issn.0258 -2724.5 6.1.23 
 
Research article  
 
Computer and Information Science  
 
 
THE PERSONAL DATA PROTECTION OF INTERNET USERS IN 
INDONESIA  
 
印度尼西 亚互联网用户的个人数据保 护 
 
Dewa  Gede  Sudika  Mangku  *, Ni Putu  Rai Yuliartini, I . Nengah  Suastika, I. Gusti Made Arya Suta 
Wirawan  
Faculty  of Law and  Social  Sciences, Universitas Pendidikan Ganesha  
Jalan Udayana No. 11 Singaraja Bali Indonesia , sudika.mangku@undiksha.ac.id  
 
Received: November 4, 2020 ▪ Review: December 7, 2020 ▪ Accepted: January 3, 2021  
 
This article is an open -access article distributed under the terms and conditions of the Creative Commons  
Attribution License ( http://creativecommons.org/licenses/by/4.0 ) 
 
Abstract  
The emergence and rapid development of information and communication technolo gy has brought 
about  various opportunities and challenges. One of them is the active interaction between individuals and 
the digital -based information service providers. In modern economic development, related information 
including personal data  or also known as digital dossier —the collection of large amounts of an 
individual’s information using digital technology —are valuable asset s due to their high economic value 
since they are  widely utilized by businesses . In this regard  and due to the increasing number of cellphone 
and internet users , there is a need to study the issue s on the importance of protecti ng one’s  personal data. 
In Indonesia, there is no specific regulation regarding the protection of personal data. Therefore, it is 
essential to  come up with specific and comprehensive legislation related to personal data protection  as 
legal basis for better implementation of personal data protection in Indonesia in the future. The purpose  of 
this research  is to find out and analyze  the current policies on  protection  of personal data of internet users 
in Indonesia. This study uses a normative  juridical  method  with a statutory  approach  and utilizes literature 
study. The result  shows that the concept  of personal data protection  implies  that individuals  have  the right  
to determine  whether  one will join an online community , share  or exchange personal data with  another, 
and the conditions  that must  be met in order to  do so. The study likewise found that t he threat  of personal 
data leakage  is increasingly  occurring because of the development  of the e -commerce  sector in Indonesia.  
 
Keywords : Protection, Personal Data, Information, Communication, Technology  
 
 
 
摘要  信息和通信技 术的出现和迅速发展带来了各种机遇和挑 战。其中之一是个人与基于数字的信
息服务提供商之 间的积极互动。在现代经济发展中，相关信息包括个人数据或也称 为数字档案（
使用数字技 术收集大量个人信息）是有价 值的资产，因为它们具有很高的 经济价值，因为它们已203             Mangku et al. / Journal of Southwest Jiaotong University / Vol.56 No.1 Feb. 2021  
 被企业广泛使用。在 这方面，由于手机和互 联网用户的增加，有必要研究有关保 护个人数据重要
性的问题。在印度尼西 亚，没有关于保 护个人数据的具体 规定。因此，有必要提出与个人数据保
护有关的具体而全面的立法，作 为将来在印度尼西 亚更好地实施个人数据保 护的法律基 础。这项
研究的目的是 找出并分析印尼 现行的互联网用户个人数据保 护政策。本研究采用 规范性的法律方
法和法定方法，并利用文献研究方法。 结果表明，个人数据保 护的概念意味着个人有 权确定一个
人是否将加入在 线社区，与另一个人共享或交 换个人数据，以及必 须满足的条件。 该研究同样发
现，由于印度尼西 亚电子商务部门的发展，个人数据泄露的威 胁越来越多。  
关键词 : 保护，个人数据，信息，通信，技 术 
 
I. INTRODUCTION  
The emergence and rapid development of 
information and communication technology has 
brought about various opportunities and 
challenges. One of them is the active interaction 
between individuals and digital -based 
information service providers. Currently, various 
sectors and industries use information systems, 
such as trade and business (e -commerce), 
transportation, tourism, government (e -
government), the financial industry (e -payment 
for electronic commerce), educatio n (e -
education), health (e -health), search engines, 
social networks, smartphones and mobile internet, 
and the development of the cloud computing 
industry.  
In modern economic development, related 
information including personal data or also 
known as digital dossier —the collection of large 
amounts of an individual’s information using 
digital technology —are valuable assets due to 
their high economic value since they are widely 
utilized by businesses [1]. Accordingly, as well 
as due to the increasing number of c ellphone and 
internet users, there is a need to study the issues 
on the importance of protecting one’s personal 
data. In the virtual world, actions of a user are 
almost limitless. However, in accessing the 
virtual world via the internet, even if it is free , a 
user must have an “identity card,” which is called 
an internet protocol (IP). An IP differentiates one  
internet user from one another . 
Registering one’s data  is dangerous because if 
a user’s personal data is made public, other 
people can access and kno w an individual’s 
personal identity. Accordingly, this may result in 
cybercrimes such as when a person may access 
without authorization another user’s Instagram 
account or even his ATM card, fraud, or 
pornographic crimes. Thus, it is of utmost 
importance t hat legislation be issued in protecting 
one’s personal data.  
The collection and dissemination of personal 
data is a violation of privacy [2]. Personal data is an asset or commodity with high economic value. 
There is also a correlative relationship between 
the level of trust and the protection of certain data 
from personal life. In Indonesia, the protection of 
personal data is currently not regulated by any 
specific statutory regulation but is scattered in 
various laws and regulations. Legal provisions 
related to personal data protection remain partial 
and sectoral. To date, these laws and regulations 
have not been able to provide optimal and 
effective protection of personal data as part of 
privacy as well as a form of protection of human 
rights (HAM) [3].  
Article 28G of the Constitution of the 
Republic of Indonesia 1945 stipulates that, 
“every person has the right to protection of 
himself, family, honor, dignity and property 
under his control, and has the right to sense safe 
and protection from the threat of fear to do or not 
do something is a human right.” Thus, this article 
mandates to establish laws and regulations that 
protect personal data.  
Regulations regarding personal data 
protection will minimize threats in violations of 
personal data privacy in diffe rent fields including 
the banking industry, online friendship sites such 
as Facebook, WhatsApp, Twitter, and Instagram, 
and electronic ID card programs. If the privacy 
policy is violated by one of the parties, especially 
social media service providers, it will cause a 
condition known as misuse of personal data.  
By avoiding the threats, personal data 
protection arrangements are intended to protect 
personal interests and provide economic benefits 
for Indonesia. It will further encourage and 
strengthen the pos ition of Indonesia as a trusted 
business center, which is a key strategy for 
Indonesia's national economy. Therefore, the 
regulation regarding the protection of personal 
data is an important matter that will ensure that 
personal data of the Indonesian peop le remain 
private and protected . 
Dealing with this matter, the importance of 
protecting personal data starts to strengthen along 204 
 with the increasing number of cellphone and 
internet users. This is dangerous because when 
the personal data is open to the pub lic, other 
people can access and know someone's personal 
identity. Therefore, several cases have emerged, 
especially related to the leakage of personal data 
of a person and lead to fraud or pornographic 
crimes. Thus, this is important in relation to a 
legal umbrella in protecting internet users' 
personal data in Indonesia by applying principles 
that uphold user privacy protection at the 
regulatory and technical level.  
 
II. LITERATUR E REVIEW  
 
A. The Concept of Personal Data Protection  
The development of technology has had a 
very significant impact on social life, especially 
on the speed of internet connectivity. This also 
has implications for accessibility related to the 
technological advances that raise questions about 
the right of individuals to maintain the 
confi dentiality of some information. Easy and 
fast dissemination of information through 
technology creates a threat to privacy by 
providing big opportunities for those who access 
personal information.  
The concept of personal data protection 
implies that individ uals have the right to 
determine whether that person will join the 
community and then share or exchange personal 
data between them and the right to determine 
what conditions must be met to conduct it [4]. 
Personal data protection laws, in general, involve 
safeguarding measures to protect the security of 
personal data and allow their users by other 
people as long as they comply with 
predetermined conditions.  
Several international instruments have 
regulated data protection principles, and many 
have been ratif ied as part of national law. The 
Council of Europe Convention for the Protection 
of Individuals with regard to Automatic 
Processing of Personal Data (No. 108), 1981; the 
Organization for Economic Cooperation and 
Development Guidelines on the Protection of 
Privacy and Transborder Data Flows of Personal 
Data (1980); and the Guidelines for the 
regulation of computerized personal data files 
(General Assembly resolution 45/95 and E / 
CN.4 / 1990/72) are some examples of 
international instruments that regulate th e data 
protection [5].  
Data protection is fundamental to human 
rights. Several countries have recognized data 
protection as a constitutional right or in the form 
of data habeas, namely the right of a person to obtain security for the data they have and jus tify 
when the errors are found in the data [6]. 
Portugal is an example of a country that has 
recognized data protection as a constitutional 
right. Besides, Armenia, the Philippines, Timor -
Leste, Colombia, and Argentina are countries 
with historical and cul tural differences that have 
also recognized the role of data protection in 
facilitating democratic processes and have 
guaranteed data protection privacy [ 6]. 
Meanwhile, the ASEAN region countries also 
clearly recognize the right to privacy data as 
stipulat ed in Article 21 of the ASEAN Human 
Rights Declaration [ 6]. 
The concept of data protection is often treated 
as part of privacy protection. For example, the 
rules protect personal data. Data protection can 
relate specifically to privacy, and the idea can be  
applied to a broader category of privacy. 
Regarding data protection as part of privacy, it is 
consistent with the understanding that privacy is 
a form of confidentiality or the right to disclose 
or withhold information [ 7]. 
 
B. The Thre at of Leakage Related to Personal 
Data  
Threats related to leakage of personal data 
have increasingly been proposed along with the 
development of Indonesia's e -commerce sector. 
When President Joko Widodo has launched the 
1000 Start -Up movement, one of them is an 
effort to develop the digital economy [ 8]. What is 
expected to encourage the growth of 4 (four) 
start-ups of unicorn originating from Indonesia, 
including Go -Jek, Tokopedia, Traveloka, and 
Bukalapak?  
This digital startup's growth has also triggered 
a massive collect ion of personal consumer data, 
personal data, and consumer behavior 
(shopping/activity) data. Referring to the terms of 
services of e -commerce in Indonesia, they collect 
personal consumer data. Suppose a potential user 
wants to run almost all applications.  In that case, 
it will force the user to provide access to other 
data, such as access to personal identity, contact 
list, location, SMS, photos/media/files [ 9]. 
Thus, if the user wants to run the application, 
they do not have a choice except it has to agree to 
access the data. Unfortunately, the absence of a 
Law on Personal Data Protection results in the 
absence of standardization of data protection 
principles, which results in minimal recognition 
of the data subject's right [1 0]. 
As an example of Indon esia's case regarding 
the leakage of personal data, namely in early 
September 2019, consumer data from Lion Air 
subsidiaries, namely Malindo Air and Thai Lion 205           Mangku et al. / Journal of Southwest Jiaotong University / Vol.56 No.1 Feb. 2021  
 Air, experienced a leak. This was disclosed by 
cybersecurity company Kaspersky Lab, and at 
least 21 million passenger data was leaked and 
uploaded to online forums [1 1]. 
In addition to the above cases, there are other 
cases, namely a case that did not exist long ago, 
in May 2020, which happened to one of the 
Tokopedia e -commerce sites hacked by an 
unknown person. A total of 91 million user data 
and more than seven million Tokopedia merchant 
data were reportedly sold on dark sites, for 5,000 
US dollars or around Rp 75 million. Data that has 
been compromised, such as user names, e -mail 
addresses, and pho ne numbers with the rest of the 
data, such as payment data for Tokopedia users, 
such as credit cards, or digital finance such as 
OVO, remains safe [1 2]. 
Responding to these cases, we need a 
regulation that regulates Personal Data Protection 
to safeguard or  at least provide a clear wall for 
the security of individual’s personal data. 
Because currently, violations of personal data 
were only regulated in a Ministerial Regulation, 
it is not at the level of a Law.  
As a result of this legal vacuum, the parties 
(digital service providers) as data controllers 
avoid the obligations that should be attached as 
personal data controllers. In general, data 
controllers should maintain the personal data 
security infrastructure of their service users, 
which includes the impl ementation of 
pseudonymization and encryption of personal 
data and providing guarantees of the continuous 
confidentiality, integrity, availability, and 
resilience of processing systems and services  
[13]. 
 
III. METHODS /MATERIAL  
Research is the main means of dev eloping 
science, including technology. The research aims 
to reveal the truth systematically, 
methodologically, and consistently. Thus, the 
research was conducted through analysis and 
construction of existing data. Research is an 
effort to explore an unclea r, obscure object or 
even without explanation for it. The logical and 
systematic study of the principles that guide 
scientific research (methodology) is intended as 
basic principles and not methods (methods or 
designs for conducting research).  
The author' s legal research regarding The 
Personal Data Protection of Internet Users in 
Indonesia is normative and prospective legal 
research. This research can qualify as normative 
legal research because it examines the rules or 
norms in the law regarding personal d ata in using 
the Internet in Indonesia, including related laws and regulations and those relevant to it. This 
research can also qualify as prospective legal 
research because it is expected to provide 
conceptual input on Indonesian government 
policies in pr otecting personal data on the 
Internet.  
 
IV. RESULT AND  DISCUSSION  
 
A. Legal Basis Concerning Current Protection 
of Personal Data in Indonesia  
There is no regulation in Indonesia that 
specifically regulates the protection of personal 
data. However, the laws and regulations do 
include some aspects of protection, including:  
1) Law No. 23 of 2006 Conce rning Population 
Administration  
Law No. 23 of 2006 concerning Population 
Administration states that the state is obliged to 
store and provide protection for the personal data 
of residents. Thus, administration officers and 
agencies that collect personal data on residents 
with access rights are obliged to maintain the 
confidentiality of such data, where the detailed 
arrangements of this are described in Presidential 
Regulat ion No. 67 of 2011, namely concerning 
National Identity Card Based on Identity Number. 
However, this regulation still does not 
accommodate the protection of residents' 
personal data (storage and use) related to post -
scanning and recording of data related t o 
fingerprints and retinal scans of residents.  
According to Article 1 point 22 of Law No. 24 
of 2013, concerning Amendments to Law 
Number 23 of 2006 concerning Population 
Administration, it is stated that personal data as 
individual data must be stored and  maintained for 
the truth and protected in its confidentiality. It is 
also stated in Article 85 that the state has an 
obligation to keep and provide protection for the 
personal data of these residents. This is as stated 
in Article 79, which obliges the sta te to provide 
protection and appoint a minister as the person in 
charge of access to citizens' personal data.  
2) Law No. 49 of 2009 Concernin g Archiving  
In the context of archiving, in Article 3 of 
Law no. 49 of 2009 concerning Archives, it is 
determined that  one of the objectives of archiving 
is to ensure the safety and security of archives as 
evidence of accountability in the life of society, 
nation and state. This law also regulates the 
retention period of data, ranging from 10 to 25 
years. A data/informati on archive that has passed 
the retention period of 25 years can be extended, 
destroyed, or shared to the public, provided that it 
does not disclose any confidential or personal 
data.  206 
 3) Law No. 8 of 199 9 Conc erning Company 
Documents  
This is the law that regulates company data, 
where Article 1 paragraph (2) determines that 
company documents include data, records, and/or 
information created and/or received by the 
company in the context of carrying out its 
activities. This documentation is written or 
recorde d in any form that can be seen, read, or 
heard.  
4) Law No. 10 of 1998 Concerni ng Banking  
Article 1 point 28 of this Act states that bank 
secrecy includes everything related to 
information regarding bank deposit customers 
and their deposits. Provisions relatin g to personal 
data protection in the Banking Act relate to bank 
confidentiality matters . Banks are required to 
keep information confidential regarding 
depositors and their deposits based on Article 40 
of Law Number 10 Year 1998, except in the 
cases referre d to in Articles 41, 41A, 42, 43, 44, 
and 44A. These exclusions are for cases where 
the implementation is used for tax purposes, 
settlement of bank receivables, judicial interests 
in criminal cases, as well as at the request, 
approval, or power of attorney  of the depositing 
customer. In these instances, the bank may 
violate the provisions concerning bank secrecy 
by following certain procedures.  
5) Law No. 21 of 2011 Conc erning the Financial 
Services Authority (OJK)  
According to this law, the OJK has 
supervisor y authority over all financial service 
providers, including banks that were previously 
supervised by the central bank. This law is one of 
the few controls covering the confidentiality of a 
customer’s personal data. This provision was 
later reinforced by OJ K Regulation (POJK) No. 1 
/ POJK.07 / 2013 concerning consumer 
protection in the financial services sector. In this, 
Article 2D states that the basic principles of 
consumer protection that the OJK must embrace 
are based on the principles of confidentiality  and 
the security of consumer data. This POJK also 
contains a special chapter that regulates the 
supervision of consumer protection in the areas 
of the financial services sector that are fully 
under the authority of the OJK [1 4]. 
6) Law No. 36 of 199 9 Conc erning 
Telecommunications  
This law regarding telecommunications 
regulates several matters relating to the 
confidentiality of information. These regulations, 
among others, are stated in Article 22, which 
stipulates that every person is prohibited from 
committing acts without rights,  illegality, or 
manipulation of: (a) access to telecommunications networks; and/or (b) access 
to telecommunications services; and/or (c) access 
to special telecommunications networks.  
7) Law No. 11 of 2008 Concernin g Information 
and Electronic Transactions  
Article 9 states that business actors who offer 
products through electronic systems must make 
available full and true information about 
contractual conditions, producers, and offered 
products. Article 26 paragraph (1) states that, 
unless provided otherwise b y the rules, the use of 
any information through electronic media that 
involves personal data must be made with the 
consent of the person concerned. Paragraph (2) 
then states that any person whose rights are 
infringed as indicated in paragraph (1) may lodge  
a claim for damages incurred under this law. 
Elucidation of Article 26 paragraph (1) states that 
in the use of information technology, the 
protection of personal data is one part of  privacy 
rights.  
 
B. Urgency on Internet User Personal Data 
Protection in Indonesia  
The internet has indirectly changed the way 
that people communicate. Information 
technology has changed the lifestyles of people 
globally, which has resulted in significant 
changes in social, cultural, economic, and legal 
frameworks. Indonesia is a  developing country 
that has a very large number of people using 
technology and modern communication systems; 
however, it does not yet have a comprehensive 
legal framework for regulating the protection of 
internet users and their data.  The impact is that 
there is an urgency to address legal issues related 
to personal data protection, which increases along 
with the increasing use of technology. This is 
because often existing laws cannot work 
optimally and effectively in keeping up with 
technological developm ents. This causes the law 
to run often slower than the development of 
society, including technological developments 
related to protecting personal data for internet 
users in Indonesia.  
On the other hand, the reality is that people 
have not placed their per sonal data as part of the 
property that must be protected. This is 
evidenced by the number of posts containing 
personal data content, both on several social 
media platforms and in various social networking 
groups. Besides, users, in general, do not fully 
understand the privacy policy, terms, and 
conditions of service of each of these applications, 
especially those related to the use of personal 
data when they access several  electronic 207           Mangku et al. / Journal of Southwest Jiaotong University / Vol.56 No.1 Feb. 2021  
 platforms (e -commerce, online transportation, 
and many more) [1 0]. 
One of  the efforts that can be made to 
overcome this problem is to strengthen the legal 
framework for protecting personal data, namely, 
by creating a protection system applying 
principles that uphold the protection of user 
privacy at the regulatory or technical level [1 5]. 
The principles that uphold the protection of user 
privacy are as follows [1 5]: 
1. Proactive, instead of reactive: This 
principle focuses on anticipation and prevention.  
2. Prioritizing user privacy: This principle 
maps the efforts to provide maximum privacy 
protection by ensuring that personal data is 
automatically protected in IT systems.  
3. Integration of privacy protection into the 
design: This principle requires that the protection 
of personal data should be embedded in 
technology designs.  
4. Optimizing  the function: It emphasizes 
the provision of risk mitigation standards for 
electronic systems whose obligations are not only 
for the security of the company but also for the 
privacy of the owners of personal data.  
5. Provision of a total security system: Thi s 
principle strengthens the security system from the 
beginning to the end.  
6. Transparency: This ensures that 
practically any existing business or technology 
can operate following the agreed rules and be 
disclosed to the public . Digital -based service 
provider s must comply with a verification 
process that has been carried out by an 
independent party.  
7. Respecting user privacy: This principle is 
the most vital and is realized by giving the 
owners of personal data an active role to manage 
their data . 
In general, th ese seven principles are 
contained and spread at the technical, regulatory 
level in Indonesia. Take, for example, the 
proactive, instead of reactive principle that has 
been stipulated in Article 15 and Article 16 of 
Law No. 11 of 2008 concerning Electronic  
Information and Transactions, which regulates 
that electronic system providers must provide a 
reliable and secure electronic system, and are 
responsible for system operation, and establish 
minimum requirements for the implementation of 
the system. The pri nciple of providing total 
security is also contained in Article 26 letter a of 
the Financial Authority Authority (OJK) 
Regulation Number 77 / POJK.01 / 2016 
concerning Information Technology -Based 
Borrowing and Lending Services, which 
stipulates that it re quires the maintenance of confidentiality, integrity, and availability of 
personal data from the beginning to destroyed 
[16]. 
However, specific and comprehensive 
arrangements related to the seven principles are 
not determined by the Law's level and are 
scattered in different implementing regulations. 
This contrasts with the increasing development of 
technology and the increasing number of digital -
based service users who demand personal data 
protection efforts that require a stronger legal 
basis, namely in p roviding guarantees for 
people's rights to the security of their personal 
data.  
Therefore, specific and comprehensive laws 
and regulations related to protecting personal 
data in Indonesia are very important. This is 
intended so that Indonesia will have a legal basis 
to implement better personal data protection in 
Indonesia in the future.  
 
V. CONCLUSION  
The concept of personal data protection 
implies that individuals have the right to 
determine whether that person will join the 
community and be able to  share o r exchange 
personal data with one another or not, as well as 
the right to determine what conditions must be 
met in order to do this. The threat of personal 
data leakage is increasingly occurring in line with 
the development of the e -commerce sector in 
Indonesia. The growth of this digital startup has 
also triggered a massive collection of consumer 
data, which is not only limited to personal data, 
but also behavioral (shopping/activity) data from 
consumers.  
Until now, no one has specifically regulated 
the pr otection of personal data in Indonesia. 
However, the aspect of protection has been 
reflected in other laws and regulations, including 
Law No. 23 of 2006 concerning Population 
Administration; Law No. 49 of 2009 concerning 
Archives; Law No. 8 of 1999 concern ing 
Company Documents; Law No. 10 of 1998 
Banking; Law No. 21 of 2011 concerning the 
Financial Services Authority (OJK); Law No. 36 
of 199 9 concerning Telecommunications; Law 
No. 11 of 2008 concerning Electronic 
Information and Transactions.  
Referring to the issues of the existing problem, 
the importance of protecting personal data is 
starting to strengthen along with the increasing 
number of cellphone and internet users. 
Therefore, it is necessary to have specific and 
comprehensive legislatio n related to personal 
data protection in Indonesia. It is intended that 
Indonesia should have  a legal basis in 208 
 implementing better personal data protection in 
Indonesia in the future . 
 
REFERENCE S 
 
[1] IBRAHIM , R. (2021 ) Jurisdiksi 
Dunia Maya (Cyberspace) dalam  Sistem 
Hukum Nasional Abad XXI.  Ius Quia  Iustum 
Law Journal , 10 (24), pp. 119-127. 
[2] NUGRAHA, R.A. (2012 ) Analisis 
Yuridis Mengenai Perlindungan Data 
Pribadi dalam Cloud  Computing System 
Ditinjau dari UU Informasi dan Transaksi 
Elektronik . Jakarta : Universitas Indonesia . 
[3] RIZAL, M.S. (2021 ) Perbandingan 
Perlindungan Data Pribadi Indonesia dan 
Malaysia . Jurnal Cakrawala Hukum,  10 (2), 
pp. 218-227. 
[4] MUTIARA , U. and MAULANA, R. 
(2020 ) Perlindungan Data Pribadi Sebagai 
Bagian Dari Hak Asasi Manusia Atas 
Perlindungan Diri Pribadi . Indonesian 
Journal  of Law and  Policy  Studies , 1 (1), pp. 
43-55. 
[5] DEWI , S. (2015 ) Privasi atas Data 
Pribadi: Perlindungan Hukum dan Bentuk 
Pengaturan  di Indonesia . Jurnal De Jure , 15 
(2), 23. 
[6] BANISAR,  D. ( 2000 ) Privacy  & 
Human Rights : An International Survey  of 
Privacy  Laws  and Developments . 
Washington , District of Columbia : Electronic  
Privacy  Information Centre . 
[7] CNN INDONESIA  (2018)  Risiko 
Ketika Data Pribadi Dicuri . [Online] 
Available from: 
https://www.cnnindonesia.com/teknolo gi/201
81226210103 -185-356593/risiko -ketika -
data-pribadi -dicuri  [Accessed 0 4/01/21].  
[8] SAAD, A.R. (2005 ) Personal Data &  
Privacy  Protection . Hunters Hill : 
Puddingburn  Publishing.  
[9] PURWANTO . (2007 ) Penelitian 
Tentang Perlindungan Hukum Data Digital . 
Jakarta : Badan Pembin aan Hukum Nasional . 
[10] DJAFAR, W. (n.d.)  Hukum 
Perlindungan Data Pribadi di Indonesia: 
Lanskap, Urgensi dan Kebutuhan 
Pembaruan . [Online] Available from: 
https://law.ugm.ac.id/wpcontent/uploads/sites
/1043/2019/08/Hukum -Perlindungan -Data -Pribadi -di-Indonesia -Wahyudi -Djafar.pdf  
[Accessed 0 4/01/21]. 
[11] HERIANI, F.N. (2019) Data 
Pengguna Lion Air Bocor, UU Perlindungan 
Data Pribadi Dibutuhkan . [Online] Available 
from:  
https://www.hukumonline.com/berita/baca/lt
5d8947d 7aa783/data -penumpang -lion-air-
bocor --uu-perlindungan -data-pribadi -
dibutuhkan/  [Accessed 0 4/01/21]. 
[12] PERTIWI, W.K. (2020) Kasus 
Kebocoran Data di Indonesia dan Nasib UU 
Perlindungan Data Pribadi . [Online] 
Available from:  
https://tekno.kompas.com/read/2020/05/05/1
9080067/kasus -kebocoran -data-di-indonesia -
dan-nasib -uu-perlindungan -data-
pribadi?page=all  [Accessed 0 4/01/21]. 
[13] SITOMPUL,  A. (2001 ) Hukum 
Internet, Pengenalan Mengenai Masalah 
Hukum Di Cyberspace . Bandung : PT Citra 
Aditya Bakti.  
[14] MAKARIM, E. (2010 ) Tanggung 
Jawab Hukum Penyelenggara Sistem 
Elektronik . Jakarta : Raja Grafindo Persada . 
[15] SAUTUNNIDA, L. (2018 ) Urgensi 
Undang -Undang Perlindungan Data Pribadi 
di Indonesia: Studi Perbandingan Hukum 
Inggris dan Malaysia . Kanun Jurnal Ilmu 
Hukum , 20 (2), pp. 369-384. 
[16] HAMZAH, A. (1987 ) Aspek -aspek 
Pidana di Bidang Komputer, Cet. I . Jakar ta: 
Sinar Grafika.  
 
 
参考文 : 
[1] IBRAHIM ，R.（2021）网络管辖权
（网络空间） 达拉姆 ·西斯泰姆 ·胡库姆 ·阿
巴德。因为那是正确的法律期刊 ，法律杂
志，10（24），第 119-127页。  
[2] NUGRAHA ，R.A。（ 2012）我国个人
数据保护的法律分析 云计算系统 电子信息
大学和电子学系 。雅加达：印度尼西 亚大
学。  
[3] RIZAL ，硕士（ 2021）印尼个人 资料
和马来西亚。 贾卡纳尔地平线法 ，10
（2），第 218-227页。  
[4] MUTIARA ，U。和 MAULANA ，R。
（2020）作为保护个人人权的一部分保护209           Mangku et al. / Journal of Southwest Jiaotong University / Vol.56 No.1 Feb. 2021  
 个人数据 。印尼法律和政策研究 杂志， 1
（1），第 43-55页。  
[5] DEWI ，S.（2015）私人数据：印度尼
西亚的法律保护和表格安排 。朱尔纳·德·
朱尔，15（2）， 23。 
[6] BANISAR ，D。（ 2000）隐私与人
权：隐私权法律和发展的国际调查 。哥伦
比亚特区华盛顿：电子隐私信息中心。  
[7] 有线电视新闻网 印度尼西 亚（ 2018）
个人数据被盗的 风险。[在线]可从以下网
站 获得：
https://www.cnnindonesia.com/teknologi/201
81226210103 -185-356593/risiko -ketika -
data-pribadi -dicuri [访问时间 04/01/21] 。 
[8] SAAD ，A.R。（ 2005）个人数据和 隐
私保护。猎人山：布丁本出版社。  
[9] PURWANTO 。（ 2007）数字数据保 护
法研究。雅加达： 国家法律 发展局。 
[10] DJAFAR ，W.（n.d.）印度尼西 亚的
胡库姆 ·佩林登甘 资料库：兰斯卡普，乌
尔根西 ·丹·基布图汉 ·彭巴鲁。 [在线]可从
以 下 网 站 获得：
https://law.ugm.ac.id/wpcontent/uploads/sites
/1043/2019/08/Hukum -Perlindungan -Data -
Pribadi -di-Indonesia -Wahyudi -Djafar.pdf [ 已
访问 04 / 01/21] 。 
[11] HERIANI ，F.N。（ 2019）狮航用户
数据泄漏，需要 个人数据保 护法。[在线]
可 从 以 下 网 站 获得：
https://www.hukumonline.com/berita/baca/lt
5d8947d7aa783/data -penumpang -lion-air-
bocor --uu-perlindungan -data-pribadi -
dibutuhkan/ [ 访问时间： 04/01/21 ] 。 
[12] PERTIWI ，W.K.（2020）印度尼西 亚
的数据泄漏案件 和个人数据保 护法的命运 。
[在线]可 从 以 下 网 站 获 取 ：
https://tekno.kompas.com/read/2020/05/05/1
9080067/ kasus -kebocoran -data-di-indonesia -
dan-nasib -uu-perlindungan -data-
pribadi?page= 全部 [Accessed 04/01/21] 。 
[13] SITOMPUL ，A。（ 2001）互联网法，
法律问题概论 网络空间。万隆： PT柠檬
阿迪亚。 
[14] MAKARIM ，E。（ 2010）电子系统
操作员的法律责任 。雅加达： 拉贾·格拉
芬多 ·佩尔萨达。 [15] SAUTUNNIDA ，L.（2018）印度尼
西亚保护法的紧迫性 数据：马来西亚，马
来西亚。 法学杂志法典 ，20（2），第
369-384页。  
[16] HAMZAH ，A.（1987）计算机领域的
犯罪方面 ，著作。 一世 .雅加达： 西纳尔 ·
格拉菲卡 。 "
DataPrivacy,3002.txt," 1   Author:  Martin Tisné, Managing Director, Luminate  Editor: Marietje Schaake, International Policy Director, Cyber Policy Center     The threat of digital discrimination  On March 17, 2018, questions about data privacy exploded with the scandal of the previously unknown consulting company Cambridge Analytica. Lawmakers are still grappling with updating laws to counter the harms of big data and AI.   In the Spring of 2020, the Covid-19 pandemic brought questions about sufficient legal protections back to the public debate, with urgent warnings about the privacy implications of contact tracing apps.1 But the surveillance consequences of the pandemic’s aftermath are much bigger than any app: transport, education, health systems and offices are being turned into vast surveillance networks. If we only consider individual trade-offs between privacy sacrifices and alleged health benefits, we will miss  1 See John Thornhill; Naomi Klein 
“The collective nature of big data means people are more impacted by other people’s data than by data about them. Like climate change, the threat is societal and personal.”  2 the point. The collective nature of big data means people are more impacted by other people’s data than by data about them. Like climate change, the threat is societal and personal.   In the era of big data and AI, people can suffer because of how the sum of individual data is analysed and sorted into groups by algorithms. Novel forms of collective data-driven harms are appearing as a result: online housing, job and credit ads discriminating on the basis of race and gender, women disqualified from jobs on the basis of gender and foreign actors targeting light-right groups, pulling them to the far-right.2 Our public debate, governments, and laws are ill-equipped to deal with these collective, as opposed to individual, harms.   Data is the new CO2  As with CO2, data privacy goes far beyond the individual. We are prisoners of other people’s consent. If you compare the impact of data-driven harms to those of CO2, it becomes clear how impacts are societal, not individual. My neighbour’s car emissions, factory smoke from a different continent, affect me more than my own small carbon footprint ever will. This collective threat of climate change is well reflected in environmental law and it underpins the (political) logic of emissions reductions and the Paris accords.3        Individuals may enjoy short-term benefits from what will harm the collective in the long term. Thinking optimistically, the Coronacrisis could open the path to laws dealing with collective data-driven harms. More likely, the clash between society’s immediate and understandable healthcare fears will be pitted against privacy protections. For example, the UK health minister said that “no one should constrain work on responding to coronavirus due to data protection laws”.4 Even the European Commission’s Data Strategy focuses mostly on empowering individuals with regards to “their” data.5 The need for collective data rights continues to be ignored.   From collective to individual rights, and back  Data rights were historically not as individualized as they are today. Human rights law at the end of the Second World War focused largely on protecting groups. The Nazi regime had oppressed and massacred Jews, Roma and other persecuted peoples on the basis of  2 Ali, Sapiezynski, Bogen, Korolova, Mislove and Rieke, “Discrimination through optimization: How Facebook's ad delivery can lead to skewed outcomes”, 2019 https://arxiv.org/abs/1904.02095 3 More recently in Holland, the Urgenda Climate Case against the Dutch Government established that the government had a legal duty to prevent dangerous climate change and must significantly reduce emissions to protect human rights. 4 https://twitter.com/MattHancock/status/1240189379676712960?s=20 5 https://ec.europa.eu/digital-single-market/en/policies/building-european-data-economy “The era of machine learning effectively renders individual denial of consent meaningless.”  3 their belonging to a minority group. The collective harm wrought by a pernicious state was articulated with the concept of genocide: a new concept to describe crimes committed ""with intent to destroy, in whole or in part, a national, ethnical, racial or religious group."" The aim was then to protect groups from future genocidal crimes.6   In the 1970s, the pendulum began to swing in the direction of individual privacy, with the rise of computing. The Organisation for Economic Development and Cooperation (OECD) developed a set of privacy guidelines in 1980. These guidelines popularized the notion that individuals should give informed consent for any information used for and about them. 7 During the same period, the 1978 French data protection law enshrined the notion that people’s personal data must be collected and processed fairly and lawfully for specified, explicit, and legitimate purposes, and with the consent of the person themselves (referred to as the “data subject”). 8 The French law in turn inspired the European Union 1995 Directive on personal data protection, which inspired the 2018 General Data Protection Regulation (GDPR) often called the gold standard of data protection laws. Today, data rights are seen as 'individual rights' and individualisation of data rights has become a cornerstone of data protection laws around the world.9   The irony of history is that as governments and laws moved from protecting groups to protecting individuals, technology firms were moving the other direction, from analysing individual behaviour towards that of groups. The era of machine learning effectively renders individual denial of consent meaningless. Even if I refuse to use Facebook or Twitter or Amazon - the fact that everyone around me has joined means there are just as many datapoints about me to target.        As engineers and companies began to deploy increasingly complex algorithms, coupled with data gathered at scale, the market has evolved beyond transacting individual data,  6 Samantha Power, “A Problem From Hell”, 2002. 7 https://www.oecd.org/sti/ieconomy/privacy-guidelines.htm 8 https://www.cnil.fr/sites/default/files/typo/document/Act78-17VA.pdf 9These range from the right to be informed, the right to access data, to rectify it, erase it, restrict its processing. https://ico.org.uk/for-organisations/guide-to-data-protection/guide-to-the-general-data-protection-regulation-gdpr/individual-rights/ https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:12012P/TXT  As per the Charter of Fundamental Rights of the European Union: “everyone has the right to the protection of personal data concerning him or her.” “Even if I refuse to use Facebook or Twitter or Amazon – the fact that everyone around me has joined means there are just as many data points about me to target.” 
“Our societies need collective and individual level data rights, similarly to non-discrimination law which covers individuals and groups.”  4 towards extracting value from collective data. The fact that laws remain focused on the individual puts them out of touch with the rapidly unfolding reality that technology and artificial intelligence creates. Our societies need collective and individual level data rights, similarly to non-discrimination law which covers individuals and groups.10   Why the individualist fallacy suits Big Tech  When media design professor David Carroll sought to retrieve data about him from Cambridge Analytica, he filed a legal claim under the UK’s data protection law. Prof. Carroll then challenged the company’s liquidation, citing the public interest in accountability and independent oversight. Court documents show that he believed learning more about how his individual data was being collected and used would shed light on the impact of big data and AI on the collective, on democracy. His appeal was dismissed.11 The case shows how hard it is for individuals to seek remedy for collective harms, as opposed to personal privacy invasions.  The value of an individual’s data to Google or Facebook is marginal. For companies, the value lies in the inferences drawn from your interaction with others.12 In 2018, Facebook generated $10/year income per active daily user.13 The harms that the individual can demonstrate are thus minimal. Blending individuals into a class and tracking how that class responds to different stimuli means Google cannot say how data about you has been used. But the value of their processing of collective data is enormous. From those $10 per person per year, Facebook generated an annual net income of $22bn in 2018, while Alphabet generated $30bn. Companies with data analytics capabilities were found by PwC to have higher stock market values than peers within the same industry.14  The laws and thinking developed in the 1970s are no longer suited to deal with today’s reality. The issue here is a fundamental mismatch between the logic of the market and the logic of the law.15 Contemporary technology markets extracts value from collective data. Our laws respond to individual harms and have not changed to reflect changes in technology. Governments should change legal regimes to match the logic of the market. Perhaps urgency has been lacking so far because the nature of the collective harms – much like CO2 pollution – is invisible to the average person. Algorithms are cloaked in secrecy, their effects omnipresent but invisible. The notion of injustice, which can lead to awareness and legal claims, is evanescent when the injustice was committed invisibly, by  10 I am grateful to Prof. Sandra Wachter for this comment. Please see Affinity Profiling and Discrimination by Association in Online Behavioural Advertising https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3388639 11 The liquidation judge noting that “the relevant ‘interests’ are Prof. Carroll’s interests as a creditor, not his interests as a curious academic or as someone leading a campaign to establish a principle about the use of data or as someone who is unsettled by what might have happened to his data in the past. https://www.judiciary.uk/judgments/vincent-john-green-mark-newman-v-cambridge-analytica-uk-limited-others/ 12 A Right to Reasonable Inferences: Re-Thinking Data Protection Law in the Age of Big Data and AI https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3248829 13 https://investor.fb.com/investor-news/%20press-release-details/2019/Facebook-Reports-Fourth-%20Quarter-and-Full-Year-2018-Results/ 14 https://www.bennettinstitute.cam.ac.uk/publications/value-data-summary-report/ ; pwc.co.uk/issues/ data-analytics/insights/putting-value-on-data.html  15 See Julie Cohen, “Between Truth and Power”, 2019  5 a computer model (though designed by humans).16 Collective action is therefore also less likely to take place.17 The task at hand is to understand the nature of novel harms and make the invisible visible.   Making the invisible visible: collective data-driven harms   The more collective the harm, the less people are protected and the less visible it is. The more the harm is individual, the more visible its impacts are and the more people are legally protected. If a person is discriminated against because of protected characteristics such as their age, gender or ethnicity, it will be visible to them and they will hopefully be in a position to seek redress. When a person is discriminated against due to an algorithmic decision, it is likely to be less visible and, currently, hard to seek redress.18   People tend to suffer from data-driven harms in three main ways. First, there are purely individual harms. For example, an individual is seen as unfit for employment due to data directly related to them (e.g. their age). Protections against these types of harms are well established in law.  Second, there are inferred harms. This is where the individual is inferred to be part of a group or category of people but the person whose data is used is not harmed. Consider people uploading public photos of themselves on a popular American dating website, as these were used by researchers controversially developing algorithms to ascertain people’s sexuality based on their facial characteristics. 19 Individuals whose photos are used are not the only ones harmed necessarily. People whose sexuality is ""identified"" (however spuriously) via these techniques are the ones harmed via inferences made as a result of data collected and processed.20   Third, there are optimized harms. These are harms suffered as a result of how machine learning systems are optimized. YouTube’s algorithm has concluded that people are drawn to content that is more extreme than what they are currently viewing and leads them to a path that, as academic and activist Zeynep Tufekci has written, might be harmless (from jogging to ultra-marathons) or damaging (from political rallies to conspiracy theories).21 People are unwittingly profiled by the algorithm. As with all optimisation systems, YouTube’s algorithm is single-mindedly focused on its users and does not focus on its externalities on non-users, minorities and anyone who is not on the system (i.e. society at large).  16 A recent study of the use of AI in hiring in the UK determined that the auditing tools used to ensure compliance were not able to accurately determine bias in an AI system. Why Fairness Cannot Be Automated: Bridging the Gap Between EU Non-Discrimination Law and AI https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3547922 17 I am grateful to Prof. Wachter for this insight. 18 Pioneers in this field include E. Bloustein’s “Group Privacy: The Right to Huddle” https://heinonline.org/HOL/LandingPage?handle=hein.journals/rutlj8&div=24&id=&page= ; Taylor, Floridi and van der Sloot (editors) of “Group Privacy: New Challenges of Data Technologies” https://www.springer.com/gp/book/9783319466064; Mittelstadt “From Individual to Group Privacy in Big Data Analytics” https://link.springer.com/article/10.1007/s13347-017-0253-7  19 https://www.economist.com/science-and-technology/2017/09/09/advances-in-ai-are-used-to-spot-signs-of-sexuality 20 See Dr. Sandra Wachter “A right to reasonable inferences: re-thinking data protection law in the age of big data and AI” https://www.law.ox.ac.uk/business-law-blog/blog/2018/10/right-reasonable-inferences-re-thinking-data-protection-law-age-big 21 https://www.nytimes.com/2018/03/10/opinion/sunday/youtube-politics-radical.html  6  Our countries’ legal systems and policy arsenals are ill-equipped to respond to the latter two data-driven harms. Data protection, as currently framed, is premised on a relationship between data controllers and data subjects. As technology becomes increasingly sophisticated, that connection between data controllers and data subjects falters. It is not always clear who the controller is nor which subject has been harmed. A legal vacuum will arise – and possibly already exists – and accountability falls away.22   
As the world moves further online due to the Coronavirus, companies and governments will collect a lot more information about people through data gathering. This will likely increase the use of automated decisions, for example on how to allocate resources. And with more automation, there will be even greater equity implications. Data processing may decide who gets to have access to education, welfare or to the judicial system. Research over the past five years has shown how the negative impacts of automated decision-making on people fall disproportionately on those already marginalised in society, such as people of colour, women and immigrants. 23   The 21st century catch to the data privacy and discrimination problem is that the members of the public no longer know which group they are part of or not, only the algorithm does. Many people will not even know that they are being profiled or discriminated.24 The conversation needs to be reframed around automation and power and which groups will be adversely impacted.  Solutions lie in hard accountability, strong regulatory oversight of data-driven decision making, and the ability to audit and inspect the decisions and impacts of algorithms on society.   Regulating automation is regulating power: the case for hard accountability   Rather than regulating how people consent to their data being used in order to protect their privacy, policymakers should regulate automation, starting with black box algorithms that collect, sort and classify data. That will take a whole new method of regulation. Members of the public need information, public scrutiny and accountability on  22 See Ravi Naik, 2020 https://jolt.law.harvard.edu/digest/the-gentle-civilizer-of-technology 23 https://www.theguardian.com/technology/series/automating-poverty 24 Why Fairness Cannot Be Automated: Bridging the Gap Between EU Non-Discrimination Law and AI https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3547922 “Solutions lie in hard accountability, strong regulatory oversight of data-driven decision making, and the ability to audit and inspect the decisions and impacts of algorithms on society.”  7 and for the disparate impacts of the huge amounts of automation that are pointed at them every second of the day.   In the European Union, the GDPR is weak on automation and collective harms.25 The accountability of algorithmic decision systems are mainly covered by articles 13-15 and 22 but these are limited to decisions that are wholly automated, that use personal data, and that are deemed “significant decisions” thus eluding many of the smaller harms detailed earlier, which cumulatively amount to significant collective harms.26 GDPR further individualises data-driven harms by requiring the person who suffered the harm to be at the centre of any claim resulting from it. That would be like requiring that a case on the Co2 emissions of an entire country depend on its provable impacts on one person.27   Three elements are needed to ensure hard accountability: (1) clear transparency about where and when automated decisions take place28 and their impact on people and groups, (2) the right to give meaningful public input and call those in authority to justify their decisions, and (3) the ability to enforce sanctions.29. A Public Interest Data Bill should encapsulate these three points.   Clear Transparency   The focus should be on public scrutiny of automated decision making and the types of transparency that lead to accountability.30 This includes revealing the existing, purpose and training data behind the algorithms, as well as their impacts – whether they led to disparate outcomes, and on which groups. Clear and targeted transparency sheds light on the algorithms and the institutions that deploy them, e.g. revealing information about institutional performance (e.g. use of facial recognition cameras by the police and their impact), and are explicit about what gets measured, by whom and how. But transparency remains a necessary but not sufficient condition for accountability. 31 For that, meaningful public input and the possibility to enforce sanctions are needed.   Public participation    25 https://gdpr-info.eu/ 26 “Why a Right to Explanation of Automated Decision-Making Does Not Exist in the General Data Protection Regulation” Wachter, Floridi, Mittelstadt 2017  https://academic.oup.com/idpl/article/7/2/76/3860948; “Enslaving the algorithm: from a right to an explanation to a right to better decisions” Edwards and Veale 2018, https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3052831; 27 The law does allow for NGOs to take up complaints without being mandated by a specific person (the “data subject”) but, again after intense lobbying, that section of the law was made optional and only three out of twenty-eight European countries chose to enact it. When NGOs can bring systemic claims on behalf of the public without needing to have a mandate from an individual, data-driven harms could be collectively safeguarded in the same way that environmental harms are. GDPR Section 80. (2)  GDPR’s main remedy to countering collective data-driven harms is either when the violation of a person’s individual rights is symptomatic of the same violation being suffered by all or when a class action can be mounted. There are few such cases. That seemingly obscure section of the law points to a potentially interesting future trend.  I am grateful to the team at Privacy International for their time spent explaining this point. 28 Sometimes referred to as the “Blade Runner Law” requiring an automated system or bot to declare itself as such and not camouflage itself as a human. 29 See Prof. Jonathan Fox’s distinction between hard and soft accountability here https://www.tandfonline.com/doi/full/10.1080/09614520701469955 30 http://omidyar.com/sites/default/files/file_archive/Public%20Scrutiny%20of%20Automated%20Decisions.pdf 31 Jonathan Fox, “The uncertain relationship between transparency and accountability”, 2007,  ttps://www.tandfonline.com/doi/full/10.1080/09614520701469955; Fung, Graham, Weil “Full Disclosure: The Perils and Promise of Transparency”, 2007   8  The public has a fundamental right to call those in power to justify their decisions. This “right to demand answers” should not be limited to consultative participation where people are asked for their input and officials move on. It should include empowered participation where public input is mandated prior to the roll-out of an algorithm in society. For example, algorithmic impact assessments should provide members of the public the possibility to give meaningful input into the use of automated decision making, expanding such assessments as a tool for community-driven decision making.   Sanctions   Finally, the power to sanction is key for these reforms to succeed and for accountability to be achieved. The GDPR has been hobbled by the lack of funding and capacity of data protection commissioners across Europe. Despite the GDRP’s power to impose fines of up to 4% of a company’s annual turn-over, few such fines have been meted out and half of Europe data protection regulators only have five or fewer technical experts.32 But data protection or information commissions cannot be solely responsible for the accountability of algorithms as our societies are transformed by artificial intelligence. Companies and governments need laws that restrict data usage and automation, above and beyond implications for people’s personal data. For this, societies will also need the modernisation of sectoral laws such as labour law, criminal law, genetic law, environmental law and discrimination law.33 For example, laws that regulate the public administration could already be applied here. Administrative law could be used to mandate greater accountability of automated decision making used by the public sector.34 Labour laws could be adapted to account for the role of technology in managing employer/employee relations.35   Precedent   Examples exist of draft bills that have sought to fill this gap. In the United States, an effort was undertaken in 2019 to enact an Algorithmic Accountability Act, that subsequently stalled in Congress, aiming to determine whether private sector algorithms resulted in discrimination or not. The Act would have required firms to undertake algorithmic impact assessments in certain situations to check for bias or discrimination.36 In France, the  32 “GDPR accused of being toothless because of lack of resources”, Financial Times 20th April 2020, https://www.ft.com/content/a915ae62-034e-4b13-b787-4b0ac2aaff7e 33 For an update of GDPR to cover such issues, see the conclusion of “A right to reasonable inferences: re-thinking data protection law in the age of Big Data and AI” https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3248829 34 Jennifer Cobbe, “Administrative Law and The Machines of Government”, 2018 https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3226913 35 In response to the optimisation used on Amazon workers, the best answer may be workers’ rights and protections rather than laws specifically geared towards the technology used. 36 https://www.congress.gov/bill/116th-congress/house-bill/2231/all-info The bill was a promising start but also criticised for relying on the relatively weak enforcement power of the Federal Trade Commission, for not providing an opportunity for meaningful public input as environmental impact assessments do, and for failing to mandate a clear level of public transparency for the results of algorithmic impact assessments.   9 Digital Republic Law (Loi Pour Une République Numérique) today applies to administrative decisions taken by public sector algorithmic systems but could provide a blueprint for future laws. It provides access to how important automation was to the ultimate decision. It also opens up access to the data used and its source, as well as any treatment parameters and weightings if used in decisions that affected people and provides information on the outcome of the automated process. In contrast, GDPR provides restrictions but only on the use of personal data in fully automated decisions.37   A Public Interest Data Bill   1. Clear Transparency: 1. Require that firms and governments open up the data and source code behind high-risk algorithms and define which are deemed “high-risk” in relation to evidence on the disparate impacts of those algorithms on the population (e.g. whether they fall disproportionality on marginalised communities).  2. Require that firms and governments publish algorithmic impact assessments assessing the outcomes of the algorithmic treatment on groups as well as any collective data-driven harms. Ensure the results of such assessments are published openly. Ensure these precede the roll-out of high-risk AI deployments and renew these on a regular schedule.38  3. Ensure full transparency and accountability of automation:  1. Tweaks to algorithms that might seem small or insignificant when considered alone, can add up to substantial collective impact when taken together- they would be included. These should not be limited to ‘decisions’ made by an algorithm nor to those decisions needing to be ’significant’ as is currently the case with GDPR article 22.39  2. Apply both to decisions that are fully, as well as partly automated.40  3. Require transparency and accountability for how a decision was made based on a computer model, not simply explaining the model in abstract. (The degree and the mode of contribution of the algorithmic processing to the decision taken.41)  37 The French Digital Republic Law (Loi Pour Une République Numérique) is under-researched given the over-focus of the machine learning field on Anglo-Saxon examples and case studies. The law today applies to administrative decisions taken by public sector algorithmic systems but provides a blueprint for future laws. The French law provides access to how important automation was to the ultimate decision. It also opens up access to the data used and its source, as well as any treatment parameters and weightings if used in decisions that affected people. It also provides information on the outcome of the automated process. For example, a person could have access to data and the source code used in an algorithm that decided whether to award them a place to a public university or not, and how that decision was made and weighted (e.g. were their grades more important than where they live?). In contrast, GDPR provides restrictions but only on the use of personal data in fully automated decisions. also ref to Edwards and Veale, 2018 38 https://ainowinstitute.org/aiareport2018.html 39 As Dr. Michael Veale says ""decisions that seem “insignificant” at the individual level may actually be very impactful at group level."" 40 GDPR provisions for explainability and accountability of algorithms are restricted to decisions that are 100% automated. In reality, most automated decisions have a human involved at some point even if their involvement is superficial or substantially biased by the verdict of the algorithm. 41 https://www.legifrance.gouv.fr/affichCodeArticle.do?cidTexte=LEGITEXT000031366350&idArticle=LEGIARTI000034195881  10 4. Cover decisions beyond those that use personal data. For example, this would cover self-driving cars, or data that was once personal and then supposedly anonymised. People are impacted by data that is not personal, and by personal data that is not about them.  2. Public participation:  1. Provide members of the public the possibility to give meaningful input into the use of automated decision making (including but not limited to input into algorithmic impact assessments).  2. Ensure that public participation is empowered and not merely consultative.  3. Sanctions: 1. Ensure the ability to enforce sanctions for non-compliance.  2. Fund and resource accountability bodies adequately, including oversight bodies for sectoral laws such as labour law, criminal law, genetic law, environmental law and discrimination, in addition to data protection agencies.   4. Relevance to groups as well as individuals: 1. Enable persons as well as organisations to lodge requests.42 2. Provide access to the treatment parameters and, where appropriate, their weighting, applied to the situation of the person(s) or groups concerned.   Conclusion  Privacy concerns surrounding COVID-19 brought to the surface a number of systemic mismatches between individual privacy law and the value of collective data processing. The pandemic accelerates the risk of inequality and new harms dramatically as surveillance and data gathering are accelerated in the name of ending the health crisis. Most of those suffering will be already marginalised and vulnerable in our societies. Similar to the collective nature of the threat of climate change, our governments and policy makers must change the way they think about the regulatory response. They need to consider data’s collective as well as the individual impact.            42 See footnote 27   11 About the Author:   Martin Tisné is Managing Director at Luminate, a global philanthropic organization. He is responsible for Luminate’s Data & Digital Rights impact area, work in Europe, and policy and advocacy. Alongside the Obama White House, Martin founded the Open Government Partnership and helped it grow to a 70+ country initiative. He also initiated the International Open Data Charter, the G8 Open Data Charter, and the G20’s commitment to open data principles. Martin is the co-founder of Publish What You Fund, a global campaign for foreign aid transparency, and Integrity Watch Afghanistan, the country’s leading anti-corruption NGO. Twitter: @martintisne  About Luminate:   Luminate is a global philanthropic organisation focused on empowering people and institutions to work together to build just and fair societies. We support innovative and courageous organisations and entrepreneurs around the world, and we advocate for the policies and actions that will drive change across four impact areas: Civic Empowerment, Data & Digital Rights, Financial Transparency, and Independent Media. We work with our partners to ensure that everyone has the opportunity to participate in and to shape the issues affecting their societies, and to make those in positions of power more responsive and accountable. Luminate was established in 2018 by philanthropists Pierre and Pam Omidyar. The organisation was founded by The Omidyar Group. www.luminategroup.com    Acknowledgments:  I am extremely grateful to Salmana Ahmed, Madeleine Clare Elish, Kate Crawford, Polly Curtis, Jonathan Fox, Janet Haven, Swee Leng Harris, Gus Hosein, Karen Levy, Jim Peacock, Ravi Naik, David Robinson, Marietje Schaake, Ben Scott and Sandra Wachter for reviewing this paper in draft form and their hugely helpful comments.   I would also like to give special thanks to Adrien Abecassis, Julia Angwin, Azeem Azhar, Solon Barocas, Ailidh Callander, Simon Chignard, Sylvie Delacroix, Alix Dunn, Alex Goodman, Seda Gürses, Kieron O’Hara, Gry Hasselbalch, Carly Kind, Neil Lawrence, Sean Macdonald, Aiha Nguyen, Tanya O’Carrol, Reema Patel, Seeta Peña Gangadharan, Imogen Parker, Phil Sheldrake, Martha Spurrier, Katarzyna Szymielewicz, Linnet Taylor, Jeni Tennison, Zeynep Tufekci, Michael Veale, Henri Verdier, Stefaan Verhulst, Adrian Weller, Glen Weyl, Meredith Whittaker, and Caroline Wilson Palow for their advice. I am very grateful for their time.     
 12 About the Editor:   Marietje Schaake is the international policy director at Stanford University’s Cyber Policy Center and international policy fellow at Stanford’s Institute for Human-Centered Artificial Intelligence. She was named President of the Cyber Peace Institute.  Between 2009 and 2019, Marietje served as a Member of European Parliament for the Dutch liberal democratic party where she focused on trade, foreign affairs and technology policies. Marietje is affiliated with a number of non-profits including the European Council on Foreign Relations and the Observer Research Foundation in India and writes a monthly column for the Financial Times and a bi-monthly column for the Dutch NRC newspaper.      
"
DataPrivacy,3003.txt,"User Perspective on Usages and Privacy of 
eHealth Systems in Bangladesh: A Dhaka based 
Survey 
Dr. Biplob Ray  
School of Engineering and Technology 
Central Queensland University  
Melbourne, Australia 
b.ray@cqu.edu.au 
 
Milon Biswas 
Department of Computer Science & Engineering, 
Bangladesh University of Business and Technology, 
Dhaka, Bangladesh. Dr. Kamal Kumar Saha  
School of Engineering and Technology 
Central Queensland University  
Melbourne, Australia 
k.saha@cqu.edu.au 
 
Md Mahbubur Rahman 
Faculty of CSE 
Patuakhali Science and Technology University  
Patuakhali, Bangladesh
Abstract - Electronic Health (eHealth) systems and devices 
are getting popular in developing countries due to many benefits to the users. To improve wider users’ acceptance of eHealth systems in developing countries, eHealth community needs a clear understanding of user’s expectations and needs which may not be the same as developed countries due to differences in education, socio-economic status, social values and beliefs. This paper contributes to the understanding of users’ perspectives on usages and privacy aspects of the 
eHealth systems and devices in Bangladesh. In this study, the 
participants who were 18 years or older were randomly selected across Dhaka for the electronic response to the survey questions. The survey result shows that 64% of the young generation under 35 years of age are not seriously concern about the privacy of their general and personal health information, though it is a primary concern to most of the people in Bangladesh. Regarding the consequences of 
disclosing health information to others, female respondents were more reactive than their counterparts. 
Key words - Electronic health, Privacy, Consequence, 
mHealth. 
I.    INTRODUCTION 
Due to the rapid growth of population, developing 
countries like Bangladesh are facing a shortage of qualified 
health professionals [11, 12]. At the same time, the uses of 
technology to maintain health are growing due to the 
continuous growth of internet users in developing 
countries. The circumstances  changed rapidly in recent 
times due to the faster aggression of telecommunication applications among the mass population in developing 
countries. Nowadays, health consciousness is not a fashion 
of wealthy people but also a passion for middle-class 
people who mostly thrives their life from hard working 
professions. The electronic health systems, such as mHealth applications and devices, web-based health 
portals, and health blogs, may provide easy access to 
quality healthcare for the disadvantaged population. 
Many studies have been conducted to understand the 
usages patterns and privacy concerns for the users of 
eHealth systems and devices [4, 7, 18, 13, 14, 22]. 
However, almost all of them ar e considered for the users of 
eHealth systems in developed countries like Germany [14], US [13], China [22] and Hong Kong [4]. Understanding 
users’ perspectives on usages and privacy aspects of the 
eHealth systems and devices in developing countries is also important. With this aim, in this paper, we have surveyed 
1,499 participants from Dhaka to understand users’ views 
towards usages and privacy of eHealth systems and 
devices. Data from the survey ed result are processed and 
analysed to identify the user s’ perspectives on usage and 
privacy of the eHealth system. 
With further explanation and discussion, the rest of the 
paper has been organised as follows. Section II presented 
the background and significance of this study followed by 
methodology and data collection in section III. In section 
IV, we have presented statistical analysis and discussion. 
Finally, the conclusion and limitations have been described 
in section V. 
II.   BACKGROUND AND SIGNIFICANCE 
Literature in the field of mHealth privacy can be 
classified into four main categories: privacy law [11, 12]; 
privacy policy [2, 11, 12, 16]; identification of privacy risks 
[1, 10, 17]; and user’s views towards mHealth privacy and 
electronic transmission of medi cal information [8, 20, 21]. 
There is also literature on users’ prevalence and perception 
of mHealth usages [4, 13, 14, 22 ], technical review [19] and 
solution [3] to address the privacy risks. This section is 
prepared after an extensive literature review to highlight the 
current state-of-the-art and the significance of the study. 
Several studies [2, 11, 12, 16] investigated the impact 
and standardisation of privacy policies for mHealth 
applications. Most investigations found that the privacy 
policies of mHealth applications were either confusing or 
did not exist. McCarthy, M. in [11] identified that, despite 
the efforts taken by the government to educate consumers and to provide guidance to developers, confusion persists 
amongst consumers due to large gaps in policies with 
respect to information access, security, and privacy. In the 
survey, A. et al. [2] identified that the majority of health 
applications (70%) do not have a privacy policy, and for 
those that do, focuses on the ap plication itself (rather on the 2020 IEEE Asia-Pacific Conference on Computer Science and Data Engineering (CSDE) | 978-1-6654-1974-1/20/$31.00 ©2020 IEEE | DO I: 10.1109/CSDE50874.2020.9411593
Authorized licensed use limited to: Carleton University. Downloaded on June 05,2021 at 20:49:18 UTC from IEEE Xplore.  Restrictions apply. company and website). According to [2], most of the 
available policies do not make the information transparent on privacy practices and are often too wordy to understand 
for general users. The authors suggested the necessity of 
developing international regulations for mobile health 
information collection, storage, and privacy. The article 
[16] focused on mobile health apps for diabetes and again 
found that many of these apps do not have a privacy policy. 
They pointed out that users might mistakenly believe that 
health information entered an application is private, 
particularly if the app has a privacy policy, but that 
generally is not the case. Worryingly, many of these 
mHealth apps access various se rvices of mobile devices 
that are not related to their functionality, such as camera access or modifying the conten t of external USB devices. 
According to the study by [7], most apps share information 
with third parties that are not directly under the developers’ 
control, such as data aggregators or advertising networks. 
Wen D. et al. [21] investigated user’s perspectives on 
the usage of wearable devices. The data of [21] shows that 
59.8% of users in China are discouraged to use health 
applications due to privacy concerns. The paper by [20] identified that older adults place greater emphasis on health 
care providers as opposed to personal access for managing 
their mHealth data and privacy. At the same time, younger 
adults are highly likely to withhold information in mHealth 
system out of concerns for privacy and security. O’Donnell, H. C., et al. [8] investigated consumers’ attitudes towards 
health information exchange between institutes and 
personal mHealth devices. The article has concluded that 
mHealth applications need to address consumer’s security 
concerns and disparities in standards for health information 
exchange. 
The articles [3, 5, 6, 19] have studied various patterns 
of mHealth app use and perception of mHealth users in the US, Germany, Hong Kong, and China. Several articles [5, 
6] reported that the use of mHealth applications have an 
inverse correlation with age among adults but have a linear 
correlation with education, socio-economic factors, and 
social status. On the contrary, Rasche, P. et al. [3] reported 
that gender or education did not show any significant 
differences in the use of h ealth apps in Germany. In 
Germany [3] adults are likely to  use health apps if they are 
at the lower end of the age range and have a high level of 
technical readiness which is in contrast with the findings in 
previous national surveys in the United States [19] and 
Hong Kong [5]. 
Information privacy [3, 14, 19], accuracy [9, 13] and 
standardisation [13, 14, 22] are some common expectations 
of users that are required to fulfil before mHealth systems 
can be adopted by mass population. A unique example in 
this line is reported by Krebs P. et al. [13] which described 
that users of mHealth systems in the US have high confidence in the accuracy and safety of their data, despite 
the authors’ acknowledged the lack of standardization, 
monitoring, and third-part y access to mHealth data. 
Dehling T. et al. [19] considered over 24,405 health-related 
apps (iOS; 21,953; Android; 2452) for information security and privacy risk assessments. He described that most of the 
apps that access health related critical information and provide health assessments are putting its users in the 
information security and privacy risks. 
Fatema K. et al. [20] have studied mHealth Services of 
Chakaria, a rural sub-district in Bangladesh. They have 
concluded that knowledge of existing mHealth services was 
well accepted among participants . The authors claimed that 
the compliance with the prescriptions was almost similar 
for advice received through mHealth devices, hence, there is a clear indication that surveyed society is looking 
forward to embracing mHealth technology. However, this 
result is done in a small sub-district which does not reflect 
the opportunity or view of the greater part of Bangladeshi 
population. 
Hannah et al. [21] conducted an investigative study in a 
remote district of Bangladesh to explore the behavioural 
impact and barriers in the adaptation of mHealth in mass people. They adopted both the COM-B (Capability, 
Opportunity, and Motivation-Behaviour) model and the 
Transtheoretical Domains Framework (TDF) to formulate 
a concrete guideline in the formative research of behaviour 
change, which might be helpful to conceptualise the awareness-rising and behaviour change in mHealth. They 
focused on the quality of equipment in diabetic 
management and control in Bangladesh, which was found 
very low.  
As of today, the adaptation rate of mHealth 
technologies among the elderly people in comparison to the 
other group of people are very low [15, 22]. In this study, 
the researchers applied the Un ified Theory of Acceptance 
and Use of Technology (UTAUT) to develop a model on 
the acceptance of mHealth tec hnologies among the elderly 
people of Bangladesh. They conducted a face to face study 
among the elderly and another group of people. The study 
results revealed some indicators such as performance expectancy, effort expectancy, social influence, technology 
anxiety and resistance to change, which has significant 
impact on the behavioural attitudes of elderly people.   
As detailed above, users of eHealth systems are 
concerned about the privacy of their data and confused 
about current security protection measures and relevant 
privacy laws. Furthermore, a very little study has been done 
to understand a larger segment of upcoming eHealth users who can be most benefitted from its capabilities.  This 
shows the need for a user centr ic approach to address the 
privacy issues of eHealth systems. As a step towards this, 
we have collected and analysed  users’ views of usage and 
privacy of eHealth systems. 
III. METHODOLOGY AND DATA COLLECTION 
This paper reports on the data collected from an online 
survey conducted by CQUniversity, Australia and 
Bangladesh University of Business and Technology, 
Bangladesh. The survey aims to obtain public opinion on a 
range of topics from the selected random sample of 
Bangladeshi residents in Dhaka. Random samples from all 
Authorized licensed use limited to: Carleton University. Downloaded on June 05,2021 at 20:49:18 UTC from IEEE Xplore.  Restrictions apply. the four Dhaka city corporat ions were selected following 
the Bangladesh Bureau of Statistics [3] database and contacted via email for an onlin e survey. Out of all invited 
people, only 30% agreed to participate in the survey, out of 
which 20% filled the entire online survey. We have used 
Survey Monkey to conduct our online survey which only 
considered participants from the 18 to 70 age group. 
A. Data collection process 
The survey was conducted between 2nd of November 
2017 to 9 January 2018. Data collection was conducted via 
telephone interviews, which comprised of a standard 
introduction and asking some demographic and core health 
questions. In total there were 10 questions that were pilot 
tested on 83 randomly selected students to create the final 
questionnaire. The questions were on consumer 
perspectives of personal electronic health privacy, with 
some of the questions specifically about electronic 
healthcare records, and others about data on personal 
healthcare devices and applications.   
The estimated sample error,  at the 95% confidence 
interval, is 
±2.8  percentage points. Several questions were 
used for quality assurance. On a scale of 1 (very poor) to 10 
(excellent), the quality of the survey had a mean score of 
8.6. Almost all respondents (96%) found the survey 
questions very easy or mostly easy to understand (4-level 
Likert scale).  
B. Representativeness of the samples 
A cluster sampling approach was considered to decide 
the sample units of this study. Dhaka City Corporation is 
divided into two parts as Dhaka City Corporation North (DNCC) and Dhaka city Corporation South (DSCC). A 
total of 1499 sample units comprised of 329 units from 
DNCC and 1170 units from DSCC were selected on a 
random basis to ensure the representativeness of the 
sample. For a further check on this issue, the chi-square analysis was conducted for the category of age and gender. 
The results for both categories are (/g1876
/g2870 (/g1856/g1858 = 2, /g2009 =
.05)  =  .456, /g1868 = .797) /g1853/g1866/g1856 ( /g1876/g2870(/g1856/g1858 = 1, /g2009 = .05)  =
 38.756, /g1868 = .00) . The result for age supports the 
representativeness of the sample to the population, whereas 
the gender result does not support it. But the sex ratio in the 
sample is completely matched with the sex ratio in the 
population of Dhaka city (104 males for 100 females), 
which can also be considered as the indicator of 
representativeness of the sample to the population.  
IV.  STATISTICAL ANALYSIS AND 
DISCUSSION 
Data were analysed using Statistical Package for the 
Social Sciences (SPSS) version 24 (IBM Corp).  The 
survey data consists of different characteristics of 
participants including demographic (age, gender) and case 
specific from the questionnaire. Below are the discussions 
of the results with appropriate graphs and interpretations. 
 A. Demographic characteristics of the Participants and 
their uses of health apps 
The total number of participants in this study was in 
1499. The range of age was 18 to 76 years with mean 43.95 
years and standard deviation 15.29. For a better presentation of the participants by age, we considered three 
age categories such as age between 18 to 35 years, 36 to 53 
years, and over 54 years.  The number of participants in 
three age groups were 516 (34.4%), 508 (33.9%), and 475 
(31.7%) respectively as illustrated in Figure 1. Respondents 
were diversified into male and female as 870 (58%) male 
and 629 (42%) females as presented in Figure 1.  
 
Fig. 1. Distribution of participants’ age and gender. 
Regarding participants’ preferences of several ways to store 
their health information, most of the 855 (57.2%) participants were found to like electronic medium to record 
it.  
B. Respondents’ attitude on the privacy of the recorded 
health information 
Health related information is usually considered as a 
personalized issue. Disclosure of personal health 
information to others can also be considered as a matter of 
compliance in most of the countries in the world. To know 
the preferences of the respondents on this issue they were asked: “how much they are concern about the privacy of 
health information”. For analysis purposes, respondents 
were categorised as per their age and gender. Regardless of 
all categories almost all of them (97.9% to 99.7%) highly 
concern about privacy of their health information. More 
specifically both the personal and general health 
information’s privacy issues were equally important to 
them. We conducted Chi-square tests between the privacy 
issue and the different categories of the respondents such as 
age, gender, and likeliness of health apps. Most of the test 
results show insignificant except the tests between privacy 
issue and likeliness of using different health applications. The Chi-square results for two kinds of health information 
are found as personal (22.617, /g1856/g1858 = 4, /g1868 /g3398 /g1874/g1853/g1864/g1873/g1857 =  0.0)  
and general (12.988, /g1856/g1858 = 4, /g1868 /g3398 /g1874/g1853/g1864/g1873/g1857 =  0.011) . Both 
the results indicate that th ere are statistically valid 
associations between the privacy issue and respondents’ likeliness of using health apps. 
The survey data were analysed by using a statistical 
cross tabulation procedure between the privacy issue and how often they review the privacy of health apps and how 
concern they are if the health information known by others. 
Authorized licensed use limited to: Carleton University. Downloaded on June 05,2021 at 20:49:18 UTC from IEEE Xplore.  Restrictions apply. The results show no significant difference in the attitude 
about the privacy of health apps among the different levels of respondents. The chi-square test result between the 
privacy issue of health apps and how much they are 
concerns if the health information known by others appears 
as significant (226.046, /g1856/g1858 = 4, /g1868 /g3398 /g1874/g1853/g1864/g1873/g1857 =  0.00) . 
Therefore, we can conclude that there is a statistically valid association between these two issues.     
According to the results of the study, nearly 61% of the 
respondents to whom the privacy of health apps is important did not like to use the health apps. On the 
contrary, 73% of the respondents to whom privacy of the 
health apps is not so important did like to use health apps. 
Both the facts highlight a lack of efficiency of privacy 
features of the health apps, which could be an important 
message to the manufacturing companies of those devices.  
C. Respondents’ attitudes on the security issues of health 
apps 
Health information should be secured and safe from 
being corrupted and disclosed. This is a vital issue for the 
health-conscious people who are the users of health apps. 
To investigate this issue respondents were asked: “how 
much they are a concern if their health information is 
known by others”. The responses were analysed with cross 
tabulation technique between this security question and 
different categories of respond ents, which revealed same 
results with very little differences. For example, 
respondents who are likely (88.2%) to use health 
application concern (84.1%) about sharing their health 
information. Also, the chi-sq uare analysis between these 
two issues appears as significant (103.68, /g1856/g1858 = 4, /g1868 /g3398
/g1874/g1853/g1864/g1873/g1857 =  0.00) . Therefore, there is a statistically valid 
association between these two issues. 
 
Fig. 2. Consequences due to disclosure of health 
information 
The consequences of disclosure of health information 
could be diverse in nature. These could be the reflection of 
the community culture of the respondents, who participate 
in this study. The current study investigated the concerns of 
the respondents on this by mentioning some specific 
examples of consequences in a question with multi-
selection option. Comparatively more male respondents 
pointed to “physical harm”, “changes to the cost of health 
insurance” and “impact on mental  health” than females. On 
the other hand, females were more reactive on “losing a job”, “losing other income”, “loss of social connection of friend/ colleagues” “waste of  time” and “embarrassment” 
than their counter parts.   
Female respondents reacted to  more consequences due 
to disclosure of health information than their male 
counterparts, which underpin an important conclusion that females were more concerned than male on this issue. The 
frequencies of choosing the consequences from the given 
list were “physical harm” (741, 49.50%), “impact on 
mental health” (727, 48.6%); “embarrassment” (718, 48%), 
“changes to the cost of health insurance” (714, 47.7%, “waste of time” (707, 47.2%) and so on. 
Responses on the security issues also imposed some 
negative questions on the security features of the available health apps. This can be established from the results that 
61% of the respondents who are concerned about their 
health information aware that others do not like to use the 
health applications. Manufacturer s of health apps can also 
consider this message for their future production.  
D. Respondents’ cross-sectional attitudes on both the   
         privacy and security issues of health apps 
The health apps are residi ng on technical devices and 
subject to sudden malfunction due to various reasons. The people who are concerned about the privacy of their health 
information review the security related information of their 
used health apps very frequently. But this current study 
revealed some otherwise behaviours of the health apps 
users. It shows very little or no differences among the 
respondents’ preferences to review the health apps  (‘never 
-33%’, ‘sometimes –34%’ and ‘most times- 31%’) security 
information even they treat the privacy of their health 
information very important.  These results could be the 
reflection of the respondents’ unawareness about the virus 
attack or any software related issues to the health apps.    
V. CONCLUSION 
The concept of mHealth or eH ealth system is still at a 
developing stage despite the rapid awareness of technology in every sector in Bangladesh.  The analysis of collected 
survey data depicts that al most all the users except the 
youth cohorts are concern about the privacy of their health 
information. It could be due to a lack of knowledge of the young generation about the consequences or they are 
technically enough sound to manage their eHealth system 
to protect privacy. One important finding came out from the study on the attitude of female users of the eHealth system 
in Bangladesh. They are more concerned about the 
consequences of disclosing their health information than 
their counterpart. Strong protective social culture or 
religious bindings may be the cause of such kind of attitude of female users.  Finally, we acknowledge that a larger 
sample size would be more useful to understand the 
relationship among other demographics, such as participant 
health status and field of employment, with responses to 
privacy questions.  
  
Authorized licensed use limited to: Carleton University. Downloaded on June 05,2021 at 20:49:18 UTC from IEEE Xplore.  Restrictions apply. REFERENCES 
[1] A. Mense, P. Urbauer, S. Sauermann, and H. Wahl,  
“Simulation environment for testing security and privacy of 
mobile health apps. In: Rozenblit J, Sametinger Journal  
edition” Proceedings of the Modeling and Simulation in Medicine Symposium, Pasadena, California, 2016, pp. 1-5.  
[2] A. Sunyaev, T. Dehling, P. L. Taylor, and K. D. Mandl, 
“Availability and quality of mobile health app privacy 
policies” Journal of the Am erican Medical Informatics 
Association, 2015, vol. 22(e1), pp. e28-e33. 
[3] B. Brumen, M. /g43/g72/g85/g76/g252/g78/g82, A. /g54/g72/g89/g254/g81/g76/g78/g68/g85/g15/g3J. Završnik, and  
M. Hölbl, “Outsourcing Medical Data Analyses: Can 
Technology Overcome Legal, Privacy, and Confidentiality 
Issues?”  Journal of Medical Internet Research, 2013, vol. 
15(12), pp. e283. 
[4] C. Shen, M. P. Wang, J. T. Chu, A. Wan, K. Viswanath, 
S. S. Chan, “Health app possession among smartphone or tablet owners in Hong Kong: population-based survey”, 
JMIR Mhealth Uhealth, Jun 2017, vol. 5(6), pp. e77. 
[5] EU GDPR. GDPR Key Changes. Accessed on 7th 
December 2018 from URL:https://eugdpr.org/. Archived 
by WebCite® at http://www.webcitation.org/74TilQrBM 
on 7th December 2018. 
[6] F. Khatun, SMA. Hanifi, M. Iqbal M, S. Rasheed, M. S. 
Rahman, T. Ahmed, S. Haque, T. Sharmin, N. U. Z. Khan, 
S. S. Mahmood, and D. H. Peters, “Prospects of mHealth 
Services in Bangladesh: Recen t Evidence from Chakaria, 
PLoS one, 2014, vol. 9, pp. e111413. 
[7] Google Trends. mHealth and health privacy trend. 
Accessed on 7th December 2018 from URL: 
https://trends.google.com/trends/explore?geo=AU&q=mh
ealth,Health%20Privacy. Arch ived by WebCite® at 
http://www.webcitation.org/74TfGuaIS on 7th December 
2018.  
[8] H. C. O’Donnell, V. Patel, L. M. Kern, Y. Barrón, P. 
Teixeira, R. Dhopeshwarkar, R. Kaushal, “Healthcare 
Consumers’ Attitudes Towards Physician and Personal Use 
of Health Information Exchange.” Journal of General 
Internal Medicine, 2011, vol. 26(9), pp. 1019 -26. 
[9] H. M. Jennings, J. Morrison, K. Akter, A. Kuddus, N. 
Ahmed, S. K. Shaha, T. Nahar, H. Haghparast-Bidgoli, AK. 
A. Khan, K. Azad & E. Fottrell, “Developing a theory-
driven contextually relevant mHealth intervention.” Global Health Action, 2019, vol. 12(1).  
[10] H. Ratnayake, “Privacy risk assessment in relation to 
health and wellbeing apps: Towards harmonizing the 
European laws, industry interests and the privacy 
expectations of the users. (Master's thesis), University of 
Oslo, Norway 2015 
[11] M. McCarthy, “Federal privacy rules offer scant 
protection for users of health apps and wearable devices” 
British Medical Journal, 2016, vol. 354, pp. i4115. [12] N. F. BinDhim, L. Trevena, “Health-related 
smartphone apps: regulations, safety, privacy and quality” 
BMJ Innovations, 2015, vol. 1(2), pp. 43-45. 
[13] P. Krebs, D. T. Duncan, “Health app use among US 
mobile phone owners: a national survey” JMIR Mhealth 
Uhealth, 2015, vol. 3(4), pp. e101. 
[14] P. Rasche, M. Wille, C. Bröhl, S. Theis, K. Schäfer, 
M. Knobe, and A. Mertens, “Prevalence of Health App Use 
Among Older Adults in Germany: National Survey” JMIR 
MHealth and UHealth, 2018, vol. 6(1), pp. e26. 
[15] R. Hoque, and G. Sorwar, “Understanding factors 
influencing the adoption of mHealth by the elderly: An 
extension of the UTAUT model.” International journal of 
medical informatics, 2017, vol. 101, pp.75-84.  
[16] S. R. Blenner, M. Köllmer, A. J. Rouse, N. Daneshvar, 
C. Williams, and L. B. Andrews, “Privacy policies of 
android diabetes apps and sharing of health information” 
JAMA, 2016, vol. 315(10), pp. 1051-1052. 
[17] S. Safavi, Z. Shukur. “C onceptual Privacy Framework 
for Health Information on Wearable Device” PLOS ONE 
2014; vol. 9(12): pp. e114306. 
[18] Statista. mHealth (mobile health) industry market size 
projection from 2012 to 2020 (in billion U.S. dollars). 
Accessed on 9th October 2017 from URL: 
https://www.statista.com/statistics/628190/global-
mhealth-devices-and-servi ces-revenue-worldwide/. 
Archived by WebCite® at   
 http://www.webcitation.org/74TfahBVm on 7th 
December 2018.  
[19] T. Dehling, F. Gao, S. Schneider, A. Sunyaev, 
“Exploring the far side of mobile health: information 
security and privacy of mob ile health apps on iOS and 
Android”, JMIR Mhealth Uhealth, 2015, vol. 3, pp. e8. 
[20] T. Le, H. Thompson, and G. Demiris, “An examination 
of electronic health information privacy in older adults” 
Studies in Health Technology and Informatics, 2013, vol.  
192, pp. 709-713. 
[21] W. Dong, Z. Xingting, and L. Jianbo, “Consumers’ 
perceived attitudes to wearable devices in health monitoring in China” A survey study, Computer Methods 
and Programs in Biomedicine, 2017, vol. 140, pp. 131-137. 
[22] Z. Xie, A. Nacioglu, C. Or, “Prevalence, Demographic 
Correlates, and Perceived Imp acts of Mobile Health App 
Use Amongst Chinese Adults: Cross-Sectional Survey 
Study”, JMIR mHealth and uHealth, 2018, vol. 6(4). 
 
Authorized licensed use limited to: Carleton University. Downloaded on June 05,2021 at 20:49:18 UTC from IEEE Xplore.  Restrictions apply. "
DataPrivacy,3005.txt," 
Volume 1 4, 2018 
Accepted by Editor Fay Sudweeks │Received: March 10, 2018│ Revised: March 11, March 21, 2018 │ Accep t-
ed: March 23, 2018.  
Cite as: Tshiani, V ., & Tanner, M. (2018). South Africa’s quest for smart cities: Privacy concerns of  digital n a-
tives of  Cape Town, So uth Africa.  Interdisciplinary Journal of  e -Skills and Lifelong Learning , 14, 55-76. 
https://doi.org/10.28945/3992  
(CC BY -NC 4.0 ) This article is licensed to you under a Creative Commons Attribution -NonCommercial 4.0 International 
License . When you copy and redistribute this paper in full or in part, you need to provide proper attribution to it to ensure 
that others can later locate this work (and to ensure that others do not accus e you of plagiarism). You may (and we encou r-
age you to) adapt, remix, transform, and build upon the material for any non -commercial purposes. This license does not 
permit you to use this material for commercial purposes.  
SOUTH AFRICA ’S QUEST FOR SMART CITIES :  
PRIVACY CONCERNS OF DIGITAL NATIVES  
OF CAPE TOWN , SOUTH AFRICA  
Valerie Tshiani  University of  Cape Town, Cape Town, 
South Africa  TSHVAL003@myuct.ac.za   
Maureen Tanner * University of  Cape Town,  Cape Town, 
South Africa   mc.tanner@uct.ac.za   
* Corresponding author  
ABSTRACT  
Contribution  This study contributes to scientific literature by detailing the impact of  specific 
factors on the privacy concerns of  citizens living in an African city . 
Findings  The findings reveal that the more that impersonal data is collected by the Smart 
City of  Cape Town, the lower the privacy concerns of  the digital natives. The 
findings also show that the digital natives have higher privacy concerns when 
they express a strong need to be aware of  the  security mea sure put in place by 
the city.  
Recommendations  
for Practitioners  Practitioners (i.e. , policy makers) should ensure that it is a legal requirement to 
have security measures in place to protect the privacy of  the citizens while co l-
lecting data within the s mart city  of  Cape Town. These regulations should be 
made public to appease any apprehensions from its citizens towards smart city 
implementations. Less personal data should also be collected on the citizens.  
Recommendation  
for Researchers  Researchers should further investigate issues related to privacy concerns in the 
context of  African developing countries . Such is the case since the population 
of  these countries might have unique cultural and philosophical perspectives that might influence how they perceive privacy.  
Impact on Society  Cities are becoming “smarter ” and, in the context of  developing countries , pri-
vacy issues might not be such a major concern  as is the case in the developing 
world.  
Future Research  Further qualitative studies should be conducted to better understand issues r e-
lated to perceived benefits, perceived control, awareness of  how data is collec t-
ed, and level of  privacy concerns of  digital natives in developing countries.  Smart Cities and Privacy Concerns  
56 Keywords  smart cities, privacy concerns, digital natives,  developing countries , South Africa  
INTRODUCTION  
Several cities around the world are moving towards acquiring the “smart city ” status and several Afr i-
can cities are j oining this quest  (Anthopoulos &  Fitsilis, 2013; Watson, 2015). A smart city is a city 
that incor porates the use of  Information and Communication T echnologies (ICT) and the Internet 
of  Things  (IoT)  in the management and monitoring of  city resources ( J. Lee, Hancock , & Hu, 2014). 
Cities such as Vienna and Barcelona are establishing themselves as world leaders in the smart city 
movement ( March & Ribera -Fumaz, 2016 ; Schleicher, Vogler, Inzinger , & Dustdar, 2015 ).  
The concept of  smart cities in the African context is new and still needs more exp loration (Chourabi  
et al., 2012; Nfuka & Rusu, 2010) . Moreover,  smart city implementations in African cities are still 
lagging behind  those of  European cities (Watson, 2015). Nonetheless , many African cities such as 
Cape Town in South Africa and Nairobi in Kenya have implemented smart city projects such as free 
Wi-Fi in public places and cashless payment systems for public transport (Albino,  Berardi , & Da n-
gelico, 2015; J. Lee  et al., 2014). In addition, m any of  the smart city services that can be found in self -
described European smart cities such as Barcelon a can also be found in Cape Town (Volkwyn, 2017).  
Data is constantly being produced and consumed in smart cities (Kitchin, 2015). Data from different 
sources are integrated together, with the aim of  acquiring a full picture of  the city ’s status in terms of  
safety and  economic vitality and in terms of  resources usage (March &  Ribera -Fumaz ., 2016). A s a 
result, generating and using this data often raises issues around  data security and data privacy (Li, 
2012).  
RESEARCH PROBLEM  
With this current period being coined as ‘the digital age ’, it is not surprising that one of  the challen g-
es of  our time is privacy (Acquisti, Taylor , & Wagman, 2016). In today ’s society, the threat of  perso n-
al data being abused for financial gain, social discriminat ion, or coercion is real (Edwards , Hofmeyr , 
& Forrest, 2016). In the past , there have been several instances where people ’s personal information 
was collected by government agencies without their awareness . An example of  such an incident was 
when it was re vealed  by WikiLeaks that a United States government agency called the National Sec u-
rity Agency (NSA) collected personal information such as phone records, emails, bank tra nsactions, 
travel records, and I nternet searche s of  millions of  people (N. Lee, 2015). It cannot be as sumed that 
these highly publicis ed incidents of  people ’s information being used without their awareness  and 
consent went unnoticed by digital natives in Cape Town, South Africa.  
Smart city projects have been known to bring benefits to  cities such as sustainable economic deve l-
opment (Castro, Jara , & Skarmeta, 2013). H owever , one may wonder how certain factors influence 
the privacy concerns that come along with the implementation of smart cities , particularly in the A f-
rican context. In a  time when information can be easily transferred, accessed ,and even shared, it is 
no surprise that people may have inclinations to be very protective of  their personal information 
(Elhai, Levine , & Hall, 2017).  
RESEARCH PURPOSE  
Many studies have  been done  on smart cities in Europe  and in emerging economies like the Far East. 
However, Backhouse (2015) states that “little has been done to understand how this concept is pla y-
ing out on the African continent, although many African cities are pursuing smart city  agendas ” 
(p. 1). The objective of  this study is , therefore , to investigate the impact of  awareness, perceived be n-
efits, types of  collected  data, and perceived control on the privacy concerns of  digital natives living in 
the smart city of  Cape Town, South Africa.  Tshiani & Tanner  
57 In line with the objective of  the study, t he research question is:  
What is the impact of  need for awareness, awareness of data collection method, perceived 
benefits, types of data collected , and perceived control on the privacy concerns of digital 
natives living in the smart city of  Cape Town, South Africa?  
The study specifically investigated the privacy concerns of  the citizen s (foreign & local)  of  Cape 
Town born between the years of  the late 1980s to mid -1990s . While the privacy concerns of  non -
citizens (i.e., visitors) are also relevant, the study specifically focuses on citizens  as they are the ones 
who reside in the city on a long term basis. This was investigated using a quantitative ap proach.  
IMPORTANCE OF THE STUDY  
It is important to study the implementation of  smart cities in Africa as research shows that by 2050 
almost 70% of  the world’ s people will live in cities (Shanahan  et al. , 2017). Law makers of  African 
cities need to find efficient ways to accommodate all these people and encourage them to use smart 
cities technolog ies. The purpose of  this is to reduce the likelihood  of  African cities investing massive 
amounts of  money into a smart city projects that may end up eventually f ailing (Backhouse, 2015).  
LITERATURE REVIEW  
This section examines the current literature relating to the characteristics and implementations  of  
smart cities  in Cape Town, South Africa  and the privacy concerns associated with smart cities.  
CH ARACTERISTICS  OF SMART CITIES  
Cities around the world are attempting to transform into smart cities, in order to be more economi-cally competitive and promote sustainable growth (Roche, Nabian, Kloeckl, & Ratti, 2012) . Accor d-
ing to past studies, the most common characte ristics of  Smart Cities are “Smart ” Governance, 
“Smart ” Environment, “Smart ” Living Environment, and a “Smart ” Economy.  
• “Smart ” Governance  relates to the use of  digital technology in public and government or-
ganisations as well as  in social services  (Rosati & Conti, 2016) . Having an efficient and inte l-
ligent transportation systems is what makes a city  ‘smart ’ in relation to governance  (Kond e-
pud et al., 2016) . An example of  a city ’s smart governance initiative is the usage of  Infor-
mation Technology ( IT) (e.g., Smart LEDs)  to monitor and lower  the Energy consumption 
within  the city (Chourabi et al., 2012) .  
• “Smart ” Environment relates to  sustainable resource management through the use of  ICT 
(Caragliu, Nijkamp , & Del Bo, 2011) . A city with a “smart ” environment make s use of big 
data, IoT , and various other technologies in the ru nning and planning of  the city ’s infr a-
structure and  during  the provision  of  city services  (Chourabi  et al., 2012) . 
• A “Smart ” Living E nvironment provides benefits for the peopl e living in the city such as 
free public Wifi, access to e - health, access to smart building services and access to e -
education solutions  (Kondepud et al., 2016) .  
• A “Smart ” Economy promotes the use of  electronic business processes in the cit y such as 
e-banking, e -shopping , and e-auction  (Kondepud et al., 2016) .  
SMART CITY IMPLEMENTATION IN CAPE TOWN  
Cape Town has often declared that it aims to be a more competitive city with rapid economic growth 
and ec onomic development (Anthopoulos  Fitsilis , 2013; Maumbe, Owei, & Alexander, 2008). Urban 
development is a priority in Cape Town, as the city continuously receives flocks of  South Africans from rural areas and non -south Africans looking for employment and accommodation (Odendaal, Smart Cities and Privacy Concerns  
58 2006). E-governme nt is one the main focus points of  Cape Town ’s smart city strategies (Lourie, 
2017). Another focus area in Cape Town ’s smart city agenda is to provide social and economic deve l-
opment to its citizens by improving ICT skills (Lourie, 2017).  
Cape Town has started to use more technology in its day -to-day management (Volkwyn, 2017). In 
particular, smart metering is being used for electricity and water in 65% of  the city ’s large  administr a-
tive buildings (Baud , Scott, Pfeffer, Sydenstricker -Neto, & Denis , 2015) . In the past, Cape Town has 
also introduced smart city projects such as the SMART Cape Access (Khati, 2013). The aim  of  the 
SMART Cape Access Projects was to provide Cape Town citizens with free access to technology 
(Valentine, 2004). The project was star ted in July 2002 and resulted in the installation of  36 Internet 
enabled computers in six public libraries in poorer  areas across the city (Valentine, 2004).  
More recently, Cape Town city management has rolled out public Wi -Fi in many areas around the cit y 
and actively endeavoured  to improve the city ’s broadband infrastructure  in order to reduce  the d igital 
divide (Volkwyn, 2017). According to Volkwyn (2017), in order to m ake Cape Town “Smarter ” there 
has also been an increase in the usa ge of  Closed -Circuit Television ( CCTV ) cameras in the city. In 
particular, city management has installed 560 cameras in and around the city in order to make the city 
safer (Volkwyn, 2017).  
Reports on smart city projects in Cape Town often  omit  information on privacy concerns of  the cit i-
zens (Smit, Makanga, Lance , & de Vries, 2009). One possible reason for this omission is that smart 
city initiatives often overshadow the privacy concerns that these projects may bring about (van Z o-
onen, 2016).  
PR IVACY CONCERNS IN SMART CITIES  
In a city that relies on smart technologies, everyday activities performed by people  leave trails of data 
on their interests, h abits, and intentions (Acquisti et al., 2016). Research shows that people around 
the world are unknowingly constantly  revealing information about themselves to commercial entities, 
governments , and sometimes to hackers  (Acquisti  et al., 2016). However, studies show that what data 
is deemed as private and sensitive varies from person to person, based on one ’s own cultural and s o-
cietal def inition of  privacy (Alashoor, Aryal , & Kenny,  2016; Taddicken, 2014).  
When individuals have higher privacy concerns , they will more likely want to protect their data and 
are less likely to disclose their information to entities that they do not trust (Beuker, 2016). Nonethe-
less, several research findings also show that , despite the possible privacy concerns in smart cities , 
these are often overshadowed by the possible benefits of the data collected (van Zoonen, 2016). 
These findings are in line with the “The Price of  C onvenience (PoC)”  theory (Ng -Kruelle,  Swatman, 
Rebne , & Hampe,  2002) which states that most people are w illing to forgo their privacy concerns and 
privacy rights for the conveniences received in return (Hann, Hui, Lee , & Png , 2007).  
PR IVACY CONCERNS IN SOUTH AFRICA  
Threats related to the leakage of  personal information of  citizens in South Africa may not be com-
mon but still occur. Olinger , Britz, and Olivier  (2007) write that , in June 2004, it was reported by se v-
eral South African newspapers that “the South African Post office would sell the personal infor-
mation of  the registered citizens contained in its National Address Database (NAD) ” (p. 32). The 
personal information compromised of  data such as the individual ’s name, identity number, home 
address, and telephone numbers that were taken from the Ministry of  Home affairs in South Africa 
(Olinger et al., 2007). Even though there are legal protections in South Africa against the misuse of 
personal information by someone else, illegal interceptions of  electronic communication by v arious 
people and institutions in South Africa still occur (Harris , Goodman, & Traynor , 2013). In order to 
specifically look at the privacy concerns of  citizens in an African city in relation to smart city impl e-
mentation, Backhouse (2015) states that futur e empirical studies are needed to unpack the various 
nuances of  smart city agendas in the African context.   Tshiani & Tanner  
59 PR IVACY CONCERNS IN AFRICA  
Research has shown that the widely acknowledged and practiced African philosophy of  Ubuntu has 
greatly influenced the pe rception s around  privacy in Africa in terms of  values and social thinking 
(Anteneh, Belanger, Borena , & Ejigu, 2015) . The philosophy identifies “personhood ”, which is the 
rights afforded to a person, as being determined not only by the individual person bu t largely by the 
community he or she lives in (Muyia & Nafukho, 2017) . Ubuntu values  initiatives and actions that 
will collectively benefit the community even more than the rights of  the individual  (Makulilo, 2015) . 
In Africa , it can thus be perceived that privacy is determined by the collective society as opposed to 
being determined by one ’s own personal beliefs  (Borena, Belanger , & Egigu, 2015) , which is in co n-
tradiction with current western values when it comes to the usage o f  technologies (Borena et al., 
2015) . It is said that the concept of  privacy and the concept of  an individual being self -autonomous 
is a western concept that has no roots in the African culture (Makulilo, 2015) . 
By virtue of  the fact that it is a generally accepted theory that Africans live in and have a collectivist culture (Harris  et al. , 2013) , it is also theoris ed that Africans put no value on digital privacy (Olinger  
et al., 2007) . However, Makulilo (2015 ) notes that this assumption is not necessarily true , and it does 
not mean that because of the collectivist culture in Africa, there is a lack of understanding and value of  privacy in Africa n countries. Makulilo (2015) hypothesis ed that the desire for African countries to 
engage in the global e -Commerce market, will force many African governments and people to rec on-
sider their ideas of  privacy.  
THE MULTIDIMENSIONALITY OF PR IVACY CONCERNS IN SMART CITIES  
Research shows that people ’s informational privacy conce rns are affected by not only personal ps y-
chological factors, bu t also external influences (Hsu  & Shih, 2009). Consensus in exi sting research 
indicates that there are four main factors that influence informational privacy concerns . These are 
Perceived Benef its, Type of  Data Collected, Awareness , and Perceived Control.  
• Perceived Benefits: Research shows that , for most individuals, if  they perceive more benefits 
than risks, they are then willing to accept the risks and thereupon disclose informa tion. 
Beuker (2016) and Barth and  de Jong (2017) state that , even though people may claim to va l-
ue their informational privacy , in truth , once they estimate the benefits from the information 
disclosure, personal information can then be traded. In line with the t heory o f “The Price of  
Convenience (PoC) ”, it is often found that the calculated value of  the benefits often ou t-
weighs the estimated costs of  infor mation disclosure (Culnan & Bies , 2003; Olivero & Lunt , 
2004).  
• Type of  Data Collected: According to research, privac y concerns are often determined by the 
type of  information that is being disclosed. Van Zoonen (2016) states that most people co n-
sider certain data type s as more personal than others, and what people deem as personal i n-
formation is not always consistent and is dependent on individuals ’ own definition. Genera l-
ly speaking, the types of  information can be categorised in two groups : personal information 
and impersonal information (van Zoonen, 2016). Impersonal information includes data such as gender, languages  spoken , and home town while personal information contains data such 
as profile pictures, emails, phone numbers , and personal views/preferences (Beuker, 2016).  
Research shows that many people believe and fear  that snippets of  personal information can 
easily be combined into highly personal consumer profiles (Harri s et al., 20 13; Tene & Jules , 
2013). Impersonal data , on the other ha nd, is likely to illicit low levels of  privacy concerns as 
this data tends to reveal nothing about individual people (van Zoone n, 2016).  
• Awareness: Uncertainties on how  personal information is collected, used , and shared result in 
high levels of  privacy concerns (Beldad, 2011). An individual’ s decisions of  whether or not 
to share personal data in a particular environment often depends on an awareness and eval u-
ation of  the information supplied about the privacy policies implemented in that particular Smart Cities and Privacy Concerns  
60 environment (Beldad, 2011). According to Dinev and Hart (2014), most people need to have 
an awareness of  the risks involved before making a decision.  
Uncertainties often stem from environments that are ambi guous and complex (Dinev & 
Hart, 2014). Research shows that peoples ’ uncertainties regarding the usage and the pr o-
cessing of  their personal information often triggers feeling of  information privacy viol a-
tions, which could lead them to a disengage from the environment or cause disruptions in 
the environment (Wel tevrede , 2011). Being aware of  the me thods and technologies used in 
the data collection of  one ’s personal information is also another factor that influences priv a-
cy concerns (van Zoonen, 2016).  
• Perceived Control: According to Beldad (2011), most people tend to favour the ability to filter 
the flow of  their personal data regardless of the environment they are in. Beldad (2011) fu r-
ther theorises that when people have control of  how their personal information is  collected, 
used and shared, this  lower s their privacy concerns. Several studies  have identified that an i n-
dividual having a perception of  control over their personal  data is an important factor in e n-
couraging people to partake in smart city initiatives (Stewart  & Segars , 2002). Control over 
one’s personal information can be exercised in tw o ways : before and after the information is 
disclosed (Beldad, 2011). Moreover, c ontrol over one ’s personal information can be achieved 
through various means such as choice, consent, access to the data , and the ability to provide 
correct ion of  the data (Be ldad, 2011).  
DIGITAL NATIVES IN CAPE TOWN  
Research shows that in the western world it is often perceived that because digital natives grew up with technology, the way they think, behave , and act is shaped by digital technologies (Gu, Z hu, & 
Zuo 2013; Hoffmann , Lutz, & Meckel,  2014). Identifying digital natives in South Africa and other 
developing countries is not as easy as it is in the western world due to vast wealth gaps in many de-
veloping countries (Thinyane, 2010). A digital native in South  Africa was born in a time where there 
was an increase in the usage of  technologies in the world, but because of  their economic background 
and racial discrimination, they might not exhibit the same technological proficiencies and inclinations 
as those of  digital natives in western world (Thinyane, 2010). The legacy of  Apartheid in South Afr i-
ca also had an impact on the uptake of  technology by people who fit in the age group of  the world ’s 
digital natives (Thinyane, 2010). Many of South Africa ’s digital nati ves still do not have access to the 
technologies that were made popu lar when they were born (Maumbe  et al., 2008). Understanding the 
patterns of access and use of  technology by South African digital natives is essential in to hypothe-sise and u nderstand  their privacy concerns (Thinyane, 2010).  
Research shows that age influences the behaviour  and perceptions of  a user of  technology. Brown 
and Czerniewicz (2010) argue that this view is not true in the South African context. Brown and 
Czerniewicz (2010, p.48) agree with Thinyane (2010) in arguing that in South Africa “age is not a 
determining factor in students ’ digital lives; rather, their familiarity (awareness) and experience (exp o-
sure) using ICTs is more relevant ”. In South Africa, those who demonstrate dig ital native attribut es 
are an elite minority (Brown  & Czerniewicz, 2010). The digital native population in South Africa co n-
sists of  a varied group that has a wide variety of  information technology  competences (Thinyane, 
2010). However , research shows that although in the African region only one in ten young people 
may be digital natives, it is these young people who are often their nation ’s drivers in terms of using 
the Internet and technologies (International Telecommunication Union, 2013).  
SUMMARY OF THE LITERATURE  
The literature review found that there is a growing trend of  incorporating smart city initiatives in cities all around the world. Although smart city initiatives have the potential to bring various benefits, 
literature shows that  these benefit can sometimes be overshadowed by security and privacy concerns. Tshiani & Tanner  
61 In the African context, these privacy and security concerns differ from those of  the western world 
due to differences in culture, economic capabilities , and access to technologi es. Cape Town is cur-
rently involved in several  smart city projects . Therefore,  in order to encourage  participation in these 
smart city initiatives , privacy concerns need to be investigated .  
THEORETICAL FRAMEWORK  & HYPOTHESES  
THEORETICAL FRAMEWORK AND CONS TRUCTS  
As discussed in the previous section, the main factors that might influence the privacy concerns of  people living in smart cities are Perceived Control, Awareness, Perceived Benefits , and Type of  Data 
Collected. The awareness construct was split in two so as to separately investigate the citizens ’ need 
for awareness of  practices and policies and their actual awareness of  methods and technologies used to collect data. A summary of  these factors are provided in Table 1.  
Table 1. Factors impacting privacy concerns in smart cities  
Constructs  Description  
Perceived Control  The extent to which citizens perceive that they have control over how their 
personal information is collected and if  they have an option to opt- out of  
the data collection process (Stewart & Segars, 2002) . 
Awareness  of  data 
collection  This construct relates to an awareness of  the methods and technologies that 
are used to gather/collect data in a smart city (Stewart & Segars, 2002) . 
Need for awar e-
ness The extent to which citizens need to be aware of  smart city ’s privacy pra c-
tices and polices – in terms of  rules determining access of  the data ( Beldad, 
2011). 
Perceived benefits  The perceived benefits and conveniences received in return for granting 
access to personal data  (Ng-Kruelle et al., 2002) . 
Type of  data co l-
lected  The perceived nature of  the data that is collected  (Ng-Kruelle et al. , 2002).  
 
The framework that was used in this study is shown in Figure 1. The dependent variable is the levels 
of  privacy concerns of  citizens living in the smart city of  Cape Town. The independent variable s 
were summaris ed in Table 1.  
 
                                                                                  
 
 
 
  
 
 
 
  
Figure 1: Theoretical Framework  Perceived Control  (H3)  Levels of  Privacy Concerns  
of  digital natives in the 
smart city of  Cape Town  
 Perceived Benefits  (H1)  
Awareness  of  data collection 
(H4)  Type of  data collected (H2)  
Need for Awareness  (H5)  Smart Cities and Privacy Concerns  
62 HYPOTHESIS  
The hypotheses derived for the study are:  
H1: The more digital natives perceive to receive benefits from providing personal data to the smart 
city of  Cape Town, the lower their privacy concerns.  
H2: The more impersonal  the data being collected by the smart city of  Cape Town  is, the lowe r the 
privacy concerns of  the digital natives.  
H3: The more digital natives perceive to have control on the data collected about them by the smart 
city of  Cape Town, the lower their privacy concerns  
H4: The more the digital natives are aware of  how their personal data is collected by the smart city 
of  Cape Town, the lower their privacy concerns  
H5: The higher the need to  be aware of  the security measures put in place during the collection and 
storage of  their personal information by the smart ci ty of  Cape Town, the higher the  privacy co n-
cerns of  the digital natives  
RESEARCH METHODOLOGY  
RESEARCH APPROACH  AND PHILOSOPHY  
The study adopted a positivist philosophy as the data to be collected is objectively observable and 
quantifiable. In particular, the study involves finding a cause and effect type of  relation . A deductive 
approach to theory was also chosen as positivists and quantitative studies are usually deductive in  
nature ( Greener & Martelli, 2014). The research strategy was quantitative and involved the  collection 
of  numerical data for the purpose of  getting precise measurements which can then be converted into useable statistics (Bhattacherjee, 2012).  
Since the aim of  this study was to identify the current privacy concerns of  a certai n group of  people 
in Cape Town, a cross - sectional approach was best suited as it provides a present -day “ snapsh ot” of  
a point of  view ( Greener & Martelli, 2014 ). 
RESEARCH METHODS  
Instrument  
An online survey questionnaire of  closed questions  was the chosen research instrume nt in this study. 
An online questionnaire was chosen as it provided anonymous access to groups and individuals who would otherwise be difficult to reach in a short amount of  time and in large numbers through other channels. Online surveys also kept the cos ts for the study to a minimum (Wright, 2005).  
In addition to demographics questions, the survey consisted of  questions that correspond to each of  
the constructs in the research framework , namely , Perceived benefits, Type of  data collected, Pe r-
ceived Contro l, Awareness  of  Data Collection, Need for Awareness , and Levels of  Privacy Concerns.  
A Likert scale was used to allow the respondents to indicate their opinions, attitudes , or feeling s 
about a particular issue. A Likert scale with an odd number of  options was used. In particular, a 5 
point Likert scale of  1 =Strongly disagree, 2 =Disagree, 3 = Both Agree and Disagree, 4 =Agree,  
5 = Strongly agree was used.   
In order to protect the identity and views of the resear ch population used, all surveys responses were 
anonymous. Importance was put in maintaining the confidentiality of  the data collected and pr o-
duced in this research. The codes of ethics prescribed by the University of  Cape Town were followed 
while researchi ng was being done.  Tshiani & Tanner  
63 The online survey questionnaire was sent with a brief  introduction note, stating the purpose of  the 
study and a brief  description of  the study. The introductory note stated that participation in the r e-
search was voluntary and participants could opt out of  the survey questionnaire at any time.  
Participant r ecruitment  
In developed countries, people born in the late 1980s and early 1990s are considered to be digital natives as they are presumed to have grown up in the age of  technology (Boy d, 2013). However, due 
to large wealth inequalities within the South African population, there exist differences in exposure to 
technologies wi thin this age group (Triegaardt, 2009). Hence, the research population of  this study 
has been limited to Cape Tow n citizens born in the late 1980s and early 1990s but who also grew up 
with an exposure and awareness of  the popular technologies. Participants were allowed to partake in the survey if  their response to the following question was “yes”: “Did you ha ve acces s to a computer 
and/or I nternet during your primary school years (access of  this can come from anywhere and is not 
limited to home access)? ”  
A combination of  non- probability convenience sampling and Judgmental sampling (purposive sa m-
pling) approach was us ed. Convenience sampling selects cases based on their availability and willin g-
ness to participate  in the study ( Palinkas  et al., 2015). A Judgment sampling approach was used as the 
researchers had to rely on logic and judgment to pick  the sample population ( Palinkas  et al., 2015). 
The sample size was that of  100 responses.  
The survey questionnaire was distributed using the online survey tool Qualtrics. The link to the Qua l-
trics questionnaire was sent via email and on social media in order to target the required population. The emails were sent to mostly university students i n the Cape Town as it can be assumed that a large 
number of  digital natives can be found on a university campus. The universities were randomly ch o-
sen. 
Data analysis methods  
Once the data gathering period was concluded the data was extracted from Qualtrics  and imported 
into the statistical analysis software Statistica for basic statistics tests that summarised and described 
the characteristics of  the data gathered. Before being imported, the data was cle aned and coded in 
Excel in order to ensure it was free  of  errors and  was analys able. 
Reliability and item analysis was don e using Cronbach’ s alpha tests.  The relationships between the 
independent and dependent variables were found using Spearman Rank Order correlation analysis 
and multiple regression analysis tests.  
DATA ANALYSIS  
The findings are presented in this section . First,  the reliability  and consi stency of  the model was  test-
ed through the C ronbach’ s Alpha test. The data wa s then tested for normality . A d escriptive tests 
analysis of  the data was also conducted . The hypotheses were tested using correlation and multiple 
regression tests based on the n ormality of the dat a. Correlation analysis measured  the strength of  the 
relationships between variab les whereas regression estimated  what the relati onship wa s (Saunders, 
Lewis, & Thornhill, 2009).  
RELIABILITY TESTING  
Cronbach’ s alpha tests were performed for each of  the constructs (i.e. , Perceived Benefits, Types of  
Data Collected, Awareness and Perceived Control) and were used to measure the internal consistency 
of  responses pertaining to each construct (Bhattacherjee, 2012).  A Cronbach’ s alpha of  at leas t 0.6 is 
normally required (Bhattacherjee, 2012). Initial results of the Cronbach ’s Alpha test are shown in 
Table 2.  Smart Cities and Privacy Concerns  
64 Table  2. Cronbach ’s Alpha Reliability Test Results  
Construct  Cronbach ’s alpha  
Perceived Benefits  0.45 
Awareness  0.81 
Types of  data collected  -0.46 
Perceived Control  0.72 
 
As can be seen in Table 2, the Cronbach’ s Alpha values for the Awareness and Perceived Control 
constructs were acceptable (i.e. , > 0.6). This was not the case for the Perceived Benefits and Types of  
Data Collected constructs. Further investigations into the results for the Perceived Benefits co n-
struct, showed that the second question on benefits  (I consider better distribution of  city resources such as 
water, and electricity a benefit of  living in a smart city ) had to be omitted so that the Cronbach’ s alpha for 
that cons truct could increase to 0.58  (approx. 0.6).  
The negative Cronbach’ s Alpha result for the Types of  Data Collected construct was due to having 
two similar questions which were inversely formulated:  
1. I do not mind it when Cape Town uses data that specifically identifies me in the running of  the city,  
and  
2. I do not mind it when Cape Town uses data about me if  it does not specifically identify me (impersonal da-ta/unidentifiable data).  
The first question was omitted resulting in a Cro nbach ’s Alpha result of  0.6  for the construct.  
An overall instrument ’s reliability was then tested to assure the reliability and consistency of  the i n-
strument as a whole and produce d a Cronbach ’s Alpha of  0.9  and a sta ndardis ed alpha value of  0.9 . 
Table 3 shows the final results of  the Cronbach ’s Alpha tests performed for each construct and for 
the overall instrument’ s reliability after relevant questions were removed from the model.  
Table 3 . Fina l Cronbach ’s Alpha Reliability T est Results  
Construct  Cronbach ’s alpha  
Perceived Benefits  0.6 
Awareness  0.9 
Types of  data collected  0.6 
Perceived Control  0.7 
Overall instrument ’s reliability  0.8 
TEST FOR CONSTRUCT NORMALITY AND DISTRIBUTION  
Each construct was tested for normality using the Shapiro- Wilk’s (SW  ) and Kolmogorov -Smirnov. 
With a p – value < 0.001, the SW test is stat istically significant for the test items in the constructs. 
Consequently, all the constructs are non -normally distributed.  
DESCRIPTIVE TESTS  
63% of  the participants were female, 34% were male , and 3% chose not to disclose their gender. In 
addition, t he par ticipants consisted of  digital natives from Cape Town born between 1985 and 1995. 
Most of  the participants (77%) were b orn between 1991 and 1995 and 23 % between 1985 and 1990.  Tshiani & Tanner  
65 HYPOTHESES TESTING  
Since the objective of  this study is to assess the relationship between the independent variables Pe r-
ceived Benefits, Types of  Data Collected, Awareness , and Perceived Control on the dependent vari a-
ble (i.e. , Level of  Privacy Concerns), Spearman correlation tests were first conducted. The value o b-
tained fr om the Spearman correlation test is called the correlation coefficient (Saunders , Lewis,  & 
Thornhill , 2009). A positive correlation coefficient indicates a positive relationship between the two 
variables, while negative correlation coefficients indicate a negative relationship (Saunders et al., 
2009). The closer the Spearman value is to ±1 the stronger the monotonic relationship between the 
variables  (Saunders  et al., 2009). The Spearman correlation test is useful for this study as it can be 
used on data th at comes from Likert Scale variables and can be used for data t hat is non -parametric 
(Saunders  et al., 2009).  
Multiple regression analysis was also employed. Multiple regression analysis was used to determine 
the probability of  the relationship between each individual independent variables and the dependent 
variable “Levels of  Privacy Concerns ” occurring by chance (Sa unders et al. , 2009).  
H1: The more digital natives perceive to receive benefits from providing personal 
data to the smart city of  Cape Town, the lower their privacy concerns.  
As can be seen in Table 4 , no support can be found for Hypothesis 1 as there is no statistically signi f-
icant negative correlation between the Level of  P rivacy Concerns of  Cape Town smart citizens and 
the two of  the questions related to Perceived Benefits (Q3: 0.43, Q4: 0.38 ). Only Question 1 on Pe r-
ceived Benefit has a negative correlation with Level of  Privacy Concern (Q1: -0.23). 
Table 4 . Spearman ’s Rank  Order Correlations: Perceived benefits and  
Levels of  Privacy Concerns  
 Spearman Rank Order Correlations   
MD pairwise deleted  
Marked correlations are significant at  
p <.05000  
Variable  Benefit Q1  Benefit Q 3 Benefit Q4  
Level of  Privacy Concern  - 0.23 0.43 0.38 
 
Moreover, the results of  multiple regression analysis r evealed an R value of  0.50 . The degree of  co r-
relation is therefore moderate , and only 25.5% of  the total variation in the dependent variable Levels 
of  Privacy Concerns can be explained by the independent variable Perceived Benefits. Hence, the 
study indicates that Perceived B enefits does not significantly influence the levels of  p rivacy concerns 
of  digital natives living in the smart city of  Cape Town.  
This result is not in line with the findings of  other studies such as those in the van Zoonen (2016) 
study on informational privacy concerns. These results are also in contrast to the theory concepts of  
“Price of  Convenience (POC) ” talked about in the literature review of this paper.   
H2: The more impersonal  the data being co llected by the smart City of  Cape Town, 
the lower the privacy concerns of  the digital natives  
As can be seen i n Table 5 , there is a signification negative correlation between the Type of  Data Co l-
lected and the Level of  Privacy Concerns in suppo rt of  Hypothesis 2(Q1: -0.41) at 95% confidence 
level.  
 
 
 Smart Cities and Privacy Concerns  
66 Table 5 . Spearman ’s Rank Order Correlations:  
Type of  data collected and Levels of  Privacy Concerns  
 Spearman Rank Order Correlations   
MD pairwise deleted  
Marked correlations are significant at p <.05000  
Variable  Impersonal Data Q2  
Level of  Privacy Concern  -0.41 
 
The results of  the regression analysis also revealed that the more impersonal data is collected by the 
Smart City of  Cape Town, the lower the privacy concerns of  the digital na tives. An R value of  -0.39 
was obtained indicating that the association is inversely correlated. The “types of  data collecte d” 
construct shows a statistically significant correlation of  a confidence level of  0.00007 (>99%).  
The research findings of  this study therefore indicate that the more digital natives felt that imperso n-
al data was being collected by the smart City of Cape  Town, the lower their privacy concerns. This is 
consistent with research findings from the van Zoonen (2016) study.  
H3: The more control digital natives perceive to have control on the data collected 
about them by the smart city of  Cape Town, the lower their privacy concerns  
The results from Table 6  indicate that no support can be found for Hypothesis 3 as there is no stati s-
tically significant negative correlation between the Level of  Privacy Concerns of  Cape Town digital 
natives and three of  the questions  related to Perceived Control (Q2: 0.06, Q3: 0.86, Q4: 0.86 ). Only 
Question 1 on Perceived Control has a negative correlation with Level of  Privacy Concern (Q1: -
0.01). 
Table 6 . Spearman ’s Rank Order Correlations:  
Perceived control and Levels of  Privacy Concerns  
 Spearman Rank Order Correlations   
MD pairwise deleted  
Marked correlations are significant at p <.05000  
Variable  Control Q1  Control Q2  Control Q3  Control Q4  
Level of  Privacy Co n-
cern -0.01 0.06 0.86 0.86 
 The results of  the regression analysis also indicate that the variable Perceived Control does not si g-
nificantly influence the levels of  privacy concerns. In par ticular, a R value of  0.10  and a positive 
B coefficie nt of  2.41  was obtained.  
Hence, the findings indicate that there is no st atistically significant negative correlation between Pe r-
ceived Control and the level of  privacy concerns of  digital natives living in the smart City of  Cape 
Town. This result is different from those of  other international studies (e.g. , Beldad (2011). This  dif-
ference might be explained by the practiced  African philosophy of  Ubuntu. Ubuntu places the rights 
afforded to a person as being determined not only by the individual person but largely by the co m-
munity he or she lives in. This entices an individual to  not put an importance on having control of  
what information is known about them or how it is used (Anteneh et al. , 2015).  Tshiani & Tanner  
67 H4: The more the digital natives are aware of  how their personal data is collected by the 
smart city of  Cape Town, the lower their pr ivacy concerns  
Table 7  indicates that no support can be found for Hypothesis 4 as there is no statistically significant 
negative correlation between the Level of  Privacy Concerns of  Cape Town digital natives and all of  
the questions related to awareness of  the method of  data collection (Q1: 0.004 , Q2: 0.020 , Q3: 0.86 ).  
Table 7 . Spearman ’s Rank Order Correlations:  
Awareness of  Method of  Data Collection and Levels of  Privacy Concerns  
 Spearman Rank Order Correlations   
MD pairwise deleted  
Marked correlations are significant at p <.05000  
Variable  Awareness Method Q1  Awareness Method Q2  
Level of  Privacy Co n-
cern 0.004  0.02 
 
The results of  the regression analysis also indicate that the Awareness of  Data Collection Method 
does not significantly influence the levels of  privacy concerns. In particular, a R value of  0. 09 and a 
positive B  coefficient of  2.08 was obtained.  
Results from the test carried out on this hypothesis , therefore , indicate that the test item does not 
show a statistically significant correlation to the dependent variable of  Levels of  Privacy Concerns of  digital natives living in the smart City of  Cape Town. This result is different from those of  intern a-
tional studies (e.g. , Beldad (2011)) and could possibly also be explained by th e African philosophy of  
Ubuntu. In the widely adopted African philosophy of  Ubuntu, whether or not an individual has an 
awareness of  what rights pertain to them as a person does not influence the actual rights that are 
afforded to them as an individual (An tenehet al., 2015).  
H5: The higher the need to be aware of  the security measures put in place during the colle c-
tion and storage of  their personal information by the smart city of  Cape Town, the higher 
their privacy concerns of  the digital natives  
As shown  in Table 8 , support could be found for Hypothesis at a 95% confidence level. The table 
shows that there is a statistically significant positive correlation between the Level of  Privacy Co n-
cerns of  Cape Town digital natives and all of the questions related to the need to be awareness of  
security measures put in place during data collection (Q1: 0.22, Q2: 0.72).  
Table 8 . Spearman ’s Rank Order Correlations:  
Awareness of  Security Measures and Levels of  Privacy Concerns  
 Spearman Rank Order Correlations   
MD pairwise deleted  
Marked correlations are significant at p <.05000  
Variable  Awareness Security Q1  Awareness Security Q2  
Level of  Privacy Concern  0.22 0.72 
 The results of  the regression analysis also revealed that the digital natives ’ need to be aware of  the 
security measures positively significantly correlates to the levels of  privacy concerns of  living in a smart city at a confidence level of  87%.  In particular, an R value of  0 .32 and a positive B value of  
1.98 were obtained.  Smart Cities and Privacy Concerns  
68 Since the correlation between the dependent and independent variables is a positive one, the more 
the respondents placed an importance on knowing the security measures put in place in the colle c-
tion and storage of  data collected about them, the higher thei r privacy concerns. This is in line with 
other international studies (e.g. , Stewart & Segars, 2002).  
DISCUSSION  
The study was conducted using a positivist and quantitative approach and five hypotheses were tes t-
ed. Of  these five hypotheses, only two were sh own to have statistical significance as shown in T a-
ble 9.  
The findings reveal that the more impersonal data is collected by the Smart City of  Cape Town, the 
lower the privacy concerns of  the digital natives. This is in line with van Zoonen (2016) who also 
found that the collection of  impersonal data illicit s low levels of  privacy concerns, given the type of  
data that is being collected (e.g. , gender, language spoken etc.).   
The findings also show that the digital natives  of  Cape Town  have higher privacy c oncerns when 
their need to be aware of  the security measure put in place by the city is high.  This is in line with past 
studies which also found that people ’s uncertainties regarding how their personal data is used and 
processed often triggers feeling of  i nformation priva cy violations (Weltevrede , 2011).  
Table 9. Hypotheses Results  
Hypothesis  Result  
H1: The more digital natives perceive to receive benefits from providing personal 
data to the smart city of  Cape Town, the lower their privacy concerns.  
 Rejected  
H2: The more impersonal  the data being collected by the smart city of  Cape 
Town, the lower the privacy concerns of  the digital natives.  
 Accepted  
H3: The more control digital natives perceive to have control on the data collec t-
ed about them by t he smart city of  Cape Town, the lower their privacy concerns  
 Rejected  
H4: The more the digital natives are aware of  how their personal data is collected 
by the smart city of  Cape Town, the lower their privacy concerns  
 Rejected  
H5: The higher the need to be aware of  the security measures put in place during 
the collection and storage of  their personal information by the smart city of  Cape 
Town, the higher their privacy concerns of  the digital natives  
 Accepted  
 In the study Hypothesis 1 was rejected. This is not in line with past studies which found that people 
are willing to accept risks and disclose information when they perceive more benefits than risks in doing so (Beuker, 2016; Barth & de Jong, 2017). Hypothese s 3 and 4 were  also re jected, in contrast to 
what has been said in literature (Beldad, 2011; Stewart  & Segars , 2002).  
Possible reasons as to why these hypotheses were rejected might relate to the philosophy of  Ubuntu 
and other cultural beliefs that have rendered privacy concerns not as crucial or central to one ’s life in 
Africa, as compared to Western Cultures  (Borena et al., 2015) . Furthermore , it has been stated in li t-
erature that  what data is deemed as private and sensitive varies from person to person, based on one ’s 
own cultural and societal definition of  privacy (Alashoor  et al., 2016; Taddicken, 2014).  
It is important to understand the privacy concerns of  the digital natives because, in a world that is 
more and more driven by sophisticated technologies, it can be ar gued that these digital natives are 
best able to navigate and use these technologies in the future. Cities need the technologically wise and Tshiani & Tanner  
69 experienced to define, discuss , and evaluate the impact of different technologies in our societies in 
terms of  ethi cs and liabilities (Prensky, 2009). In addition, cities should also consider the concerns of 
digital natives because recent history has shown that digital natives have been known to use their 
technological abilities to drive movements and unrest for change  in the cities, which has now been 
coined as digital activism (Weltevrede, 2011).  
This study contributes to scientific literature by detailing the impact of  specific factors on the privacy 
concerns of  citizens living in an African city. For city law makers in Cape Town, this study has pract i-
cal implications as it provides information on relevant focus areas on which the law makers could pay attention to, with the aim to encouraging and appeasing any apprehensions from its citizens towards 
smart city i mplementations.  
While these study ’s findings add to the body of  knowledge, there were , however , limitations in the 
findings. Most of the participants in the study were university students, and a wider net should have 
been cast on finding participants. The sample size in future similar studies should be bigger. Income 
bracket, work industry, race, and gender are some of  the attributes which were not taken into consi d-
eration in this study and should be taken into consideration in future similar studies.  
CONCL USION  
Past studies indicate that there are four main factors that influence the privacy concerns of  citizens living in smart cities. These factors are Awareness, Perceived Benefits, Types of  Data Collected , and 
Perceived Control. However, these studies hav e mostly been conducted in developed countries and 
little is known about their impact in emerging smart cities in developing countries . This study invest i-
gated the impact of  Perceived Benefits, Type of  Data Collected, Perceived Control , and Awareness 
on the Level of  Privacy Concerns of  digital natives living in Cape Town, South Africa.  
Only two of  the five hypotheses were shown to have statistical significance. For the purpose of  del v-
ing deeper into the reasons  why some of  the hypotheses could not be prove n, it would be useful to 
make use of  open ended questions . The open ended questions could be implemented through a 
mixed method  research approach that would allow for an in -depth exploration on how the independ-
ent variables in this study influence the depe ndent variable. The use of  open ended questions in a 
qualitative study could also be used to further identify other factors that might influence privacy co n-
cerns of  digital natives living in the smart city of  Cape Town.  
REFERENCES   
Albino, V ., Berardi, U., & Dangelico, R. (2015). Smart cities: Definitions, dimensions, performance, and initi a-
tives. Journal of  Urban Technology , 22(1) , 3-21. https://doi.org/10.1080 /10630732.2014.942092  
Anthopoulos, L., & Fitsilis, P . (2013). Using classification and roadmapping techniques for smart city viability ’s 
realization. Electronic Journal of  e -Government, 11 (1), 326-336.  
Acquisti, A., Taylor, C., & Wagman, L. (2016). The eco nomics of  privacy. Journal of  Economic Literature, 54 (2), 
442-492.  https://doi.org/10.1257/jel.54.2.442  
Alashoor, T., Aryal, A., & Kenny , G. (2016). Understanding the privacy issue in the dig ital age: An expert p er-
spective. Information Systems Security and Privacy , 18. 
Anteneh, S., Belange, F., Borena, B., & Eji gu, D. (2015). Conceptualizing information privacy co ncern in low-
income countries: A n Ethiopian language instrument for social network sites  context . Twenty -first Americas 
Conference on Information Systems,  1-12. Puerto Rico . 
Backhouse, J. (2015). Smart city agendas of  African cities. Proceedings of  the African Conference on Information Systems 
and Technology (ACIST) , 7-8 
Barth,  S., & de Jong, M. (2017). The privacy paradox –  Investigating discrepancies between expressed privacy 
concerns and actual online behavior  – A systematic literature r eview. Telematics and Informatics , 34(7), 1038-
1058.  https://doi.org/10.1016/j.tele.2017.04.013  Smart Cities and Privacy Concerns  
70 Bhattacherjee, A. (2012). Social science research: Principles, m ethods, and  practices . USF Tampa Library Open 
Access Collections , 1-159, http://scholarcommons.usf.edu/oa_textbooks/3   
Baud, I., Scott, D., Pfeffer, K., Sydenstricker -Neto, J., &  Denis, E. (2015). Reprint of: D igital and spatial 
knowledge m anagement in urban governance: E merging issues in India, Brazil, South Africa, and Peru. 
Habitat International, 46 , 225-233.  https://doi.org/10.1016/j.habitatint.2015.01.018   
Beldad, A. (2011, March 17). Trust and informati on privacy concerns in electronic government.  Retrieved from University 
of  Twente theses . https://ris.utwente.nl/ws/portalfiles/portal/6066855   
Beuker. (2016). Privacy paradox : Factors in fluencing disclosure of  personal information among German and Dutch SNS users. 
University of  Twente Student Theses.  
Borena, B., Belanger, F., & Egigu, D. (2015). Information privacy protection practices in Africa: A review 
through the lens of  critical soci al theory. 48th Hawaii International Conference on System Sciences , 3490-3497. 
https://doi.org/10.1109/HICSS.2015.420  
Boyd, V . (2013). Natives, immigrants, residents or visitors? Developing  a student -led understanding of  the role 
of  digital literacies in the curriculum.  In International Enhancement Themes Conference , 11 - 13. Gla sgow: The 
Glasgow School of  Art.  
Brown , C., & Czerniewicz, L. (2010). Debunking the ‘digital native ’: Beyond digit al aparthe id, towards digital 
democracy . Journal of  Computer Assisted Learning 26 (5), 357-369.  https://doi.org/10.1111/j.1365-
2729.2010.00369.x  
Caragliu, A., Nijkamp, P ., & Del Bo , C. (2011). Smart c ities in Europe. Journal of  Urban Technology , 18(2), 65–82. 
https://doi.org/10.1080/10630732.2011.601117   
Castro, M., Jara, A. , & Skarmeta, A. (2013). Smart lighting solutions  for smart c ities. 27th International Conference 
on Advanced Information Networking and Applications Workshops . https://doi.org/10.1109/WAINA.2013.254  
Chourabi, H., Nam, T., Walker, S., Gil -Garcia, J., Mellouli, S., Nahon, K., . . . Sc holl, H. (2012). Understanding 
smart cities: An integrative f ramework. 2012 45th Hawaii International Conference on System Sciences. Maui: Sy s-
tem Science (HICSS).  
Culnan, M ., & Bies, R. (2003). Consumer privacy: Balancing economic and justice c onsiderations. Journal of  
Social Issues , 59(2), 323– 342. https://doi.org/10.1111/1540-4560.00067  
Dinev, T ., & Hart, P . (2014). Internet privacy c oncerns a nd social awareness as determinants of  intention to 
transact. International Journal of  Electronic Commerce , 10(2), 7-29. https://doi.org/10.2753/JEC1086-
4415100201  
Edwards, B., Hofmeyr, S., & Forrest, S. (2016). Hype and heavy tails: A closer look at data breaches. Journal of  
Cybersecurity , 2(1), 3-14. https://doi.org/10.1093/cybsec/tyw003  
Elhai, J., Levine, J., & Hall, B. (2017). Anxiety about electronic data hacking: Predictors and relations with digital 
privacy protection behavior. Internet Research , 27(3), 631-649.  https://doi.org/10.1108/IntR-03 -2016-0070  
Greener,  S., & Martelli, J. (2015). An introduction to business research methods  (2nd ed.), Bookboon.com. 
http://bookboon.com/en/introdu ction-to -research -methods- ebook   
Gu, X., Zhu, Y., & Guo, X. (2013). Meeting the ” digital natives ”: Understanding the acceptance of  technology 
in classrooms. Journal of  Educational Technology & Society , 16(1), 392-402.  
Harris, A., Goodman, S., & Traynor, P .  (2013). Privacy and security concerns associated with mobile money 
applications in Africa. Washington Journal of  Law, Technology Arts , 8(3), 245-264.  
Hann, I. -H., Hui, K.-L., Lee, S.-Y ., &  Png, I. (2007). Overcoming online information privacy concerns: An i n-
formation -processing theory approach. Journal of  Management Information Systems,  24(2), 13-42.  
https://doi.org/10.2 753/MIS0742-1222240202  
Hoffmann, C., Lutz, C., & Meckel, M. (2014). Digital natives or digital immigrants? The impact of  user chara c-
teristics on online trust. Journal of  Management Information Systems, 31 (3), 138-171.  
https://doi.org/10.1080/07421222.2014.995538  Tshiani & Tanner  
71 Hsu, S. -F., & Shih, D.-H. (2009). The factors influencing individual ’s behavior on privacy protection. WSEAS 
Transactions on Information Science and Applications 6 (9), 1591-160 0. 
International Telecommunication Union. (2013). Measuring the information society. Geneva . Retrieved from 
https://www.itu.int/en/ITU -
D/Stat istics/Documents/publications/mis2013/MIS2013_without_Annex_4.pdf   
Khati, P . (2013). The role of  public libraries in bridging the digital  divide: A  Cape Town case study . Doctoral dissertation, 
University of  Western Cape.  
Kitchin, R. (2015). Making sense of  smart cities: A ddressing present shortcomings. Cambridge Journal of  Regions, 
Economy and Society,  8(1), 131-136.  https://doi.org/10.1093/cjres/rsu027  
Kondepud, S., Campbell, K., Carriero, D., Dzikus, A., Reynolds, A., Robinson, R., & Stonor, T. (2016). Smart 
Cities and Infrastructure. United Nations Commission on Science and Technology for Development Inter -sessional Panel 
2015- 2016, 1 -61, Budapest: United Nations Commission on Science and Technology for Development.  
Lee, N. (2015). The afterlife of  total information awareness and Edward Snowden’s NSA leaks.  In N. Lee,  
Counterterrorism and Cybersecurity  (pp. 151-182 ). Springer.  https://doi.org/10.1007/978-3 -319-17244-6_7  
Lee, J., Hancock, M., & Hu, M. -C. (2014). Towards an effective framework for building smart cities: Lessons 
from Seoul and San Francisco. Technological Forecasting and Social Change , 89, 80-99.  
https://doi.org/10.1016/j.techfore.2013.08.033  
Li, Y . (2012). Theories in online information privacy research: A critical review and an integrated framework. 
Decision Support Systems, 54 (1), 471-481.  https://doi.org/10.1016/j.dss.2012.06.010  
Lourie, G. (2017, August 22). Why Cape Town is One of  the World’s Smart C ities? Retrieved from Talk IoT, Reliable 
IoT News . https://talkiot.co.za/2017/08/22/cape -town -one-worlds -smart -cities/   
Makulilo, A. (2015). Myth and reality of  harmonisation of  data privacy policies in Africa. Computer Law & Sec u-
rity Review , 31(1), 78 -89. https://doi.org/10.1016/j.clsr.2014.11.005  
March , H., & Ribera -Fumaz, R. (2016). Smart contradictions: The politics of  making Barcelona a s elf-sufficient 
city. European Urban and Regional Studies , 23(4), 816 -830. https://doi.org/10.117 7/0969776414554488  
Maumbe, B., Owei, V ., & Alexander, H. (2008). Questioning the pace and pathway of  e -government develo p-
ment in Africa: A case study of  South Africa ’s Cape Gateway project. Government Information Quarterly, 
25(4), 757-777.  https://doi.org/10.1016/j.giq.2007.08.007  
Muyia, M., & Nafukho, F. (2017). Leadership development for frontier s ocieties: Reflections from Kenya.  In M. 
Muyia  & F. Nafukho , Leadership development in emerging market economies  (pp. 229-244) . New York: Palgrave 
Macmillan.  https://doi.org/10.1057/978-1 -137-58003-0_13  
Nfuka, E., & Rusu, L . (2010). Critical success factors for effectiv e IT governance in the public sector organiz a-
tions in a developing country: The c ase of  Tanzania. European Conference on Information Systems( ECIS).  AIS 
Electronic Library  (AISeL).  
Ng-Kruelle, G., Swatman, P ., Rebne, D., & Hampe, J. (2002). The price of  co nvenience: Privacy and mobile 
commerce. Quarterly Journal of  Electronic Commerce 3 , 273- 286. 
Odendaal, N. (2006). Towards the digital city in South Africa: Issues and constraints.  Journal of  Urban Technology, 
13(3), 29-48.  https://doi.org/10.1080/10630730601145997  
Olinger, H., Britz, J., & Olivier, M. (2007). Western privacy and/or Ubuntu? Some critical comments on the 
influences in the forthcoming data privacy bill in South Africa. The International Information & Library Review,  
39(1), 31-43.  https://doi.org/10.1080/10572317.2007.10762729  
Olivero, N., & Lunt, P . (2004). Privacy versus willingness to disclose in e -commerce exchanges: The effect of  
risk awareness on the relative role of  trust and control. Journal of  Economic Psychology, 25 (2), 243-263.  
https://doi.org/10.1016/S0167-4870(02)00172 -1 
Palinkas, L. A., Horwitz, S. M., Green, C. A., Wisdom, J. P ., Duan, N., & Hoagwood, K. (2015). Purposeful 
sampling for qualitative data collection and analysis in mixed method implementation research.  Administr a-
tion and Policy in Mental Health and Mental Health Services Research, 42(5), 533-544.  
https://doi.org/10.1007/s10488-013-0528-y  Smart Cities and Privacy Concerns  
72 Prensky, M. (2009). H. sapiens digital: From digital immigrants and digital natives to digital wisdom. Journal of  
Online E ducation, 5 (3), 1. 
Roche, S., Nabian, N., Kloeckl, K., & Ratti, C. (2012). Are ‘smart cities ’ smart enough. In Global Geospatial C on-
ference , 215-235.  
Rosati, U., & Conti, S. (2016). What is a smart city project? An urban model or a corporate business p lan? Proce-
dia-Social and Behavioral Sciences , 223, 968-973.  https://doi.org/10.1016/j.sbspro.2 016.05.332  
Saunders, M., Lewis,  P ., & Thornhill,  A. (2009). Research methods for business students. Essex: Pearson Education.  
Schleicher, J., Vogler, M., Inzinger, C., & Dustdar, S. (2015). T owards the internet of  cities: A  research roadmap 
for next -genera tion smart cities. Proceedings of  the ACM First International Workshop on Understanding the City 
with Urban Informatics . Melbourne: ACM Digital Library.  https://doi.org/10.1145/2811271.2811274  
Shanahan, D. F., Cox, D. T., Fuller, R. A., Hancock, S., Lin, B. B., & Anderson, K. (2017). Variation in exper i-
ences of  nature across gradients of  tree cover in compact and sprawling cities. Landscape and Urban Pla n-
ning, 157, 231-238.  https://doi.org/10.1016/j.landurbplan.2016.07.004  
Stewart, K., & Segars, A. (2002). An empirical examination of  the concern for information privacy instrument.  
Information Systems Research , 13(1), 36-49.  https://doi.org/10.1287/isre.13.1.36.97  
Smit, J., Makanga, P ., Lance, K., & de Vries, W . (2009). Exploring relationships between municipal and provi n-
cial government SDI implementers in South Africa. In Global Spatial Data Infrastructure (GSDI)  11th World 
Conference . 
Taddicken, M. (2014). The ‘Privacy Paradox ’in the social web: The impact of  privacy concerns, individual cha r-
acteristics, and the pe rceived social relevance on different forms of  self -disclosure. Journal of  Computer -
Mediated Communication, 19 (2), 248-273.  https://doi.org/10.1111/jcc4.12052  
Tene, O., & Jules, P . (2013). Big Data for all: Privacy and user control in the age of  a nalytics. Northwestern Journal 
of  Technolo gy and Intellectual Property , 11(5), 274. 
Thinyane, H. (2010). Are digital natives a world -wide phenomenon? An investigation into South African first 
year stude nts’ use and experience with technology. Computers & Education, 55 (1), 406-414.  
https://doi.org/10.1016/j.compedu.2010.02.005  
Triegaardt, J. ( 2009). Pursuing a social development age nda in  the context of  globalisation: A  South African 
perspective. Social Work/Maatskaplike Werk,  45(1).  
Valent ine, S. (2004). E-Powering the people: South Africa ’s smart c ape access p roject. Council on Library and Info r-
mation Resources.  
van Zoonen, L. (2016). Privacy concerns in smart cities. Government Information Quarterly, 33 (3), 472– 480. 
https://doi.org/10.1016/j.giq.2016.06.004  
Volkwyn, M. (2017). Cape Town is the smartest city in Africa.  Cape Town Magazine . Retrieved from  
http://www.capetownmagazine.com/cape -town-smartest-city -in-africa   
Watson, V . (2015). The allure of  ‘smart city ’ rhetoric. Dialogues in Human Geography , 5(1), 36-39.  
https://doi.org/10.1177/2043820614565868  
Weltevrede, E (2011). Digital methods to study digital natives with a cause.  In N. Shah &  E. Jansen (Eds.), Digi-
tal alternatives with a cause?  Book two: T o think  (pp: 10-23) . Bangalore: The Centre for Internet and Society . 
Hivos .  
Wright, K. B. (2005). Researching i nternet -based populations: Advantages and disadvantages of  online survey 
research, online questionnaire authoring software packages, and web survey services.  Journal of  Computer -
Mediated Communication , 10(3). https://doi.org/10.1111/j.1083- 6101.2005.tb00259.x   
  Tshiani & Tanner  
73 APPENDIX  - QUESTIONNAIRE  
 
 
 
Where you born between the years of  1985 to 1995  Yes No 
Please indicate your age   
Please indicate your gender  Male  Female  
Do you have a place of  residence in the city of  Cape town?(You do not have to 
own the residence and can be renting or living with family or in university res i-
dence)  Yes No 
Did you have access to a computer/ and or internet during your primary school 
years? (access of  this can come from a nywhere, and is not limited to home a c-
cess)  Yes No 
 
 
Benefits  
Kindly indicate the extent to which you 
agree or disagree with the following stat e-
ments  Strongly  
disagree Disagree  Both Agree 
and Dis a-
gree Agree  Strongly 
agree  
1. I consider better city service delivery a 
benefit of living in a smart city.  
2. I consider better distribution of  city r e-
sources such as water, and electricity a be n-
efit of  living in a smart city.  
3. I believe that smart city initiatives will 
threaten rights to privacy.  
4. I believe that smart city initiatives will 
negatively affect people’s rights to conf i-
dentiality.       
 
 A smart city is a city that incorporates the use of  Information communication technologies (ICT) and 
the internet of  things, in the managing and monitoring of  city resources. A smart can use several tec h-
nologies such as surveillance cameras and smart energy consumption meters in the running of  the city. 
The concept of  smart cities is used in various city management areas such as public safety, infrastru c-
ture design and infrastructure monitoring.  
Data is constantly being produced and consumed in smart cities, data such as vehicle movements, vis i-
tor movements, energy consumption, neighbourhood sentiments and crowd control data. Data from different sources in smart cities is often integrated together in the aim of  gaining a picture o f  the city’s 
status in terms of  safety, economic vitality and in terms of  city resources usage.  Smart Cities and Privacy Concerns  
74 Types of  data collected  
Kindly indicate the extent to which you 
agree or disagree with the following 
statements  Strongly  
disagree Disagree  Both Agree 
and Disa-
gree Agree  Strongly 
agree  
5. I do not mind it when city of  Cape 
Town uses data that specifically ident i-
fies me in the running of  the city.  
6. I do not mind it when the city of  
Cape Town uses data about me if  it 
does not specifically identify me (impe r-
sonal data/unidentifiable data).       
 
 
 
 
 
 
 
 Control  
Kindly indicate the extent to which you 
agree or disagree with the following 
statements  Strongly  
disagree Disagree  Both Agree 
and Dis a-
gree Agree  Strongly 
agree  
7. I need to have control over what data 
is collected about me by the city of  Cape Town.  
8. I need to have the option of  being able to opt out of  data being collected 
about me by the city of  Cape Town.  
9. I need to have control over the tec h-
nologies used to collect data about me by the city of  Ca pe Town  
10. I need to have control over how data is collected about me by the city of  
Cape Town       
Awareness (Methods of  data Collection )  
Kindly indicate the extent to which you 
agree or disagree with the following 
statements  Strongly  
disagree Disagree  Both Agree 
and Dis a-
gree Agree  Strongly 
agree  
11. Being aware of  how data about me 
is collected by the city of  Cape Town is 
important to me.  
12. Being aware of  what technologies 
are used in the collection of  data about 
me by the city of  Cape Town is im-
portant to me.       Tshiani & Tanner  
75 Awareness (Need to be aware)  
Kindly indicate the extent to which you 
agree or disagree with the following 
statements  Strongly  
disagree Disagree  Both Agree 
and Disagree  Agree  Strongly 
agree  
13. Knowing how the data collected  about 
me by the city of  Cape Town is distribu t-
ed to other departments in the City of  
Cape Town and other entities is important 
to me.  
14. Knowing how the data that is collec t-
ed about me is used by the city of  Cape 
Town is important to me.       
 
 
Privacy concerns  
Please rate your level of  privacy concerns in the following scenarios, 5 being 
very concerned and 1 being no concern . 1 2 3 4 5 
15. The city of  Cape Town collects personal data (identifiable data) about me.   
16. The city of  Cape Town collects impersonal data about me (non- identifiable 
data).  
17.  My personal data is collected by the city of  Cape Town and is used for bettering city management, city planning, enhancing city services and provi d-
ing better support to local citizens.  
18.  I am aware  of  who, and/or what is collecting data about me in the City of 
Cape Town.  
19. The method of  data collection by the City of  Cape Town involves survei l-
lance and specifically identifies individuals.  
20.  The method of  data collection by the city of  Cape To wn involves survei l-
lance but does not specifically identify individuals.  
21. When I am aware of  the methods used in the collection of  data on me by the city of  cape town.  
22. When I am aware of  how data collected about me by the city of  Cape Town is used.  
23. When I feel that I have control over what data is collected about me by the city of  Cape Town.  
24. When I feel I have control over how data is collected about me by the city of  Cape Town.  
25. When I feel I have control over how much data is collected about me by 
the city of  Cape Town  
26. When I am aware of  how data collected about me by the city of  Cape 
Town is shared among departments in the city of Cape Town and with exte r-
nal entities.  
27. When I do not know the security measures put place in the col lection and 
storage of  my personal data by the City of  Cape Town.  
28. When I know the accuracy of  the data collected about me by the city of  
Cape Town.  
29. When I know who has legitimate access to the personal information col-
lected about me by the City of  Cape Town.       
 Smart Cities and Privacy Concerns  
76 BIOGRAPHIES  
Valerie Tshiani  completed a BCom (Honors) in Information Systems at 
the University of  Cape Town in 2017. She also completed a B.Sc in Co m-
puter Science and Business Computing in 2015. Her research interests are 
related to issues around Smart Cities implementations and qua ntitative 
studies.  
 
 
 
 
 
 
 
A/Prof  Maureen Tanner  has been teaching systems analysis and design 
at the Department of  I nformation Systems of  the University of  Cape 
Town since 2009. Her research interests lie in Social Media, Agile sof t-
ware development related issues (for both collocated and distributed 
teams), UML, software engineering and social aspects of  social enginee r-
ing, global software development, virtual teams, and team collaboration.  
 
 
 
 
 
"
DataPrivacy,3006.txt,"Factors that In ﬂuence Workers ’Participation
in Unhygienic Cyber Practices: A Pilot
Study from Nigeria
Princely I ﬁnedo1(&), Nigussie Mengesha1,
and Olumide Longe2
1Brock University, St. Catharines, Canada
piﬁnedo@brocku.ca
2American University of Nigeria, Yola, Nigeria
Abstract. Participation or engagement in unhygienic cyber practices could
ultimately harm an organization ’s information and communication technologies,
if unchecked. This present study used concepts from the theory of planned
behavior and organizational control theory to examine the effects of factors such
as attitude, subjective norms, organizational facilitators, monitoring, and self-efﬁcacy on workers ’participation in unhygienic cyber practices. A cross-
sectional survey of Nigerian professionals was used to test the formulated
hypotheses. Partial least squares technique of structural equation modeling(SEM) was used for data analysis. The results indicate that attitude toward cyber
hygiene has a negative effect on worker ’s participation in unhygienic cyber
practices; similarly, subjective norms have a negative effect on engagement insuch acts. The data did not show that organizational facilitators, self-ef ﬁcacy,
and monitoring had a meaningful impact on Nigerian workers ’participation in
unhygienic cyber practices. Implications of the study were discussed and con-tribution to the extant literature noted.
Keywords: Cyber hygiene
/C1Information security /C1Employee /C1
Survey /C1Nigeria
1 Introduction
Information and communication technologies (ICT) enhance societal development and
advancement across the globe [ 1]. Private and public organizations from Accra to
Zanzibar have deployed and used ICT and other digital platforms for their activities andoperations [ 2]. When processes and activities hinged on ICT platforms are compromised,
either by non-malicious mistakes or malicious attacks, the consequences of such acts canhave disastrous effects [ 3,4]. To ensure the safety of data resources, savvy organizations
and business operators often provide workers with guidelines and instructions on how to
properly use organizational ICT and other digital assets [ 3–6]. Prior research has
examined employee ICT misuse [ 5], ICT abuse [ 4], compliance, and noncompliance with
information systems (IS) security procedures [ 3–8]. These foregoing themes do not
speciﬁcally focus on employee engagement or involvement in cyber hygiene malprac-
tices [ 9,10]. Additionally, information on which factors in ﬂuence or discourage partic-
ipation in such behavior is not readily available in the noted studies.
©IFIP International Federation for Information Processing 2019
Published by Springer Nature Switzerland AG 2019
P. Nielsen and H. C. Kimaro (Eds.): ICT4D 2019, IFIP AICT 552, pp. 303 –315, 2019.
https://doi.org/10.1007/978-3-030-19115-3_25The proposed research study contributes to prior research by investigating the
effects of attitude, monitoring, subjective norms, organizational facilitators, and self-efﬁcacy on employee participation in unacceptable or ill-sanctioned cyber practices.
Speci ﬁcally, this study is designed to address the following research questions:
(a)What is the effect of attitude toward cyber practices on employees ’participation
in unacceptable cyber hygiene practices ? (b) What is the effect of subjective norms on
employees ’participation in unacceptable cyber hygiene practices ? (c) What is the
effect of organizational facilitators on employees ’participation in unacceptable cyber
hygiene practices ? (d) What is the effect of employees ’self-ef ﬁcacy regarding cyber
practices on their participation in unacceptable cyber hygiene practices ? (e) What is
the effect of monitoring on employees ’participation in unacceptable cyber hygiene
practices?
The study is relevant because the majority of previous work in the area of end-user
security behaviors has been conducted in the developed West [ 3–8]. Not much research
has been done in Africa [ 11–13]. Information systems (IS) issues in advanced societies
should not to be con ﬂated with those in developing parts of the world, including Africa
[11–13]. Factors that in ﬂuence employee involvement in unacceptable cyber practices in
Africa, with the Nigerian worker as an exemplar, may not necessarily be the same for aGerman or American worker. In this study, we provide a perspective of IS securitymanagement issue from a region of the world that has not been well-represented in the
extant literature. Moreover, ﬁndings from a study such as this one could provide useful
insights for the national cyber security frameworks recently launched in Africa,including Nigeria [ 14]. As well, the management of such behaviors among workers, in
the region, also bene ﬁts from empirical studies of this nature. To the best of our
knowledge, no previous research has explored the relationships between the effects ofattitude, self-ef ﬁcacy, subjective norms, organizational facilitators, and monitoring on
employee participation in unacceptable cyber practices, as this present study aims to do.
2 Literature Review
2.1 Information on End User Security Behavior and Cyber Hygiene
Various taxonomies on individual IS/ICT security behaviors are available in the extant
IS security management literature [ 15–17]. For example, Magklaras and Furnell [ 15]
discussed a model for predicting insider threats by focusing on IS misuse and abusewith examples, including data theft and stress. We build on the study by Loch et al.
[16], who identi ﬁed sources of information security threats to an organization, and
Stanton et al. [ 17], who proposed a taxonomy of end-user computer security behaviors.
The study focuses solely on human sources, e.g., employees and non-malicious acts.Actions of malicious entities, i.e., hackers, are outside the scope of this study, so arenatural disasters, i.e., ﬂood,ﬁre, and so on. Stanton et al. [ 17] categorized the nature or
acts of threats as either malicious or non-malicious. Thus, malicious end-user securitybehaviors include, for example, an employee who breaks into an employer ’s protected
IS to steal trade secrets; non-malicious end-user security behaviors include items such
as responding to spam email. A few researchers have investigated similar issues in304 P. I ﬁnedo et al.developing countries, e.g., Nigeria [ 11–13]. For example, Longe et al. [ 13] presented
an overview of criminal uses of ICTs in Sub-Saharan Africa with special emphasis onthe Nigerian 419 scam. I ﬁnedo et al. [ 12] reported on top non-malicious, counter-
productive computer security behavior engagements among employees in Nigeria.
Empirical information on workers ’cyber practices and potential determinants of such
in Africa are not readily available.
In essence, cyber hygiene refers to the practices, precautions, and steps users of
computers and other digital devices take to maintain, safeguard, and secure dataresources from intrusions and outside attacks. Here, “cyber hygiene practices ”refers to
the positive or favorable notion of the phenomenon while unhygienic cyber practices
connote unfavorable and ill-advised acts. In developing an illustrative list of unhygieniccyber practices for the study (see Table 1), we consulted prior academic literature and
practitioners ’reports on the subject matter [ 6,9,10,18].
3 Theoretical Foundations
A plethora of theories have been used to explore factors affecting end-user security
behaviors [ 19]. It is not possible to include all relevant theoretical frameworks in this
preliminary study. For illustrative purposes, we will fuse common theories such as the
theory of planned behavior (TPB) [ 20] and monitoring from the organizational control
theory (OCT) [ 21], to understand factors that impact employee involvement in unac-
ceptable cyber hygiene practices.
3.1 Theory of Planned Behavior
Ajzen [ 20] proposed the theory of planned behavior (TPB). Its three proximal pre-
dictors of behavioral intention and behavior are attitude, subjective norms, and per-ceived behavioral control. Attitude refers to an individual ’s positive or negative
feelings toward engaging in a speci ﬁed behavior. Subjective norms refer to an indi-
vidual ’s perception of what people important to him/her think about a given behavior.
Perceived behavioral control (PCB) refers to an individual ’s beliefs regarding the
efﬁcacy and resources needed to facilitate a behavior. Two sub-constructs, i.e., orga-
nizational facilitators and self-ef ﬁcacy, are used to represent PCB as research shows
these constructs are similar to it [ 8]. The former relates to resources that an organization
provides to encourage or discourage engagement in a target behavior. The latter relatesto an individual ’s ability to organize and execute courses of action required to
produce/perform a speci ﬁc behavior [ 22]. Behavior refers to an individual ’s observable
response in a given situation with respect to a given target. For the purpose of thisstudy, behavior is represented by unhygienic cyber practices. Several researchers have
used TPB to study employee compliance with acceptable IS security behaviors and
intention to use protective technologies [ 19].Factors that In ﬂuence Workers ’Participation 3053.2 Organizational Control Theory
Organizational control theory (OCT) is a multifaceted framework that describes the
process by which one party attempts to in ﬂuence the behavior of another within a given
system [ 21]. Here, it includes management mechanisms through which an organization
manages and directs the attention of its members, as well as motivates and encouragesthem, to act in accordance with its goals and objectives. The speci ﬁc aspect of OCT
considered for this present study is monitoring. Here, monitoring is the observing andchecking of workers ’computing practices over a regular basis. Other components of
OCT, e.g., reward and speci ﬁcation, will be considered in future research inquires.
Monitoring was chosen for this initial study because it provides an opportunity toinvestigate the impact of organizational efforts aimed at checking workers ’adherence
to prescribed IS security guidelines. Moreover, past researchers [e.g., 5] found moni-
toring to be relevant in understanding workers ’compliance with desired IS security
behaviors.
4 Research Model and Hypotheses
The study ’s research model is presented in Fig. 1. Discussions on the formulated
hypotheses are provided as follows:
In tune with the tenets of TPB, positive attitudes in ﬂuence favorable behavior and
conversely, negative attitudes will diminish a target behavior [ 20]. Past studies show
that employees who have positive attitudinal beliefs about their organization ’sI S
security rules are the ones that readily comply with such guidelines [ 8,19]. Thus, it is
expected that employees with positive attitudes toward cyber hygiene will have fewercompulsions to engage in unacceptable cyber hygiene practices.
H1: Attitude toward cyber hygiene negatively affect participation in unhygienic
cyber practices .
Evidence exists to support the view indicating that an individual ’s behavior is
inﬂuenced or motivated by what he or she observes to be the norm in his or her
environment [ 6,20]. With regard to following acceptable organizational IS security
rules, employees are more likely to adhere to their organization ’s ISSP if they notice
that those around them, i.e., superiors, peers, and subordinates, are complying withUnhygienic cyber practicesAttitude toward cyber hygiene
Subjective norms
Organizational facilitators
Self-efficacy
Monitoring
Fig. 1. The research model.306 P. I ﬁnedo et al.such guidelines [ 6–8,19]. Past studies [ 7,8] found that subjective norms signi ﬁcantly
affect ISSP compliance in organizations.
H2: Subjective norms related to cyber hygiene negatively affect participation in
unhygienic cyber practices .
The availability of organizational resources facilitates a target behavior such as
compliance [ 20]. Organizational facilitators, e.g., provision of acceptable IS use policy
and the availability of IS security awareness programs, play a signi ﬁcant role in
shaping individual perceptions of acceptable ICT use in organizations [ 3,5,18]. It is
reasonable to expect that workers, in contexts where adequate organizational resourcesexist, will have less tendency to engage in ill-sanctioned computing practices andrelated acts.
H3: Organizational facilitators negatively affect participation in unhygienic cyber
practices .
An individual ’s con ﬁdence in their knowledge and skills plays an import role in
engaging in a target behavior [ 20]. With respect to compliance with acceptable IS
security guidelines and rules, past studies demonstrated that employees with higherlevels of skills and knowledge of IS security issues and consequences related to poorchoices are less inclined to indulge in unfavorable ICT use practices [ 3,8]. Those with
lower levels of knowledge do not readily follow IS security rules [ 23] and may
knowingly or unknowingly engage in unhygienic cyber practices.
H4: Self-ef ﬁcacy negatively affects participation in unhygienic cyber practices .
In accordance with OCT, management often evaluates and monitors the actions of
employees to ensure compliance with desired goals and objectives [ 21]. If a worker
believes that management does not monitor his/her computing practices (and their useof technologies at work), s/he is more likely to ﬂout acceptable directives. D ’Arcy et al.
[5] found that when employees know that computer monitoring is in place, incidents of
IS misuse drop signi ﬁcantly. Thus, it is expected that employees ’participation in
unhygienic cyber practices will likely be low if they know their organization monitors
their ICT use practices.
H5: Monitoring negatively affects participation in unhygienic cyber practices .
5 Research Methodology
5.1 Study Design, Data Collection, and Subjects
To test the formulated hypotheses, a survey research methodology was adopted. A pilot
survey was initially conducted among 25 MBA students in a local university to
enhance the content and face validities of the items used for the study. Questionnairesused in the ﬁnal survey were administered to working MBA students in a university in
Lagos, which is the commercial capital of Nigeria; participation was voluntary. Of the125 questionnaires distributed, 76 were returned; thus, the effective response rate forthe study was 60.8%. The response rate is considered adequate for a study such as thisone. Incomplete responses and poorly completed responses were excluded from sub-sequent data analysis. In all, 71 responses were used for the study.Factors that In ﬂuence Workers ’Participation 307Demographic information about the respondents is presented as follows: 37 are
males (52.1%) and 20 are females (28.2%); the data has missing entries. Many of them(49%) have bachelor ’s degrees and 25% have other master ’s degrees. In the sample,
42.3%, 18.3%, and 15.5% of respondents were in the 21 to 30, 31 to 40, and 41 to 50
age ranges, respectively. The participants ’average years of computer use is 11.5 years
(standard deviations [S.D.] = 6.8) and they have 4.6 years (S.D. = 3.4) tenure at theircurrent organizations. Some participants noted their job titles as accountant, softwareengineer, system analyst, project manager, internet scammer, lecturer, and customerservice manager. Forty-two (42) are IT professionals and the rest are non-IT personnel.Diverse industries such as IT, manufacturing, banking, education, and so forth wererepresented in the sample. The data sample included an even distribution of organi-zation size and annual revenue.
The survey collected both independent and dependent data from the same source;
this could lead to common method bias (CMB) [ 24]. The procedures recommended to
account for the effects of CMB were followed [ 24]. For example, respondent anon-
ymity was assured and questions in the survey were ordered in a randomized manner.Additionally, two post-hoc statistical analyses to further reduce concerns related to thepresence of CMB were used. First, Harman ’s one-factor test was conducted for the
reﬂective, independent constructs. The results showed that ﬁve factors were extracted;
theﬁrst factor explained 39.8% of the variance. Second, Pavlou et al. [ 25] suggest that
an inter-construct correlation higher than 0.9 is a possible indicator of CMB. There
were no correlations in Table 2above 0.90 to further show CMB was not a problem for
our data. Both tests indicated that CMB was not problematic for the collected data.
5.2 Operationalization of the Constructs
Measuring items used to represent the unhygienic cyber practices construct were taken
from the following sources [ 6,9,10,18]. The dependent construct was modeled as a
formative construct because its constituting variables measure differing phenomena.
Table 1shows the questionnaire items used for the formative construct and their
descriptive statistics. The study ’s participants were asked the question: “Please indicate
how often you participate in the listed unhygienic cyber practices listed in Table 1.”
Their responses were assessed on a seven-point scale ranging from “Almost never ”
(1) to “Almost always ”(7). The four (4) measures for attitude toward cyber hygiene,
which include “Following the organization ’s IS security policy is a good idea ”, were
taken constructs that have been validated [ 8,22]. Four (4) items for the subjective
norms construct were adapted from [ 7,8] as well with an example: “My boss thinks
that I should follow the organization ’s IS security policy ”. For the four (4) measuring
items used for organizational facilitators, we used items such as “My organization has
established rules of behavior for use of computer resources and other digital assets ”;
this was modi ﬁ
ed from [ 5,18]. The self-ef ﬁcacy construct has items adapted from
Bandura [ 22]; items in the construct include “I have basic knowledge on how to avoid
unhygienic cyber practices. ”Five (5) measuring items adapted from D ’Arcy et al. [ 5]308 P. I ﬁnedo et al.were used to operationalize the monitoring construct. An example includes “I believe
that my organization monitors its employees ’cyber practices and engagements. ”As
indicated, the measuring items used for the re ﬂective, independent constructs have been
validated in prior studies. All the items were assessed on a seven-point scale ranging
from “Strongly disagree ”(1) to “Strongly agree ”(7).
Table 1. Questionnaire items used for the formative construct and their descriptive statistics.
Item Unhygienic cyber practices Mean SD Weight P-value VIF
1 Responding to spam (i.e.,
unsolicited emails3.28 2.29 +++ +++ 3.879
2 Using weak passwords at work 3.63 1.92 +++ +++ 4.014
3 Not updating work-related
passwords regularly3.82 2.08 0.155 0.087 2.078
4 Visiting non-related websites at
work4.05 1.69 0.115 0.015 1.285
5 Not updating anti-virus and/or
anti-spyware software at work4.15 1.96 +++ +++ 4.569
6 Not logging out of secure systems
after use3.33 2.16 0.147 0.010 1.911
7 Not always treating sensitive data
carefully3.07 2.01 0.109 0.068 1.843
8 Allowing others (e.g., family) to
play with work laptop3.34 2.26 0.094 0.021 1.794
9 Downloading unauthorized
software (i.e., freeware) onto workcomputer3.80 2.18 0.159 0.081 1.634
10 Pasting or sticking computer
passwords on of ﬁce desks2.85 2.57 +++ +++ 4.963
11 Disclosing work-related
passwords to others2.95 2.38 +++ 0.086 1.918
12 Leaving one ’s work laptop
unattended3.66 2.27 +++ 0.081 2.016
13 Not backing up work ﬁles 4.38 1.81 +++ +++ 3.987
14 Logging unto unsecure networks
outside work, e.g., WIFI4.19 1.90 0.153 0.09 1.666
15 Using unauthorized or personal
USB at work4.16 2.12 0.172 0.065 2.619
16 Storing work ﬁles in the cloud
without authorization3.51 2.25 0.163 0.075 1.648
Note: +++ represents entries excluded from ﬁnal data analysis.Factors that In ﬂuence Workers ’Participation 3096 Data Analysis
Data analysis was done using the Partial Least Squares (PLS) technique of structural
equation modeling (SEM), which is suitable for theory testing [ 26]. PLS supports the
use of small sample sizes and does not impose data normality requirements [ 26,27].
WarpPLS 5.0 software was used for this study [ 27]. PLS supports both formative and
reﬂective models and recognizes two components of a causal model: the measurement
and structural models.
6.1 Measurement Model
For the re ﬂective constructs, item reliability, composite reliability, and convergent and
discriminant validities were examined. Item loadings above 0.7 are recommended [ 26]
in assessing item reliability. Item loadings from 0.700 to 0.933 were obtained for thestudy (they were not presented due to space limitations). Composite reliability higherthan 0.707 for each construct is preferred [ 26]; results obtained in this regard are
presented in Table 2to satisfy this criterion. In addition, convergent and discriminant
validities were assessed using the following criteria: (a) the average variance extracted
(AVE) should be no less than 0.707 (i.e., the AVE should be above the threshold valueof 0.50); (b) the square root of AVE should be larger than the correlations between thatconstruct and all other constructs; and (c) the items should load more strongly on theirrespective constructs than on other constructs. This requirement for “c”was met but not
included due to space consideration; however, information relating to the AVEs isprovided in Table 2. All AVEs are above the recommended threshold of 0.50.
Table 2. Composite reliability, AVEs, and inter-construct correlations.
COM AVE 1 2 3 4 5 6
ATT 0.85 0.59 0.77 0.63 0.52 −0.05 0.48 0.28
SUB 0.87 0.62 0.63 0.79 0.52 −0.05 0.45 0.60
FAC 0.93 0.76 0.52 0.52 0.87 0.06 0.55 0.34
CYB na na −0.05 −0.05 0.06 na 0.16 −0.03
MON 0.91 0.66 0.48 0.45 0.55 0.16 0.81 0.12
SEF 0.90 0.76 0.28 0.60 0.34 −0.03 0.12 0.87
Note: (a) COM = composite reliability; AVE = average variance
extracted; (b) Off-diagonal elements are correlations among
constructs; (c) the bold fonts in the leading diagonals are thesquare root of AVEs; (d) ATT = Attitude, SUB = Subjectivenorms, FAC = Organizational facilitators, MON = Monitoring,
SEF = Self-ef ﬁcacy, CYB = Unhygienic cyber practice310 P. I ﬁnedo et al.For the formative construct, i.e., unhygienic cyber practices, the presence of mul-
ticollinearity is checked and the item weights evaluated. Excessive collinearity withinformative scales is problematic for a construct. To assess multicollinearity among thevariables, the variance in ﬂation factors (VIF) are checked. VIFs below the conservative
cutoff of 3.33 are considered adequate [ 28]. Items weights show how signi ﬁcantly
linked item indicators are to their speci ﬁed constructs; weights with statistical signif-
icance are preferred [ 28]. Table 1shows that VIFs and item weights used to capture the
dependent variable are adequate. Namely, all VIFs are below 3.33 and the weights aresigniﬁcant at p < 0.10 level.
6.2 Structural ModelF
The structural model provides information about the path signi ﬁcance ( b) of hypoth-
esized relationships and the coef ﬁcient of determination, squared R (R
2)[26]. WarpPLS
5.0 results for the bs and the R2are shown in Fig. 2(* indicates signi ﬁcance at p < 0.5
level). The independent variables explained 19% of the variance in the dependent
variable to show the model has signi ﬁcant value [ 26]. WarpPLS 5.0 also provides
information on Goodness of Fit (GoF), which is a global ﬁt measure that accounts for
both measurement and structural model performance [ 29]. The GoF obtained for this
study is 0.33, which is close to the cut-off value of 0.36 for large effect sizes [ 30].
Consistent with H1, attitude toward cyber hygiene has a negative effect on par-
ticipation in unhygienic cyber practices ( b=−0.21, p < 0.05). H2, which predicted
that subjective norms would have a negative effect on participation in unhygienic cyberpractices ( b=−0.12, p < 0.05), was con ﬁrmed as well. The data did not support H3;
namely, organizational facilitators were not found to have a negative effect on par-ticipation in unhygienic cyber practices ( b= 0.03, p = 0.41). Support was not found
for H4, which predicted that self-ef ﬁcacy negatively affected participation in unhy-
gienic cyber practices ( b= 0.09, p = 0.17). H5 was uncon ﬁrmed; monitoring was not
found to have a negative effect on participation in unhygienic cyber practices
(b= 0.28, p < 0.01). The path is statistically signi ﬁcant; however, the result is
inconsistent with the stated prediction.
Fig. 2. The PLS result.Factors that In ﬂuence Workers ’Participation 3117 Discussions and Conclusion
The study ’s main objective was to examine the effects of relevant factors taken from
TPB and OCT on workers ’participation in unhygienic cyber practices. Our result
conﬁrmed that Nigerian workers with favorable attitudes toward cyber hygiene were
more likely to shun participation in unhygienic cyber practices. This result supports
prior studies [ 6–8] that showed individual attitudes towards end-user security practices
are an important factor that modi ﬁes engagement in desired IS security behaviors. We
found that the sampled Nigerian workers were more likely to avoid participation inunhygienic cyber practices if they believed signi ﬁcant others, i.e., colleagues in their
workplaces, did not approve of such practices or acts. This ﬁnding is consistent with
the espoused viewpoint indicating that group approval of safe and acceptable com-puting behaviors augurs well for compliance with the sanctioned organization ’sI S
procedures and rules [ 6–8].
The result indicating that organizational facilitators mattered less for Nigerian
workers with respect to their engagements in unhygienic cyber practices could beexplained by contextual factors. The result is at odds with observations in a developedcountry that indicated that organizational facilitators help to prevent employeeengagement in nonmalicious IS security acts [ 18]. It is possible that the sampled
participants are employed in organizations where organizational resources are inade-quate or lacking. For example, during an informal discussion with the researchers, one
participant commented that “the attention of my company is on how to increase its
market capitalization; issues like IS security is not [company X] priority. ”We found no
meaningful association between Nigerian workers ’self-ef ﬁcacy and participation in
unhygienic cyber practices. The result in this aspect might indicate that the sampledworkers may not believe they possess suf ﬁcient skills and knowledge to help them deal
with cyber issues or related practices. This might be discouraging given that paststudies [ 6–8,19] from developed countries have shown that adequate levels of skills,
capabilities, and knowledge of end-user IS security issues are pertinent for suppressing
involvement in ill-sanctioned computing behaviors. It is somewhat surprising that the
relationship between monitoring and the dependent construct was unsupported in ourresearch setting. Prior IS security studies that used monitoring found it to be animportant mechanism for shaping behavioral intentions to comply with acceptable rules[5]. Our result shows that more monitoring seems to lead to more participation in
unhygienic cyber practices. A plausible explanation for the lack of support for H5might be due to extraneous factors. For example, it is possible that the sampled par-ticipants are unperturbed by IS security directives and monitoring in their organizations
or are able to circumvent such efforts through neutralization techniques [ 31]. To some
degree, their profession or occupation might also have an in ﬂuential role. Recall most
of the study ’s participants are IT professionals and four (4) of them candidly indicated
they are internet scammers. Evidence exists to show that employees likely to ﬂout
organizational IS security directives are those with more advanced IT know-how [ 3].312 P. I ﬁnedo et al.7.1 Contributions to Research and Implications for Practice
This study is one of the ﬁrst of its kind to investigate worker ’s participation in
unhygienic cyber practices by using perspectives from TPB and OCT. No previousstudy has considered the effects of the selected variables on the dependent constructwith data collected from Africa. This study offers support for the applicability of TPBand OCT in understanding employee participation in unhygienic cyber practices inwork settings. Findings of the study lend credence to prior studies emphasizing theroles of attitudinal beliefs and subjective norms in shaping desired behaviors. There are
implications of the study ’sﬁndings for practice, in particular, the management of
workers in Nigeria in relation to their discouraging participation in unhygienic cyberpractices in work environments. Management can better control workers ’behaviors
with respect to the phenomenon by proactively providing incentives (e.g., campaigns,training) that can enhance positive attitudes towards favorable cyber hygiene practices.Well-tailored communication could also in ﬂuence attitudes toward desired behavior.
Given the importance of subjective norms in reducing employees ’engagement in
unhygienic cyber practices, management should ensure concerns related to acceptable
cyber practices are regularly discussed at department meetings and widely situated in
the social functioning of the enterprise. In ﬂuential persons in the organization could be
given the responsibility to act as “champions ”of the cause of promoting good cyber
hygiene practices [ 7]. It is likely that the amount of variance explained in the research
model could increase further if more favorable organizational facilitators are madeavailable to workers. Similarly, identifying speci ﬁc measures that can enhance work-
er’s self-ef ﬁcacy in relation to cyber practices could help produce more fruitful results.
Reliance on monitoring mechanisms may not be totally effective in a context where
workers possess above average ICT/IS knowledge; deterrence and sanction mecha-
nisms may be needed to ensure compliance [ 4,7].
7.2 Study ’s Limitations and Future Research Directions
There are several limitations to this study. First, the data came from a cross-sectional
ﬁeld survey; longitudinal data may facilitate more insight. Second, the data was
obtained from Nigerian workers. Findings in this preliminary study may not beapplicable to workers in other parts of Africa; perceptions may vary across the con-tinent. Third, the sample is small. Fourth, although CMB was not problematic for thisstudy, it is still possible that participants might have provided “socially desirable
responses ”[24] to some of the issues being investigated. Future studies could over-
come the noted shortcomings in this study. Comparative studies on the continent and
elsewhere could be conducted. Attention should be paid to other end-user securitybehaviors such as those related to malicious acts. Other aspects of OCT, e.g., reward,could be explored. Likewise, other relevant theories in the area of IS security man-agement [ 19] could be used to study the phenomenon and case studies could be used to
enhance insights.Factors that In ﬂuence Workers ’Participation 313References
1. Niebel, T.: ICT and economic growth –Comparing developing, emerging and developed
countries. World Dev. 104, 197 –211 (2018)
2. Haftu, G.G.: Information communications technology and economic growth in Sub-Saharan
Africa: A panel data approach. Telecommunications Policy (2018, in press)
3. Iﬁnedo, P.: Roles of organizational climate, social bonds, and perceptions of security threats
on IS security policy compliance intentions. Inf. Resour. Manag. J. 31(1), 53 –82 (2018)
4. Hu, Q., Xu, Z., Dinev, T., Ling, H.: Does deterrence work in reducing information security
policy abuse by employees? Commun. ACM 54(6), 54 –60 (2011)
5. D ’Arcy, J.P., Devaraj, S.: Employee misuse of information technology resources: testing a
contemporary deterrence model. Decis. Sci. 43(6), 1091 –1124 (2012)
6. Guo, K.H., Yufei, Y., Archer, N.P., Connelly, C.E.: Understanding nonmalicious security
violations in the workplace: a composite behavior model. J. Manag. Inf. Syst. 28(2), 203 –
236 (2011)
7. Iﬁnedo, P.: Information systems security policy compliance: an empirical study of the effects
of socialisation, in ﬂuence, and cognition. Inf. Manag. 51(1), 69 –79 (2014)
8. Bulgurcu, B., Cavusoglu, H., Benbasat, I.: Information security policy compliance: an
empirical study of rationality-based beliefs and information security awareness. MIS Q.
34(3), 523 –548 (2010)
9. Aldorisio, J.: What is Cyber Hygiene? A De ﬁnition of Cyber Hygiene, Bene ﬁts, Best
Practices, and More. https://digitalguardian.com/blog/what-cyber-hygiene-de ﬁnition-cyber-
hygiene-bene ﬁts-best-practices-and-more . Accessed 20 Aug 2018
10. CITI: Clearswift Insider Threat Index (CITI). US Edition. http://pages.clearswift.com/rs/591-
QHZ-135/images/Clearswift_Insider_Threat_Index_2015_US.pdf . Accessed 7 Jan 2017
11. Njenga, K., Brown, I.: Conceptualising improvisation in information systems security. Eur.
J. Inf. Syst. 21(6), 592 –607 (2012)
12. I ﬁnedo, P., Longe, O.B., Amaunam, I.: Top exemplars of non-malicious, counterproductive
computer security behaviours (CCSB) engagements among employees in Nigeria: recom-
mendations for management. In: The 8th iSTEAMS, Lagos, Nigeria, pp. 5 –12 (2017)
13. Longe, O., Ngwa, O., Wada, F., Mbarika, V., Kvasny, L.: Criminal uses of information &
communication technologies in sub-Saharan Africa: Trends, concerns and perspectives.
J. Inf. Technol. Impact 9(3), 155 –172 (2009)
14. Technology Times, Nigeria adopts framework for national security policy. https://
technologytimes.ng/nigeria-adopts-framework-national-cyber-security-policy/ . Accessed 20
Aug 2018
15. Magklaras, G.B., Furnell, S.M.: Insider threat prediction tool: evaluating the probability of
IT misuse. Comput. Secur. 21(1), 62 –73 (2002)
16. Loch, K.D., Carr, H.H., Warkentin, M.E.: Threats to information systems: today ’s reality,
yesterday ’s understanding. MIS Q. 16(2), 173 –186 (1992)
17. Stanton, J.M., Stam, K.R., Mastrangelo, P., Jolton, J.: Analysis of end user security
behaviors. Comput. Secur. 24(2), 124 –133 (2005)
18. I ﬁnedo, P. and Cashin, J.: Using social cognitive theory to understand employees ’
counterproductive computer security behaviors (CCSB): a pilot study. In: The 27thInternational Business Research Conference (IBRC), Toronto, Canada (2014)
19. Sommestad, T., Hallberg, J., Lundholm, K., Bengtsson, J.: Variables in ﬂuencing information
security policy compliance: a systematic review of quantitative studies. Inf. Manag. Comput.Secur. 22(1), 42 –75 (2014)314 P. I ﬁnedo et al.20. Ajzen, I.: The theory of planned behavior. Organ. Behav. Hum. Decis. Process. 50(2), 179 –
211 (1991)
21. Eisenhardt, K.M.: Control: organizational and economic approaches. Manage. Sci. 31(2),
134–149 (1985)
22. Bandura, A.: Social foundations of thought and action: A social cognitive theory. Prentice-
Hall, Englewood Cliffs, NJ (1986)
23. Yazdanmehr, A., Wang, J.: Employees ’information security policy compliance: a norm
activation perspective. Decis. Support Syst. 92,3 6–46 (2016)
24. Podsakoff, P.M., MacKenzie, S.B., Lee, J.Y., Podsakoff, N.P.: Common method biases in
behavioral research: a critical review of the literature and recommended remedies. J. Appl.
Psychol. 88(5), 879 –903 (2003)
25. Pavlou, P.A., Liang, H., Xue, Y.: Understanding and mitigating uncertainty in online
exchange relationships: a principal-agent perspective. MIS Q. 31(1), 105 –136 (2007)
26. Hair, J.F., Tomas, G., Hult, M., Ringle, C.M., Sarstedt, M.: A Primer on Partial Least
Squares Structural Equation Modeling (PLS-SEM). Sage, Thousand Oaks (2014)
27. Kock, N.: WarpPLS 5.0 User Manual, ScriptWarp Systems, http://cits.tamiu.edu/WarpPLS/
UserManual_v_5_0.pdf . Accessed 27 Feb 2017
28. Petter, S., Straub, D., Rai, A.: Specifying formative constructs in information systems
research. MIS Q. 31(4), 623 –656 (2007)
29. Tenenhaus, M., Vinzi, V.E., Chatelin, Y.-M., Lauro, C.: PLS path modeling. Comput. Stat.
Data Anal. 48(1), 159 –205 (2005)
30. Wetzels, M., Odekerken-Schr öder, G., Van Oppen, C.: Using PLS path modeling for
assessing hierarchical construct models: guidelines and empirical illustration. MIS Q. 33(1),
177–195 (2009)
31. Siponen, M., Vance, A.: Neutralization: New insights into the problem of employee
information systems security policy violations. MIS Q. 34(3), 487 –502 (2010)Factors that In ﬂuence Workers ’Participation 315"
DataPrivacy,3007.txt,"/gid00030/gid00035/gid00032/gid00030/gid00038/gid00001/gid00033/gid00042/gid00045 /gid00001
/gid00048/gid00043/gid00031/gid00028/gid00047/gid00032/gid00046Citation: Shahid, J.; Ahmad, R.;
Kiani, A.K.; Ahmad, T.; Saeed, S.;
Almuhaideb, A.M. Data Protection
and Privacy of the Internet of
Healthcare Things (IoHTs). Appl. Sci.
2022 ,12, 1927. https://doi.org/
10.3390/app12041927
Academic Editors: Hamid Mcheick
and Gianluca Lax
Received: 10 October 2021
Accepted: 27 January 2022
Published: 12 February 2022
Publisher’s Note: MDPI stays neutral
with regard to jurisdictional claims in
published maps and institutional afﬁl-
iations.
Copyright: © 2022 by the authors.
Licensee MDPI, Basel, Switzerland.
This article is an open access article
distributed under the terms and
conditions of the Creative Commons
Attribution (CC BY) license (https://
creativecommons.org/licenses/by/
4.0/).
applied  
sciences 
Review
Data Protection and Privacy of the Internet of Healthcare
Things (IoHTs)
Jahanzeb Shahid1
, Rizwan Ahmad1,*
, Adnan K. Kiani2
, Tahir Ahmad3
, Saqib Saeed4
and Abdullah M. Almuhaideb5
1School of Electrical Engineering and Computer Science, National University of Sciences and
Technology (NUST), Islamabad 44000, Pakistan; 13msccsjshahid@seecs.edu.pk
2Essex Pathways Department, University of Essex, Colchester CO4 3SQ, UK; a.kiani@essex.ac.uk
3Security and Trust Unit, Fondazione Bruno Kessler, 38123 Trento, Italy; ahmad@fbk.eu
4SAUDI ARAMCO Cybersecurity Chair, Department of Computer Information Systems, College of Computer
Science and Information Technology, Imam Abdulrahman Bin Faisal University, P .O. Box 1982,
Dammam 31441, Saudi Arabia; sbsaed@iau.edu.sa
5SAUDI ARAMCO Cybersecurity Chair, Department of Networks and Communications, College of Computer
Science and Information Technology, Imam Abdulrahman Bin Faisal University, P .O. Box 1982,
Dammam 31441, Saudi Arabia; amalmuhaideb@iau.edu.sa
*Correspondence: rizwan.ahmad@seecs.edu.pk
Abstract: The Internet of Things (IoT) is an emerging ﬁeld consisting of Internet-based globally
connected network architecture. A subset of IoT is the Internet of Healthcare Things (IoHT) that
consists of smart healthcare devices having signiﬁcant importance in monitoring, processing, storing,
and transmitting sensitive information. It is experiencing novel challenges regarding data privacy
protection. This article discusses different components of IoHT and categorizes various healthcare
devices based on their functionality and deployment. This article highlights the possible points and
reasons for data leakage, such as conﬂicts in laws, the use of sub-standard devices, lack of awareness,
and the non-availability of dedicated local law enforcement agencies. This article draws attention to
the escalating demand for a suitable regulatory framework and analyzes compliance problems of
IoHT devices concerning healthcare data privacy and protection regulations. Furthermore, the article
provides some recommendations to improve the security and privacy of IoHT implementation.
Keywords: IoHT; data privacy; healthcare systems; security and privacy; healthcare regulations
1. Introduction
The IoT is an emerging technology that facilitates consumers by exchanging informa-
tion with devices connected to the Internet. The International Telecommunication Union
(ITU) [ 1] deﬁnes IoT as the network of sensor devices interacting with the environment.
The spectrum of IoT has been broadened and encloses many applications that are used in
different scenarios like security, remote monitoring, electrical appliances control, military
use, and other electronic equipment. One primary use case of IoT is in the healthcare
sector, i.e., the Internet of Healthcare Things (IoHT), designed to monitor, store, or transmit
healthcare information. In simple words, IoHT is a sub-class of IoT speciﬁcally dealing
with healthcare that includes devices, services, and software [2].
The IoHT describes uniquely identiﬁable devices connected to the Internet, communi-
cating with each other, used in the medical area. IoHT devices help to monitor individuals’
medical conditions by generating clinical data by forwarding it to a remote server or service
with the help of wireless network infrastructure [ 3]. Like any other Internet-based device,
IoHT devices have a unique identiﬁer such as an IP address which enables them to connect
with the network and to forward/receive data to/from intended devices [ 4]. The central
server manages this collected information and responds accordingly to diagnose patients’
Appl. Sci. 2022 ,12, 1927. https://doi.org/10.3390/app12041927 https://www.mdpi.com/journal/applsciAppl. Sci. 2022 ,12, 1927 2 of 22
diseases. A high-level general working ﬂow diagram of an IoHT implementation is pre-
sented in Figure 1. The idea is to provide reliable, efﬁcient, and cost-effective healthcare
services by facilitating physicians and medical staff by remotely monitoring their patients.
IoHT implementations also enable individuals to manage their health data easily and assist
them in how to use wearable health monitors [5,6].
Appl. Sci.  2022 , 12, 1927  2 of 23 
 
them to connect with the network and to forward/receive data to/from intended devices 
[4]. The central server manages this collected information and responds accordingly to 
diagnose patients ’ diseases. A high -level general working flow diagram of  an IoHT im-
plementation is presented in Figure 1. The idea is to provide reliable, efficient, and cost -
effective healthcare services by facilitating physicians and medical staff by remotely mon-
itoring their patients. IoHT implementations also enable indivi duals to manage their 
health data easily and assist them in how to use wearable health monitors [5 ,6]. 
 
Figure 1. High -Level Architecture of IoHT Implementation . 
Data privacy is considered to be a fundamental requirement for consumer ac-
ceptance , which can be ensured through the data flow representation, authentication, and 
authorization of the performed activities such as data collection, retention, processing, 
and transmission. Data privacy risks are directly related to unauthorized collection, usage, 
access, storage, and sharing activities. These activities might be the reason for personal 
data leakage and compromis ing the user’s privacy, especially concerning  healthcare data, 
as it has different priorities and is highly valuable and sensitive. In this regard, appropri-
ate protection and security measures are required [7–10]. 
Moreover, accessibility and availability of personal healthcare information on the In-
ternet also cause privacy problems. In June 2015, a censorious privacy violation attack w as 
launched when malware exploited vulnerabilities in blood gas analyzer devices to gain 
access to hospital networks and leaked private data [11]. Despite this, the privacy frame-
work for IoHT systems and services is expected  to be transparent to patients, making 
available updated information to ensure the protection of patients ’ data [6]. The healthcare 
systems collect most of the data from the sensing devices and forward it via intermediate 
devices to the management layer. During this process, several prot ocols and encoding 
schemes are used to communicate data reliably. It is easier to find vulnerabilities in dif-
ferent components of the healthcare system by using search engines like Shodan, which 
assist s attackers in searching the connected vulnerable devic es on the Internet [12]. Simi-
larly, a worksheet containing millions of records with user healthcare information can be 
exchanged in a fraction of a second, seamlessly, and without leaving any consistent trace 
[10].  
Some IoT -related data privacy and protect ion policies are used to protect data and 
user s’ privacy. However, these legal frameworks have not produced the intended results 
and the actual level of healthcare data privacy protection is insufficient for the issues men-
tioned earlier [9]. There are also  some limitations and missing aspects of healthcare data 
privacy laws that do not provide a particular set of instructions to protect IoHT data.  
In this paper, we discuss technological, legal, and structural problems of IoHT sys-
tems with some analysis, and compliance issues of the healthcare data privacy and pro-
tection framework in the developing countries. The paper uses a layered architecture to 
highlight the data protection and privacy issues in the IoHT. It further identifies protec-
tion and privacy issu es at different layers from technical and legal perspectives. Further-
more, the study of different components of the existing healthcare system in terms of se-
curity related to data protection discusses possible points of data leakage, missing aspects 
of hea lthcare policies , and issues in the enforcement of healthcare systems and policies. 
Figure 1. High-Level Architecture of IoHT Implementation.
Data privacy is considered to be a fundamental requirement for consumer accep-
tance, which can be ensured through the data ﬂow representation, authentication, and
authorization of the performed activities such as data collection, retention, processing, and
transmission. Data privacy risks are directly related to unauthorized collection, usage,
access, storage, and sharing activities. These activities might be the reason for personal
data leakage and compromising the user’s privacy, especially concerning healthcare data,
as it has different priorities and is highly valuable and sensitive. In this regard, appropriate
protection and security measures are required [7–10].
Moreover, accessibility and availability of personal healthcare information on the
Internet also cause privacy problems. In June 2015, a censorious privacy violation attack
was launched when malware exploited vulnerabilities in blood gas analyzer devices to
gain access to hospital networks and leaked private data [ 11]. Despite this, the privacy
framework for IoHT systems and services is expected to be transparent to patients, making
available updated information to ensure the protection of patients’ data [ 6]. The healthcare
systems collect most of the data from the sensing devices and forward it via intermediate
devices to the management layer. During this process, several protocols and encoding
schemes are used to communicate data reliably. It is easier to ﬁnd vulnerabilities in different
components of the healthcare system by using search engines like Shodan, which assists
attackers in searching the connected vulnerable devices on the Internet [ 12]. Similarly, a
worksheet containing millions of records with user healthcare information can be exchanged
in a fraction of a second, seamlessly, and without leaving any consistent trace [10].
Some IoT-related data privacy and protection policies are used to protect data and
users’ privacy. However, these legal frameworks have not produced the intended results
and the actual level of healthcare data privacy protection is insufﬁcient for the issues
mentioned earlier [ 9]. There are also some limitations and missing aspects of healthcare
data privacy laws that do not provide a particular set of instructions to protect IoHT data.
In this paper, we discuss technological, legal, and structural problems of IoHT systems
with some analysis, and compliance issues of the healthcare data privacy and protection
framework in the developing countries. The paper uses a layered architecture to highlight
the data protection and privacy issues in the IoHT. It further identiﬁes protection and
privacy issues at different layers from technical and legal perspectives. Furthermore, the
study of different components of the existing healthcare system in terms of security related
to data protection discusses possible points of data leakage, missing aspects of healthcare
policies, and issues in the enforcement of healthcare systems and policies. To highlight
the utility of this work we compare it with recent similar works in the IoHT literature in
Table 1. The main contributions of this work can be summarized as follows:
 We deﬁne a ﬁve-layer reference architecture for IoHT, which is derived from known
architectures used in other IoHT related research articles [13–15];Appl. Sci. 2022 ,12, 1927 3 of 22
 We present IoHT classiﬁcation, identify the vulnerabilities in IoHT implementation
and map the security problem on the deﬁned ﬁve-layer IoHT architecture;
 We discuss major existing healthcare legislative and regulatory initiatives, compare
various legislative approaches and identify the gaps and governance challenges.
 We conclude with the recommendations on both frontiers (i.e., technical and legislative).
Table 1. Comparison of this work with the recent IoHT literature.
Papers with AuthorsIoHT
ArchitectureLegislative and
RegulatoryCommunication
TechnologiesStandards Security and Privacy
S. Ketu et al., 2021 [16] 4 4Challenges
4Countermeasures
4
M. Mamdouh, 2021 [17] 4 4 4
R. Somasundaram, 2021 [ 18] 4 4
R. Sivan et al., 2021 [19] 4 4 4
This Paper 4 4 4 4 4 4
The remainder of the paper is organized as follows. Section 2 introduces the Internet
of Healthcare Things (IoHT) and presents a layered architecture and working of IoHT.
Section 3 highlights the vulnerabilities in the healthcare system and identiﬁes the possible
points from where the data can be leaked. Section 4 presents various global governance
initiatives regarding privacy and data protection in IoHT and also identiﬁes the gaps and
governance challenges that hinder the widespread adoption of IoHT. Section 5 recommends
some measures for effective IoHT implementation, while Section 6 draws some conclusions.
2. Internet of Healthcare Things (IoHT)
Health management problems are increasing with the growing population, especially
with the increasingly larger aging population. Sometimes no response from the hospital
for emergencies creates social issues. Similarly, the medical staff in rural areas do not
have sufﬁcient resources for treatment and do not have the expertise to diagnose complex
diseases. Due to these reasons, people in rural areas focus on big hospitals for proper
medical attention, increasing the load on hospitals. The late detection of diseases and
severe health problems of older people also complicate the diagnosis process. Therefore,
there is a need to provide better medical facilities using an optimized healthcare system
that includes body sensors and medical devices to remotely monitor and diagnose medical
problems [20,21].
2.1. IoHT Classiﬁcation Based on Architecture
In this subsection, we present a detailed classiﬁcation of IoHT based on the IoHT
architecture in Figure 2. A generic IoHT architecture consists of IoHT devices [ 22–30],
communication protocols [ 31–36], and networks [ 37–40]. The IoHT devices are further
classiﬁed as wearable devices and implant devices. The data of IoHT devices are then
transmitted by using different communication protocols like IEEE 802.15.6, WBAN, IEEE
802.15.1 Bluetooth, IEEE 802.11 WiFi, and LoRaWAN. The destination of this medical data
is a cloud service, or a remote server designed for intensive processing tasks. Finally, data
can be transmitted to a physician or an Emergency Response Team (ERT) via a mobile
communication service such as 4G or 5G.
Once the information is collected from different IoHT devices where the sensors
exist, it travels toward the near and far edge of the network to be stored, analyzed, and
additionally processing [41].
Using a fog node network can give a health care system more computing power
that smaller and battery-oriented IoHT devices might not be able to achieve. In the IoHT
architecture, data operations, such as classiﬁcation and compression, are completed on the
intermediate devices or often single remote servers that allow for the fast processing of data
that mobile devices cannot do. In cloud-based networks, the majority of the computation isAppl. Sci. 2022 ,12, 1927 4 of 22
performed on the cloud servers because the cloud has a higher computing capacity than the
fog node networks. Cloud-based networks utilize multiple servers for parallel computing
and data analysis. Moreover, the cloud has data centers that allow for more data storage
that is sometimes needed for patient records.
Appl. Sci.  2022 , 12, 1927  4 of 23 
 
can be transmitted to a physician or an Emergency Response Team (ERT) via a mobile 
communication service such as 4G or 5G.  
 
 
Figure 2. IoHT Architecture Classification.  
Once the information is collected from different IoHT devices where the sensors exist, 
it travels toward the near and far edge of the network to be stored, analyzed , and addi-
tional ly processing [ 41]. 
Using a fog node network can give a health care system m ore computing power that 
smaller and battery -oriented IoHT devices might not be able to achieve. In the IoHT ar-
chitecture, data operations, such as classification and compression, are completed on the 
intermediate devices or often single remote servers tha t allow for the fast processing of 
data that mobile devices cannot do. In cloud -based networks, the majority of the compu-
tation is performed  on the cloud servers because the cloud has a higher computing capac-
ity than the fog node networks. Cloud -based netw orks utilize multiple servers for parallel 
computing and data analysis. Moreover, the cloud has data centers that allow for more 
data storage that is sometimes needed for patient records.  
2.2. IoHT Components  
An IoHT implementation can simply consist of a single sensor device like a heartbeat 
monitor connected to a mobile device via Bluetooth. The mobile device should have a 
specific application to generate alarms after analyzing data. It could be complex, consist-
ing of multiple sensors, intermediate device s, and centralized servers connected with a 
central management system. The medical Emergency Response Team (ERT) monitors the 
patient’s health conditions and responds in the case of any emergency.  
Figure 3 shows a generic IoHT implementation where differen t devices at different 
layers communicate with each other with the help of various protocols. Intermediate de-
vices like cell phones or PDAs communicate with sensors/actuators using short -range pro-
tocols and with healthcare servers with the help of the Inte rnet. Servers take the 
Figure 2. IoHT Architecture Classiﬁcation.
2.2. IoHT Components
An IoHT implementation can simply consist of a single sensor device like a heartbeat
monitor connected to a mobile device via Bluetooth. The mobile device should have a
speciﬁc application to generate alarms after analyzing data. It could be complex, consisting
of multiple sensors, intermediate devices, and centralized servers connected with a central
management system. The medical Emergency Response Team (ERT) monitors the patient’s
health conditions and responds in the case of any emergency.
Figure 3 shows a generic IoHT implementation where different devices at different
layers communicate with each other with the help of various protocols. Intermediate
devices like cell phones or PDAs communicate with sensors/actuators using short-range
protocols and with healthcare servers with the help of the Internet. Servers take the
appropriate actions by updating the database or contacting the relevant physicians or quick
response teams in case of an emergency. It is a simple architecture used in IoT application
development [ 41]. Each connected device has limitations like limited processing, memory,
and battery resources, etc., and the applications are developed for better utilization of these
limited resources. The reference architecture model shown in Figure 3 highlights a layered
approach for describing IoHT functionalities and their problems. In addition, the layered
architecture is used for identifying IoHT data security and privacy issues in technical and
legal aspects. This ﬁve-layer architecture helps to identify IoHT data security and privacy
issues at different layers, what type of devices and entities (medical team, physician, and
emergency services) are involved at a speciﬁc layer, and what threat vectors are involved at
each layer.Appl. Sci. 2022 ,12, 1927 5 of 22
Appl. Sci.  2022 , 12, 1927  5 of 23 
 
appropriate actions by updating the database or contacting the relevant physicians or 
quick response teams in case of an emergency. It is a simple architecture used in IoT ap-
plication development [41]. Each connected device has limit ations like limited processing, 
memory , and battery resources, etc., and the applications are developed for better utiliza-
tion of these limited resources. The reference architecture model shown in Figure 3 high-
lights a layered approach for describing IoHT functionalities and their problems. In addi-
tion, the layered architecture is used for identifying IoHT data security and privacy issues 
in technical and legal aspects. This five -layer architecture helps to identify IoHT data se-
curity and privacy issues at different layers , what type of devices and entities (medical 
team, physician, and emergency services) are involved at a specific layer , and what threat 
vectors are involved at each layer.  
 
Figure 3. Five -Layer Architecture for IoHT . 
Here, we will discuss each layer with its components.  
2.2.1. Device Layer  
Actuators are devices that can perform some actions based on the data generated by 
sensors, e.g., electronic motors, drug pumps, etc. On the other hand, according to their 
installation type, these devices  can be divided into three categories (i.e., implantable, 
wearable, and fixed devices). These categories are briefly discussed here.  
Implantable Medical Devices are implanted into the human body as shown in Figure 
4. The most common devices belonging to th is category are the Pacemaker [42], Neuro -
stimulators [43], Insulin Pumps, Glucose Monitoring Systems [44], Gastric Stimulators 
[45], Foot Drop Implants [46], Cochlear Implants [47] , and Drug Pumps [48], etc.  
Figure 3. Five-Layer Architecture for IoHT.
Here, we will discuss each layer with its components.
2.2.1. Device Layer
Actuators are devices that can perform some actions based on the data generated
by sensors, e.g., electronic motors, drug pumps, etc. On the other hand, according to
their installation type, these devices can be divided into three categories (i.e., implantable,
wearable, and ﬁxed devices). These categories are brieﬂy discussed here.
Implantable Medical Devices are implanted into the human body as shown in Figure 4.
The most common devices belonging to this category are the Pacemaker [ 42], Neuro-
stimulators [ 43], Insulin Pumps, Glucose Monitoring Systems [ 44], Gastric Stimulators [ 45],
Foot Drop Implants [46], Cochlear Implants [47], and Drug Pumps [48], etc.
Fixed Medical Devices are related to the third category of devices that can be used for
different tests in the laboratory, such as X-ray machines. The medical devices in this layer
can be classiﬁed according to their functionalities or installation type. According to their
functionalities, these devices can be divided into two types (i.e., sensors and actuators)
brieﬂy discussed here.
Sensors generate data by sensing physical parameters from their environment, e.g., tem-
perature, pressure, etc. [ 49], ventilators, and ECG machines installed in medical labs and
diagnostics rooms. These machines are not mobile because of their sizeable physical di-
mension. These machines are computerized, controlled, and connected to the networks
for remote monitoring. The data sent from these devices are not secure and prone to
data-stealing threats. It is essential to ensure the security of these devices because these
devices gather and forward the data of multiple patients on an hourly basis compared
to the devices working remotely. Therefore, these devices draw more attention from the
intruders. A summary of data transmission rate, frequency spectrum, range, etc., is given
in Table 2.Appl. Sci. 2022 ,12, 1927 6 of 22
Appl. Sci.  2022 , 12, 1927  6 of 23 
 
 
Figure 4. Implantable and Wearable IoHT Devic es. 
Fixed Medical Devices are related to the third category of devices that can be used 
for different tests in the laboratory , such as X -ray machines . The medical devices in this 
layer can be classified according to their functionalities or installation ty pe. According to 
their functionalities, these devices can be divided into two types (i.e., sensors and actua-
tors) briefly discussed here.  
Sensors generate data by sensing physical parameters from their environment, e.g., 
temperature, pressure, etc . [49], v entilators, and ECG machines installed in medical labs 
and diagnostics rooms. These machines are not mobile because of their sizeable physical 
dimension. These machines are computerized, controlled, and connected to the networks 
for remote monitoring. The data sent from these devices are not secure and prone to data -
stealing threats. It is essential to ensure the security of these devices because these devices 
gather and forward the data of multiple patients on an hourly basis compared to the de-
vices workin g remotely. Therefore, these devices draw more attention from  the intruders. 
A summary of data transmission rate, frequency spectrum, range, etc., is given in Table 2.  
Table 2. IoHT Devices Technical Details.  
IoHT Devices  Protocol  Range  Frequency  
Spectrum  Data  
Transmission 
Rate  Security  
Protocols  
Pacemaker  BLE/WiFi/Cel-
lular  400–500 
m 2.4-5 GHz, ISM 
Band, 700 –2100 
MHz  1–3 Mbit/s  Secure SDN, 
NIST Standard  
Hear Rate Mon-
itor ANT +/BLE  400 m  2.5 GHz  60 Kbps–3 Mbit/s  8-Byte Network 
Key, 128 -bit 
AES  
Temperature 
Sensor  IEEE 
802.15.4/Zigbe
e 10 m  2.4-2.48 GHz  250 Kbps  Symmetric 
Cryptography  
Figure 4. Implantable and Wearable IoHT Devices.
Table 2. IoHT Devices Technical Details.
IoHT Devices Protocol RangeFrequency
SpectrumData
Transmission RateSecurity
Protocols
Pacemaker BLE/WiFi/Cellular 400–500 m2.4-5 GHz, ISM Band,
700–2100 MHz1–3 Mbit/sSecure SDN, NIST
Standard
Hear Rate Monitor ANT +/BLE 400 m 2.5 GHz 60 Kbps–3 Mbit/s8-Byte Network Key,
128-bit AES
Temperature Sensor IEEE 802.15.4/Zigbee 10 m 2.4-2.48 GHz 250 KbpsSymmetric
Cryptography
ECG Sensor WiFi 50 m2.4–2.5 GHz, ISM
Band1–3 Mbit/s WPA-2
Blood Pressure
MonitorBluetooth 3.0 + EDR
Technology10 m 2.45 GHz 3 Mbit/sAES-CMAC
Encryption
EMG Sensors BLE 400 m 2.45 GHz 1 Mbit/sLink Layer
Encryption
PPG Sensors BLE 400 m 2.45 GHz 1 Mbit/sLink Layer
Encryption
Position Sensors BLE 400 m 2.45 GHz 1 Mbit/sLink Layer
Encryption
Cufﬂess B.P . Sensors BLE 400 m 2.45 GHz 1 Mbit/sLink Layer
Encryption
Motion Sensors Radio Frequency 150 m 433.92 MHz 10 Kbps SPECK/SIMON
Air Flow Sensors Bluetooth 3.0 100–150 m 2.45 GHz 1–3 Mbit/sSymmetric
Cryptography
2.2.2. Protocol Layer
The protocol layer consists of communication protocols and wireless standards used to
enable communication for wireless devices. The ﬁrst dedicated standard for Wireless Body
Area Networks (WBANs) is the IEEE 802.15.6 released in 2012 [ 31]. IEEE 802.15.6 is de-
signed to support both medical and non-medical applications and can be easily conﬁgured
based on application requirements [ 50]. It ensures communication inside and around the
human body and is speciﬁcally designed keeping in mind sensor devices that consume less
energy and have a low transmission range. Another upcoming standard in this context is
the European Telecommunications Standards Institute (ETSI) SmartBan [27,30] . In additionAppl. Sci. 2022 ,12, 1927 7 of 22
to IEEE 802.15.6, there are other commonly used standards and technologies such as IEEE
802.15.1 (Bluetooth), IEEE 802.15.4 (ZigBee), IEEE 802.11 (WiFi) [ 35], Bluetooth Low Energy
(BLE), NFC, LoRaWAN, UWB [ 36], RuBee [ 51], and Z-Wave [ 52]. The choice of the stan-
dard used is based on many factors like data rate, transmission range, number of devices
supported, interference due to the coexistence of different technologies, etc. The technical
details about these protocols, such as frequency, communication range, data transfer rate,
energy consumption, and its security features, are summarized in Table 3.
Table 3. IoHT Devices Technical Details.
Protocol Frequency RangeData Transfer
RateEnergy
ConsumptionSecurity
Bluetooth 2.40–2.48 GHz 10–50 m 1–3 Mbps 0.01–50 WSAFER Block
Cipher
BLE 2.40 GHz 400–1000 m 125 Kbps–2 Mbps 50–100 micro W AES-CCM Cipher
ZigBee 860 MHz–2.40 GHz 10–100 m 20–250 Kbps 10–100 micro W AES-CCM/CBC
LoRaWAN 433–923 MHz 2–7 km 27 Kbps 50–80 micro W AEA-CMAC
ANT 2.40 GHz 30 m 60 Kbps 42–72 micro W AES-CBC
UWB 4.3 GHz 10 m 1 Mbps 5.31 micro W CRC
RuBee 131 kHz 15 m 9.6 Kbps 40 nano W AES
2.2.3. Intermediate Layer
Devices in this layer have the responsibility to transmit data to potent computing
resources like cloud servers. These devices act like gateways that enable data ﬂow from
the sensor devices to the cloud or central servers for storage and further analysis. These
devices can run multiple communication protocols such as WiFi, Bluetooth, GSM, etc., [ 53]
and forward the collected data to the server. Some intermediate devices can store data as
well for pre-processing algorithms to evaluate whether the data is clinically relevant or
not [45]. Some of the intermediate devices are discussed here brieﬂy.
Smart Hub is used to make communication easy with smart IoT devices but has many
vulnerabilities that draw the attention of attackers into snifﬁng the trafﬁc of the smart
hub. If they can access networks, they will be able to spot IoT healthcare devices in the
network [54].
Access Points facilitate a wireless connection between different healthcare devices and
connect them with remote servers.
IoT Gateways primarily act as the bridge to connect sensor networks with conventional
communication networks, enabling protocol conversion and device management.
The System on Chip (SoC) is a device that integrates all components of a computer
system, helps to normalize data collected from different sensors, and controls actuators
based on applications. In this way, it reduces the load on a central server and minimizes
the communication [55].
Personal Digital Assistants (PDAs) are extensively used in the healthcare domain to
support healthcare providers [56]. Such systems receive data from different wearable and
implanted devices and process them through a variety of software applications [57].
2.2.4. Management and Data Link Layer
Management and database servers are the centralized part of the healthcare system
that receives and updates the patient’s data collected by the sensors and can also help the
physicians manage the quantity of medicine or prescribe a new one for the patient. In
emergencies, the servers send alerts to physicians and the emergency response teams for
appropriate actions.Appl. Sci. 2022 ,12, 1927 8 of 22
2.2.5. Feedback Layer
The physicians and the emergency response teams (ERT) provide feedback in any
emergency and play a signiﬁcant role in the healthcare system. At this layer, doctors and
ERT respond to the system when they receive an alert from it.
Physicians are an integral part of the healthcare system and can observe their patients
anytime, anywhere, and change treatment when required. An IoT device for a physician
gives real-time information about the patient under observation. The IoT gateway device
gathers information from different healthcare devices and forwards it to PDA devices held
by physicians.
Emergency Response Team (ERT): In case of any medical emergency, the responsibility
of the ERT is to provide medical attention to patients on-premises or remotely. In the
context of IoHT, medical devices monitor patients’ conditions and generate alerts in case of
a medical emergency. Emergency care is a critical part of medical services and is inﬂuenced
by the contextual information’s time, availability, and accuracy [58].
2.3. IoHT Working
Figure 4 presents the data workﬂow diagram of a generic IoHT implementation
system. This ﬁgure follows the conventions of a workﬂow diagram where the Start and
End processes are shown with an oval shape. Different systems processes are demonstrated
with rectangle shapes, and conditional methods are shown with the diamond shapes. The
IoHT system consists of a Sensor, Intermediate System, Actuator, Server, and ERT module.
A physician is also a part of the system that receives information from the server about a
patient’s health condition.
Figure 5 presents a three-level generic IoHT implementation framework. Level 1
consists of sensors and actuators reading physiological parameters and at times performing
interventions. The acquired data is then processed in level 2 before it is forwarded to level
3 for decision-making.
Appl. Sci.  2022 , 12, 1927 9 of 23 
 
 
Figure 5. Workflow Diagram of IoHT Implementation.  
3. IoHT Security Landscape 
IoHT security goes beyond device security, therefore, rather than only physical inter-
faces and firmware on the device, its scope must include web, mobile/cloud interfaces, 
network services, local storage, and 3rd part y APIs as well. Furthermore, diverse needs 
and varied intended usage of healthcare devices  by residential and industrial consumers 
makes it more complex [59]. A list of top ten vulnerabilities related to IoHT devices has 
been published by The Open Web Application Security Project (OWASP). This includes 
lack of authorization/insufficient authentication, insecure web interfaces, lack of transport 
encryption, insecure network services, privacy concerns, insecure cloud interfaces, inade-
quate security configuration, insecure software or firmware, poor physical security, and insecure mobile interfaces [60]. Interested readers can refer to [61,62] for detailed security 
issues concerning IoHT. 
3.1. Security Vulnerabiliti es in IoHT Implementations 
There are several known security vulnerabilities in the existing IoHT implementa-
tions [12,63]. It is essential to discuss them to get a better understanding of the resulting security problems. 
3.1.1. IoT Device’s Operating System 
Due to the specialized requirements of IoHT  devices and limitations of existing op-
erating systems, specialized op erating systems such as RIOT , Contiki, FreeRTOS, and Ti-
nyOS have been developed for IoT devices [64, 65]. The constrained computational power, 
memory, and limited power of IoT devices make them vulnerable to the system and net-
work attacks. Furthermore, constrained reso urces do not allow the implementation of 
complex encryption and authentication sche mes on these devices as they may signifi-
cantly consume the computational resources and cause a long delay, resulting in the de-
graded performance of the regular operation of these devices which is critical, especially 
for real-time IoT devices. This scenario facili tates attackers in using memory vulnerabili-
ties and compromises the security of such devices [66]. With inadequate resources on 
lightweight IoT devices, it becomes challenging to implement strong encryption and au-thentication schemes. 
3.1.2. Communication Protocols 
IoHT devices come with fewer safety checks , and it is the firmware of these devices 
that have security vulnerabilities like hardcoded keys. The urgency to roll out IoHT cloud 
platforms and the limited user experience of new IoT applications may result in the de-velopment of protocols by I.T. companies havi ng many potential security loopholes. Due 
Figure 5. Workﬂow Diagram of IoHT Implementation.
3. IoHT Security Landscape
IoHT security goes beyond device security, therefore, rather than only physical inter-
faces and ﬁrmware on the device, its scope must include web, mobile/cloud interfaces,
network services, local storage, and 3rd party APIs as well. Furthermore, diverse needs
and varied intended usage of healthcare devices by residential and industrial consumers
makes it more complex [ 59]. A list of top ten vulnerabilities related to IoHT devices has
been published by The Open Web Application Security Project (OWASP). This includes
lack of authorization/insufﬁcient authentication, insecure web interfaces, lack of transport
encryption, insecure network services, privacy concerns, insecure cloud interfaces, inade-
quate security conﬁguration, insecure software or ﬁrmware, poor physical security, andAppl. Sci. 2022 ,12, 1927 9 of 22
insecure mobile interfaces [ 60]. Interested readers can refer to [ 61,62] for detailed security
issues concerning IoHT.
3.1. Security Vulnerabilities in IoHT Implementations
There are several known security vulnerabilities in the existing IoHT implementa-
tions [ 12,63]. It is essential to discuss them to get a better understanding of the resulting
security problems.
3.1.1. IoT Device’s Operating System
Due to the specialized requirements of IoHT devices and limitations of existing oper-
ating systems, specialized operating systems such as RIOT, Contiki, FreeRTOS, and TinyOS
have been developed for IoT devices [ 64,65]. The constrained computational power, mem-
ory, and limited power of IoT devices make them vulnerable to the system and network
attacks. Furthermore, constrained resources do not allow the implementation of complex
encryption and authentication schemes on these devices as they may signiﬁcantly consume
the computational resources and cause a long delay, resulting in the degraded performance
of the regular operation of these devices which is critical, especially for real-time IoT de-
vices. This scenario facilitates attackers in using memory vulnerabilities and compromises
the security of such devices [ 66]. With inadequate resources on lightweight IoT devices, it
becomes challenging to implement strong encryption and authentication schemes.
3.1.2. Communication Protocols
IoHT devices come with fewer safety checks, and it is the ﬁrmware of these devices
that have security vulnerabilities like hardcoded keys. The urgency to roll out IoHT
cloud platforms and the limited user experience of new IoT applications may result in the
development of protocols by I.T. companies having many potential security loopholes. Due
to the diversity of IoT devices, it is not easy to develop a standard security protocol for
heterogeneous devices and it leads to problems of how to discover and urgently address
the security vulnerabilities among IoT devices.
3.1.3. Insecure Middleware
To make IoT applications development more manageable, various IoT middleware
platforms have been developed. These platforms offer distributed system services with
standard programming interfaces and protocols and minimize problems associated with
heterogeneity, distribution, and scale in IoT applications development. These services are
called ‘middleware’ as they sit ‘in the middle’, in a layer above the operating system and
networking software and below domain-speciﬁc applications [67,68].
3.2. Possible Points for Data Leakage
In the healthcare system, there are two states of data from where it can be stolen.
These states are the persistent data (i.e., data at rest) and transient data (i.e., in motion).
The overall description of data leakage points in a healthcare system is given in Figure 5.
The arrows with dotted lines represent transient data whereas the arrows with solid lines
represent persistent data. The details of each state are as follows.
3.2.1. Persistent Data
Persistent data refers to the data stored on different healthcare system components,
such as sensing and actuating devices, etc., and is prone to theft. Here, the features of the
healthcare system where the data can be stolen are discussed.
Healthcare devices such as sensors and actuators store the events logs (latest reading,
conﬁguration change, connection history, etc.) in the memory. As these devices usually
have weak security conﬁgurations and store data in non-encrypted forms, an attacker can
steal the data by exploiting these weaknesses.Appl. Sci. 2022 ,12, 1927 10 of 22
IoHT servers are the most attractive components of healthcare systems for attackers to
steal health care data from as they contain the complete history of all the patients with their
biodata.
Physicians/Response Team devices are mostly mobile devices that physicians or
response teams use to monitor and process healthcare data. These devices are susceptible
to memory leakage attacks. Personal computers, mobile phones, tablets, or any speciﬁc
devices to monitor healthcare data fall into this category. Due to direct communication
with IoHT data servers, these devices are highly vulnerable to different security attacks.
Figure 6 is presenting attacks an attacker can execute on devices operating at different
layers. Effect of one compromised device can propagate at different layers because these
devices are communicating with each other.
Appl. Sci.  2022 , 12, 1927  11 of 23 
 
 
Figure 6. Possible Sites for Data Leakage in an IoHT implementation . 
3.2.2. Transient Data  
Transient data refers to the data on the move, as in the IoHT implementations, the 
information is tran sferred from devices to servers using different protocols and network-
ing devices. Here, we discuss such devices and protocols from where the data can be sto-
len during transmission.  
Communication protocols are used to transmit data from an IoT device to a d evice. 
IoT devices are primarily lightweight, low computational, and battery -oriented. Low -
range protocols with weak security features transfer data from IoT devices to a gateway 
device.  
Intermediate devices collect data from end devices using a specific p rotocol  to aggre-
gate and transform this data into another format that can be underst ood by the next de-
vice. A mobile phone or a PDA connected to the wear er acts like an intermediate device. 
These intermediate devices use multiple protocols to communicate with IoT endpoint de-
vices, data servers, and monitoring devices. These protocols have security issues; IoT gate-
ways, smart hubs, mobile phones, and WiFi dev ices enable IoT devices to connect with 
the Internet. These devices are limited in storage and are not used to store data ; therefore , 
they  forward it as it is received by changing its format. These devices communicate with 
IoT devices with different design s like LoRaWAN, ZigBee, Bluetooth, etc., but forward 
this collected data by using TCP/IP or UDP to the Internet.  
Access Points/Smart Hubs/IoT Gateways enable communication between IoT and 
data servers. They are not storage devices; however, they forward re ceived data to the 
intended destination. Due to the contact with different types of IoT devices, these devices 
operate multiple protocols. Security features like encryption, hashing, and password 
checking are used with these protocols. However, there are n o established guidelines to 
comply with any healthcare laws describing the security levels needed for IoT healthcare 
intermediate devices.  
3.3. Essential Security Features of IoHT  
Following are some of the essential security features of IoHT.  
Use of Standa rdized Devices: To ensure data privacy and security in healthcare sys-
tems, it is essential to use standardized devices with reliable security features. These de-
vices should meet healthcare standards and policies. Data storage, transmission, format 
intercha nges, terminologies, and presentation standards should be well -defined. No med-
ical devices, either implantable or wearable, are allowed to be manufacture d if they do 
not meet data standards.  
Figure 6. Possible Sites for Data Leakage in an IoHT implementation.
3.2.2. Transient Data
Transient data refers to the data on the move, as in the IoHT implementations, the
information is transferred from devices to servers using different protocols and networking
devices. Here, we discuss such devices and protocols from where the data can be stolen
during transmission.
Communication protocols are used to transmit data from an IoT device to a device.
IoT devices are primarily lightweight, low computational, and battery-oriented. Low-range
protocols with weak security features transfer data from IoT devices to a gateway device.
Intermediate devices collect data from end devices using a speciﬁc protocol to ag-
gregate and transform this data into another format that can be understood by the next
device. A mobile phone or a PDA connected to the wearer acts like an intermediate device.
These intermediate devices use multiple protocols to communicate with IoT endpoint
devices, data servers, and monitoring devices. These protocols have security issues; IoT
gateways, smart hubs, mobile phones, and WiFi devices enable IoT devices to connect with
the Internet. These devices are limited in storage and are not used to store data; therefore,
they forward it as it is received by changing its format. These devices communicate with
IoT devices with different designs like LoRaWAN, ZigBee, Bluetooth, etc., but forward this
collected data by using TCP/IP or UDP to the Internet.
Access Points/Smart Hubs/IoT Gateways enable communication between IoT and
data servers. They are not storage devices; however, they forward received data to the
intended destination. Due to the contact with different types of IoT devices, these devices
operate multiple protocols. Security features like encryption, hashing, and password
checking are used with these protocols. However, there are no established guidelines toAppl. Sci. 2022 ,12, 1927 11 of 22
comply with any healthcare laws describing the security levels needed for IoT healthcare
intermediate devices.
3.3. Essential Security Features of IoHT
Following are some of the essential security features of IoHT.
Use of Standardized Devices: To ensure data privacy and security in healthcare sys-
tems, it is essential to use standardized devices with reliable security features. These
devices should meet healthcare standards and policies. Data storage, transmission, format
interchanges, terminologies, and presentation standards should be well-deﬁned. No medi-
cal devices, either implantable or wearable, are allowed to be manufactured if they do not
meet data standards.
Log Management and Compliance: Every management activity performed on these
healthcare devices should be appropriately logged on the healthcare servers. For data
privacy, there is a need to know who is accessing what data from which system. The best
practices are gathering log ﬁles and saving them for future references as long as you need
them. In distributed systems, these reports should be shared with other systems to prove
compliance.
Updating Technologies: Technologies are considered the workforce behind the ad-
vancement in healthcare systems, and recent innovations have proven this. Technologies
have changed the healthcare system to increase the accessibility of treatment that leads
to improved care and efﬁciency. The latest components are more efﬁcient and secure as
compared to the previous ones. Therefore, the new devices with the latest technologies
should be preferred over the old ones.
Patch Management: In addition to upgrading hardware, updating the software of IoT
device’s ﬁrmware is also essential; if it is not updated, it may be vulnerable. The software
vendors release patches from time to time to overcome the ﬂaws and loopholes. Every
device which is connected to the Internet needs to be updated, whether it is a personal
computer, smartphone, or any other device such as IoT [20].
User’s Proﬁling: The users should be identiﬁed and authenticated; the system must
know what resources a user can access. This process is called authorization. This process
ensures that the behavior of the users is constrained according to the user’s permission.
User proﬁling is more critical in healthcare applications. Healthcare organizations must
comply with international standards and procedures regarding user authentication and
authorization. The exchange of patient data must permanently preserve privacy constraints
with professional liabilities.
Generate Alerts: There should be some proper mechanism for IoHT to generate alerts
for any kind of malicious event. Intermediate devices can play an essential role in this
regard with the help of some applications. Warnings should be raised for critical events and
emergencies. These devices can analyze received data; after the detection of any malicious
activity, alerts can be directly transmitted to the monitoring server or system. Availability
of IoHT should be the top priority; if any critical IoHT device goes off for any reason,
intermediate devices communicate with them directly; therefore, generating alerts by these
devices could help the emergency response team to respond more quickly.
Fines and Penalties: A mechanism should be implemented in the healthcare sector
against attackers that attack healthcare systems and devices and try to leak or steal health-
care information. Healthcare data is conﬁdential, and if an attacker tries to breach the
healthcare system for the purpose of data-stealing by any means they should be charged
with penalties or imprisonment.
4. Healthcare Data Protection Legislation and Framework
In developed countries, there are data privacy and protection laws implemented to
securely process citizens’ personal data. In this section, we discuss some of them brieﬂy to
highlight their aims and features. These laws and legislations are compared with each other
to see how they provide security features at different layers of the proposed healthcareAppl. Sci. 2022 ,12, 1927 12 of 22
architecture. Implementation of IoT data privacy regulations is very important for legal
matters, human rights, and social norms. Generally, IoT data privacy regulations are re-
quired to support core privacy goals like fairness, purpose satisfaction, proportionality, and
accountability. Government and private organizations can work together to achieve these
goals. American, European, and other leading countries’ law enforcement organizations are
working to ﬁnd a common ground for solving healthcare data privacy problems while also
making a more effective existing legal framework. An effective legal framework should
ensure the user’s awareness and their control over the IoT healthcare products with their
services. Compliance with other international data privacy frameworks makes it more
adequate [6,69].
4.1. Major Healthcare Initiative
A brief overview of healthcare initiatives related to data protection, for different
regions, is given below.
4.1.1. Health Insurance Portability and Accountability Act (HIPAA)
HIPAA was enacted by the US government to implement the security and privacy
of healthcare data for American citizens. It has separate rule sets for security and privacy.
The privacy rules enable the privacy of the health data to protect the data from disclosure.
The security rules provide security of the individuals’ health information by adopting
advanced technologies to acquire more efﬁcient means of patient care. The HIPAA security
and privacy rules are implemented to healthcare and non-healthcare organizations that
store, transmit, and process healthcare data of US citizens by any means.
The privacy rule protects the following health information processed by concerned entities:
 Common Identiﬁer (e.g., name, address, birth date);
 Past, present, or future physical and mental health or condition;
 Past, present, or future payment provision for healthcare;
 Provision of healthcare to individuals.
The mentioned entities are allowed to be processed or disclosed for research and
public interest and should have the authorization to process or disclose health information
except for treatment purposes, like payment and relevant health care operations.
The Security Rule protects personal health information that has been created, re-
ceived, transmitted, and managed electronically. These security rules have the following
characteristics:
 Making sure of the integrity and availability of personal healthcare information;
 Detects and protects against known threats to conﬁdentiality, integrity, and availability;
 Protect against processes not permitted and the disclosure of information.
The security rule requires the administrative protection of management processes
personnel. Proper administrative controls should be maintained for devices and personnel.
This rule also includes audit control, access control, integrity control, and transmission
security.
4.1.2. The Health Information Technology for Economic and Clinical Health Act (HITECH)
The Health Information Technology for Economic and Clinical Health Act (HITECH)
is an extension of security and privacy guidelines described in HIPAA signed in 2009.
However, it has not been completely enforced in the healthcare industry. It only provides
legal liability for noncompliance. Besides applying the HIPAA rules, it is responsible for the
notiﬁcation of breaches and unauthorized access to healthcare data. It enables individuals
with a right to get their electronic healthcare data and they can grant privileges to others to
receive healthcare information [70].Appl. Sci. 2022 ,12, 1927 13 of 22
4.1.3. Personal Information Protection and Electronic Document Act (PIPEDA)
The federal government of Canada introduced the Personal Information Protection
and Electronic Document Act (PIPEDA) in 2001. Its main purpose is to provide individuals
control, to some extent, over their personal information by enforcing policies on organiza-
tions that process and disclose the personal information of Canadian citizens. These policies
include informing citizens concerning personal information processed or disclosed and
this information is protected by using adequate security measures. The PIPEDA applies
to all personal data, healthcare data, and other data that holds the private information of
individuals. If any organization collects data, then it is fully accountable for the protection
of the collected data. The PIPEDA is not applicable in all states of Canada. Every province
has the right to enforce rules and policies as long as they are similar to PIPEDA [ 71].
PIPEDA provides basic guidelines to organizations for collecting and processing personal
information for business purposes. The interconnected devices should use the security
standards of data privacy principles according to PIPEDA.
4.1.4. EUROHEALTH
In late 1990, the WHO Regional Committee for Europe established a healthcare pro-
gram called EUROHEALTH. It provides medium-term needs for the Countries of Central
and Eastern Europe (CCEE) for material, managerial, and technical resources for long-term
health strategies. The primary objectives of this program are to make collaborative agree-
ments with CCEE, fundraising, and coordination with international organizations. The
program’s work has generated an information ﬂow for better cooperation and coordination
between all the organizations working in the health sector. This program is realistic, ﬂexible,
and accessible to the CCEE [72].
4.1.5. General Data Protection Regulation (GDPR)
In May 2018, a new General Data Protection Regulation (GDPR) replaced directive
95/46 [ 73], consolidating and innovating data protection rules. The introduced GDPR is
considerably more comprehensive and establishes requirements for internal compliance
mechanisms that did not exist in the legislation. It applies to all sectors of the economy, all
broadly deﬁned personal data, and every sector that controls or processes data. Moreover,
it applies protective standards throughout the lifespan of the data. GDPR is designed to
enable people to better control their data. Although the EU already established its data
protection directive in 1995, it was not completely reasonable for all the member states of
the EU. To remove all the reservations, the GDPR has been established and is applicable
throughout the EU.
The liability of the GDPR’s impact is likely to be for health organizations, hospitals, and
other healthcare organizations that process healthcare data. GDPR requires accountability
for how healthcare data is processed. Data controllers are not only responsible for the
compliance, but processors are also accountable for any data breach of their work and
direction. Furthermore, health organizations will need to be clearer and attentive in
proﬁling data processing activities and will require their staff to ensure a secure ﬂow of
data [74].
4.1.6. The Privacy Act (Australia)
It entails the set of principles of Australian legislation to protect the personal infor-
mation of Australian citizens. These principles refer to the usage, storage, and disclosure
of personal information. Moreover, individuals have the right to the access and correc-
tion of their personal information. This law also includes data security, data quality, and
cross-border data ﬂow policies [75].
Healthcare data is a subset of personal information and needs extra security policies
and protection. All organizations that provide a health service and hold health information
(other than an employee record) are covered by the Privacy Act, whether or not they are
small businesses. In certain circumstances, the Privacy Act permits the handling of healthAppl. Sci. 2022 ,12, 1927 14 of 22
information and personal information for health and medical research purposes, where it
is impracticable for researchers to obtain individuals’ consent.
4.1.7. Saudi Health Information Exchange Policies (SHIEP)
The main objective of this policy is to present the permissible usage of the KSA
(Kingdom of Saudi Arabia) health information exchange like patient care, public health, and
quality. This policy applies to all individuals and organizations who have access to the Saudi
Health Information Exchange managed records such as participating healthcare subscribers,
business associates, health information services providers, and subcontractors [76].
Personal Health Information (PHI) will be available for treatment, healthcare op-
erations, and public health, but it may be permitted for research and education. This
policy shall not permit the usage of healthcare information for market studies and legal
investigation or inquiry.
Table 4 presents a comparison of regulations in light of the proposed ﬁve layers
architecture for IoHT. It is interesting to observe that none of the above regulations are
providing security at the intermediate and feedback layer. However, both these layers
are vulnerable to data leakage threats due to human intervention accessibility to physical
devices.
Table 4. Comparison of Healthcare Regulations.
RegulationCountry/
RegionDevice
LayerProtocol
LayerIntermediate
LayerManagement
LayerFeedback
Layer
HIPAA America Yes Yes No Yes No
PIPEDA Canada Yes No No No No
EURO
HEALTHEU Yes No No No No
GDPR EU Yes Yes No No No
The Privacy Act Australia Yes Yes No No No
SHIEP KSA Yes Yes No Yes No
4.2. Data Protection Issues/Aspects Not Covered in Healthcare Laws
There are various issues and aspects of IoHT that are not covered in the already
discussed legislative initiatives.
Big Data Issues Integrating healthcare data notably raises security and privacy issues.
Patient information is processed at different levels of security in data centers. In America,
most organizations have HIPAA certiﬁcation, but this does not guarantee a patient’s record
safety because HIPAA is more inclined towards ensuring security policies rather than
implementation. Moreover, the transmission of large data sets from different locations
poses an extra burden on processing and storage. Conventional security solutions are
inadequate for large and inherently changing data sets. With the emergence of cloud
healthcare solutions, security demands are becoming more complex and there are no
speciﬁed policies written in existing healthcare data protection laws [77].
Data Governance related to the governance of healthcare data should be the initial
step in managing healthcare data. It is due to the need of moving the healthcare industry
towards a value-based business model. It demands common data representation that
encircles different security standards (e.g., ICD, CPT, and LOINC) [ 78]. Currently, data
generated in the healthcare industry is diverse and would demand a proper governance
model. There are no policies for healthcare data standardization and normalization for
proper data governance.
Privacy-preserving analytics in the healthcare industry is grasping IoT devices to
monitor and transmit vitals to healthcare clouds. Therefore, it needs to process and analyze
data in an ad-hoc decentralized manner. However, the execution of resource-exhausting
operations with privacy preservation is becoming a challenge. As new healthcare dataAppl. Sci. 2022 ,12, 1927 15 of 22
analytics are becoming popular, healthcare data privacy laws need revision, and new laws
should be drafted to illustrate all processes involved in the usage of patients’ private data.
Identiﬁcation of the Relevant Privacy Violation privacy in IoT-based devices can be
violated at many stages. Firstly, it is violated by collecting data by third parties. Secondly,
the usage and distribution of private data, and thirdly, the data is combined with other
data. The third possibility is not known by the users who are using IoT-based medical
devices to process and generate data. By combining newly generated information with
existing data about a patient or health activity, it raises the high commercial value for many
data-hungry organizations and commercial ﬁrms. Most of the data is often generated by
automated medical devices, therefore, a higher trust level should be maintained for this
data than on manually entered human data. This is important because medical insurance
companies are monitoring the health conditions of their customers with the help of medical
devices to ascertain the speciﬁc risks associated with their customer’s health. These devices
are also tracking users’ geo-locations and such data must be protected through adequate
device safety measures as well as legislative limitations on data usage.
Data and context quality are mostly overlooked issues even if these facts play a
signiﬁcant role in the privacy debates in the context of IoT. The quality of data highly
depends on the environment in which it is collected. The quality of context may be
unknown where there is no or incomplete information about the context. It may also be
ambivalent as there is a chance of contradictory information from different context sources.
Context quality generates new problems of conﬁdentiality that have not been addressed by
current research. Context quality is related to the information that is not to be processed by
hardware components that likely provide the information. It is better to protect context
quality as it is sensitive information. Change in context quality is also sensitive information.
IoT devices generate data based on context and do not allow users to shut down the system
or to easily disconnect from it.
To enhance transparency of the healthcare systems, not only healthcare data that
is propagated from different devices need to be controlled, but also the data generated
automatically by the healthcare devices need to be managed. Despite this important issue,
no law has been made in this regard. There is a need to develop a combined approach with
technical standards and existing regulatory frameworks to ensure data transparency.
Privacy violating interactions and presentation in IoT-based healthcare applications
like heartbeat monitors, geo-tracking devices, automated insulin pumps, and other health-
care devices envisage and require strong interaction with the patient. In such devices, the
information will be provided through sensors or other recorded medical device readings.
This information goes through different devices to reach its ultimate destination and be-
comes a threat to privacy when this sensitive information is exchanged through different
systems. In smart cities, for instance, an individual could make a query for the way to a
speciﬁc health clinic. Such a query should not be answered, for instance, by showing the
way to a health clinic nearby, visible to any passerby, another example of such medical
devices that do not encrypt data while transmitting to the remote server. Any adversary
intending to sniff that data could easily use this information for a malicious purpose. Due to
the close interaction and presentation techniques, the threat of privacy-violating interaction
and presentation is a major challenge in healthcare laws.
Life cycle transition privacy is compromised when private information is disclosed
by IoT devices during the life cycle transition. These devices hold information like vital
sign readings, drug dosage, and actuator functions. Healthcare data is highly sensitive, but
also the collection of simple usage data (e.g., location, duration, frequency) could disclose a
lot about the life cycle of people. Despite evident problems with healthcare devices, the
life cycle transition problem has never been addressed. The life cycle of healthcare devices
is still modeled as buy-once-own-forever and solutions have not evolved beyond a total
memory wipe (e.g., before selling a wearable) or physical destruction. There is a need to
identify the requirements for ﬂexible solutions to implement convenient privacy life cycle
management mechanisms.Appl. Sci. 2022 ,12, 1927 16 of 22
Linkage refers to the linkage of different previously separated systems like combin-
ing forms of revealed data sources. When data is gathered from different sources with
different permissions and contexts it causes loss of context and poor judgment. Threats
of linking different systems and data sources are not novel. Online social networks and
integrated third-party applications are facing the same problems. However, IoT networks
and services rely on the interaction and collaboration of many coequal systems. Managing
the numerous devices in IoHT systems and their connectivity with other systems will
raise more challenges in linkage threats. The threat of linkage will cause problems in the
IoT evolution process. There are mainly two reasons for it. First, the horizontal linkage
of different companies and manufacturers systems to create a heterogeneous distributed
system-of-systems delivering new services that no single system can provide. Successful
linkage will make data exchange more agile and controllable between different parties.
However, horizontal linkage also causes more local data ﬂows than vertical linkage that
could improve privacy. These problems should be properly addressed in IoT healthcare
laws to prevent passive monitoring and intrusive data collection by IoT devices.
4.3. IoHT Governance Challenges
Based on the analysis of the major global healthcare initiatives, we identiﬁed the
following IoHT governance challenges that hinder the widespread adoption of IoHT
systems.
4.3.1. Conﬂicts in Laws
After the implementation of the General Data Protection Regulation (GDPR) for the
enforcement of data security and privacy, standards have been widened. For the protection
of European citizens’ data outside of Europe, their data is allowed to be used under strict
conditions: If any data privacy legislation enacted by a non-EU country accepts Standard
Contractual Clauses (SCC) and Binding Corporate Rules (BCR). These terms and conditions
cause challenges for developing countries because BCR’s and SCC are time-consuming and
costly [79].
A country may or may not have a law for user data protection, but the healthcare
system should follow the laws of the country from which the user belongs. The same thing
applies to the social and cultural norms, which should be applied according to the laws of
the state to which the user belongs. In the case of data residing on cloud servers, again, the
privacy laws will be applied according to the region of the patients.
4.3.2. Data Protection Issues in Healthcare Systems
The above discussion leads us to ﬁnd the four major issues in protecting healthcare
data from the national policy level to an organizational level. These issues are (i) the
absence of laws for healthcare data protection, (ii) the use of non-standard healthcare
devices and communication protocols by a healthcare facility, (iii) compliance issues to
implement healthcare policies locally, (iv) and the absence of a dedicated enforcement
agency for inspection and to deal with complaints and violations [80].
4.3.3. Absence of Conﬂict of Laws for Healthcare Data Protection
As we have discussed, the most common reason for violations in healthcare laws
is the absence of law in many developing countries. There is no law for healthcare data
protection, therefore, patients’ data can be used by the government and private agencies.
They use patients’ medical records for research purposes without their consent. No mon-
itoring facility can ensure the integrity of the medical records after being used by these
organizations.
4.3.4. Non-Standard Healthcare Devices and Protocols
The development and production process of various smart devices over a short period
reduces the security considerations. Small businesses have less expertise and resources inAppl. Sci. 2022 ,12, 1927 17 of 22
terms of security implementations and testing the devices as well as applications for the
healthcare domain. Usage of non-standard healthcare devices and protocols is the major
concern of healthcare data privacy and protection, which, in context, is the usage of locally
manufactured healthcare devices that do not have sufﬁcient security features according
to healthcare standards. Most of these devices are unable to update or patch against new
threats and attacks. There must be some laws and policies included in these healthcare care
laws that declare guidelines for manufacturing healthcare devices and security features
must be included in them [81].
4.3.5. Other Considerations
Apart from the above-mentioned challenges, there are several local issues worth
consideration for the adoption of IoHT governance initiatives.
 Awareness: There is a lack of awareness among the users of healthcare systems about
the importance of the security of healthcare data;
 System Management Staff: Most of the time healthcare data is processed and for-
warded by system management staff in healthcare institutes. They work on inter-
mediate devices like data servers and gateway devices. The staff is not qualiﬁed
enough to understand the complexities of data privacy and legal aspects. They are
unaware of the latest threats because their primary tasks are network conﬁgurations
and the availability of data at the endpoints. Mostly, they do not update healthcare
systems and leave them unpatched until an issue is raised. They have little exposure
to awareness programs and practices for data security and privacy;
 Doctors and Healthcare staff: One of the critical facts is not knowing much about the
security and privacy laws of healthcare data by the healthcare staff and emergency
response teams in hospitals. Speciﬁcally, the doctor uses personal devices such as cell
phones or laptops to view the data, therefore, these personal devices also need to be
secure. Moreover, they are not even familiar with the consequences of healthcare data
leakage. There is no proper framework implemented that enforces healthcare staff to
follow rules and policies to share and process patients’ healthcare data legally and
securely;
 Patients: These are the central entities in the healthcare system. However, they are
less attractive to attackers due to having less information i.e., about themselves only.
They are conscious about their privacy and all these legalities are there to protect their
healthcare privacy. However, they do not have any idea about how their data is shared
with other organizations and what their rights are regarding their healthcare data. If
their data is shared without conferring with them, it begs the question of what the
legal liberties are that can be taken by concerned organizations about data sharing;
 Enforcement Difﬁculties: There is no enforcement authority or body established that
helps to enforce data privacy laws in the healthcare sector. All the healthcare institutes
should be obliged to follow instructions by some authority. The authority should
implement healthcare laws;
 Low Budget: In developing countries, the medical budget is very limited. The trend
of using IoHT devices is emerging in big cities. Mostly, there is no central system
or facility provided to facilitate data privacy in developing countries. A very low
or limited budget is allocated for new and innovative technologies in the healthcare
sector. Therefore, the authorities consider that there is no need to enforce data privacy
policies if the usage of such devices is limited;
 Lack of Qualiﬁed Staff: The IT staff does not conﬁgure/enable security functions in
IoHT due to inadequate qualiﬁcations and expertise. The main reason behind it is
the lack of security training programs for medical staff. They are only interested in
the functional requirements of medical devices but do not take care of non-functional
requirements of a medical device like communication security and data privacy;
 No Internal Auditing: There is no IoHT audit like IT audit, conducted in healthcare
organizations and hospitals. If any organization is processing healthcare data, then itAppl. Sci. 2022 ,12, 1927 18 of 22
is unlikely to make checks and balance the healthcare data. The internal audit ensures
what data is being processed by which organization for what purposes. The auditing
activities reduce the risk of data leakage and unauthorized usage;
 No Special Enforcement Authority: One of the difﬁculties in the enforcement of data
privacy laws and regulations is the absence of special enforcement authorities in the
healthcare sector. Formation and implementation of data privacy laws should be
the primary responsibilities of the authority. It can also ensure the compliance of
state-level or national-level policies with international policies.
5. Recommendations
After careful analysis of data privacy and protection policies and compliance issues
for IoHT-based systems, some recommendations can assist in improving this system. These
recommendations assist in resolving healthcare data security problems due to compliance
issues between different legal frameworks.
 Fines and Penalties: IoT devices gather a huge amount of information and there are
several privacy risks associated with the usage and access of the data. Speciﬁcally,
individual identiﬁcation and behavior monitoring are major concerns. As the usage
of IoT devices is increasing in the healthcare sector, a huge amount of private data
is processed and stored. There is a need to introduce new privacy safeguards. The
health information collected from devices like Fitbit/Jawbone [ 82] can be used to
detect disease correlations with new treatment options with remote monitoring;
 Data Anonymization: IoT devices gather most of the data aggregated from the envi-
ronment and forward it via a router or intermediate device for processing. During
this process, several protocols and compression schemes are used as the storage space
on the devices is limited and cannot handle big headers like that used for Internet
Protocol IPv6. This data is sanitized as closely as possible to the device that created it
since this communication avoids safety risks;
 Healthcare System Design: The healthcare system should be designed in such a way
that it provides the controls in a user-friendly manner. An end-user must have full
control over his/her collected data at any moment i.e., to whom it can be or cannot be
shared. At any moment, the user should be given the possibility to know and control
who has his data, what data have been collected, and for what purposes they will be
used for the legitimate initial purpose;
 Privacy by Design: Privacy embedded into the design is an essential component
integrated into the whole IoHT core system. The privacy safeguard framework must
be implemented from the beginning of the system engineering process. The health-
care devices operate with user interactions or web interfaces. There are no privacy
protection guidelines available while designing device interfaces. There are several
vulnerabilities in web-based interfaces that are prone to data leakage and information
leakage attacks. Most of the devices do not have authentication features or have
default passwords that are difﬁcult to enter due to their small size interfaces;
 Communication Security: There are several communication protocols used in IoT
healthcare devices. There are no speciﬁc guidelines provided in data privacy laws
about protocol security or what type of encryption or anonymity standards should be
adopted for IoT devices, which operate on low memory and computation resources.
These privacy laws should provide transparent policies about the communication
security of these devices, especially for use in hospitals;
 Dispute Resolution: There is a need to resolve regional and international disputes
regarding data protection. There are different versions of healthcare data privacy laws
enforced regionally and internationally. If the healthcare data of a citizen is processed
in a different country or state where different data privacy laws are enforced, then
what are the possible legal issues that should apply to that person’s processed data?
These types of disputes should be resolved in the national healthcare policies;Appl. Sci. 2022 ,12, 1927 19 of 22
 Awareness Programs: Awareness programs are very signiﬁcant to highlight the im-
portance of data privacy, especially in the healthcare sector. IT staff, management
staff, and other related staff of a healthcare facility should be aware and carry out
the practices of secure processing of healthcare data. They must be aware of the
consequences in the case of data leakage and what penalties they would be charged in
the case of carelessness. Doctor and emergency response teams should be trained for
the secure usage of their devices (i.e., laptops and cellphones, etc.) linked to healthcare
systems, and they should share their experiences and difﬁculties while using these
devices securely with healthcare organizations.
6. Conclusions
In this paper, we presented a detailed IoHT classiﬁcation along with the general
architecture of IoHT systems. The architecture aids in highlighting data security and
privacy challenges. Along with discussion about the known security vulnerabilities, we
also discussed the healthcare regulations and afﬁliated issues. We also highlighted major
reasons causing the failure of data protection and possible points of data leakage in IoHT
systems. The study also discussed and compared various data protection regulations and
highlighted their limitations. Lastly, we proposed some recommendations regarding data
privacy and security for IoHT implementations. We believe this research will help the
industrial and governing bodies to design and implement IoT-enabled healthcare systems
while protecting the security and privacy of individuals.
As future work, we plan to explore cybersecurity risk assessment approaches with
respect to IoHT to aid organizations and governments in better protecting themselves
against pertinent risks. The risk with simply extending existing assessment methodologies
will be possibly being blind to new risks arising in the healthcare ecosystem. These risks
could be related to the high sensitivity of healthcare data, the ﬂow of information, and
compliance with regional and global regulatory approaches.
Author Contributions: J.S. and R.A. were involved in the conceptualization of the key idea; A.K.K.
and T.A. helped with the methodology and deﬁning the contents of the paper; J.S., R.A., A.K.K. and
T.A. were involved in the writing—original draft preparation; J.S., R.A., A.K.K., T.A., S.S. and A.M.A.
were involved in reviewing and editing. The improvements in the write-up were contributed to by
all. All authors have read and agreed to the published version of the manuscript.
Funding: The authors would like to thank SAUDI ARAMCO Cybersecurity Chair, Imam Abdulrah-
man Bin Faisal University for funding this project.
Institutional Review Board Statement: Not applicable.
Informed Consent Statement: Not applicable.
Data Availability Statement: Not applicable.
Conﬂicts of Interest: The authors declare no conﬂict of interest.
References
1. International Telecommunication Union Yearbook of Statistics, 1991–2000 ; ITU: Geneva, Switzerland, 2001.
2. Ahmad, T.; Ranise, S. Validating Requirements of Access Control for Cloud-Edge IoT Solutions (Short Paper). In International
Symposium on Foundations and Practice of Security ; Springer: Cham, Switzerland, 2018.
3. Culler, D.; Chakrabarti, S.; Infusion, I.P . 6LoWPAN: Incorporating IEEE 802.15. 4 into the IP Architecture, IPSO Alliance; White
Paper. 2009. Available online: https://www.omaspecworks.org/wp-content/uploads/2018/03/6lowpan.pdf (accessed on 1
October 2021).
4. Al Alkeem, E.; Yeun, C.Y.; Zemerly, M.J. Security and privacy framework for ubiquitous healthcare IoT devices. In Proceedings of
the 10th IEEE International Conference for Internet Technology and Secured Transactions (ICITST), London, UK, 8–10 December
2015; pp. 70–75.
5. Miorandi, D.; Sicari, S.; de Pellegrini, F.; Chlamtac, I. Internet of things: Vision, applications and research challenges. Ad. Hoc.
Netw. 2012 ,10, 1497–1516. [CrossRef]
6. Porambage, P .; Ylianttila, M.; Schmitt, C.; Kumar, P .; Gurtov, A.; Vasilakos, A.V . The quest for privacy in the internet of things.
IEEE Cloud Comput. 2016 ,3, 36–45. [CrossRef]Appl. Sci. 2022 ,12, 1927 20 of 22
7. Solanas, A.; Patsakis, C.; Conti, M.; Vlachos, I.S.; Ramos, V .; Falcone, F.; Postolache, O.; P érez-Mart ínez, P .A.; Di Pietro, R.; Perrea,
D.N.; et al. Smart health: A context-aware health paradigm within smart cities. IEEE Commun. Mag. 2014 ,52, 74–81. [CrossRef]
8. Martinz-Ballest, A.; P érez-Mart ínez, P .A.; Solanas, A. The pursuit of citizens’ privacy: A privacy-aware smart city is possible.
IEEE Commun. Mag. 2013 ,51, 136–141. [CrossRef]
9. Eckhoff, D.; Wagner, I. Privacy in the smart city- applications, technologies, challenges, and solutions. IEEE Commun. Surv. Tutor.
2017 ,20, 489–516. [CrossRef]
10. Alghanim, A.A.; Rahman, S.M.M.; Hossain, M.A. Privacy analysis of smart city healthcare services. In Proceedings of the 2017
IEEE International Symposium on Multimedia (ISM), Taichung, Taiwan, 11–13 December 2017; pp. 394–398.
11. Storm, D. MEDJACK: Hackers Hijacking Medical Devices to Create Backdoors in Hospital Networks Computer World. 2015.
Available online: https://www.computerworld.com/article/2932371/medjack-hackers-hijacking-medical-devices-to-create-
backdoors-in-hospital-networks.html (accessed on 1 October 2021).
12. McMahon, E.; Williams, R.; El, M.; Samtani, S.; Patton, M.; Chen, H. Assessing medical device vulnerabilities on the Internet of
Things. In Proceedings of the IEEE International Conference on Intelligence and Security Informatics (ISI), Beijing, China, 22–24
July 2017; pp. 176–178.
13. Wang, L.; Ali, Y.; Nazir, S.; Niazi, M. ISA evaluation framework for security of internet of health things system using AHP-TOPSIS
methods. IEEE Access 2020 ,8, 152316–152332. [CrossRef]
14. Rahman, M.A.; Hossain, M.S.; Showail, A.J.; Alrajeh, N.A.; Alhamid, M.F. A secure, private, and explainable IoHT framework to
support sustainable health monitoring in a smart city. Sustain. Cities Soc. 2021 ,72, 103083. [CrossRef]
15. Rahman, M.A.; Hossain, M.S.; Islam, M.S.; Alrajeh, N.A.; Muhammad, G. Secure and provenance enhanced Internet of health
things framework: A blockchain managed federated learning approach. IEEE Access 2020 ,8, 205071–205087. [CrossRef]
16. Ketu, S.; Mishra, P .K. Mishra Internet of Healthcare Things: A contemporary survey. J. Netw. Comput. Appl. 2021 ,192, 103179.
[CrossRef]
17. Mamdouh, M.; Awad, A.I.; Khalaf, A.A.; Hamed, H.F. Authentication and Identity Management of IoHT Devices: Achievements,
Challenges, and Future Directions. Comput. Secur. 2021 ,111, 102491. [CrossRef]
18. Somasundaram, R.; Thirugnanam, M. Review of security challenges in healthcare internet of things. Wirel. Netw. 2021 ,27,
5503–5509. [CrossRef]
19. Sivan, R.; Zukarnain, Z.A. Security and Privacy in Cloud-Based E-Health System. Symmetry 2021 ,13, 742. [CrossRef]
20. Parashar, A.; Rishishwar, S. Security challenges in IoT. In Proceedings of the Third International Conference on Advances in
Electrical, Electronics, Information, Communication and Bio-Informatics (AEEICB), Chennai, India, 27–28 February 2017; pp.
446–449.
21. Morghan, H.; Hashmi, U.S.; Imran, A. Edge computing in smart health care systems: Review, challenges, and research directions.
Trans. Emerg. Telecommun. Technol. 2019 , e3710. [CrossRef]
22. Cao, Y.; Hou, P .; Brown, D.; Wang, J.; Chen, S. Distributed analytics and edge intelligence: Pervasive health monitoring at the era
of fog computing. In Proceedings of the 2015 Workshop on Mobile Big Data (Mobidata), Hangzhou, China, 21 June 2015.
23. Hu, R.; Pham, H.; Buluschek, P .; Gatica-Perez, D. Elderly people living alone: Detecting home visits with ambient and wearable
sensing. In Proceedings of the 2nd International Workshop on Multimedia for Personal Health and Health Care (MMHealth),
Mountain View, CA, USA, 23 October 2017.
24. Baktir, A.C.; Tunca, C.; Ozgovde, A.; Salur, G.; Ersoy, C. SDN-based multi-tier computing and communication architecture for
pervasive healthcare. IEEE Access 2018 ,6, 56765–56781. [CrossRef]
25. Brito, C.; Pinto, L.; Marinho, V .; Paiva, S.; Pinto, P . A Review on Recent Advances in Implanted Medical Devices Security. In
Proceedings of the 2021 16th Iberian Conference on Information Systems and Technologies (CISTI), 2021, Chaves, Portugal, 23–26
June 2021; pp. 1–6. [CrossRef]
26. Thakar, A.T.; Pandya, S. Survey of IoT enables healthcare devices. In Proceedings of the 2017 International Conference on
Computing Methodologies and Communication (ICCMC), Erode, India, 18–19 July 2017.
27. Li, X.; Huang, X.; Li, C.; Yu, R.; Shu, L. EdgeCare: Leveraging edge computing for collaborative data management in mobile
healthcare systems. IEEE Access 2019 ,7, 22011–22025. [CrossRef]
28. Perez, A.J.; Zeadally, S. Recent Advances in Wearable Sensing Technologies. Sensors 2021 ,21, 6828. [CrossRef] [PubMed]
29. Qu, Y.; Zheng, G.; Ma, H.; Wang, X.; Ji, B.; Wu, H. A Survey of Routing Protocols in WBAN for Healthcare Applications. Sensors
2019 ,19, 1638. [CrossRef]
30. Baker, S.B.; Xiang, W.; Atkinson, I. Internet of things for smart healthcare: Technologies, challenges, and opportunities. IEEE
Access 2017 ,5, 26521–26544. [CrossRef]
31. Saboor, A.; Mustafa, A.; Ahmad, R.; Khan, M.A.; Haris, M.; Hameed, R. Evolution of Wireless Standards for Health Monitoring.
In Proceedings of the 2019 9th Annual Information Technology, Electromechanical Engineering and Microelectronics Conference
(IEMECON), Jaipur, India, 13–15 March 2019; pp. 268–272. [CrossRef]
32. Saboor, A.; Ahmad, R.; Ahmed, W.; Kiani, A.K.; Moullec, Y.L.; Alam, M.M. On Research Challenges in Hybrid Medium-Access
Control Protocols for IEEE 802.15.6 WBANs. IEEE Sens. J. 2019 ,19, 8543–8555. [CrossRef]
33. Taleb, H.; Nasser, A.; Andrieux, G. Wireless technologies, medical applications and future challenges in WBAN: A survey. Wirel.
Netw. 2021 ,27, 5271–5295. [CrossRef]Appl. Sci. 2022 ,12, 1927 21 of 22
34. Hämäläinen, M.; Paso, T.; Mucchi, L. ETSI SmartBAN in Medical IoT. In Proceedings of the 2021 XXXIVth General Assembly
and Scientiﬁc Symposium of the International Union of Radio Science (URSI GASS), Rome, Italy, 28 August–4 September 2021.
[CrossRef]
35. Negra, R.; Jemili, I.; Belghith, A. Wireless body area networks: Applications and technologies. Procedia Comput. Sci. 2016 ,83,
1274–1281. [CrossRef]
36. Khajenasiri, I.; Zhu, P .; Verhelst, M.; Gielen, G. A low-energy ultra-wideband internet-of-things radio system for multi-standard
smart-home energy management. IEIE Trans. Smart Process. Comput. 2015 ,4, 354–365. [CrossRef]
37. Mukherjee, A.; Ghosh, S.; Behere, A.; Ghosh, S.K.; Buyya, R. Internet of Health Things (IoHT) for personalized health care using
integrated edge-fog-cloud network. J. Ambient. Intell. Hum. Comput. 2021 ,12, 943–959. [CrossRef]
38. Mamdouh, M.; Awad, A.I.; Hamed, H.F.A.; Khalaf, A.A.M. Outlook on Security and Privacy. In IoHT: Key Challenges and Future
Vision, Proceedings of the International Conference on Artiﬁcial Intelligence and Computer Vision (AICV 2020). Advances in Intelligent
Systems and Computing, Cairo, Egypt, 8–10 April, 2020 ; Hassanien, A.E., Azar, A., Gaber, T., Oliva, D., Tolba, F., Eds.; Springer:
Cham, Switzerland, 2020; p. 1153. [CrossRef]
39. Meyer, J.; Kazakova, A.; Büsing, M.; Boll, S. Visualization of complex health data on mobile devices. In Proceedings of the 2016 ACM
Workshop on Multimedia for Personal Health and Health Care (MMHealth), Amsterdam, The Netherlands, 16 October 2016.
40. Rolim, C.O.; Koch, F.L.; Westphall, C.B.; Werner, J.; Fracalossi, A.; Salvador, G.S. A cloud computing solution for patient’s data
collection in health care institutions. In Proceedings of the 2010 Second International Conference on eHealth, Telemedicine, and
Social Medicine, St. Maarten, The Netherlands, 2–16 October 2010.
41. Perera, C.; McCormick, C.; Bandara, A.K.; Price, B.A.; Nuseibeh, B. Privacy-by-design framework for assessing internet of things
applications and platforms. In Proceedings of the 6th International Conference on the Internet of Things, Stuttgart, Germany, 7–9
November 2016; pp. 83–92.
42. Stork, M.; Vancura, V . Hidden pacemaker pulses detection based on wavelet and Hilbert-Huang transform. In Proceedings of the
IEEE International Conference on Applied Electronics, Pilsen, Czech Republic, 9–10 September 2014; pp. 285–288.
43. Samani, M.M.; Mahnam, A. Design and implementation of an ultra low power wireless neuro-stimulator system. In Proceedings
of the 17th Iranian Conference of Biomedical Engineering (ICBME), Isfahan, Iran, 3–4 November 2010; pp. 1–4.
44. Lucisano, J.Y.; Routh, T.L.; Lin, J.T.; Gough, D.A. Glucose monitoring in individuals with diabetes using a long-term implanted
sensor/telemetry system and mode. IEEE Trans. Biomed. Eng. 2016 ,64, 198–1993.
45. Hiremath, S.; Yang, G.; Mankodiya, K. Wearable internet of things: Concept, architectural components and promises for
person-centered healthcare. In Proceedings of the International Conference on Wireless Mobile Communication and Healthcare-
Transforming Healthcare Through Innovations in Mobile and Wireless Technologies (MOBIHEALTH), Athens, Greece, 3–5
November 2014; pp. 304–307.
46. Birgit, L.; Andrei, P . ActiGait®: A Partly Implantable Drop-Foot Stimulator System. In Introduction to Neural Engineering for Motor
Rehabilitation ; Farina, D., Jensen, W., Akay, M., Eds.; IEEE: Piscataway, NJ, USA, 2013; pp. 421–423.
47. Hansen, J.H.; Ali, H.; Saba, J.N.; Charan, M.R.; Mamun, N.; Ghosh, R.; Brueggeman, A. Cci-mobile: Design and evaluation of a
cochlear implant and hearing aid research platform for speech scientists and engineers. In Proceedings of the 2019 IEEE EMBS
International Conference on Biomedical & Health Informatics (BHI), Chicago, IL, USA, 19–22 May 2019; pp. 1–4.
48. Caffey, S.; Po-Ying, L.; Jeffrey, B. Remote-Controlled Drug Pump Devices. U.S. Patent 8,285,328, 9 October 2012.
49. Zhao, Y.; Wang, J.; Zhang, Y.; Liu, H.; Chen, Z.A.; Lu, Y.; Dai, Y.; Xu, L.; Gao, S. Flexible and Wearable EMG and PSD Sensors
Enabled Locomotion Mode Recognition for IoHT Based In-home Rehabilitation. IEEE Sens. J. 2021 ,21, 26311–26319. [CrossRef]
50. Rao, S.; Dubey, S.; Deb, S.; Hughes, Z.; Seo, Y.S.; Nguyen, M.Q.; Tang, S.J.; Abell, T.; Lahr, C.; Chiao, J.C. Wireless gastric
stimulators. In Proceedings of the Texas Symposium on Wireless and Microwave Circuits and Systems, Waco, TX, USA, 3–4 April
2014; pp. 1–4.
51. Zareei, M.; Zarei, A.; Budiarto, R.; Omar, M.A. A comparative study of short range wireless sensor network on high density
networks. In Proceedings of the 17th Asia-Paciﬁc Conference on Communications, APCC, Sabah, Malaysia, 2–5 October 2011; pp.
247–252.
52. Fouladi, B.; Ghanoun, S. Security Evaluation of the Z-Wave Wireless Protocol ; Black Hat: Las V egas, NV , USA, 2013; V olume 24, pp. 1–2.
53. Fatima, I.; Ahmad, A.; Ali, S.; Ali, M.; Baig, M. ITriple-Band circular polarized antenna for WLAN/WiFi/Bluetooth/WiMAX
applications. Prog. Electromagn. Res. C 2021 ,109, 65–75. [CrossRef]
54. Varshney, G.; Gupta, H. A security framework for IOT devices against wireless threats. In Proceedings of the 2nd International
Conference on Telecommunication and Networks (TEL-NET), Noida, India, 10–11 August 2017; pp. 1–6.
55. Xie, L.; Yang, G.; Mantysalo, M.; Xu, L.L.; Jonsson, F.; Zheng, L.R. Heterogeneous integration of bio-sensing system-on-chip and
printed electronics. IEEE J. Emerg. Sel. Top. Circuits Syst. 2012 ,4, 672–682. [CrossRef]
56. Lindquist, A.; Johansson, P .; Petersson, G.; Saveman, B.I.; Nilsson, G. The use of the personal digital assistant (PDA) among
personnel and students in health care: A review. J. Med. Internet Res. 2008 ,10, e1038. [CrossRef]
57. Jung, J.Y.; Lee, J. Automatic discovery and installation of wearable bio signal devices in ubiquitous healthcare system. In
Proceedings of the 9th International Conference on Advanced Communication Technology, Gangwon, Korea, 12–14 February
2007; pp. 412–414.
58. da Costa, C.A.; Pasluosta, C.F.; Eskoﬁer, B.; da Silva, D.B.; Righi, R.d. Internet of health things: Toward intelligent vital signs
monitoring in hospital wards. Artif. Intell. Med. 2018 ,89, 61–69. [CrossRef]Appl. Sci. 2022 ,12, 1927 22 of 22
59. Rajit, N.; Thanachayanont, A. A 1-V CMOS low-power resistor-based temperature sensor for human body temperature monitoring.
In Proceedings of the 34th International Technical Conference on Circuits/Systems, Computers and Communications (ITC-CSCC),
JeJu, Korea, 23–26 June 2019; pp. 1–4.
60. Yousefzadeh, B.; Shalmany, S.H.; Makinwa, K.A. A BJT-based temperature-to-digital converter with inaccuracy from  55C to
+125C in 0.16. IEEE J. Solid State Circuits 2017 ,52, 1044–1052. [CrossRef]
61. Bai, B.; Nazir, S.; Bai, Y.; Anees, A. Security and provenance for Internet of Health Things: A systematic literature review. J. Softw.
Evol. Process. 2021 ,33, e2335. [CrossRef]
62. Esha, N.H.; Tasmim, M.R.; Huq, S.; Mahmud, M.; Kaiser, M.S. Trust IoHT: A Trust Management Model for Internet of Healthcare
Things. In Proceedings of the International Conference on Data Science and Applications, Kolkata, India, 10–11 April 2021; 2021;
pp. 47–57.
63. MacDermott, A.; Kendrick, P .; Idowu, I.; Ashall, M.; Shi, Q. Securing things in the healthcare internet of things. In Proceedings of
the Global IoT Summit (GIoTS), New York, NY, USA, 7–21 June 2019; pp. 1–6.
64. Baccelli, E.; Hahm, O.; Günes, M.; Wählisch, M.; Schmidt, T. OS for the IoT-goals, challenges, and solutions. In Proceedings of the
Interdisciplinaire sur la S écurit éGlobale (WISG2013) Workshop, Troyes, France, 22 January 2013; pp. 1–6.
65. Chung, B.; Kim, J.; Jeon, Y. On-demand security conﬁguration for IoT devices. In Proceedings of the International Conference on
Information and Communication Technology Convergence (ICTC), Jeju, Korea, 19–21 October 2016; pp. 1082–1084.
66. Zhou, W.; Jia, Y.; Peng, A.; Zhang, Y.; Liu, P . The effect of IoT new features on security and privacy: New threats, existing solutions,
and challenges yet to be solved. IEEE Internet Things J. 2018 ,6, 1606–1616. [CrossRef]
67. Foukia, N.; Billard, D.; Solana, E. PISCES: A framework for privacy by design in IoT. In Proceedings of the 14th Annual Conference
on Privacy, Security and Trust (PST), Auckland, New Zealand, 12–14 December 2016; pp. 706–713.
68. van Rest, J.; Boonstra, D.; Everts, M.; van Rijn, M.; van Paassen, R. Designing privacy-by-design. In Proceedings of the 1st
Annual Privacy Forum, Lecture Notes in Computer Science, Limassol, Cyprus, 10–11 October 2012; Springer: Berlin/Heidelberg,
Germany, 2012; Volume 8319, pp. 55–72.
69. Weber, R.H. Internet of things-new security and privacy challenges. Comput. Law Secur. 2010 ,26, 23–30. [CrossRef]
70. Neuhaus, C.; Polze, A.; Chowdhury, M.M. Survey on Healthcare IT Systems: Standards, Regulations and Security ; University Potsdam:
Potsdam, Germany, 2011.
71. Swartz, N. Canada reviews PIPEDA. Inform. Manag. 2007 ,41(2), 8.
72. Danzon, M.; Litvinov, S.K. EUROHEALTH Programme. World Health Stat. Q. Rapp. Trimest. De Stat. Sanit. Mond. 1993 ,46,
153–157.
73. de Hert, P .; Papakonstantinou, V . The proposed data protection regulation replacing directive 95/46/EC: A sound system for the
protection of individuals. Comput. Law Secur. Rev. 2012 ,28, 130–142. [CrossRef]
74. Georgiou, D.; Lambrinoudakis, C. Compatibility of a security policy for a cloud-based healthcare system with the EU general
data protection regulation (GDPR). Information 2020 ,11, 586. [CrossRef]
75. Maeve, M. E-government in Australia: The challenge to privacy of personal information. Int. J. Inf. Technol. 2002 ,10, 327.
76. Balkhair, A. Kingdom of Saudi Arabia The National eHealth Program. 2014. Available online: https://www.itu.int/ITU-D/cyb/events/
2012/e-health/Nat_eH_Dev/Session%204/KSA-MOH-Presentation-SaudiArabia%20FINAL.pdf (accessed on 1 October 2021).
77. Zheng, M. Surveillance and disease control in COVID-19: Big data application in public health. In Proceeding of the Inter-
national Conference on Applications and Techniques in Cyber Security and Intelligence, Fuyang, China, 19–21 June 2021 ; Springer:
Berlin/Heidelberg, Germany, 2021; pp. 565–570.
78. Overhage, J.M.; Ryan, P .B.; Reich, C.G.; Hartzema, A.G.; Stang, P .E. Validation of a common data model for active safety
surveillance research. J. Am. Med. Inform. Assoc. 2012 ,19, 54–60. [CrossRef]
79. Mattoo, A.; Meltzer, J.P . International data ﬂows and privacy: The conﬂict and its resolution. J. Int. Econ. Law 2018 ,21, 769–789.
[CrossRef]
80. Hemalatha, P . Monitoring and securing the healthcare data harnessing IOT and blockchain technology. Turk. J. Comput. Math.
Educ. 2021 ,12, 2554–2561.
81. Lydahl, D. Standard tools for non-standard care: The values and scripts of a person-centred assessment protocol. Health 2021 ,25,
103–120. [CrossRef] [PubMed]
82. Huang, Y.; Xu, J.; Yu, B.; Shull, P .B. Validity of FitBit, Jawbone UP , Nike+ and other wearable devices for level and stair walking.
Gait Posture 2016 ,48, 36–41. [CrossRef] [PubMed]"
DataPrivacy,3008.txt,"HAL Id: hal-03272517
https://hal.inria.fr/hal-03272517
Submitted on 28 Jun 2021
HAL is a multi-disciplinary open access
archive for the deposit and dissemination of sci-
entific research documents, whether they are pub-
lished or not. The documents may come from
teaching and research institutions in F rance or
abroad, or from public or private research centers.L’archive ouverte pluridisciplinaire HAL , est
destinée au dépôt et à la diffusion de documents
scientifiques de niveau recherche, publiés ou non,
émanant des établissements d’enseignement et de
recherche français ou étrangers, des laboratoires
publics ou privés.
Distributed under a Creative Commons Attribution| 4.0 International License
Data Privacy and Protection: The Role of Regulation
and Implications for Data Controllers in Developing
Countries
Mohammed Agbali, Abubakar A. Dahiru, G. Daniel Olufemi, Inuwa A.
Kashifu, Olatunji Vincent
T o cite this version:
Mohammed Agbali, Abubakar A. Dahiru, G. Daniel Olufemi, Inuwa A. Kashifu, Olatunji Vincent.
Data Privacy and Protection: The Role of Regulation and Implications for Data Controllers in De-
veloping Countries. 16th International Conference on Social Implications of Computers in Developing
Countries (ICT4D), Jun 2020, Manchester, United Kingdom. pp.205-216, ￿10.1007/978-3-030-65828-
1_17￿. ￿hal-03272517￿ 
Data Privacy and Protection: The Role of Regulation and 
Implications for Data Controllers in Developing 
Countries  
Mohammed Agbali , Abubakar A. Dahiru  [0000 -0001 -9858 -5796 ], Daniel  Olufemi G. , Inuwa 
A. Kashifu  and Olatunji Vincent  
A. National Information Technology Development Agency, 28, P/H Crescent, Area 11, 
Garki, Abuja, Nigeria  
magbali@nitda.gov.ng ; d.a.abubakar@rgu.ac.uk ; 
odaniel@nitda.gov.ng ;kinuwa@nitda.gov.ng ; 
volatunji@nitda.gov.ng  
Abstract. Advances in new technologies present challenges to general expecta-
tions relating to collection, usage and cross -border control and transfer of per-
sonal data  in recent times. Data has become the critical component of the fourth 
industrial revolution in global economies involving governments, businesses, and 
individuals. This paper considers the recent introduction of Data Protection Reg-
ulation in Nigeria (NDPR ), which can be adjudged to have novel compliance 
structures globally. Using a qualitative approach, and further enabled by the in-
stitutional theory as a framework the paper examines the implications of the 
NDPR requirements for Data Controllers and Proces sors in key sectors of the 
economy. Findings from the study shows that there are five key components of 
the NDPR that can compel, motivate or support organizations to make significant 
structural changes such as standardization of processes, practices and I T assets to 
show conformity and/or gain legitimacy. The study equally identified  the factors 
that facilitate or inhibit the adoption and implementation of the conditions of the 
NDPR categorised in line with the three pillars of the institutional theory fra me-
work . These findings projects policy direction in enhancing the institutionalisa-
tion of NDPR measures across key sectors.  It will also inform businesses on nec-
essary cause of action and changes to ensure privacy and protection of personal 
data collected from data subjects.  
Keywords:  Data Protection , Data Privacy , Regulation , Data Controllers , Data 
Processors , Institutional Theory , Framework, ICT,  NDPR . 
1 Introduction  
Globalisation and phenomenal growth of Internet and Web access are complicating the 
challen ges of personal data privacy and protection around the world. Vulnerabilities 
due to data collection in both public and private sectors are at a potentially remarkable 
stage. For instance, governments collect and process personal information from popu-
lation census, birth certificate, voters register, and drivers’ license records. On the other 2 
hand, private corporations such as telecommunications operators, fund managers, and 
commercial banks increasingly build capacities for compiling databases of customer 
information aided by unlimited capability of new technologies with  ease of tracking 
and retrieval . 
With globalization and rapid technological advancements, there is obviously in-
creased capability for organizations or businesses to collect, analyse, store, transfer and 
interlink data for various purposes including direct marketing and personalized services 
[1]. In this regard, with identification technologies in the form of radio -frequency iden-
tification (RFID) and social media platforms such as Facebook, In stagram and other 
networks, data privacy and protection are currently being challenged [2].  
In Europe and North America, developments in personal data processing technolo-
gies evolved with data privacy legislation and several other initiatives to improve t he 
level of privacy protection. The first data privacy act by German Federal State of Hes-
sen dated back to 1970 and was followed by the adoption of Swedish Data Protection 
act in 1973 [3]. Similarly, the United States government formulated the popular Fair 
Information Practices (FIPs) since 1973 [4]. The development of modern data protec-
tion laws started with the Convention for the Protection of Individuals regarding  Auto-
matic Processing o f Personal Data,1981 (aka Convention 108). The Convention enu-
merated the core principles of data processing which are still the basis of modern data 
protection laws today. The next major intervention is the European Union Data Protec-
tion Directive, 1995. S imilarly, while issues around protection of citizen’s telephone 
conversation, correspondence, telegraphic communications, etc. are covered under sec-
tion 37 of the 1999 constitution of the Federal Republic of Nigeria [5], the National 
Information Technology  Development Agency (NITDA), pursuant to Section 6(c) of 
the NITDA Act 2007, issued a Data Protection Guideline in 2013. However, upon the 
issuance of the EU General Data Protection Regulation (EUGDPR) in 2016, and other 
international developments on priva cy protection, NITDA issued the Nigeria Data Pro-
tection Regulation (NDPR) [6] on 25th January  2019.  
In Nigeria, Data Privacy and Protection regulation has been evolving since the in-
troduction of the NDPR which is a subsidiary legislation. The NDPR aims to safeguard 
the rights of citizens and people living in Nigeria to data privacy and protection in order 
to foster the integrity of commerce and industry in the volatile data economy. It also 
aims at enhancing the secure exchange of data; improve business ope rating environ-
ment and create sustainable jobs [6] . The NDPR applies to public and private entities 
processing data of Nigerians . 
In this paper, we examine the implications of data privacy and protection regulations 
in the context of a developing country – Nigeria, with particular attention to data inten-
sive businesses. These businesses include financial service providers such as 
banks/fund managers, telecommunications companies, health -care service providers 
and a host of other operators that need to take into account the Nigeria Data Protection 
Regulation (NDPR) requirements as it relates to their businesses. To address this issue, 
a research question was raised ‘ what are the  implications of the new NDPR for data 
controllers  in Nigeria ’? 
We provide answers  to this question through a comprehensive systematic literature 
review and analysis of feedback obtained from audit reports filed by entities as well as 3 
expert’s interviews. We found five key components of the NDPR that can compel, mo-
tivate or support orga nizations to make significant structural changes such as standard-
ization of processes, practices and IT assets to show conformity and/or gain legitimacy. 
We equally identified  the factors that facilitate or inhibit the adoption and implementa-
tion of the co nditions of the NDPR categorised in line with the three pillars of the in-
stitutional theory framework adopted for this study. The findings are discussed and 
enumerated to inform policy makers on the implications of their approaches towards 
ensuring complia nce. It will also educate businesses on necessary cause of action and 
changes to ensure privacy and protection of personal data they solicit from data sub-
jects. 
The remaining part of the paper is organized into distinctive sections as follows: 
Section 2 pr esents related works in the field of privacy and data protection. Section 3 
summarizes the concept of institutional theory. Section 4 discusses the methodology 
adopted for this study. Section 5 presents the result and discussion of the findings. Fi-
nally, S ection 6 concludes the paper . 
2 Related Work  
The growing dependence by businesses on information technology (IT) to manage their 
data has led to increase in information security and privacy risks implications. The term 
‘privacy’ is a multifaceted concept cur rently gaining traction in the field of computing. 
According to Nissenbaum  [7], privacy is a contextual integrity and one of the most 
enduring social issues associated with information and communication technologies 
(ICT). The author opined that privacy is  breached if personal data is used or made avail-
able outside its intended context. Data protection on the other hand refers to the pro-
cesses of safeguarding the confidentiality of data including ensuring its privacy and 
protecting it from compromise. Accor ding to Friedewald et al. , the concept of data pro-
tection is both broader and more specific than the right to privacy  [2]. Art. 8 of the EU 
Charter of Fundamental rights is the first legislative attempt to di stinguish data protec-
tion from priv acy. According to Lynskey [8],  the right to data protection provides indi-
viduals with m ore rights than right to privacy.  The enhanced control introduced by data 
protection serves two purposes - a) it proactively promotes individual personality rights 
which are threatened by personal data processing and b) reduces the power and infor-
mation asymmetries between individuals and those who process their data. In this re-
gard, data protection is not intended only to make the protection of privacy real but also 
seeks to protect other rights relating to conscience, non -discrimination and a host of 
other concerns or interests . 
Article 1 2 of the United Nations Universal Declaration on Human Rights provides - 
no one shall be subject to arbitrary interference with his privacy, family, home or cor-
respondence, nor to attacks upon his honour and reputation. Everyone has the right to 
the protect ion of the law against such interference or attacks. The declaration is a state-
ment of intent of every member of the United Nations, including Nigeria. Article 8 of 
the European Convention for the Protection of Human Rights and Fundamental Free-4 
doms guarant ees the right to respect for private and family life, home , and correspond-
ence. The African Charter on Human and People’s Rights on the other hand does not 
provide for the right to privacy. This lacuna has been justified on the basis that Africa 
has more p ressing rights issues such as child labour, slavery, terrorism etc. However, it 
has been argued to the contrary that invasion of privacy by telecommunication compa-
nies on behalf of government is repressing fundamental rights to expression, beliefs and 
life.1 The courts have ruled that illegal monitoring of employees’ usage of computer 
such as private mails, browsing sites and social media activities is a breach of right to 
privacy2. Unlawful storage of data is also held to be a breach of privacy.3 
The gr owing concerns about automated personal data systems therefore resulted in 
the introduction of data privacy measures, regulations, and conventions in the western 
countries such as United States of America (USA) through the US Department of 
Health, Educatio n, and Welfare as well as in Europe through the Organization for Eco-
nomic Co -operation and Development (OECD) and EU  [1]. For instance, in a move to 
standardize the protection of personal data privacy, the European Union (EU) enacted 
the Data Protection Di rective in 1995  [9]. The directive which among other things pro-
hibits corporations and governments from using personal data for any purpose other 
than original purpose without permission, took effect in 1998  [10].  
Organizations are facing ever increasing regulatory interventions (e.g., GDPR, 
CCPA, NDPR, PIPEDA, HIPAA, etc.) that may lead to significant structural changes 
such as standardization of processes, practices,  and IT assets to show conformity and/or 
gain le gitimacy.  According to UNCTAD , 107 countries (of which 66 were developing 
or transition economies) have put in place legislation to secure the protection of data 
and privacy  [11]. Noteworthy is the growing level of adoption in comparing Europe, 
Asia and Af rica. 
3 Institutional Theory  
The manner in which organisations respond to changes is often dependent on the socio -
political, economic and technological influences exerted by the environment in which 
they operate as posited by Weerakkody et al., [12]. Thus,  the impacts of such external 
forces on organizational behaviour have been studied by many researchers using the 
institutional theory. The core concept of institutional theory is that organizational struc-
tures and processes tend to acquire meaning and achi eve stability in their own right, 
rather than on the basis of their effectiveness and efficiency in achieving desired ends, 
such as the mission and goals of the organization [13]. Few studies have focused on 
using the theory to understand the impact of IT -enabled change in organizations  [12]. 
According to DiMaggio & Powell, institutions exert three types of isomorphic pressures 
or effects, viz. coercive, normative,  and mimetic  [14]. Jennings and Greenwood [15]  
suggest that the notion of institutional pressu res is akin to the concept of institutional 
 
1 Privacy International at the 62nd Session of the African Commission on Human and People's 
Rights (ACHPR)   
2 Barbulescu v. Romania (No. 61496/08)  
3 Roman Zakharov v. Russia (No. 47143/06)  5 
pillars proposed by Scott, which comprises of “regulative, normative and cultural cog-
nitive elements that, together with associated activities and resources, provide stability 
and meaning to social life”  (pp. 48)  [16]. The basic similarity in all institutional theo-
retical claims however, is that something identified at a higher level is used to explain 
processes and outcomes at a lower level of analysis [17] .  
In this research, the strength of the institutional th eory is employed to determine the 
various implications of data privacy and protection regulations on data intensive busi-
nesses in the context of a developing country – Nigeria. Specifically, Scott’s [16] three 
key pillars that can make up or support instit utions viz. regulative, normative and cog-
nitive are employed . 
Table 1. Institutional Theory Framework  
 
Source: Institutions and Organizations  pp. 51  [16] 
 
Institutionalization, through the lenses of the regulative pillar can be viewed as a stable 
system of rules that can be informal or formal backed by monitoring and sanctioning 
powers and accompanied by feelings of fear or guilt and/or innocence or incorruptibil-
ity. In normative systems, not only are goals and/or o bjectives defined, appropriate 
methods of pursuing them are also defined. Norms and values can vary depending  on 
what the position is or who the actors are. Thus,  the appropriate goals or activities as-
signed to particular actors or positions leads to the c reation of roles or normative ex-
pectations of how specific actors are required to behave.  Cognition can be described as 
the psychological result of perception and learning and reasoning.  Scott  [16], DiMaggio 
& Powell [14],  and other organizational scholars  have stressed the centrality of cogni-
tive elements of institutions as being the “ shared conceptions that constitute the nature 
of social reality and the frames through which meaning is made ” [16] pp.57.  
6 
In summary, the three pillars – regulative, normative and cognitive, all have their 
distinctive features and ways in which they operate as shown on Table 1. However, 
Scott pointed out that in most empirically observed institutional forms, a combination 
of th e pillars are observed at work which can lead to the formation of a stable social 
system  [16].  
This research contributes to the few studies that have applied institutional analysis 
as a theoretical lens for studying the implications of IS/ICT regulations on organiza-
tions.  For instance, institutional theory has been used by Appari  et al to explain the 
variability in regulatory compliance prevalent in the US healthcare sector  [18]. In their 
explanation of firms’ response to information security a nd privacy issues, Greenway 
and Chan  argue that information security research could leverage socio -organizational 
theory, like the institutional theory, to frame inquiries  [19]. Similarly, D‟Arcy and 
Hovav  advocate application of institutional theory to st udy the relationship between 
organizational characteristics and security best practices  [20]. In a developing country 
context, Dahiru et al have also used the theory to determine the associative interaction 
between exciters and inhibitors to technology ado ption [21].  
4 Methodology  
The research methodology adopted by this study is a qualitative one. A systematic lit-
erature survey backed by a pilot study was conducted in Nigeria between November 
2019 and January 2020. During this phase, the focus of the study is to obtain the impli-
cations of data protection and privacy laws/regulations on data intensive businesses 
across key sectors in Nigeria. Considering the broadness of the topic as well as potential 
legal and economic implications, a systematic literature r eview was initially conducted 
to provide insights into the topic and to collect adequate qualitative data [22]. Further, 
using the selected theoretical underpinnings of the institutional theory, a data collection 
instrument was developed to allow for expan sion of the research and to provide an 
evaluation mechanism. The pilot study was carried out in line with the design empha-
sised by Naoum [23].  As Naoum posited, lessons drawn from feedback in a pilot study 
helps the researcher to refine and check the instrument before the main data collection 
exercise . 
 
4.1 Research Context  
Considering key criteria, Nigeria is Africa’s largest economy  [24].  In addit ion, Nigeria 
launched one of the most vibrant campaigns towards enforcement of data privacy and 
protection on African content in recent time. Data collection and analysis were carried 
out in Nigeria cutting across key sectors including telecommunication, b anking and 
finance, and regulatory agencies. These sectors were selected based on their direct in-
volvement in the personal data processing and management to enhance the quality of 
data used in this study. As technology researchers involved in various stake holders’  
awareness programmes, the research team was able to access qualitative data through 
top quality interviews in both public and private sectors . 
 7 
4.2 Data Collection  
The study relied on gradual but systematic literature reviews including audit reports 
and face -to-face interviews using a semi -structured interview guide. The systematic 
literature survey was conducted to elicit information required to meet the objective of 
this study. During the face -to-face interviews, selection of participants was made based 
on participant’s degree of invo lvement in the ongoing data protection and privacy rev-
olution in Nigeria. The selection process also employed snowballing approach that fa-
cilitated researcher’s contact with data privacy and protection experts across key sectors 
in Nigeria. In this pilot s tudy, seven data intensive organizations were considered  in 
both public and private sector and a total of eleven participants were interviewed. The 
participants included two executive directors, and three general managers, four senior 
managers and two CTOs . Participants profile a re summarized in table 2. All the inter-
views were conducted in English language with note taking and audio recording and 
lasted for 45 to 55 minutes.  
Table 2.  Profile of Interview Participants  
Sector  Profile  
ED GM SM CTO  
Telecommunications  0 2 1 0 
Banking/Finance  1 1 2 2 
Regulatory Agencies  1 0 1 0 
Total  2 3 4 2 
 
4.3 Data Analysis  
Data relating to Privacy and Protection advances were analyzed using institutional the-
ory discussed in the previous section. Data Privacy and Protection issues in this study 
were evaluated within each domain (as shown in table 2) on the basis of qualitative data 
elicited t hrough interviews and credible documents relating to NDPR. In view of the 
relatively small number of participants involved in this pilot study, results from the 
interviews were analyzed using qualitative manual method . 
5 Results and Discussion  
In this section, we describe the most important findings based on the feedback from 
experts’ interviews , pertinent literature and audit reports. Considering the early stage 
of Data Protection regulation in Nigeria, the analysis focused on laying out the key 
issues  relating to implications of Personal Data Protection for data controllers in key 
sectors. We achieved this through useful inputs obtained from experts in terms of Per-
sonal Data Protection implications in the context of Nigeria.   
The recent adoption of NDP R brings about new obligations compelling all data con-
trollers handling Nigerian personal data to review their existing data privacy and pro-8 
tection policies to ensure compliance with NDPR. To guide these tasks, this paper iden-
tified key implications of NDP R for data controllers. The 5 key components of NDPR 
identified in the analysis are summarised in Table 3. 
Table 3.  NDPR Implications for Data Controllers  
NDPR Implications  Requirements  
Data Controllers to 
Designate a Data Pro-
tection Officer  
 Data Controllers are required to appoint competent person 
or outsource data protection to verifiably competent firm 
to ensure adherence to NDPR.  
 
Considering condi-
tions for data pro-
cessing in interna-
tional context  Data Controllers are obliged to ensure that Data Subject 
consent explicitly to the proposed transfer, after being in-
formed of possible risks of such transfer to a third country. 
Data Controllers must  ensure that Data Subject is mani-
festly made to understan d through clear warning of the spe-
cific implications of data protection likely to be violated as 
a result of such transfer to third country.  
 
Ensuring Data Sub-
ject's right to data 
portability  
 Data Controllers are required to ensure that data subject re-
serve the right to have personal data transmitted directly 
from one controller to another.  
 
Provision of measures 
for dealing with data 
breaches  
 Data Controllers are obliged to secure personal data against 
all foreseeable hazards and breaches such as theft, cyberat-
tack, viral attack, manipulation of any kind, and damage by 
natural elements.  
 
Reckoning with pen-
alty for dealing with 
default  Under NDPR, regulatory authority (NITDA) has powers to 
impose administrative fine on defaulting Data Controllers. 
Issues of non -compliance could cost Data Controllers a 
fine of up to 10 million Naira or 2% of Annual Gross Rev-
enue of the preceding year.  
 
Awareness of NDPR implications is key and can be viewed as the starting point for the 
NDPR requirements for implementation amongst data controllers. In this area, findings 
of this study only confirmed the awareness at the top -management level. For instance, 
when prompted to confirm the level of awareness of NDPR and the potential implica-
tions on their organizati on, participants of this study confirmed that only their top man-
agement is aware of the regulation and implications of not adhering to data protection 
laws, pr ocedures and policies. Although  the interview ees tend to confirm awareness at 
the management leve l, only one participant from the banking sector confirmed compli-
ance and steps taken to engage Data Protection Compliance Organization (DPCO). This 
is an indication of the need for additional awareness creation  and sensitization . Imple-9 
mentation of NDPR in Nigeria indicates the need to designate DP COs in various or-
ganizations, which may have considerable impacts on controllers’ technical compe-
tence and may need the hiring of more expertise.  
Organizations involved in data processing in international context a re also required 
to put in place security measures including rules for the onward transfer of personal 
data to foreign country or international organization. This principle is related to a host 
of other principles enunciated in the NDPR for data processing  regarding data minimi-
zation, specific purpose, lawful and legitimacy, accuracy as well as storage and security 
of personal data (Art. 2.1a to 2.1d). Adherence to these principles can be considered 
reasonable. However, interviewees from the banking sector expressed concern regard-
ing data they share with their partners in card sub -sector that maintain data centres 
worldwide , where in most cases , location of such centres remain unknown to them.  
This is in alignment with previous studies by Dahiru et al [21] t hat indicated how data 
is transferred to international 3rd parties without recourse to its sensitivity.  
Implementation of NDPR also introduces a new right  to data portability. This right 
imposes new capability on Data Controllers to ensure there is capabi lity to provide data 
subject their personal data in compatible format when the need arises. One of the un-
derpinning principles of the NDPR is that Data Controller must comply with basic min-
imum standards of information security management. In this regard, Data Controllers 
and Data processors are to ensure Confidentiality, Integrity and Availability. Interview-
ees have all indicated that their organizations have created a new role for Data Protec-
tion Officer within the organization and have appointed or are i n the process of appoint-
ing an officer to fulfill that role.  
Regarding data breaches, implementation of NDPR imposes new obligation for Data 
Controllers to notify the regulatory authority and data subjects of any data breaches 
without delay. In this regar d, Data Controllers need to put in place notification mecha-
nisms which may require serious changes to the existing systems especially in the area 
of new technologies. When asked to indicate whether or not their organizations have a 
register for data breach es and security incidence, interviewees across the sectors inves-
tigated indicated “ NO” suggesting that they do not currently have an internal data pro-
tection policy in place to support implement ation of this NDPR commitment.  
Finally, implementation of the NDPR requires that Data Control lers and Data Pro-
cessors  reckon with sanctions in their processing principle for failure to do so may cost 
them a fine of up to 10 million Naira (Approx.  USD27, 000, or €24, 000) by the regu-
latory  authority . Response from interviewees indicate that while organizations are con-
cerned about fines in monetary terms, they are more worried with brand image damage, 
and marketing and publicity. Thus, to ensure compliance and avoid these sanction s, 
Data Controllers have started to  review their privacy and protection measure s to keep 
to this requirement . 
Further, the analysis of the audit report and feedbacks from experts’ interviews led 
to identification of several  factors arising from the adoption and implementation of 
NDPR in Nigeria. While some of the factors are positive and favourable  in facilitating 
the implementation of NDPR conditions, some are negative, and hence inhibiting the 
implementation of NDPR measures. These identified factors are categorised in line with 
the three pillars of institutional theory framework as shown in Table  4. 10 
Table 4.  Institutional Factors in the Adoption of NDPR  
Institutional 
Pillar  Facilitators  Inhibitors  
Regulative  Well -articulated policies and 
guidelines  Lack of organizational plans and 
initiatives regarding NDPR  
NDPR Compliance monitor-
ing Absence of legal framework and 
policy  
Risk Assessment mechanism  Insufficient technical staff  & 
training  
Capacity building for senior 
managers on NDPR   
Normative  Awareness and sensitization 
of key stakeholders  Lack of industry -wide direction 
on how to achieve NDPR objec-
tives  
Monitoring & Evaluation of 
NDPR Implementation  Absence of review mechanism for 
NDPR  
Funding with regards to 
NDPR enforcement  Absence of budgetary provision 
for NDPR  
Cultural -
Cognitive  Involvement of key Stake-
holders in the implementation 
of NDPR  Resistance to change and risks of 
leaving personal data unprotected  
Communicating the benefits 
of Personal Data Protection to 
stakeholders  Not realizing the importance of 
NDPR.  
 
As Scott [16] pointed out, in most empirically observed institutional forms, a combina-
tion of the pillars are observed at work . Concerning the regulative pillar, formulation 
of well -defined policies appears to be the strongest factor facilitating the level of com-
pliance with NDPR measures amongst stakeholders. Thus, formulation of such policies 
received inputs from key actors thro ugh series of stakeholders’ engagements. Inhibiting 
factors under the same pillar include insufficient human capacity especially in technical 
areas and adequate training organized by top management . 
Regarding the normative pillar, it is important to note t hat some of the data control-
lers are willing to conform to industry best practices and recognize the opportunities 
therein, however, data subjects are not yet conversant with the newly acquired rights as 
a result of the introduction of the NDPR. The findin gs from  this stud y therefore suggest 
the need for continuous awareness for data subjects and adequate training for  data con-
troller s at organizational level . A major inhibiting factor highlighted under this pillar 
relates to absence or lack of funding for N DPR activities.  
Regarding the Cultural -Cognitive pillar, involvement of key stakeholders in NDPR 
implementation and communicating the benefits of Data Protection to stakeholders 
were highlighted as facilitating factors. Under the same pillar, resistance to  changes and 
risks of leaving personal data unprotected are highlighted as inhibiting factors. It is 11 
therefore imperative to learn how to deal with the cultural changes inherent in data 
privacy and protection initiatives such as NDPR . 
6 Conclusion and Future  Work  
This study attempts to identify Data Protection Regulation requirements and implica-
tions for Data Controllers and Processors in the context of a developing country - Ni-
geria. The research study analysed these implications using  the institutional the ory 
framework. Although the NDPR was launched in January 2019, most Data Controllers 
and Processors are yet to realise the complexity of its implications . 
The identified implications of the new changes introduced by NDPR adoption and 
implementation were c ollated with 5 key aspects  prioritised by Data Controllers for 
compliance in order to avoid sanctions from regulatory authority for non -compliance. 
Specifically, the research projects that controllers need to pay attention to awareness 
creation and designat e DPOs for proactive implementation of NDPR requirements. 
NDPR also specifies procedures for data processing in international context. Thus, Data 
Controllers must  consider this requirement when transferring personal data to third 
countries or international  organisations. Similarly, the NDPR introduces obligations 
that require Data Controllers to ensure data portability. Data Controllers also need to 
ensure uniform standards or interoperability of their procedures for seamless transmis-
sion of data. Regarding  data breaches, Data Controllers equally the appointment of a 
DPO that will support the compliance through the development of an internal data pro-
tection policy and a robust data breach reporting mechanism . Finally, as the NDPR also 
imposes sanctions for n on-compliance, Data Controllers need to review their strategies 
and reckon with sanction s and adequate budgetary provision to support compliance  
when planning.  
As part of the major contributions of this paper, some guidelines believed to facilitate 
the ins titutionalisation of NDPR measures across organisations were pushed forward. 
Evidence from this study as presented in the previous section suggests that use of insti-
tutional theory can help in interpretation of different level of NDPR adoption and im-
plemen tation by Data Controllers. It can also help to project policy direction enhancing 
the institutionalisation of NDPR measures across key sectors. It is imperative therefore 
to extend the scope of this study in the future to cover more organisations in order  to 
determine the extent to which the NDPR implementation facilitate s and support  the 
digital economy transformation in Nigeria . 
References  
1. Tikkinen -Piri, C., Rohunen , A., & Markkula, J. (2018). EU General Data Protection Regu-
lation: Changes and implications for personal data collecting companies. Computer Law & 
Security Review, 34 (1), 134 -153.  
2. Friedewald, M., Wright, D., Gutwirth, S., & Mordini , E. (2010). Privacy, data protection 
and emerging sciences and technologies: towards a common framework. Innovation –The 
european journal of social science research, 23 (1), 61 -67.  12 
3. Roos, A. (2006). Core principles of data protection law.  Comp. & Int'l LJS Afr., 39, 102.  
4. Gellman, R. (2014). Willis Ware's Lasting Contribution to Privacy: Fair Information Prac-
tices.  IEEE security & privacy , 12(4), 51 -54.Ge 
5. Law of the Federal Republic of Nigeria, (2004 ). Chapter 4, Section 37  fundamental right of 
citizens to pr ivate and family life . 
6. Nigeria Data Protection Regulation (2019). Art 1.1. Retrieved from:  https://nitda.gov.ng/wp -
content/uploads/2019/01/Nigeria% 20Data%20Protection%20Regulation.pdf   
7. Nissenbaum, H. (2004). Privacy as contextual integrity. Wash. L. Rev., 79 , 119.  
8. Lynskey, O. (2014). Deconstructing data protection: the ‘added -value’of  a right to data pro-
tection in the EU legal order.  International & Comparative Law Quarterly , 63(3), 569 -597. 
9. Steinke, G. (2002). Data privacy approaches from US and EU perspectives. Telematics and 
Informatics, 19 (2), 193 -200.  
10. Birnhack, M. D. (2008). The EU Data Protection Directive: An engine of a global regime. 
Computer Law & Security Review, 24 (6), 508 -520.  
11. United Nations Conference on Trade and Development UNCTAD. (2020). Data Protection 
and Privacy Legislation Worldwide Retrieved from: 
https://unctad .org/en/Pages/DTL/STI_and_ICTs/ICT4D -Legislation/eCom -Data -
Protection -Laws.aspx Accessed On: 18 February 2020  
12. Weerakkody, V., Dwivedi, Y. K., & Irani, Z. (2009). The diffusion and use of institutional 
theory: A cross -disciplinary longitudinal literature su rvey.  Journal of Information Technol-
ogy, 24 (4), 354 -368. 
13. Miles, J. A. (2012). Management and organization theory: A jossey -bass reader. () John 
Wiley & Sons.   
14. DiMaggio, P. J., & Powell, W. W. (1983). The iron cage revisited: Institutional isomorphism 
and c ollective rationality in organizational fields.  American Sociological Review, , 147 -160.  
15. Jennings, P. D., & Greenwood, R. (2003). 6bConstructing the iron cage: Institutional theory 
and enactment'.  Debating Organization: Point -Counterpoint in Organization Studies, 195   
16. Scott, R. W. (2008). Institutions and organizations: Ideas and interests.  
17. Clemens, E. S., & Cook, J. M. (1999). Politics and institutionalism: Explaining durability 
and change.  Annual Review of Sociology, , 441 -466.  
18. Appari, A., Johnson, M. E., & Anthony, D. L. (2009). HIPAA compliance: an institutional 
theory perspective.  AMCIS 2009 proceedings , 252.  
19. Greenway, K.E., and Chan, Y.E. (2005) “Theoretical Explanations for Firms‟ Information 
Privacy Behaviors,” Journal of AIS, 6, 6, 17 1–198 
20. D‟Arcy, J. and Hovav, A. (2009) “An Integrative Framework for the Study of Information 
Security Management Research,” in Jatinder Gupta and Sushil Sharma (Eds.), Handbook of 
Research on Information Security and Assurance, Idea Group Publishing, 55 –67. 
21. Dahiru, A. A., Bass, J. M., & Allison, I. K. (2014, November). Cloud computing adoption 
in sub -Saharan Africa: An analysis using institutions and capabilities. In  International Con-
ference on Information Society (i -Society 2014)  (pp. 98 -103). IEEE.  
22. Glaser , B. G., & Holton, J. (2004). Remodeling grounded theory.  Paper presented at the 
Forum Qualitative Sozialforschung/Forum: Qualitative Social Research  Author, F.: Article 
title. Journal 2(5), 99 –110 (2016).  
23. Naoum, S. G. (2013). Dissertation Research and Writing for Construction Students  (3rd ed. 
ed. Vol. Routledge 3rd ed. 2013)   
24. National Bureau of Statistics. (201 9). Nigeria Economy Largest in Africa . [Online]. Avail-
able: http://nigerianstat.gov.ng/  
 "
DataPrivacy,3009.txt,"Big Data, Not Big Brother: 
New Data Protection Laws and the Implications 
for Independent Media Around the World
AYDEN FÉRDELINE
June 2019Big Data, Not Big Brother:
New Data Protection Laws and the Implications 
for Independent Media Around the World
JUNE 2019
ABOUT CIMA
The Center for International Media 
Assistance (CIMA),  at the National 
Endowment for Democracy, works 
to strengthen the support, raise the 
visibility, and improve the effectiveness of 
independent media development throughout 
the world. The center provides information, 
builds networks, conducts research, 
and highlights the indispensable role 
independent media play in the creation and 
development of sustainable democracies. 
An important aspect of CIMA’s work is 
to research ways to attract additional US 
private sector interest in and support for 
international media development.
CIMA convenes working groups, discussions, 
and panels on a variety of topics in the 
field of media development and assistance. 
The center also issues reports and 
recommendations based on working group 
discussions and other investigations. 
These reports aim to provide policymakers, 
as well as donors and practitioners, with 
ideas for bolstering the effectiveness of 
media assistance.
Center for International Media Assistance  
National Endowment for Democracy
1025 F STREET, N.W., 8TH FLOOR
WASHINGTON, DC 20004
PHONE: (202) 378-9700
FAX: (202) 378-9407
EMAIL:  CIMA@ned.org
URL: https://cima.ned.org
Mark Nelson
SENIOR DIRECTOR
Nicholas Benequista
MANAGING EDITOR
Daniel O’Maley
PUBLICATION EDITORABOUT THE AUTHOR
Ayden Férdeline  is a T echnology Policy 
Fellow with the Mozilla Foundation, where 
he researches the ongoing development 
and harmonization of global data protection 
standards. He previously supported the 
Internet Society’s global public policy team 
and was a researcher for the data and 
analytics group Y ouGov. He is an alumnus 
of the London School of Economics and is 
based in Berlin, Germany.Contents
Introduction  .......................................... 1
Cutting Through the Complexity: Privacy,  
Data Protection, and Personal Information  ............... 4
Key Historical Developments in Privacy Law  ............. 6
Understanding Websites and Analytics  
and Balancing Interests  .............................. 12
User T racking by Independent Media Outlets  
in Developing Countries  ............................... 15
Immediate Privacy Gains Are Possible  .................. 20
Conclusion and Recommendations  .................... 22
Appendix A—T racking Domains Identified Through Study  . .24
Appendix B—Small Publishers Studied  ................. 25
Appendix C—Large Publishers Studied  ................. 26
Appendix D—Study Setup and T esting Parameters  ....... 27
Endnotes  .......................................... 28
Cover photo:  Left side, top; © pixinoo / Shutterstock.com1 Big Data, Not Big Brother: New Data Protection Laws and the Implications for Independent Media Around the World   #mediadevBut what started as a way to improve the user experience came with 
a downside for website viewers: it entailed collecting and processing 
their personal information, often without their knowledge or consent. 
Moreover, the drive to collect data has resulted in many independent 
media outlets in the Global South unknowingly permitting third parties, 
many of which cannot be identified, to invasively monitor their visitors.
Worldwide, citizens and policymakers are increasingly cognizant of 
the risks that the burgeoning data economy poses to personal privacy. 
In recent years, a wave of next-generation data protection laws have 
emerged that seek to restrict the collection, usage, and sharing of 
personal information. This is not necessarily a good news story for 
those news institutions that had successfully harnessed the value of 
analytics to grow advertising revenue or to better understand their 
audiences. These data protection regulations have, by design, severely 
hampered the environment within which many smaller digital media 
outlets operate. While these laws do not entirely restrict the use of 
analytics, they do restrict the use of analytic applications that place 
people at risk of harm. Indeed, there is growing evidence that some of 
the tracking mechanisms employed by digital news sites are potentially 
doing just that.
T o get a better understanding of how new privacy regimes will affect 
media in the Global South, this paper assembles a new set of findings 
on the websites of 50 small, independent news publishers from 10 
developing countries. It shows that third-party trackers are collecting 
audience data when people read articles, write comments, send in news 
tips, and share pieces on social networking platforms. One independent 
publisher in Nigeria, for instance, had 523 third-party cookies on its 
homepage collecting audience information. In total, over 150 companies—
not all of which could be identified—were found to be invisibly tracking the 
visitors to these 50 websites. They were collecting IP addresses, which 
can identify geographic locations, the titles and URLs of news articles 
read, search queries, and other data. Once collected, this information Introduction
For years, the road to news media financial sustainability was said to be 
paved with data—digital news outlets were counseled to collect as many 
details about their readers as possible. Tracking audiences was considered 
essential for optimizing search engine results, creating content that people want 
to read, and supporting targeted advertising to fund journalism. 
Worldwide, citizens and 
policymakers are increasingly 
cognizant of the risks that 
the burgeoning data economy 
poses to personal privacy. 2 CENTER FOR INTERNATIONAL MEDIA ASSISTANCE  CIMA.NED.ORGcould be sold to advertisers or further exchanged with other third parties. 
It could even reach the hands of governments. 
From a media development perspective, the failure of media outlets 
to protect their visitors against invasive tracking by third parties is 
troubling for two reasons. First and most importantly, it places the 
privacy and safety of a publication’s readers in jeopardy. Readers need 
to feel confident that visiting independent news sites, especially those 
covering sensitive issues, will not put them in danger. Second, from a 
business perspective, when publishers give away information about 
their audiences for free, they cede valuable leverage for negotiating with 
advertisers. In essence, the outsourcing of analytics to third parties 
potentially puts readers at risk and weakens a site’s ability to truly take 
advantage of the interactions it has with its readers. Taken together, the 
findings of this study suggest an important new frontier for the media 
development community and the need to build stronger awareness 
about and strategies for managing the threats posed by tracking the 
readers of independent media. 
This report also provides an overview of the latest regulatory 
developments in the data protection field, such as the European 
Union’s General Data Protection Regulation (GDPR). While much of the 
public debate has been about policy changes in Europe, the impact 
has been much broader in geographic scope. The changes underway 
directly impact independent news outlets in many developing countries. 
This review of new laws is followed by a detailed description of the 
various trackers currently used on news websites, and the ways that 
they potentially come into conflict with new data protection laws 
and regulations. Then, the report analyzes research on web tracking 
technologies used by news media websites in 10 developing countries, 
exposing pervasive tracking that ultimately may not benefit either 
the readers or the news organizations themselves. And finally, it 
concludes with suggestions about how news organizations and other 
media development stakeholders might be able to take advantage of 
the global shift in data protection laws and regulations to strengthen 
independent media. The findings of this study 
suggest an important new 
frontier for the media 
development community and 
the need to build stronger 
awareness about and strategies 
for managing the threats posed 
by tracking the readers of 
independent media. MORE SENSITIVE LESS SENSITIVE
SECRET 
Passwords
Passport Number
Health Data
GPS Coordinates
Religion
Political AffiliationPERSONAL
Name
Home Address
Email Address
Home Telephone 
Gender Identity
Date of Birth  
(in some circumstances)PUBLIC
Username
Language
Device Type
Cookie  
PreferencesANONYMIZED
What I Click
What Websites 
I Visit
What I Search ForNONPERSONAL
Weather or 
Temperature
Energy 
Consumption
3 Big Data, Not Big Brother: New Data Protection Laws and the Implications for Independent Media Around the World   #mediadevDEFINITIONS
What is privacy?
A generally accepted definition of privacy is “the claim of individuals, groups, or institutions to determine 
for themselves when, how, and to what extent information about them is communicated to others. ”1
What are data protection laws?
Data protection laws are frameworks that seek to regulate the collection, storage, and processing of 
information about individuals. 
Is all data protected by data protection laws?
No. Data protection laws apply only to personal and sensitive information. Data protection laws do not 
protect nonpersonal data,2 anonymized data, or public data.3 This is an important distinction because 
many common data analytic practices do not use personal information at all.
What is personal information?
There is no universal definition of what is or is not personal information. However, a common definition 
found within many national laws and international agreements modelled after the European Union’s 
GDPR is that personal data “means any information relating to an identified or identifiable natural 
person. ”4 Some data elements very clearly count as personal or secret information, such as a name 
or passport number. But the answer is not so straightforward for other elements. For example, a date 
of birth in and of itself is not personal information. But if that can be combined with a street address 
and one’s gender, it could be used to identify someone, and in that instance should be treated as 
personal information.5Note: While privacy is a disputed concept in law and philosophy, and attitudes toward how personal information is used 
vary from individual to individual, it is generally accepted that some data elements are more sensitive than others. FIGURE 1.   The Range of Consumer Privacy Levels160
140
120
100
80
60
40
20
0
1979 1989 1999 2009 March 20199214180134
4 CENTER FOR INTERNATIONAL MEDIA ASSISTANCE  CIMA.NED.ORGMedia companies have also begun using similar data sets to understand 
reader preferences, follow up on stories, and create content that responds 
to audience demand. Personal information has come to be seen by some 
companies as an economic asset to be harvested or as a tool to better 
inform editorial decisions. At the same time, individuals have reported 
feeling powerless to stay in control of how their personal information is 
being used. Increasingly, however, there are regulatory barriers that restrict 
these activities.
As of March 2019, 134 countries had enacted data protection laws,6 
while 26 others7 had drafted legislation with some degree of government 
support. While there are exemptions within many data protection laws 
for journalistic activities like newsgathering, there are almost always 
implications within these laws for the “business side” of media companies Cutting Through the Complexity: Privacy, 
Data  Protection, and Personal Information
As the internet has grown in social and economic importance, more and 
more people have begun engaging with technologies that surreptitiously 
undermine their privacy. Businesses have emerged with business models 
that are based on gathering, using, and selling personal information without the 
data subject’s knowledge or explicit consent. 
Note: The number of countries with data protection laws has risen to 134.
SOURCE:  Graham Greenleaf, “Global Data Privacy Laws 2017: 120 National Data 
Privacy Laws, ” Privacy Laws & Business International Report 145 (2017): 
10-13, https:/ /papers.ssrn.com/sol3/papers.cfm?abstract_id=2993035 . 
Supplemented by further research by the author.FIGURE 2.  Number of Countries with Data Protection Laws Personal information has 
come to be seen by some 
companies as an economic 
asset to be harvested or 
as a tool to better inform 
editorial investments. 5 Big Data, Not Big Brother: New Data Protection Laws and the Implications for Independent Media Around the World   #mediadevand journalistic institutions. These laws are particularly likely to apply 
if a news organization’s website collects data about its audience, loads 
elements onto a webpage from a third party, or uses cookies. It would not 
be feasible for most entities to adhere to the unique laws of 134 countries; 
however, it is also not necessary to do so. Adopting the highest data 
protection standard available is the most straightforward approach to 
compliance. At this time, that standard is the European Union’s General 
Data Protection Regulation (GDPR). Achieving compliance with the GDPR 
and its principles of accountability and data minimization would place most 
media organizations in good standing, even if they operate in a different 
regulatory environment.
Although European nations represent a minority of those jurisdictions with 
data protection laws, Europe continues to have an outsized influence on 
the development of data protection laws in Africa, Asia, and the Americas. 
This is unlikely to change in the foreseeable future because the Council of 
the European Union has advised the European Commission that it cannot 
negotiate away privacy rights in trade agreements.8 Countries that wish to 
trade with the European Union, and, in particular, have data flows with the 
trading bloc, will thus face pressure to implement data protection laws that 
are influenced by the European standard. FIGURE 3.  Countries with Data Protection Laws
Note: Most data protection laws apply to both the public and private sectors, but there are 
exceptions. The enforcement of these laws varies from jurisdiction to jurisdiction. 
SOURCE:  These data are based upon original research and analysis by the author.Law applies to 
both public and 
private sector
Law applies to 
private sector only
Law applies to 
public sector only
Law has unknown 
scope of applicability
No law or no 
data available
Although European nations 
represent a minority of 
those jurisdictions with 
data protection laws, 
Europe continues to have 
an outsized influence on 
the development of data 
protection laws in Africa, 
Asia, and the Americas. 6 CENTER FOR INTERNATIONAL MEDIA ASSISTANCE  CIMA.NED.ORG
1 There must be no secret record-keeping systems. 
2 Individuals must be able to find out what information 
about them is in a record and how it is used. 
3 Information cannot be obtained for one purpose and then used for 
another purpose without the consent of the individual concerned. 
4 There must exist a right to correct inaccurate records. 
5 Organizations are responsible for ensuring that their 
record-keeping systems are secure and reliable, and 
must take precautions to prevent the misuse of data. 
Following the passage and implementation of the act, the United States 
advocated for these principles internationally. T oday, they can be found 
in every major privacy protection instrument, including the African 
Union Convention on Cyber Security and Personal Data Protection, 
the Asia-Pacific Economic Cooperation’s Cross-Border Privacy Rules, 
the Organisation of Eastern Caribbean States’ Data Protection Bill, the 
Organisation for Economic Co-operation and Development’s Guidelines 
on the Protection of Privacy and T ransborder Flows of Personal Data, the 
Council of Europe’s Convention for the Protection of Individuals with regard 
to Automatic Processing of Personal Data, the European Union’s Data 
Protection Directive, and the GDPR.
The GDPR came into effect in 2018. It was a major revision to European 
law that significantly built upon the principles contained within the US 
Privacy Act of 1974 and the values advanced within the EU Data Protection 
Directive of 1995, aiming to prohibit the excessive collection, use, and 
disclosure of personal information without disproportionately impeding Key Historical Developments in Privacy Law
Privacy rules and norms that govern action or inaction related to our personal 
information have been interpreted in a similar way around the world for 
some time. In 1974 the United States adopted the Privacy Act,9 a federal law 
that sought to safeguard information about individuals held by federal agencies. 
The act codified into law the recommendations developed by an independent 
advisory committee in 1973 that had analyzed the consequences of using electronic 
systems to maintain records about people.10 Their report shaped our contemporary 
understanding of information privacy and it remains relevant some four decades 
later. In short, the committee recommended adopting five principles: 
Data protection laws 
are frameworks that 
seek to regulate the 
collection, storage, and 
processing of information 
about individuals. 7 Big Data, Not Big Brother: New Data Protection Laws and the Implications for Independent Media Around the World   #mediadevcommerce, free expression, or freedom of association. Whether this 
balance was successfully achieved remains hotly debated, but what is 
uncontested is that this legislation forced companies around the world to 
review their data processing activities. 
GDPR in a Nutshell
The GDPR codifies into law a risk-based approach to protecting the privacy 
of natural persons. It requires privacy by design and by default, mandates 
accountability for data controllers, and grants individuals new rights, 
including the rights to erasure and to control and transparency over how 
their personal information will be used. The GDPR states that personal 
information must be retained for the shortest period of time possible and 
that there must be limits on who can access it. It also imposes significant 
restrictions on how and when personal information may be shared with 
third parties. Further, the GDPR grants new protections to sensitive 
information like medical data, and Article 7 states that if an individual 
is asked to consent to a data processing practice, their consent must 
be a “freely given, specific, and unambiguous” indication of their intent. 
Most strikingly, the penalties for noncompliance are set at €20 million 
($22.4 million) or 4 percent of global revenue, whichever is higher, even 
where there is no ill intent on the part of the data controller.
Extraterritoriality
A major difference between the GDPR and other data protection laws is 
that it has extraterritorial effect, meaning that enforcement is theoretically 
possible outside of the borders of the European Union. This has made 
Europe, in the eyes of some, the “world’s data police. ”11 The consequence 
of this is that under the GDPR, even organizations outside of the European 
Union must comply with the GDPR when they process data belonging to 
individuals in the European Union. Because of the global nature of the 
internet, it is easy to imagine a European resident who is protected by the 
GDPR visiting the website of a publisher, say, in Belarus or Mongolia. At 
least in theory according to European Union regulations, that Belarusian 
or Mongolian publisher must adhere to the GDPR if collecting analytic data 
about that European resident. If such extraterritorial enforcement actually 
happens, there would be profound implications here for the digital media 
ecosystem worldwide.
At this point in time it is difficult to know what obligations will actually 
be enforced on entities located outside of the European Union. One of 
the largest ambiguities that the internet presents, when it comes to the 
applicability of legislation, is that it is a space where conventional nation-
state borders do not exist and where traditional modes of interstate legal 
cooperation have struggled to keep pace with the realities of a Web 2.0 The GDPR codifies into law 
a risk-based approach to 
protecting the privacy of natural 
persons. It requires privacy 
by design and by default, 
mandates accountability for 
data controllers, and grants 
individuals new rights, 
including the rights to erasure 
and to control and transparency 
over how their personal 
information will be used. 8 CENTER FOR INTERNATIONAL MEDIA ASSISTANCE  CIMA.NED.ORGworld. This is why earlier data protection laws have been difficult to enforce 
and why the European Union, in the GDPR, has sought to make its legislation 
applicable in all environments. The fear, however, is that this jurisdictional 
overreach could lead to a legal arms race that could have unpredictable 
and unintended consequences. So far, the GDPR’s enforcement bodies 
have been reluctant to impose penalties on data controllers outside of the 
European Union. In one notable example, the United Kingdom’s Information 
Commissioner’s Office sent a letter to the Washington Post  advising that 
its website did not comply with the GDPR, but it did not take any formal 
enforcement action.12 This suggests that, at least for now, the European 
Union will rely on indirect means of enforcing the GDPR outside of its 
borders, incentivizing self-compliance through fear of reputational damage.
GDPR Is the New Global Standard
T en countries outside of Europe have now updated their earlier data 
protection bills to enact many (or all) of the principles contained within 
the GDPR. This trend appears to be continuing, with new or updated bills 
pending in Algeria, Indonesia, Thailand, and T unisia that appear to have 
been modelled after the GDPR. Pakistan, which does not have any data 
protection legislation at present, currently has a bill under consideration 
that would adopt large chunks of the GDPR. 
“I assume that lawmakers just copied and pasted the GDPR and left some 
things out actually, ” said Salwa Rana, legal officer at Media Matters for 
Democracy in Pakistan.13 “And these things were that you need to inform 
the data subject of any leak that takes place, that the data subject has 
the right to be forgotten, and extraterritoriality. ” Rana said the question 
of extraterritorial application is one that remains unaddressed. “This was 
one of the main questions that was raised in one of our consultations: Is 
enforcement of the law going to be limited to Pakistan? The problem is that 
we have the federal investigation authority which is going to be responsible 
for any violations under this law outside of Pakistan, yet under the 
proposed legislation, they haven’t given them any power. ” While the GDPR’s 
exemptions for journalistic activities remain in the proposed bill, media 
organizations in Pakistan have not been actively involved in drafting this 
law. “There hasn’t been much response from media companies, but I feel 
like the way this law is going, they are going to have to begin participating. ”
Regardless of whether or not more countries adopt the GDPR’s provisions, 
given the global nature of the internet and many businesses’ desire to trade 
with member states of the European Union, a need to comply with the 
GDPR has incentivized businesses such as Microsoft to voluntarily adopt 
higher privacy and data protection standards for their entire operations 
worldwide, even where they are under no legal obligation to do so.14So far, the GDPR’s 
enforcement bodies 
have been reluctant to 
impose penalties on data 
controllers outside of 
the European Union. 9 Big Data, Not Big Brother: New Data Protection Laws and the Implications for Independent Media Around the World   #mediadevGDPR Implications for the Media Industry
Article 4 (7) of the GDPR defines a data controller as “the natural or legal 
person, public authority, agency or other body which, alone or jointly 
with others, determines the purposes and means of the processing of 
personal data. ” By this broad definition, it is difficult to imagine any media 
organization with either a list of subscribers or a website with analytic 
functions that would not  be considered a data controller. As a result, any 
journalistic institution whose content is accessible to European residents 
will need to think carefully about how the GDPR may impact their 
business development activities or editorial functions. Some potential 
ramifications include the following:
Impacts on Newsgathering
The GDPR states  that the 
… processing of personal data solely for journalistic purposes, or for the 
purposes of academic, artistic or literary expression should be subject 
to derogations or exemptions from certain provisions of this Regulation 
if necessary to reconcile the right to the protection of personal data with 
the right to freedom of expression and information.15
While this language provides the media with significant leeway to be able 
to publish journalistic work, it does not assist journalists in accessing 
information for journalistic purposes.
Ioana Avadani, executive director of the Center for Independent 
Journalism in Bucharest, said the GDPR has been used in Romania to 
protect those in positions of power.16 “What we witnessed immediately 
after the GDPR is that institutions started to invoke the GDPR as a 
reason not to release information, ” she said. “They were not keen on 
releasing information before, so what they got is just another reason, and 
they are very happy that this is a legal reason to justify their less-than-
transparent attitude. ” 
Avadani pointed to an example of a protest that occurred in August 2018 
where riot police in Bucharest behaved in a violent manner and physically 
assaulted demonstrators. After a journalist asked who had authorized this 
action, the police invoked the GDPR and refused to name the authorizing 
officer. “It was a clear case of public information, and they still refused 
to say it because they wanted to protect the government, ” said Avadani. 
Their next reaction was to use the GDPR to attempt to force the journalist 
to reveal their source, claiming the police department had an obligation 
under the GDPR to investigate a data breach. “It was not a genuine 
concern for the protection of the police officer’s privacy, it was just a way 
to protect the authorities. ” 
© Urban Fenix / Shutterstock.com
“What we witnessed 
immediately after the GDPR 
is that institutions started to 
invoke the GDPR as a reason 
not to release information…
They were not keen on 
releasing information before, 
so what they got is just 
another reason, and they are 
very happy that this is a legal 
reason to justify their less-
than-transparent attitude.”
— IOANA AVADANI,
Center for Independent  
Journalism in Bucharest10 CENTER FOR INTERNATIONAL MEDIA ASSISTANCE  CIMA.NED.ORGThis is not the only case of the GDPR being abused in Romania. RISE 
Project, a non-profit investigative journalism organization, was threatened 
with a €20 million ($22.4 million) fine from Romania’s data protection 
authority after publishing a post on Facebook that accused a prominent 
Romanian politician of theft. RISE Project subsequently published a letter 
it had received from the National Supervisory Authority for Personal Data 
Processing, which demanded that it disclose within 10 days “how and when 
RISE Project obtained the information ultimately posted to Facebook, who 
their source was, how they stored the documents, and what other personal 
information RISE Project has on [the politician] and their friends, ”17 or 
face a penalty of €20 million. While it is doubtful such a fine would stand 
up in the highest courts of the European Union, for smaller media outlets 
the fear of costly, ongoing litigation could ultimately have a chilling 
effect on journalism.
“Right to Erasure” Impact
The “right to erasure, ” also known as the right to be forgotten, has garnered 
significant attention but is often misunderstood. The right is not absolute, 
with Article 17 (3) of the GDPR offering a public interest exemption 
intended to safeguard against predicted abuses. The problem is that 
the GDPR’s Recital 153 states  that “Member States law should reconcile 
the rules governing freedom of expression and information, including 
journalistic, academic, artistic and or literary expression with the right to 
the protection of personal data pursuant to this Regulation. ”18 This means 
that there could be a patchwork quilt of interpretations for how this article 
should be implemented. Unfortunately, in Romania, the data protection 
authority has settled upon a definition that seems to have prioritized the 
right to privacy over freedom of expression in all circumstances.
Ziarul de Iasi,  a local newspaper in Romania with a circulation of 5,000 
copies per week, received a right to erasure request to delete an article 
from its online archive. After the newspaper refused to remove an article 
from nine years earlier about a public figure who had engaged in improper 
behavior, the National Supervisory Authority for Personal Data Processing 
sent a letter imposing a fine of 3,000 leu ($725) per day until the article 
was deleted. “For a local newspaper this is huge, ” said Avadani. “In this 
particular case, Ziarul de Iasi  is going to challenge the request in court. 
However the editor-in-chief told me if he keeps receiving requests like this, 
he may not be able to afford to keep challenging them. ”While it is doubtful such 
a fine would stand up in 
the highest courts of the 
European Union, for smaller 
media outlets the fear of 
costly, ongoing litigation 
could ultimately have a 
chilling effect on journalism.11 Big Data, Not Big Brother: New Data Protection Laws and the Implications for Independent Media Around the World   #mediadevImpacts on Internal Operations and Website Functionality
Under the GDPR, data controllers are obligated to ensure that both their 
data processing practices and the data processing practices of third 
parties comply with the regulation. This necessarily requires that media 
organizations more closely scrutinize the activities of the third-party 
vendors they work with. 
Ala’a Alzghoul, an information systems specialist with Arab Reporters 
for Investigative Journalism in Jordan, explained how the GDPR 
prompted his organization to develop internal procedures for assessing 
how third parties handle personal information.19 “For example, we use 
Google Analytics to collect some data for the user experience. Before we 
added their plug-in, we first read the privacy policy of Google Analytics 
and asked for every detail as to what data this tool is collecting, we 
tracked what they actually do, and we mention those details in our 
privacy policy. ” Alzghoul explained that the GDPR also prompted Arab 
Reporters for Investigative Journalism to develop new procedures for 
handling personal information. “T o prevent any data leakages, we moved 
from regular databases to encrypted databases, ” he said. “We have a 
new policy to protect the personal data that we collect, and to prevent 
employees [from] just copying the data onto their laptops. But this 
happened because of the GDPR, not because we were afraid of the laws 
here in Jordan. ”Under the GDPR, data 
controllers are obligated 
to ensure that both their 
data processing practices 
and the data processing 
practices of third parties 
comply with the regulation. 
© pixinoo / Shutterstock.com1
2
3
12 CENTER FOR INTERNATIONAL MEDIA ASSISTANCE  CIMA.NED.ORGUnderstanding Websites and Analytics 
and Balancing Interests
When someone visits a newsstand and buys a printed newspaper, they 
receive a complete product. But when a visitor browses a webpage, 
their web browser does not download one file. Rather, the web browser 
reads the code, downloads the required content from various sources, and 
renders the page. This all happens in milliseconds. The output may appear to 
the reader as one complete package, but more happens behind the scenes 
than many people realize, with content typically being downloaded from both 
first-party and third-party sources.
When the webpage loads, this image is downloaded 
from https:/ /es.mercopress.com/web/img/
mp-logo.png . Because it comes directly from 
MercoPress’s website, it is first-party content.
First-party content does not currently face 
legal or regulatory challenges.
But the ads? They are downloaded from pagead2.
googlesyndication.com/pagead/js /adsbygoogle.js .
Third-party content comes from a different 
source. Because this content comes from 
GoogleSyndication.com and not MercoPress.com, 
it is third-party content.
Third-party content is impacted by the GDPR.First-party content comes from the same location 
as the webpage itself. For example, if you visit 
https:/ /es .MercoPress.com …13 Big Data, Not Big Brother: New Data Protection Laws and the Implications for Independent Media Around the World   #mediadevWhen content is being downloaded, the browser sends an HTTP request to 
either retrieve information from a server or send data to a server.20 As part 
of this interaction, the server obtains the visitor’s IP address to learn who 
it is interacting with. Y ou could think of an IP address as the return address 
on a letter you mail; it is a unique number that essentially identifies you by 
the device you’re using to connect to the internet, and can be linked to all 
the online activity you engage in on that device. 
When first-party content is downloaded, a website visitor would reasonably 
expect that they are sharing their IP address to access that content. The 
situation becomes murkier with third-party content. Because the website 
viewer’s IP address is being collected by an external source with which 
they do not have a direct relationship—and since these third-party content 
elements can either be invisible, blend into the webpage, or just load on the 
webpage without the individual’s explicit consent—their IP address would 
be collected by a third party without the individual’s knowledge or approval. 
Moreover, it is rare that only an IP address will be captured. While an IP 
address does constitute personal information, it is extremely common 
for third parties to collect information on individuals through cookies, 
web beacons, and application program interfaces, among other technical 
measures, as people browse digital properties. These allow for individuals 
to be targeted in a much more granular manner and to be tracked across 
the entire internet.When first-party content is 
downloaded, a website visitor 
would reasonably expect 
that they are sharing their IP 
address to access that content. 
The situation becomes murkier 
with third-party content. 
How Website Visitors are Tracked
Cookie A cookie is a message that a server sends to a web browser to store on the website visitor’s 
computer. This file is then sent back to the server each time the visitor’s web browser requests 
content from that particular server.
Web beacon A web beacon is a small image, usually one pixel by one pixel in size, that is discretely placed on 
a website to monitor visitor behavior. When the image loads, the web beacon passes information 
along to the server where the image is stored, including the IP address of the computer that 
retrieved the image, the time the web beacon was viewed, the type of browser that retrieved the 
image, and cookie values.
Application 
program 
interfaceAn application program interface determines how different software applications and 
components should interact with others. They are building blocks that website developers can use 
to pull and share data. For instance, Amazon’s Product Advertising application program interface 
allows another website to search Amazon’s product inventory and to then add personalized 
functions to its website advertising Amazon’s products.14 CENTER FOR INTERNATIONAL MEDIA ASSISTANCE  CIMA.NED.ORGHistorically it has even been possible for companies to track website 
viewers without using cookies or web beacons or deploying application 
program interfaces. There are various fingerprinting algorithms that 
have enabled data sets to be analyzed in a manner that, for all practical 
purposes, could uniquely identify an individual with a high degree of 
accuracy. When a visitor downloads a file from a third party, the third 
party necessarily obtains a user agent string (the website visitor’s 
operating system, web browser type, and version number) and accepts 
headers (the type, version, and capabilities of the browser that is making 
the request so that the server returns compatible data). If JavaScript 
is enabled, it can communicate the names of browser plug-ins that 
are installed, and these plug-ins can be called upon to share system-
specific attributes. Many of these attributes are, in and of themselves, 
harmless, but when aggregated, can effectively and easily lead to the 
identification of a user.21 
T o grasp how the tracking of readers involves a variety of distinct and 
independent entities, imagine a scenario in which an individual lands on 
a fictitious new site, NewsWebsite.com, to read an article on nutrition. 
The reader’s presence on the site is collected by Analytics.com, a 
third-party audience measurement tool firm that NewsWebsite.com 
has enabled on its website. Analytics.com collects data from millions 
of websites using cookies, and this data could include the visitor’s past 
shopping behavior, interests, time zone, ethnicity, browser language 
preferences, and gender, among other information. Based on these data 
compiled by Analytics.com, another third-party, Shopping.com, knows 
that the visitor is female, aged 40, and previously spent $60 on a book. 
Shopping.com could now infer that the visitor is likely to be interested 
in hardcover recipe books, and so sends a request to Advertising.com 
to load an advertisement for a hardcover recipe book on the next page 
that the visitor loads on the NewsWebsites.com site. In this scenario, 
personal data about the reader would have been circulated with at 
least four different entities, some of which the reader herself might not 
even know about.
Over the past three years web browsers and mobile devices have 
begun masking header information by default. This reduces, but does 
not entirely eliminate, the potential for reidentification through this 
manner. These changes have arguably been implemented in reaction 
to new privacy regulations like the GDPR, which oblige data controllers 
to provide individuals with an effective means of exercising their data 
rights. Users with older mobile devices or web browsers that have 
not been updated may be particularly vulnerable to identification 
through fingerprinting.There are various 
fingerprinting algorithms 
that have enabled data sets 
to be analyzed in a manner 
that, for all practical 
purposes, could uniquely 
identify an individual with a 
high degree of accuracy. 15 Big Data, Not Big Brother: New Data Protection Laws and the Implications for Independent Media Around the World   #mediadevHowever, 15 percent of the tracking devices we found on independent 
news websites had no easily identifiable ownership. Some of the most 
pervasive trackers on independent news websites in Kenya, Nigeria, and 
Ukraine, for example, actively masked their identities. In such cases the 
average website visitor would not know who is collecting their personal 
information or for what purpose. At least when the owner of a cookie can 
be identified, users can make contact with them to exercise their rights. 
Overall, the analysis included 100 news websites in Argentina, Brazil, Egypt, 
India, Indonesia, Kenya, Nigeria, Syria, Ukraine, and Uruguay—five small 
publishers and five large publishers in each country. The measurements 
were conducted using the open source OpenWPM platform, which was 
developed by scientists at Princeton University. This tool has been used 
in 22 academic studies, and it allows researchers to systematically and 
reliably quantify, understand, and uncover the ways in which website users 
are tracked across the measured websites.22 The OpenWPM tool was 
deployed using a local virtual private network (VPN) to imitate the website 
experience of a local website visitor (with the exception of Syria, which 
does not have a VPN, where sites were visited using a T urkish VPN).
The small, independent publishers chosen for analysis were selected 
based on recommendations from respected journalists and media 
policy advisors in the field. The ownership structures and funding 
sources of these websites were also taken into account to verify 
their independence. T o be included, the sites also had to be posting 
original content consistently for three or more years. For the large 
publishers, we included the five most visited news websites in the given 
country in January 2019, per Alexa Internet’s rankings. In Uruguay, 
the top three most visited news websites were Argentine, so instead 
the analysis included the five most visited news websites that were 
published out of Uruguay.User Tracking by Independent Media Outlets 
in Developing Countries
This report’s analysis of news websites in developing countries sheds 
light on the pervasiveness of user tracking on these news sites. Of the 
50 small, independent publishers studied, 92 percent contained third-
party tracking devices such as cookies and web beacons. Most of the tracking 
devices whose owners we could identify were transmitting data to the United 
States or the European Union. 
Some of the most pervasive 
trackers on independent news 
websites in Kenya, Nigeria, and 
Ukraine, for example, actively 
masked their identities. In such 
cases the average website visitor 
would not know who is collecting 
their personal information or 
for what purpose.Key Findings
250
200
150
100
50
0
Pubmatic Google Verizon Rubicon Project Brightcove
16 CENTER FOR INTERNATIONAL MEDIA ASSISTANCE  CIMA.NED.ORGWhile the study was not exhaustive, it is significant because it found that over 
150 companies—not all of whom we could identify—were invisibly tracking 
the visitors to these 50 independent news websites. They were collecting IP 
addresses, which can identify one’s geographic location, the titles and URLs 
of news articles read, search queries, and other data. Once collected, this 
information could be sold to advertisers or further exchanged with other third 
parties. Any data that are collected are also vulnerable to being stolen in a 
data breach, or obtained by a government through a court order. 
The situation was no better for large, mainstream media websites in the 
same countries. Overall, 98 percent of the large news websites that we 
analyzed contained third-party cookies. With the exception of websites 
reviewed from Brazil and Nigeria, the large news websites contained more 
third-party cookies than their independent counterparts did.
FIGURE 4.  Top Five Identifiable Tracking Companies on Independent Media Websites
Note: The top five tracking companies across the 50 independent media websites whose ownership could be identified, as 
tested on April 11, 2019. This excludes trackers—some of which would have otherwise made the top five—that operate in an 
opaque manner and do not disclose for whom they are collecting and sending data.92%
of these  
independent media 
websites contained  
third-party  
tracking devices167
unique companies 
are monitoring 
website visitors 
on independent 
media websites523
cookies were found  
on one single 
webpage of an 
independent news 
website in Nigeria1
independent news website 
in Ukraine had been 
compromised by a third party 
and was distributing invasive 
malware to website visitors180
160
140
120
100
80
60
40
20
0
Argentina Brazil Egypt India Indonesia Kenya Nigeria Syria Ukraine Uruguay
Independent Media Mass Media
17 Big Data, Not Big Brother: New Data Protection Laws and the Implications for Independent Media Around the World   #mediadevUses and Purposes of Tracking Devices
In the analysis of 50 independent news websites, seven common uses of tracking devices were 
identified—along with one uncommon, but problematic, use.FIGURE 5.  Number of Third-Party Trackers (Average on Website Homepage)
Note: With the exception of Brazil and Nigeria, the homepages of large publishers tended to contain more third-party 
trackers than the homepages of small publishers. Test conducted April 11, 2019.
Commons Uses of Tracking Devices (in alphabetical order)
Advertising Displaying online advertisements creates a very significant stream of income 
for many news websites. Tracking devices are commonly used to embed third-
party advertisements and to exchange reader data to display targeted, behavioral 
advertisements. The most common advertising networks we found were PubMatic, 
Google AdSense and Google DoubleClick, and Rubicon Project.
Potential for Privacy Violation:  HIGH 
Audience 
MeasurementNews websites deploy technical measures to determine the number of unique 
website visitors, the number of pages visited, and the average time spent on the 
webpage. Cookies are used to determine repeat visitors and data can be exchanged 
with third parties to build demographic profiles of visitors. The most common 
audience measurement instruments we found were Google Analytics and Adobe 
Experience Cloud.
Potential for Privacy Violation:  MEDIUM 
continues next page18 CENTER FOR INTERNATIONAL MEDIA ASSISTANCE  CIMA.NED.ORGCommons Uses of Tracking Devices (in alphabetical order)
Content Hosting Most news websites use third-party hosting providers, either to host their website or to cache 
content so that it loads more quickly. Common providers include Amazon Web Services and 
Cloudflare. Some publishers use third-party content libraries, such as Adobe Fonts, to improve 
the appearance of their websites. Others use tools like YouTube to host videos because it is 
either cheaper than self-hosting that content, or easier to extend the functionality of their 
websites by turning to a third-party application. In all these instances, when content is being 
loaded through an intermediary, it exposes a visitor’s IP address (or more) to the third party.
Potential for Privacy Violation:  MEDIUM 
Design 
OptimizationNews websites sometimes conduct design experiments using real audiences to understand 
what design changes could keep visitors on their website longer or improve website usability. 
These tools, like Apptimize, Optimizely, and Splitforce, do not typically collect personal 
information and data are usually accessible only by the publisher.
Potential for Privacy Violation:  LOW 
Recommendation 
SystemsSome news websites use content recommendation engines to encourage website viewers to 
remain on the same website, or to visit a partner’s website to read an article that the publisher 
believes the reader will find interesting. These tools are thought to be behind many of the 
opaque tracking devices that we found, as their content changes dynamically and is updated by 
a code that the website owner is unable to closely scrutinize. 
Potential for Privacy Violation:  HIGH 
Social Media 
Share ButtonsMany news websites have embedded social media share buttons into their websites. These 
are intended to facilitate sharing news articles via the most common social media platforms. 
The most common social media share buttons we saw were for Facebook, which, if its button is 
embedded into a webpage, results in audience information being shared with Facebook. Other 
buttons included the Twitter icon and the ShareThis widget.
Potential for Privacy Violation:  HIGH 
Visualizations Some news websites use third-party tools to attractively display their stories, for instance, by 
embedding interactive maps, using Scribd to embed annotated documents into a page, or using 
Tableau to create column graphs or pie charts.
Potential for Privacy Violation:  MEDIUM 
Uncommon Uses of Tracking Devices
Malware 
DistributionOne independent Ukrainian news website was identified as a vector for disseminating malware. 
Malware is software that is designed to allow an unauthorized actor to gain access to a 
computer. It was difficult to decisively attribute the source and creator of the malware being 
distributed, as it actively masked its identity, but this particular content was embedded into the 
Ukrainian website through a third-party advertisement and appeared to be transmitting data to 
servers in Russia.
Potential for Privacy Violation:  HIGH No Tracking
Tracking8%
92%
19 Big Data, Not Big Brother: New Data Protection Laws and the Implications for Independent Media Around the World   #mediadevNinety-two percent of the independent media websites and 98 percent of 
the mass media websites reviewed contained third-party trackers of some 
kind. In the context of independent media websites, these trackers were 
most commonly deployed for audience measurement purposes, followed 
by advertising and marketing purposes, and then by the inclusion of social 
media share buttons. 
According to Valentina Pavel, who researches data ownership at Privacy 
International, “user tracking and exploitation of data is still the default for 
news websites, but this is changing. ”23 She said readers are looking for 
something else and that publishers can turn data protection principles to 
their competitive advantage. “Be fair and clear to your readers, show them 
you have been thoughtful about the way you are handling their data, and 
collect only the type of data that is necessary for the smooth running of 
the website and explain in plain language why you made those choices. ” 
She noted that large publishers like the New York Times  have dropped 
behavioral advertisements from their websites altogether without suffering 
any revenue impact and believes this has paved the way for smaller 
publications to do the same. “People are looking for real guarantees that 
their data is not going to be exploited, so by all means, don’t sell or share 
user data, and stop or limit using first- or third-party cookies, ” said Pavel. 
“If others do it, why can’t you?”
FIGURE 6.   Third-Party Cookies on Homepages of Independent 
Media Websites
Note: It is important to note that during our test, we did not press the “consent” button 
on websites to permit the collection and use of cookies. Accordingly, these results should 
be read to show that 92 percent of websites tested had third-party tracking devices that 
activated even in the absence of visitor consent.“Be fair and clear to your 
readers, show them you 
have been thoughtful 
about the way you are 
handling their data, and 
collect only the type of 
data that is necessary for 
the smooth running of the 
website and explain in 
plain language why you 
made those choices.”
— VALENTINA PAVEL,
Privacy International20 CENTER FOR INTERNATIONAL MEDIA ASSISTANCE  CIMA.NED.ORGMoving from Third Party to First Party (in alphabetical order)
Advertising Using a third-party ad exchange, like Google AdSense, to fund journalism sites makes 
it impossible to eliminate the privacy risks that these tools present. 
However, a website owner may decide not to use an ad exchange, opting instead to 
handle advertising sales internally. This has been an approach that larger publishers 
have been taking post-GDPR, as it can also result in higher revenue from direct sales 
negotiations. If advertisements are not targeted to the individual website viewer and 
are instead of a general nature, this is fairly simple to resolve. Provided no personal 
information about the website viewers is exchanged with the advertiser, no privacy 
violations can occur. Much, if not all, of the information typically found in a media kit 
about audience demographics does not constitute personal information. 
If personal information is to be exchanged with third parties, a careful assessment 
will need to be conducted before this happens. This assessment must consider both 
the fundamental rights and freedoms of the individuals concerned, their reasonable 
expectations for how and why their personal information would be used, and the 
grounds for why and how the third party would reasonably use those data.
Ease of Migration:  DIFFICULT 
Audience 
MeasurementThere are audience measurement tools that can be self-hosted, such as Cryptolog, 
which allows both internal analytical logging and total control over how long data 
are retained. However, these tools are not as easy to use as third-party ones and are 
not as rich in features. Significant training may be required to learn how to use them 
effectively.
Ease of Migration:  DIFFICULT 
continues next pageImmediate Privacy Gains Are Possible
Around the world, data protection laws and regulations are changing the 
digital ecosystem; by extension, they are changing the online publishing 
world too. Coming into compliance with new data protection laws like 
the GDPR and other applicable regimes can be difficult, and that appears to be 
a leading reason that some website owners have not taken action. 
One simpler mechanism of coming into compliance with the GDPR is to migrate away from using third-
party tools, services, and applications and to instead use self-hosted tools. This immediately limits 
exposure to the data collection and processing practices of third parties, and grants readers a greater 
degree of privacy protection, as information that could lead to their identification will no longer be 
circulating outside of the websites they’re visiting. 21 Big Data, Not Big Brother: New Data Protection Laws and the Implications for Independent Media Around the World   #mediadevMoving from Third Party to First Party (in alphabetical order)
Content Hosting Not all content needs to be hosted by third parties. For example, if a website uses a 
third-party font library, the site could be changed to use either default browser fonts 
or self-hosted fonts to provide an immediate privacy gain for visitors.
It may not be feasible to self-host all content. Video hosting, for example, can be very 
resource intensive and expensive. However, website owners may wish to investigate 
which third-party video hosts are out there, and how (if at all) they use visitor data. 
YouTube, for example, offers “Privacy Enhancing Mode, ” which “allows you to embed 
YouTube videos without using cookies that track viewing behavior. ”24 However, it is 
disabled by default and so action is required by the website’s publisher to benefit 
from this privacy gain.
Ease of Migration:  EASY 
Design 
OptimizationSelf-hosted design testing tools may not offer all of the functions of those of third 
parties. It might be a better idea to review the privacy policy of any tools that are 
used, and to make sure they respect the privacy of visitors. As most of these services 
are paid tools and involve some kind of contractual relationship, it is likely possible to 
find a design testing tool that does not share or unnecessarily retain audience data.
Ease of Migration:  Possibly Not Necessary 
Recommendation 
SystemsRecommendation engines are a major source of third-party cookies. While they 
may look harmless and may improve the appearance and functionality of a website, 
they can also be Trojan horses, inserting hundreds of third-party trackers into 
every page of a website (including malware, in some instances). These tools can 
also be used to direct viewers to content that belongs to another publisher, or to 
inject misinformation or misleading stories onto your webpage. Website owners 
who cannot develop their own content recommendation engines may be better off 
eliminating the use of these third-party tools altogether.
Ease of Migration:  DIFFICULT 
Social Media 
Share ButtonsSome of the most significant privacy violations we saw came from deploying social 
media share buttons. Embedding a Facebook “like” button into a webpage enables 
that platform to be able to link a user’s reading and browsing history to their 
Facebook account. However, it is not necessary to use Facebook’s default “like” 
button to encourage sharing news articles on Facebook. There are self-hosted social 
sharing plug-ins that transmit less personal information to third parties that may be 
used instead. 
Ease of Migration:  EASY 
Visualizations Many third-party visualization tools offer self-hosted deployments, though some 
technical knowledge may be required to synchronize their visual interfaces with local 
deployment. Others do not pose significant privacy risks because as paid tools they 
offer contractual assurances around how they will or will not use data.
Ease of Migration:  EASY 22 CENTER FOR INTERNATIONAL MEDIA ASSISTANCE  CIMA.NED.ORGIndeed, the findings in this report reveal that the current level of 
preparedness among smaller media companies in the Global South 
to protect their readers from being identified and to protect the 
commercial value of their analytics data is low. Experts consulted for 
this report said that this is unfortunate as many of the most common 
data analytic practices that independent media outlets engage in and 
benefit from do not require the use of personal information at all. 
While it will take additional effort by website operators, safeguarding 
the privacy rights of visitors may be good for business. This, in 
turn, could help improve the commercial viability of independent 
media. Research from the World Economic Forum shows that in 
the $3 trillion global data economy, online news sites are unusually 
well-trusted relative to search engines, social networking platforms, 
and even financial institutions.25 Y et, so far, a handful of players 
like Facebook and Google have led the business of online data 
collection and marketing, which has allowed them to harness the 
value of the data that they have collected from websites whose 
content they do not control. This research reveals that independent 
media websites commonly leak personal information to third parties 
under the following scenarios: when analytic tools are used to 
measure audience demographics, when split tests are conducted 
to experiment with new website design features, when social media 
“share” buttons are embedded into webpages, or when content 
recommendation engines are deployed to personalize a website’s 
content to make it relevant to the reader. 
Maintaining and building upon the high degree of trust that exists 
between a publisher and its readers may be the long-term solution Conclusion and Recommendations
A year after a new wave of data protection regulations such as the European 
Union’s GDPR have come into effect, the websites of media outlets continue 
collecting great volumes of personal information—but often unintentionally, 
and typically for other parties. That so many media institutions have failed to 
safeguard this asset—to both protect the privacy and safety of their readers and 
to be in a better negotiating position with advertisers—suggests that education, 
capacity building, and direct support of independent news outlets is needed to 
improve their analytics activities and ensure that they safeguard reader privacy. 
While it will take 
additional effort by 
website operators, 
safeguarding the privacy 
rights of visitors may be 
good for business. 23 Big Data, Not Big Brother: New Data Protection Laws and the Implications for Independent Media Around the World   #mediadevto addressing data privacy concerns while simultaneously developing 
an effective business model. “Media sustainability in the long run 
is actually going to be based on your own user base rather than 
that of advertisers, ” said Tanja Maksic, a researcher with the Balkan 
Investigative Reporting Network.26 This, in turn, could result in digital 
publishers developing greater negotiating power and leverage with 
third-party advertising exchanges or, alternatively, being able to sell 
premium subscriptions to readers. “Look at your internal organization, 
what you are collecting, what you are doing with it, how you are 
protecting it, ” said Maksic, “and craft your economic sustainability 
around your user base and meet their demands and their standards. ” 
Louise Marie Hurel, an internet governance researcher at Brazil’s 
Igarapé Institute, agreed. “I think this is inevitable really. Enforcement 
of data protection laws may be ad hoc, but that same degree of 
uncertainty should not mark your relationship with users who are 
accessing your content. ”27
From a media development perspective, news organizations around 
the world need better support and training on how to safeguard their 
valuable audience data, both to protect the privacy rights of their 
readers and for commercial purposes. Practical primers on complying 
with data protection laws and regulations, sharing best practices, 
regularly auditing websites to understand what tracking devices 
are present and what they are doing with data, and developing and 
exchanging benchmarking information could all help smaller media 
outlets find a competitive advantage over the advertising networks and 
platforms that have absorbed the bulk of online advertising dollars to 
date. Ultimately, a new business model for journalism will require both 
savvy use of data and an abiding respect for readers’ privacy.From a media development 
perspective, news 
organizations around the 
world need better support 
and training on how to 
safeguard their valuable 
audience data, both to 
protect the privacy rights 
of their readers and for 
commercial purposes. 24 CENTER FOR INTERNATIONAL MEDIA ASSISTANCE  CIMA.NED.ORGTracking Domain Tracking Company
33across.com 33Across
undertone.com 33Across
securedvisit.com 4Cite Marketing
4finance.com 4finance
acuityplatform.com Acuity
pippio.com Acxiom
addthis.com AddThis
addtoany.com AddToAny
ipredictive.com Adelphic
adform.net Adform
adgrx.com AdGear
adblade.com Adiant
adition.com Adition
adkernel.com AdKernel
admedo.com Admedo
admixer.net Admixer
everesttech.net Adobe
demdex.net Adobe
tubemogul.com Adobe
adotmob.com Adot
adriver.ru AdRiver
akamaized.net Akamai Technologies
amazon-adsystem.com Amazon
turn.com Amobee
adnxs.com AppNexus
metadsp.co.uk Avid Media
avocet.io Avocet
widespace.com Azerion
bettingpartners.com Best Partners
betweendigital.com Between Digital
bidswitch.net BidSwitch
bttrack.com Bidtellect
bidtheatre.com BidTheatre
ml314.com Bombora
brightcove.net Brightcove
e-planning.net Caraytech
cardlytics.com Cardlytics
casalemedia.com Casalemedia
sitescout.com Centro
clevernt.com Clever Advertising
clickagy.com Clickagy
onaudience.com Cloud Technologies
cloudflare.com Cloudflare
cogocast.net Cogo Labs
colpirio.com Colpirio
fwmrm.net Comcast
scorecardresearch.
comComscore
connexity.net Connexity
dotomi.com Conversant
crazyegg.com Crazy Egg 
ctnsnet.com Crimtan
criteo.com Criteo
cxense.com Cxense
videmob.com Cydersoft
dable.io Dable
w55c.net dataxu
dochase.com Dochase
adsymptotic.com Drawbridge
media6degrees.com Dstillery
dtscout.com DTSTracking Domain Tracking Company
sociomantic.com dunnhumby
dyntrk.com Dynadmic
eboundservices.com eBound
emxdgt.com EMX
esquemas.com Esquemas
exelator.com eXelate
tribalfusion.com Exponential
exposebox.com ExposeBox
eyeota.net Eyeota
eyereturn.com Eyereturn Marketing
eyeviewads.com Eyeview
facebook.com Facebook
atdmt.com Facebook
lijit.com Federated Media 
Publishing
fidelity-media.com Fidelity Media
stickyadstv.com FreeWheel
gemius.pl Gemius
genieesspv.jp Geniee
adhigh.net Getintent
getsocial.io GetSocial
doubleclick.net Google
google.com Google
youtube.com Google
gstatic.com Google
groovinads.com GroovinAds
gumgum.com GumGum
histats.com Histats
digitru.st IAB
ibillboard.com iBILLBOARD
id5-sync.com ID5
netmng.com IgnitionOne
360yield.com Improve Digital
impdesk.com Infectious Media
innity.com Innity
innovid.com Innovid
inskinad.com Inskin
insticator.com Insticator
onthe.io IO Technologies
ispot.tv iSpot
izooto.com iZooto
justpremium.com JustPremium
daum.net Kakao
creative-serving.com KPN
latinongroup.com LatinOn
loopme.me LoopMe
crwdcntrl.net Lotame
lytics.io Lytics
list-manage.com MailChimp
mookie1.com Media Innovation Group
media.net Media.net
mathtag.com MediaMath
marketgid.com MGID
bing.com Microsoft
linkedin.com Microsoft
mixmarket.biz MixMarket
narrative.io Narrative
navdmp.com Navegg
agkn.com Neustar
unrulymedia.com News Corporation
toast.com NHNTracking Domain Tracking Company
onesignal.com OneSignal
bluekai.com Oracle
zemanta.com Outbrain
owneriq.net ownerIQ
paypalobjects.com PayPal
paystack.com Paystack
paystack.co Paystack
adrta.com Pixalate
playground.xyz Playground XYZ
powerlinks.com PowerLinks
infogram.com Prezi
pubmatic.com PubMatic
contextweb.com PulsePoint
quantserve.com Quantcast
metype.com Quintype
quora.com Quora
po.st R1Demand
gwallet.com RadiumOne
rating-widget.com RatingWidget
rawgit.com RawGit
recreativ.ru Recreativ
reddit.com Reddit
reembed.com reEmbed
republer.com Republer
reson8.com Resonate Networks
responsivevoice.org ResponsiveVoice
retargetly.com Retargetly
rubiconproject.com Rubicon Project
rundsp.com RUN
scribd.com Scribd
rutarget.ru Segmento
sharethis.com ShareThis
simpli.fi Simpli.fi
sinoptik.ua Sinoptik
serving-sys.com Sizmek
skplanet.com SK Planet
sonobi.com Sonobi
spotify.com Spotify
spotxchange.com SpotX
stackadapt.com StackAdapt
sundaysky.com SundaySky 
survata.com Survata
taboola.com Taboola
tailtarget.com Target Audiences 
and Insights Lab
teads.tv Teads
tapad.com Telenor
tickcounter.com TickCounter
adsrvr.org Trade Desk
tradingview.com TradingView
programattik.com Türk Telekom
twitter.com Twitter
tynt.com Tynt
upravel.com Upravel
advertising.com Verizon
yahoo.com Verizon
uplynk.com Verizon
adtech.de Verizon
weborama.com Weborama
weborama.fr Weborama
pushcrew.com WingifyAppendix A—Tracking Domains Identified Through Study
The following 167 companies were found to be sending data to 246 domain names by way of third-party cookies 
installed on the websites of independent media outlets.25 Big Data, Not Big Brother: New Data Protection Laws and the Implications for Independent Media Around the World   #mediadevPublisher Country
The Bubble Argentina
Revista Anfibia Argentina
La Izquierda Diario Argentina
El Cronista Argentina
Página/12 Argentina
Publica Brazil
Revista Fórum Brazil
Brasil 247 Brazil
Intercept Brasil Brazil
O Antagonista Brazil
Mada Masr Egypt
Egypt Independent Egypt
Daily News Egypt Egypt
Akhbar el-Yom Egypt
Ahram Online Egypt
Khabar Lahariya India
The Caravan India
Scroll India
The Wire India
The Quint India
Tempo Indonesia
RUAI TV Indonesia
Antara News Indonesia
Tirto Indonesia
Coconuts Jakarta IndonesiaPublisher Country
Thika Town Today Kenya
Africa Uncensored Kenya
Hivisasa Kenya
The Elephant Kenya
Talk Africa Kenya
Daily Trust Nigeria
Premium Times Nigeria
Sahara Reporters Nigeria
TheCable Nigeria
Stears Business Nigeria
Enab Baladi Syria
ANA Press Syria
Shaam News Network Syria
Aleppo Today Syria
Ain Al-Madinah Syria
Ukraine World Ukraine
Euromaidan Ukraine
Novoye Vremya Ukraine
Hromadske Radio Ukraine
Zerkalo Nedeli Ukraine
MercoPress Uruguay
LaRed21 Uruguay
Diario El Telégrafo Uruguay
Brecha Uruguay
Búsqueda UruguayAppendix B—Small Publishers Studied
We analyzed the websites of the following small publishers on April 11, 2019:26 CENTER FOR INTERNATIONAL MEDIA ASSISTANCE  CIMA.NED.ORGPublisher Country
Infobae Argentina
El Intransigente Argentina
Clarín Argentina
La Nación Argentina
Perfil Argentina
Globo Brazil
Metrópoles Brazil
Jornal do Brasil Brazil
UOL Brazil
Estadão Brazil
Al Gomhuria Egypt
Youm7 Egypt
Sada El Balad Egypt
Al Masry al Youm Egypt
El Fagr Egypt
Times of India India
Manorama Online India
NDTV India
Hindustan Times India
News 18 India
The Jakarta Post Indonesia
Tribun News Indonesia
Detik Indonesia
Kompas Indonesia
Liputan 6 Indonesia
Daily Nation KenyaAppendix C—Large Publishers Studied
We analyzed the websites of the following larger publishers on April 11, 2019:
Publisher Country
Business Daily Kenya
Capital FM Kenya
Kenya Broadcasting 
CorporationKenya
Kenya News Kenya
Punch Newspaper Nigeria
Vanguard News Nigeria
The Guardian Nigeria
This Day Nigeria
P .M. News Nigeria
SANA Syria
Syria Report Syria
Raialyoum Syria
Zamanalwsl Syria
Aks Alser Syria
Ukrainian Independent 
Information AgencyUkraine
Ukrainian News Ukraine
Segodnya Ukraine
Interfax-Ukraine 
News AgencyUkraine
Kyiv Post Ukraine
El País Uruguay
El Observador Uruguay
OK Diario Uruguay
Portal 180 Uruguay
Ecos Diarios Necochea Uruguay27 Big Data, Not Big Brother: New Data Protection Laws and the Implications for Independent Media Around the World   #mediadevSetup
T ests using OpenWPM were initially conducted 
on a sample of 40 websites on March 24, 
2019, and repeated on April 11, 2019, using 
the full group of 100 websites. OpenWPM 
was installed from GitHub using Git revision 
b3ead7e38892095950806e8bcbb2e1129c27ca96.
T ests were performed using the Kubuntu 
18.04 operating system, with Python 2.7 .15rc1 
and Python 3.6.7 and Firefox 67 .0b4. T esting was 
done under VPN connection.Appendix D—Study Setup and Testing Parameters
Testing Parameters
The OpenWPM “demo.py” script was used 
as a template and modified. The value of 
NUM_BROWSER was set to 1 to use only one web 
browser and to be sure that the websites were 
crawled in the given order. The browser was not 
headless. Flash was enabled. Cookie_instrument 
(experimental) was enabled. 
The following is the sequence of commands used 
for each website:
■■Visit the homepage and wait for 120 seconds
■■Dump flash cookies
■■Dump profile cookies
T o prevent data contamination, the previously 
generated SQLite database was deleted before 
each new recording.28 CENTER FOR INTERNATIONAL MEDIA ASSISTANCE  CIMA.NED.ORG1 Alan Westin, Privacy and Freedom  (1967), 7.
2 Nonpersonal data include information that does not concern a 
natural person. For instance, a data set of daily temperatures in a 
city would constitute nonpersonal data.
3 Anonymized data include information that was originally personal 
information but has been transformed in such a way that the 
link between it and the natural person has been cut. Many data 
analysis activities are performed on anonymized data.
4 European Union Law, “Regulation (EU) 2016/679 of the European 
Parliament and of the Council of 27 April 2016 on the protection 
of natural persons with regard to the processing of personal data 
and on the free movement of such data, and repealing Directive 
95/46/EC (General Data Protection Regulation),” Article 4 (1), 
April 27, 2016, https://eur-lex.europa.eu/legal-content/EN/
ALL/?uri=celex:32016R0679 . 
5 This is known as the Mosaic Effect. In a study published in Science  
in 2015, researchers found that four data points are enough to 
uniquely and accurately reidentify an individual in 90 percent 
of cases. See Yves-Alexandre de Montjoye, Laura Radaelli, 
Vivek Kumar Singh, and Alex “Sandy” Pentland, “Unique in the 
Shopping Mall: On the Reidentifiability of Credit Card Metadata,” 
Science  347, no. 6221 (January 2015): 536-539, http://science.
sciencemag.org/content/347/6221/536.full .
6 The number of countries with adopted data protection laws as of 
March 27, 2019, is 134. The geographical distribution of the 134 
laws is the following: Africa (26), Asia-Pacific (22), Europe (54), 
Latin America and Caribbean (23), Middle East (7), and North 
America (2). 
7 In alphabetical order: Barbados, Belarus, Brunei, Dominica, 
Ecuador, Egypt, El Salvador, Ethiopia, Falkland Islands, Grenada, 
Guatemala, Honduras, Indonesia (substantial revision to existing 
law), Jamaica, Jordan, Kenya, Montserrat, Nigeria, Saint Helena, 
Saint Kitts and Nevis, Saudi Arabia, Swaziland, Tanzania, Virgin 
Islands, Zambia, and Zimbabwe. 
8 Mark Scott and Laurens Cerulus, “Europe’s New Data Protection 
Rules Export Privacy Standards Worldwide,” Politico , January 31, 
2018, https://www.politico.eu/article/europe-data-protection-
privacy-standards-gdpr-general-protection-data-regulation/ .
9 Robert Gellman, Fair Information Practices: A Basic History (April 
10, 2017), available at SSRN: https://ssrn.com/abstract=2415020 .
10 Secretary’s Advisory Committee on Automated Personal Data 
Systems, Records, Computers and the Rights of Citizens,  Office 
of the Assistant Secretary for Planning and Evaluation, US 
Department of Health and Human Services, July 1, 1973, https://
aspe.hhs.gov/report/records-computers-and-rights-citizens . 
11 Sarah Gordon and Aliya Ram, “Information Wars: How Europe 
Became the World’s Data Police,” Financial Times , May 20, 
2018, https://www.ft.com/content/1aa9b0fa-5786-11e8-bdb7-
f6677d2e1ce8 .
12 Rebecca Hill, “Washington Post Offers Invalid Cookie Consent 
under EU Rules – ICO,” The Register,  November 19, 2018, https://
www.theregister.co.uk/2018/11/19/ico_washington_post/ .
13 S. Rana interviewed by A. Férdeline via Skype, March 14, 2019.
14 Julie Brill, “Microsoft’s Commitment to GDPR, Privacy and 
Putting Customers in Control of Their Own Data,” Microsoft, 
blog post, May 21, 2018, https://blogs.microsoft.com/on-the-
issues/2018/05/21/microsofts-commitment-to-gdpr-privacy-and-
putting-customers-in-control-of-their-own-data/ .15 General Data Protection Regulation, “Processing of Personal Data 
Solely for Journalistic Purposes or for the Purposes of Academic, 
Artistic or Literary Expression,” Recital 153, via Intersoft 
Consulting, https://gdpr-info.eu/recitals/no-153/ .
16 I. Avadani interviewed by A. Férdeline via Skype, March 19, 2019.
17 “OCCRP Strongly Objects to Romania’s Misuse of GDPR to Muzzle 
Media,” Organized Crime and Corruption Reporting Project, 
November 9, 2018, https://www.occrp.org/en/40-press-releases/
presss-releases/8875-occrp-strongly-objects-to-romania-s-
misuse-of-gdpr-to-muzzle-media .
18 General Data Protection Regulation, Recital 153.
19 A. Alzghoul interviewed by A. Férdeline via Skype, March 14, 2019.
20 Please note that this is an oversimplification of the process. 
Please refer to the following technical document for a more 
complete explanation of how HTTP requests work: R. Fielding, 
J. Gettys, J. Mogul, H. Frystyk, L. Masinter, P. Leach, and 
T. Berners -Lee, “Hypertext Transfer Protocol,” IETF, June 1999, 
https://www.ietf.org/rfc/rfc2616.txt .
21 See, for instance, P. Laperdrix,W. Rudametkin, and B. Baudry, 
“Beauty and the Beast: Diverting Modern Web Browsers to Build 
Unique Browser Fingerprints,” 37th IEEE Symposium on Security 
and Privacy, 2016, https://www.ieee-security.org/TC/SP2016 ; 
N. Nikiforakis, A. Kapravelos, W. Joosen, C. Kruegel, F. Piessens, 
and G. Vigna, “Cookieless Monster: Exploring the Ecosystem 
of Web-Based Device Fingerprinting,” 34th IEEE Symposium 
on Security and Privacy, 2013, https://ieeexplore.ieee.org/
document/6547132/ .
22 For further information on the intricacies of how the OpenWPM 
tool works, please see the Princeton Web Census website at 
https://webtransparency.cs.princeton.edu/webcensus/ .
23 V. Pavel interviewed by A. Férdeline via email, February 26, 2019.
24 “Turn on Privacy-Enhanced Mode,” YouTube Help, 2019, https://
support.google.com/youtube/answer/171780?visit_id=0-
636595692661723869-3019304114&rd=1 .
25 Vasudha Thirani and Arvind Gupta, “The Value of Data,” 
World Economic Forum, 2017, https://www.weforum.org/
agenda/2017/09/the-value-of-data/ .; In a 2014 poll, 56 percent 
of respondents in countries with less than 25 percent internet 
penetration answered 5, 6, or 7 on a seven-point trust scale 
to indicate that they trust online news sites to protect their 
personal data. This compares with 40 percent trusting search 
engine companies, 37 percent trusting companies that provide 
social networking services, and 29 percent trusting online 
marketers and advertisers. The only stakeholder group more 
trusted than the media were banks and financial institutions, 
who were trusted by 61 percent of respondents. Source: William 
H. Dutton, Ginette Law, Gillian Bolsover, and Soumitra Dutta, 
The Internet Trust Bubble: Global Values, Beliefs and Practices  
(World Economic Forum, 2014), http://www3.weforum.org/docs/
WEF_InternetTrustBubble_Report2_2014.pdf .
26 T. Maksic interviewed by A. Férdeline via Skype, February 8, 2019.
27 L. Hurel interviewed by A. Férdeline in Kobe, Japan, March 13, 
2019.EndnotesCenter for International 
Media Assistance
NATIONAL ENDOWMENT FOR DEMOCRACY
1025 F STREET, N.W., 8TH FLOOR
WASHINGTON, DC 20004
PHONE: (202) 378-9700
EMAIL: CIMA@ned.org
URL: https://cima.ned.org"
DataPrivacy,3010.txt,"      
mHealth Data Security, Privacy, 
and Confidentiality:  
Guidelines for Program Implementers  
and Policymakers  
January 2018  
 
 
 
 
  
 
 
 
 
 
 
mHealth  Data Security, 
Privacy, and Confidentiality:  
Guidelines for Program Implementers  
and Policymakers  
 
Lauren Spigel, MPH  
Samuel Wambugu, MPH, PMP  
Christina Villella, MPH  
 
January 2018  
 
MEASURE  Evaluation  
University of North Carolina at Chapel Hill  
123 West Franklin Street, Suite 330  
Chapel Hill, NC , USA  2751 6  
Phone: +1 919 -445-9350  
measure@unc.edu  
www.measureevaluation.org  
This publication was produced with the su pport of the 
United States Agency for International Development 
(USAID) under the terms of MEASURE Evaluation 
cooperative agreement AID -OAA -L-14-00004. MEASURE 
Evaluation is implemented by the Carolina Population 
Center, University of North Carolina at Cha pel Hill, in 
partnership with ICF International; John Snow, Inc., 
Management Sciences for Health; Palladium; and Tulane 
University. Views expressed are not necessarily those of 
USAID or the United States government.  MS-17-125A   
 
ISBN: 978-1-64232 -003-9 
  mHealth Data Security, Privacy, and Co nfidentiality Guidelines       i ACKNOWLEDGMENTS  
These guidelines are the result of many people ’s hard work . Lauren Spigel, Sam uel Wambugu , and 
Christina Villella —all of MEASURE Evaluation, ICF —led the development of  the guidelines from start 
to finish. We thank Steven Wanyee of IntelliSOFT K enya and Irene Okwara , who he lped coordinate the 
Kenya workshops;  Frances Baaba da -Costa Vroom , of the University of Ghana School of Public Health , 
and Debbie Mangortey (independent consultant) organized the Ghana worksh ops. We also thank all of 
the workshop participants , who reviewed the guidelines and provided invaluable feedback and 
contributions to strengthen this resource (see Appendix B).  
We recognize technical review, guidance , and support from Ana Djapovic S choll, of the United States 
Agency for International Development . We appreciate the thorough review of these guidelines by a team 
of digital health experts:  Olivia  Velez and Denise Johnson , of ICF ; Joy Kamunyori , of MEASURE 
Evaluation, ICF ; and Manish Kuma r, of MEASURE Evaluation,  University of Nor th Carolina at Chapel 
Hill. 
We are grateful to those who  gladly shared their case studies to enrich the guidelines: Cathy Mwangi, 
Tychus Nyanga, Harris Dindi , and Collins Mudogo , of MHealth Kenya limited , and Reina 
Marie -Antoinette Mwinbang Der , of FHI360 /Ghana.  
We recognize the meticulous work of ICF ’s editor, Cindy Young -Turner , and ICF’s creative services team 
for the design and layout work, and MEASURE Evaluation’s Knowledge Management team for the 
overall review , branding, publication , and dissemination  of the guidelines.     
 
 
 
 
 ii      mHealth Data Security, Privacy, and Confidentiality Guidelines  CONTENTS  
Abbreviations  ................................ ................................ ................................ ................................ .. iv 
Glossary  ................................ ................................ ................................ ................................ ............  v 
Preface ................................ ................................ ................................ ................................ ............  vii 
Introduction  ................................ ................................ ................................ ................................ ..... 1 
What is the purp ose of these guidelines?  ................................ ................................ ................................ ..........  1 
Who is the audience for these guidelines?  ................................ ................................ ................................ ........ 1 
What are security, privacy, and confidentiality?  ................................ ................................ ...............................  2 
What information is contained in these guidelines?  ................................ ................................ .......................  2 
What does the growth of mobile technology mean for health services?  ................................ .....................  3 
How do the Principles for Digital Development apply to data security, privacy, and confidentiality?  .. 5 
How were these guidelines developed?  ................................ ................................ ................................ .............  6 
Orga nization of the guidelines  ................................ ................................ ................................ ........  8 
National and Organizational -Level Leadership and Governance  ................................ ...................  9 
National -Level Leadership and Governance of mHealth Programs  ................................ ...........................  9 
Organizational -Level Leadership and Governance of mHealth Programs  ................................ ...............  11 
Technology  ................................ ................................ ................................ ................................ .... 14 
mHealth Application and Data  ................................ ................................ ................................ ........................  15 
Operating System  ................................ ................................ ................................ ................................ ...............  17 
Device  ................................ ................................ ................................ ................................ ................................ ... 17 
Network  ................................ ................................ ................................ ................................ ...............................  19 
Data Storage  ................................ ................................ ................................ ................................ ........................  20 
Case Study: Community -Based Hypertension Improvement Project in Ghana  ...........................  22 
User Behavior ................................ ................................ ................................ ................................ . 24 
Training and T echnology Literacy  ................................ ................................ ................................ ...................  25 
Designing for and with the User  ................................ ................................ ................................ ......................  26 
Case Study: mLab in Kenya  ................................ ................................ ................................ ...........  28 
Checklist  ................................ ................................ ................................ ................................ ........  30 
References  ................................ ................................ ................................ ................................ ...... 31 
Appendix A. Related MEASURE Evaluation Resources  ................................ ..............................  35 
Appendix B. Country Participants  ................................ ................................ ................................ . 36 
Appendix C. How the Guidelines Were Developed  ................................ ................................ ...... 38 
    mHealth Data Security, Privacy, and Co nfidentiality Guidelines       iii Figures  
Figure 1. Overlaps between mHealth data security, privacy, and confidentiality  ................................ ..............  2 
Figure 2. Visual model for mHealth data security, privacy, and confidentiality guidelines  .............................  3 
Figure 3. Mobile cellular subscriptions per 100 inhabitants in low - and middle -income countries,  
2001–2016 ................................ ................................ ................................ ................................ ................................ ..... 4 
Figure 4. Active mobile broadband subscriptions per 100 inhabitants, 2007 –2016 ................................ .........  4 
Figure 5. Mobile technology ecosystem  ................................ ................................ ................................ .................  15 
 
 
 
Tables  
Table 1. Intended audiences for the guidelines  ................................ ................................ ................................ ....... 1 
Table 2. Common components of eHealth strategies  ................................ ................................ .........................  10 
Table 3. eHealth strategy resources  ................................ ................................ ................................ .........................  10 
Table 4. Core elements of security, privacy, and confidentiality laws for mHealth  ................................ ........ 11 
Table 5. Recommendations to ensure data security throughout the project life cycle  ................................ ... 12 
Table 6. Resources for conducting a feasibility assessment  ................................ ................................ ................  13 
Table 7. Steps to mitigate security risks throughout the d ata life cycle  ................................ .............................  16 
Table 8. Considerations related to the security of operating systems  ................................ ...............................  17 
Table 9. Common security risks for mobile devices and possible solutions  ................................ ....................  18 
Table 10. Netw orks and mHealth data security  ................................ ................................ ................................ .... 19 
Table 11. Risks and benefits of storage on devices, local dedicated servers, and cloud -based ser vers........ 21 
Table 12. Components of an effective training to increase technological literacy  ................................ ..........  25 
Table 13. User -centered design resources  ................................ ................................ ................................ ..............  26 
Table 14. Data security, privacy, and confidentiality questions to ask during the design process  ................  27 
 
 
 
  iv      mHealth Data Security, Privacy, and Confidentiality Guidelines  ABBREVIATIONS  
ePHI  electronic personal health information  
HIS health information system (s) 
ICT information communication technology  
LMIC  low- and middle -income country  
OS operating system (s) 
PHI personal health information  
SMS Short Message Service  
 
    mHealth Data Security, Privacy, and Co nfidentiality Guidelines       v GLOSSARY  
application  Any mHealth  tool, regardless of mobile platform  
breach  “An impermissible use or disclosure under the Privacy Rule that 
compromises the security or privacy of the protected health 
information” (Office for Civil Rights, U.S. Department of Health  and 
Human Services, 2013)  
Cloud computing  A technology in which information technology  resources are provided 
as services via Internet (Kalaiselvi, Kousalya, Varshaa, & Suganya, 
2016)  
Cloud -based server  A server that is run by a third party that se lls space to organizations 
for data storage (Kalaiselvi, Kousalya, Varshaa, & Suganya, 2016)  
Confidentiality  “The obligations of those who receive information to respect the 
privacy interests of  those to whom the data relate” (Cohn, 2006)  
Dedicated local  server  A server that is reserved for serving the specific needs of a project. 
It is often owned and stored by the organization or government that 
owns the mHealth project data . 
eHealth  The use of information and communication technologies for health 
(World Health Organization, 2017)  
eHealth  strategy  The product of a country’s strategic planning process for developing 
or continuing to build out investments in digitizing its health 
informati on systems (W orld Health Organization  & International 
Telecommunication Union , 2012)  
Encryption  A process by which information is converted to a code to protect 
sensitive data  (Arora, Yttri, & Nilson, 2014)  
Geofencing  The use of GPS technology to trigger  a response if the device leaves a 
specific geographic region  
Geolocation  A process to locate a device, such as mobile phone, through the 
Internet or mobile network  
ISO20000 standards  A service management system standard  that specifies requirements for 
the service provider to plan, establish, implement, operate, monitor,  
review, maintain , and improve the system  
mHealth  Use of mobile wireless technologies for public health (World Health 
Organization, 2017)  
Personally identifiable 
information  Data relatin g to an individual who can be ide ntified directly or 
indirectly by the data or  by linking the data to other information 
reasonably available (United N ations  Development Group, 2017)  
Phishing  An attempt to gain access to sensitive information by disguising  
oneself as a trustworthy entity  vi      mHealth Data Security, Privacy, and Confidentiality Guidelines  Principles for Digital 
Development  Guidelines that can help “practitioners integrate established best 
practices into technology -enabled programs” (Principles for Digital 
Development, 2017)  
Privacy  “An individual’s right t o control the acquisition, uses, or disclosures 
of his or her identifiable health data” (Cohn, 2006) . This includes any 
information the person wants to keep private.  
Security  “Physical, technological, or administrative safeguards or tools used to 
protect identifiable health data from u nwarranted access or 
disclosure ” (Gejibo, 2015)  
Sensitive data  All personal data relating to religious, philosophical, political , and 
trade  union opinions and activities, as well as to sex life or race, 
health, social measur es, legal proceedings , and penal or administrative 
sanctions (African Union, 2014) 
Server  “A computer in a network that is used to provide services (such as 
access to files or shared peripherals or the routing of e -mail) to other 
computers in the network” (Merriam -Webster, 2017)  
Virtual private network  “A method of creating secure connections be tween mobile devices 
and the back  end while using public, often unsecured networks ”  
(Grandison , 2017 ) 
 
 
    mHealth Data Security, Privacy, and Co nfidentiality Guidelines       vii PREFACE  
Information technology is spreading fast , and its adoption in the health sector is gaining ground  rapidly . 
Under the banner of eHealth, mHealth , or digital health, mobile technology (such as laptop computers, 
mobile phones, and tablets ) has become an indispensable tool to  increase  health coverage.  As countries 
strive toward universal health coverage, mobile wireless technologies —mHealth to ols—in support of 
enumeration, registration , and unique identification  of patients , along with  maintenance of health records , 
will facilitate improved health system performance. Electronic forms and registry systems will enable 
routine monitoring of the co verage of essential interventions for individuals in relevant populations . 
Because mobile technology is widespread,  governments and organizations are harnessing their power to 
collect, collate, transmit , and present data in a timely fashion, thereby overco ming barriers inherent in 
paper -based systems. The rapid progression of technology enables the increased sharing of data between 
electronic systems. This can provide decision makers  with valuable data and improve  their ability to make 
critical decisions  on health programs.  
As healthcare organizations turn to mobile devices to improve efficiency and productivity, many are 
introducing risks that could all too easily result in a data breach and the exposure of protected health 
informat ion. Organizations aro und the world are taking note and providing guidelines on how to 
safeguard electronic personal health information (ePHI) . 
Building the infrastructure to safeguard ePHI is an evolving area. High -income countries have not found 
a lasting solution to this pro blem, much less the low - and middle -income countries (LMICs). Technology 
is changing so fast that keeping up with all the issues of security and privacy is daunting. LMICs lack 
national and comprehensive instruments such as laws and guidelines to protect t he data -rich digital health 
sector. A few high -income countries have established laws and policies for healthcare data privacy and 
confidentiality from which other countries can learn.  
Privacy generally refers to patients having substantial control over the extent, timing, circumstances, and 
sharing of information about oneself with others (Golstin, et al., 2003). Confidentiality refers to the 
treatment of identifiable information that has been disclosed to others in relation of trust and with the 
expecta tion that it will not be divulged to others except in previously agreed -upon ways. Protecting  a 
person ’s health information is particularly important for sensitive health issues such as HIV and for 
stigmatized populations that are at elevated levels of acq uiring sexually transmitted diseases, including 
HIV.  
In its October 2017 Cybersecurity Newsletter, the United States  Department of Health and Human 
Services Office for Civil Rights reminded insured  entities of the risks associated with mobile devices that 
are used to create, receive, maintain, or transmit ePHI. Entities covered by the Health Insurance 
Portability and Accountability  Act were reminded of the need to conduct an organization -wide risk 
assessment and develop a risk management plan to address all  mobile device security risks identified 
during the risk analysis and reduce them to an appropriate and acceptable level.  Although  many covered 
entities allow the use of mobile devices, some prohibit the use of those devices to create, receive, 
maintain, o r transmit ePHI.  These risks are not specific to the United States; d ata breach incidents 
continue to plague healthcare organizations  around the world. The potential cost to the healthcare 
industry could be as much as $5.6 billion annually (Patrick, 2014).  Often these devices  are also  used for 
personal digital activities , such as calling, texting, playing games, taking photos, web browsing, e -mailing, 
and accessing social media. Given that mobile devices are not always connected to secure Internet , these viii      mHealth Data Security, Privacy, and Confidentiality Guidelines  personal activities may inadvertently expose the device to viral attacks and other security risks , leading to 
data breaches. In addition, d ue to their portability, mobile devices are susceptible to breakage, loss, and 
theft. Cyber -attacks  are also increasi ngly common, especially targeting sensitive health data stored on 
digital health systems  (Goldman, 2017; Institute for Critical Infrastructure Technology , 2016) .  
The media are increasingly reporting instances of breaches in the security of large amounts of 
electronically stored personal data . Attracted by both the sensitivity and utility of health data, hackers are 
always devising new ways to gain access . They exploit vulnerabilities in the software, device , or data 
transmission channels, sometimes with fa r-reaching ramifications.  If health services data are stolen, clients 
can be expose d to social or economic risks  and their  trust in those services would diminish . In addition, if 
data are compromised, health managers may be making decisions based on inaccu rate data. Countries and 
programs must therefore continuously review and tighten their technology tools, rules , and regulations to 
protect patients’ health data.  
International law recognizes the individual right to privacy. Privacy  is a basic human right. For example, 
the International Covenant on Civil and Political Rights and the European Convention on Human Rights 
and Fundamental Freedoms acknowledge the individual right to privacy. Many countries have privacy 
laws, either as omnibus data protection regulation such as the European laws  or general personal data 
protection laws. These kinds of laws give equal weight to all types of personal data. The U.S. laws are 
different; they are more specific and are categorized by different sectors , ensuring that protection 
measures of certain types of data are more stringent  (Trustlaw Connect , et al., 2013 ). In fact, the United 
States has enacted laws to protect health and medical data through the Health Insurance Portability and 
Accountability Act . LMICs  have exp ressed interest in establishing data protection regulations , but few 
have enacted such mechanisms. The countries that have enacted these kinds of protections are Kenya, 
Mauritius, Morocco , and South Africa  (Trust Law Connect , et al., 2013).  Many other count ries have ethical 
codes of practice for medical workers who come into contact with privileged patient information.  The 
World Health Organization’s 1994  Declaration on Promotion of Patients’ Rights recognizes a patient’s 
right to privacy.  
To understand  and frame  the digital health landscape  issues of personal health information  privacy and 
confidentiality , particularly in an mHealth environment , MEASURE Evaluation, with funding from the 
United States Agency  for International Development , conducted a landscape analysis . Its result s are 
reported  in mHealth for Low - and Middle -Income Countries —Challenges and Opportunities in Data Quality, Privacy, 
and Security report (Wambugu & Villella, 2016) . The landscape analysis was conducted in two countries  
(Kenya  and Tanzania ), but the results broadly portray the situation of LMICs, especially in the African 
region. The main objective of that analysis was to help understand the  data security and privacy practices  
and the preparedness of countries  in the sub -Sahara n region to tackle emerging data ethics  issues. Th is 
landscape  analysis was supplemented by a review of gray and peer -reviewed literature on this subject . The 
findings were telling. They  show that digital systems are nascent but developing fast , and there is need for 
strong leadership for national health information systems  (HIS) to establish or improv e existing 
governance mechanisms. In addition,  standard operating procedures  were commonly cited as tools that 
countries need to guide issues of data security  and privacy  (Wambugu & Villella, 2016) .  
The landscape analysis report recommended  develop ing data security, privacy , and confidentiality 
guideline s that HIS managers and policymakers can use to guide their digital health work . That’s why  we 
developed this document : mHealth Data Security, Privacy, and Confidentiality : Guidelines  for Implementers and 
Policymakers , and its companion checklist, which is available here: 
https://www.measureevaluation.org/resources/publications/ms -17-125b .   mHealth Data Security, Privacy, and Co nfidentiality Guidelines       ix In develop ing these guidelines  over the course of a year , we reviewed the landscape analysis report ; 
searched and analyzed additional gr ay and peer -reviewed  literature ; spoke with subject matter experts ; 
and, in line with the Principles for Digital Development, engaged with country advisory teams in Kenya 
and Ghana. Ghana was included , because we intended to leverage other MEASURE  Evaluation  work in 
the countr y and assess that country’s interest in having the guidelines.  
mHealth technology comprises  many layers that can affect  data security, privacy , and confidentiality 
throughout the data life  cycle . These layers include national and organizational policy; te chnology used in 
data collection, management, storage , and use; as well as user behavior. Each layer require s careful 
analysis to identify  and protect  potential vulnerabilities. The sensitivity of health data requires that the 
developers of mobile apps for  health should build systems that have a secure back -end database; keep 
minimal or no personal health information data on the device ; and ensure that the hardware, software , 
and communication channels between the device and other systems are secure.  
These guidelines are meant to help mHealth program managers  and ministry of health officials  
systematically address mHealth data privacy and security  issues. For each of the layers  of technology , 
these guidelines explore  common vulnerabilities  and propose ways to proactively address them to reduce 
possibilities of data breaches.  
The guidelines also address overarching topics , such as national data leadership and governance , user 
behavior , and training. Other topics are technology -specific , such as mobile d evices (hardware), operating 
systems, applications, networks , and data storage.  To demonstrate how elements described in these 
guidelines  have been applied in programs , two case studies are included. In addition, a n accompanying  
checklist provides users wi th a structured mechanism to assess the strengths and gaps of each of the 
mHealth components.  
These guidelines and the corresponding checklist are meant to be implemented at the national , 
subnational,  or program level , and they are public goods  that count ries and organizations  can use to 
strengthen the security, privacy , and confidentiality of  their respective mHealth  programs  and national 
HIS. Like most other tools in the digital health space, these guidelines are living documents. They will 
undergo regular updates based on new lessons  and to keep them attuned to the ever-evolving  health and 
technology ecosystem.    mHealth Data Security, Privacy, and Co nfidentiality Guidelines       1 INTRODUCTION  
What is the purpose of these guidelines ? 
These  guidelines are intended to strengthen national HIS , by providing a tool to guide decision s on 
security , privacy , and confidentiality of personal health information ( PHI) collected a nd managed using 
mobile devices.  
Who is the audience for these guidelines ? 
These guidelines are intended for  nongovernmental  mHealth program managers as well as ministry of 
health policymakers  and HIS officers  who use mHealth programs . Each group will use the guidelines  
differently , depending on their level of  operation in the health system , priorities , and needs . This list of 
intended audiences is not comprehensive. As the use of mHealth increases, the audience for these 
guidelines  could expand . It is also important to note that the guidelines are not “one size fits all.” They  
are intended to be adapted to meet the specific needs of programs and countries.  
Table 1. Intended audiences for the guidelines  
Nongovernmental Organizations  
mHealth program managers:  mHealth  program managers are responsible for overseeing the design, 
development, deployment, and implementation of a wide range of mHealth programs. These 
guidelines specifically focus on security, privacy, and confidentiality aspects of data management, 
so mHeal th program managers will find them useful for thinking through how to safeguard data that 
are collected on mobile devices and stored in the mHealth ecosystem.  
Ministry of Health  
HIS officers:  HIS officers are responsible for supporting the health information management system of 
the country. This includes managing staff, building staff capacity, and developing HIS guidelines  and 
quality improvement protocols. HIS managers will find these guidelines useful for building capacity of 
staff whose responsibilities include  overseeing data security, privacy, and confidentiality; maintaining 
the security, privacy, and confidentiality o f patient data; and integrating guidelines in national HIS 
frameworks.  
Policymakers:  Although the position will vary from country to country, HIS officials, health information 
leads, or information communication technology managers of ministries of health are responsible for 
overseeing digital health assets and programs, as well as outlining national strategy related to digital 
health. These ministry officials will find these guidelines useful for building the capacity of their 
department to oversee the sec urity, privacy, and confidentiality of mHealth programs in the country. 
They can also use these guidelines to direct  mHealth implementers o n how to increase the security, 
privacy, and confidentiality of their mHealth programs.  
These guidelines are limite d in scope, because their goal is  not to transform a ministry official or an 
mHealth program manager into an expert on digital data security. The guidelines , instead , can serve as a 
building block that will  allow stakeholders to be informed managers of a team responsible for developing 
and implementing responsible data practices, especially data security and privacy.  
  2      mHealth Data Security, Privacy, and Confidentiality Guidelines  What are security, privacy, and confidentiality?  
Security, privacy , and confidentiality ar e interrelated, but there are distinctions between the terms , as 
follows:   
• Security  refers to the technology infrastructure that protect s sensitive information . 
• Privacy  refers to the client’s right to control what “individually identifiable health  informat ion” is 
collected, used , and shared  (HealthIT.gov, 2016) . 
• Confidentiality  refers to the obligation to keep sensitive information private . It is a mechanism 
for protecting privacy.  
What information is  contained in these guidelines?  
These guidelines are built on the premise that securing technology, improving the skills of technology 
users , and establishing a supportive environment can  improve protection  of sensitive client data from  
malicious or inadvertent  access . They f ocus on the intersection between  security,  privacy , and 
confidentiality for mHealth programs. Figure 1 provides  a visual representation of the scope of these 
guidelines.  
Figure 1. Overlaps between mHealth data security, privacy , and confidentiality  
 
In addition, t hese guidelines  focus  on mHealth programs that collect personal health data, which are 
also referred to as “sensitive data .” Consequently , mHealth programs that focus on behavior change 
communication and eLearning  are beyond the scope of these guidelines , unless they collect sensitive data 
from their users . 
The primary  assumption  of these guidelines  is that , by strengthening technological, administrative , and 
physical safeguards surrounding mobile devices, sensitiv e personal health data are also more likely to be 
kept both private and confidential. The  authors acknowledge that provider behavior  outside the mobile 
technology ecosystem also contributes to client data  privacy , security , and confidentiality, but this is  
beyond the scope of these  guideline s. The authors developed Figure 2 as a visual model  of the guidelines’ 
content . Privacy
Security Confidentiality
  mHealth Data Security, Privacy, and Co nfidentiality Guidelines       3 Figure 2. Visual model  for mHealth data security, privacy , and confidentiality guidelines  
 
What does the growth of mobile technology mean for health services?  
Mobile phone penetration worldwide is on the rise. As of 2016, low- and middle -income countries had 
94 cellular subscriptions per every 100 inhabitants ( International Telecommunications Union , 2016). In 
addition, active mobile broadba nd subscriptions are increasing at a rapid rate, growing nearly fivefold  
over the past five years (8.3  per 100  inhabitants in 2011 to 40.9 per 100 inhabitants in 2016) ( International 
Telecommunications Union , 2016). Figures  3 and 4 illustrate  the rapid growth of mobile cellular 
subscriptions and mobile broadband subscriptions.  
  Protection of 
personal
health dataNational 
guidelines, 
policies, and 
leadership
Organizational 
governance
Security 
by design
Security of 
mobile 
devices, 
software, and 
networksUser trainingUser behavior4      mHealth Data Security, Privacy, and Confidentiality Guidelines  Figure 3. Mobile cellular subscriptions p er 100 inhabitants in low- and middle -income 
countries , 2001–2016  
 
 
 
 
 
 
 
 
 
 
 
Figure 4. Active mobile broadband subscriptions per 100 inhabitants, 2007 –2016  
The use of mobile devices in healthcare can make it easier to access care, improve care delivery, empower 
patients through targeted messaging, and collect real -time data to optimize resources and decision making 
(World Bank, 2016).  Over the past decade, public health practitioners have been taking advantage of the 
growing mobile penetration rate , by incorporating mobile technology in health programs.  Practitioners 
use mobile health, or mHealth, to increase access to health services an d information in some of the 
hardest -to-reach places on earth, and mobile interventions span the entire health system (Labrique, et al., 
2013).  
As a result, health programs are exploring ways to harness mobile technology to increase health coverage, 
improve the quality of care , and reduce healthcare costs. Labrique and colleagues identified 12 common 126.7
99.7
94.1
020406080100120140
2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 2011 2012 2013 2014 2015 2016*Per 100 inhabitantsDeveloped
World
Developing
The developed/developing country classifications are based on the UN M49, see: 
http://www.itu.int/en/ITU -D/Statistics/Pages/definitions/regions.aspx.html
Note: * Estimate
Source: ITU World Telecommunication/ICT Indicators database
90.3
49.4
40.9
0102030405060708090100
2007 2008 2009 2010 2011 2012 2013 2014 2015 2016*Per 100 inhabitantsDeveloped
World
Developing
The developed/developing country classifications are based on the UN M49, see: 
http://www.itu.int/en/ITU -D/Statistics/Pages/definitions/regions.aspx.html
Note: * Estimate
Source: ITU World Telecommunication/ICT Indicators database  mHealth Data Security, Privacy, and Co nfidentiality Guidelines       5 mHealth and  information communication technology ( ICT) applications, which range from behavior 
change communication and diagnostics to electronic health records, data collection , and vital events 
tracking (Labrique, et al., 2013). Box 1  outlines the 12 common mHealth and ICT applications.  These 
guidelines are  most relevant to the  mHealth and ICT applications  that appear in bold . 
Box 1 . 12 common mHealth and ICT applications  
• Client education and behavior change  
• Sensors and point -of-care diagnostics  
• Registries and vital events tracking  
• Data collection and report ing 
• Electronic health records  
• Electronic decision support  
• Provider -to-provider  
• Provider work planning and scheduling  
• Provider training and education  
• Human resource management  
• Supply chain management  
• Financial transactions and incentives  
As mHealth  programs become ubiquitous, policymakers  and program implementers are beginning to take 
a closer look at data security, privacy, and confidentiality concerns related to mHealth programs. Data 
need to be credible and consistent, and they should be collecte d and stored securely in a trusted electronic 
health record with managed access for patients, caregivers, and healthcare professionals (Kumar & 
Wambugu, 2015).  As data collected by mobile devices contain protected health information or personally 
identifia ble information, there are considerable concerns around data confidentiality  (Sacks, J., et al. 
2015). Personal  and sensitive health information store d on mobile devices and servers can affect  the 
landscape of data security policies and procedures.  
These guidelines are  a practical tool for policymakers  and program implementers , to ensure that mHealth 
programs protect  sensitive health information . There are several layers of security that an mHealth 
program should  focus on to improve data security. Because  the security of mHealth data relies on the 
security of each layer , these guideline s cover the layers of mobile device  security : that is,  the application, 
the operating software, the device, the network, and the servers (TrustLa w Connect, et al., 2013). These 
guidelines  go beyond the technology and address  security issues related to user behavior and national 
policies . 
How do the Principles for Digital Development apply to data security, privacy , 
and confidentiality ? 
These guidelines were developed in line with the Principles for Digital Development (Principles for 
Digital Development, 2017) , which outline best practices for digital health programming. The Principles 
for Digital Development  are written by and for international development professionals  and are freely 
available for use.  Broadly, these guidelines operationalize three of  the eight  Principles for Digital 
Development that ensure responsible data practices through out the data life cycle , from  collection to 
disposal. These Principles are : 
• Design with the user: Engage users during the design process to build systems that protect 
sensitive information, keeping in mind user priorities, needs and contexts. See the Designing for 
and with the User  section of these guidelines for more information.6      mHealth Data Security, Privacy, and Confidentiality Guidelines  • Understand the existing ecosystem : To ensure protection of sensitive information, implemente rs should 
make sure that their mHealth programs abide by national and organizational policies related to data 
security, privacy , and confidentiality. One way to do this is by engaging with t echnical working groups and 
national eHealth committees. See the  National and Organizational -Level  Leadership and Governance  
section of these guidelines for more information.  
• Ensure  data privacy and security : Data privacy, security , and confidentiality considerations should be 
incorporated in all aspects of your mHealth program. See Table 5, Recommendations to ensure data 
security throughout the project life cycle,  for more information.  
How were these guidelines developed?  
These guidelines were developed as a follow -up to one of the recommendations of the most recent report in 
MEASURE  Evaluation ’s mHealth data security series, mHealth for Health Information Systems in Low - and Middle -Income 
Countries: Challenges and Opportunities in Data Quality, Privacy , and Security  (Wambugu & Villella , 2016) . One of the main 
recommendations in this report was  the need for a follow -up guideline for policymakers , decision makers, and 
mHealth program implementers to strengthen the  security, privacy, and confidentiality of mHealth program s. This 
report and its  recommendation s resulted from  information gleaned both from key informant interviews in Kenya 
and Tanzania  and from a literature  review  of data security and privacy practices and risks in using mHealth in HIS 
strengthening in LMICs. In Kenya and Tanzania , informants working with mHealth programs identified varying 
levels of capacity and preparedness to protect data collected via mHealth programs. The literature review revealed  
the unique data security and priv acy risks that come with using mobile devices to collect, transmit, and store 
sensitive data . The report identified a lack of guidelines or best practices for protecting mHealth data in 
strengthening HIS. More information about the results of the report, t he report can be found at this link: 
https://www.measureevaluation.org/resources/publications/tr -16-140. 
Although  more stakeholders in the development sector have identified  the need to have responsible data practices 
and have created subsequent resources  to guide implementation of these practices , such as  Oxfam’s Responsible Data 
Training Pack  (Hastie & Bolton , 2017) and the Responsible  Data F orum ’s The Hand -Book of the Mode rn Development 
Specialist: Being a Complete Illustrated Guide to Responsible Data Usage, Manners & General Deportment (2016) , we found that 
some resources were more focused on a project lifecycle and not on routine HIS. We also did not find resources 
speci fic to the unique challenges in protecting data that is collected using mobile devices. This gap in practical 
guidance and interest from countries for more information on this topic led to the development of these guidelines.  
Additionally, t his work builds on MEASURE Evaluation’s previous work on  digital health ’s privacy and security. 
See Appendix A  for a list of related MEASURE Evaluation resources.  
The development of the mHealth Data Security, Privacy and Conf identiality Guidelines followed these steps : 
 
• Meetings with digital health stakeholders to assess interest and need in guidelines related to mHealth data 
security, privacy , and confidentiality  
• Literature review  
• Two half -day workshops with 50 Kenyan and Ghanaian digital health stakeholders to provide feedback on 
the draft guidelines  
• Draft guidelines review by a panel of technical experts in digital health, HIS, and mHealth data security, 
privacy, and confidentiali ty 
Additional information regarding the development of the guidelines is available in Appendix C . 
   mHealth Data Security, Privacy, and Co nfidentiality Guidelines       7  
A resident enumerator in Niamey, Niger, prepares for the first round of data collection for PMA2020, a mobil e technology -based survey 
project that supports routine, rapid -turnaround, high -quality data on family planning and other health indicators.  
Photo: © 2015 PMA2020/Shani Turke , courtesy of Photoshare   8      mHealth Data Security, Privacy, and Confidentiality Guidelines  ORGANIZATION OF THE GUIDELINES  
These guidelines are organized in three broad sections , which are outlined as follows :  
• National and Organizational -Level Leadership and Governance:  How leadership, policy 
and governance at the national and organizational levels can strengthen protections around 
sensitive inf ormation  
• Technology:  How to safeguard sensitive data from vulnerabilities intrinsic to mobile systems 
throughout the data lifespan, including the mHealth application, the operating system, the device, 
the mobile network , and data storage services  
• User  Beha vior: What users of mHealth applications and tools need to know to ensure the 
security, privacy , and confidentiality of sensitive data stored on mHealth devices  
Each section provides an overview of best practices and considerations for protecting sensitive  data in the 
mHealth context . 
    mHealth Data Security, Privacy, and Co nfidentiality Guidelines       9 NATIONAL  AND  ORGANIZATIONAL -LEVEL  LEADERSHIP AND 
GOVERNANCE  
Although  technology has a variety of tools that 
can be applied to  safeguard sensitive data, 
strong leadership  and organizational and 
national governance  are necessary to ensure the 
protection of sensitive data that are collected, 
stored, and transmitted through  mobile devices 
in mHealth programs. This section describes 
key components of national - and 
organizational -level governance  that, if 
implemented, c ould strengthen mHealth data 
security, privacy , and confidentiality.  
 
 
National -Level  Leadership and Governance  of mHealth Programs  
The integration of mobile technologies in national and local health systems has security and privacy 
implications at the policy level. This section outlines common components of national e Health strategies, 
as well as core elements of privacy laws to consider while developing policies to oversee mHealth 
programs.  
Questions Answered in This Section  
• What components are usuall y included in a national eHealth strategy?  
• What core elements of health data privacy laws do mHealth project managers need to know?  
To oversee and coordinate the influx of digital technology in their HIS, many ministries of health are 
developing their own e Health strategies , which outline their visions, strategic objectives, and 
implementation and oversight plans for incorporating digital technologies in their health systems. 
mHealth is recognized by the Global Observatory of eHealth as a key component of eHealth to support 
universal health coverage (World Health Organization, 2015). In its National eHealth Strategy Toolkit, 
the World Health Organization  and the International Telecommunication Union identify mHealth as an 
example of eHealth that would be  addressed in an eHealth strategy  (2012) . Therefore, an eHealth strategy 
in a country can have important governance implications for mHealth programs in the country.  Countries 
have different priorities, but there are similarities across the ir eHealth strategies. We reviewed eHealth 
strategies from Ghana, Kenya, Malawi, Tanzania, Uganda, and Zambia , and outlined the common 
components of their eHealth strategies  (see Table 2). Countries can use this as a tool to include or 
improve data security sections of their eHealth strategies. Some countries  choose also to develop an 
mHealth strategy, outlining their plans for the use of mHealth in the health sector , as a defined subset of 
eHealth activities in the country . For example, South Africa’s mHealth Strategy f or 2015 –2019 was 
embedded in its eHealth Strategy and linked to the eHealth Strategy objectives (National Department of 
Health, Republic of South Africa, 2015).  
10      mHealth Data Security, Privacy, and Confidentiality Guidelines  Table 2. Common components of eHealth strategies  
Year published  Uganda  
2013  Ghana  
2009  Malawi  
2014  Kenya  
2011  Tanzania  
2013  Zambia  
2014  
Standardize s health data through 
HMIS or eHMIS        
Build s mHealth capacity and focus 
on eLearning        
Develop s eHealth/mHealth 
Governance Committee        
Focus es on national awareness 
campaigns        
Uses a phased approach to 
implement eHealth strategy        
Includes  a monitoring and evaluation 
plan        
Considers privacy and security issues        
Bridge s equity gap through ICT        
Outlines how eHealth will be funded        
Move s towards a paperless reporting 
system        
Mentions designing for the user        
HMIS=health management information system  
For more information on how to develop a national eHealth strategy, the World Health Organization  and 
United Nations Global Pulse  offer ad ditional res ources , as shown in Table 3 . 
 
Table 3. eHealth strategy resources  
National eH ealth Strategy Toolkit  
The World Health Organization developed a toolkit  that Ministries of Health can  use to help them 
create their own national -level eHealth strategies  
Privacy a nd Data Protection Principles  
United Nations Global Pulse outlined key principles  of data privacy and protection t hat organizations 
and decision makers can use as a guide for creating their own principles.   
A key component of many  eHealth strateg ies is the development of an eHealth  and mHealth  governance 
committee, tasked with developing  policies and overseeing the ir implementation. Having strong leadership 
is crucial for ensuring that mHealth programs abide by data security, privacy, and confidentiality 
guidelines.  eHealth strategies should  also articulate issues such as who is accountable for breaches  of 
privacy and under what circumstances; whether technology companies take responsibilities for weak 
hardware, software , and communication technologies;  whether national and subnational health au thorities 
in a country are responsible for not putting in place failsafe mechanisms on information systems when 
data security is breached ; and the recourse for health system clients when their privacy is breached.  
In addition, h ealth data p rivacy regulati on is critical in providing  a legal framework on which eHealth 
operates.  According to  the mHealth Alliance  and TrustLaw Connect ’s report  addressing privacy law in 
mHealth, national policies on data security, privacy, and confidentiality should include the components 
described in Table 4. Key 
Yes  
No  
   mHealth Data Security, Privacy, and Co nfidentiality Guidelines       11 Table 4. Core elements of security, privacy , and confidentiality  laws for mHealth  
Component  Content  
Coverage  What persons or entities  are obligated to comply  
with national laws / policies ? 
What p ersonal information is covered ? 
What is the s cope of the coverage ? 
Information/ notification requirements  What are the consent requirements ? 
Data security obligations  What p olicies are needed regarding  the retention 
or disposal  of data ? 
What are the t echnical and organization security 
requirements (including cloud storage) ? 
What are the b reach notification obligations ? 
Data transfer (including cross -border 
requirements)  What are the p olicies related to how sensitive data 
are transferred from device to server ? 
Enforcement and sanctions  How will privacy laws be enforced ? 
Adapted from TrustLaw Connect, et al., 2013  
Organizational -Level  Leadership and Governance  of mHealth Programs  
Governments are responsible for creating  national -level policies to oversee the growing mHealth sector, 
and implementing organizations have an obligation to follow national policies  and best practices to 
protect sensitive health information that are collected and managed  within an mHealth system. This 
section outlines key activities and recommendations that can enhance the security of mHealth programs  
through leadership and governance within implementing organizations . 
Questions Answered in This Section  
• What should my organization do before, during, and after implementing an mHealth project to 
make sure that we protect sensitive data?  
• What resources will help my organization prepare for data security needs of an mHealth project?  
• How do organizational poli cies align with national data protection laws and policies?  
Organizations implementing mHealth programs should consider  the recommendations listed in Table 5 
to ensure data security, privacy , and confidentiality.  12      mHealth Data Security, Privacy, and Confidentiality Guidelines  Table 5. Recommendations to ensure data security throughout the project life cycle  
Project stage  Recommendation  
Conceptual stage  Form a technical working group or engage with an existing e Health and 
mHealth  governance committee and other local stakeholders. This group 
would have the authority to oversee, review , and advise on data 
security. Data breaches would be reported to this group.  
Check national and local policies, guidelines , and laws related to 
management of sensitive information.  For data that passed through 
Internet service providers, u nderstand the data that they retain, how 
long they keep them, and how they are used . 
Conduct a feasibility study to understand the mHealth  security 
landscape, requirements , and strengths and weaknesses of your mHealth 
program.  For example, use the mHealth Assessment and Planning for 
Scale  Toolkit  and the Centers for Disease Control and Prevention’s  
Checklist for Assessment of Data Security and Confidentiality Protections 
to guide your feasibility study ( see Table 6 ).  
Consider hiring or contracting the services of trained security 
professionals.  
Write  or adapt  a standard operating procedure outlining how data 
would be recovered if lost , how devices with data will be disposed of , 
how organizations will respond to theft or loss of device  or data , how 
organizations will respond to data breaches , how organizations will 
respond to protocol violations , and what security provisions  to use . 
During the design phase of your mHealth program, incorporate elements 
of “security by design” to ensure that security features reflect user needs 
and context. See the User Behavior  section  for more information.  
Develop memorandums of understanding between collaborating  
partners, outlining their roles and responsibilities.  
Set up a rigorous monitoring and evaluation system to monitor data 
breaches and ensure quick response.  
Implementation 
Stage  Implement a m echanism for m onitoring and evaluation  for continuous 
improvements . 
Establish o ngoing collaboration with the technical working group, the 
eHealth governance committee, local stakeholders , and other 
partnering organizations . 
Monitor, d ocument , and respond to data breaches or privacy  
challenges . 
Provide ongoing trainings and refresher trainings for users. For more 
information, see the  Training and Technology Literacy  section.  
Provide feedback to the overseeing body or committee . 
Post-
Implementation 
Stage  Conduct an evaluation  with a focus on responsible data practices , 
including data security and privacy measures for the project.  
Share lessons learned with all involved parties . 
Dispose of data and devices according to the country’s established 
standard operating procedures .   mHealth Data Security, Privacy, and Co nfidentiality Guidelines       13 Table 6. Resources for conducting a feasibility assessment  
mHealth Assessment and Planning f or Scale Toolkit  
The World Health Organization, the UN Foundation, and Johns Hopkins Global mHealth Initiative 
developed the mHealth Assessment and Planning for Scale  Toolkit  to help mHealth programs track 
key components they need to scale (WHO, UN Foundation,JHU GmI, 2015) . Several domains include 
indicators that can be used to implement and monitor protocols that protect mHealth data security, 
privacy, and confidentiality.  
Checklist f or Assess ment of Data Security a nd Confidentiality Protections  
The Centers for Disease Control and Prevention developed guidelines and an accompanying 
checklist  to assess the data security and confidentiality protections for programs related to HIV, viral 
hepatitis, sexually transmitted diseases, and tuberculosis (Centers for Disease Control and Prevention, 
2011). The checklist provides programs with guidelines on how to:  
• Identify key individuals and designate a leadership team.  
• Review current policies and gather resources.  
• Identify weaknesses and barriers.  
• Assess physical security and define the secure area.  
• Assess electronic security, protections, and methods of data transfer and storage.  
• Assess training needs  
 
  14      mHealth Data Security, Privacy, and Confidentiality Guidelines  TECHNOLOGY  
 
 
Having a detailed security protocol that addresses 
the most common reasons for data loss will 
prevent most  data security, privacy, and 
confidentiality breaches (Arora, Yttri, & Nilson, 
2014). This section contains  suggestions for 
increasing the security of mHealth data, but 
implementers and policymakers  should find a 
balance between the level of security and the 
functionality of the mHealth program (Arora, 
Yttri, & Nilson, 2014).  
Mobile technology  operates within a mobile 
ecosystem,  so mHealth implementers and 
policymakers  should consider the many layers of 
security configurations available within an 
mHealth ecosystem. This section outline s the levels of a mobile ecosystem that affect mHealth data 
security and measures that program implementers can take to ensure the security of personal health 
information stored in this system.  
A 56 -year -old auxiliary nurse midwife in Badag aon block, Jhansi district, Uttar Pradesh, India, learns how to register a beneficiary using 
her tablet -based mSakhi application, a mobile phone -based job aid developed by IntraHealth International under the Manthan Project.  
Photo: © 2015 Girdhari Bora fo r IntraHealth International, Courtesy of Photoshare  
  mHealth Data Security, Privacy, and Co nfidentiality Guidelines       15 Figure 5 illustrates  the interaction between the components of an mHealth ecosystem. The mobile device  
transmits data to remote data storage through the mobile network. This interaction may take mere 
seconds or minutes  (or longer, depending on the strength of the network) , but it can leave sensitive data 
vulnerable to data breaches.  Decisions  made by  mHealth managers  regardi ng which device, operating 
system, app, mobile network , and storage system to use in their programming  could have implications for 
data security, privacy , and confidentiality. This section will explore these decision  points  and potential 
vulnerabilities . 
Figure 5. Mobile technology ecosystem  
 
Each component in the ecosystem  is interconnected, and the security of the overall mHealth system relies 
on the security of each individual part.  
mHealth  Application and Data  
A mobile software application, or app, is  a program designed to perform a specific function or set of 
tasks or activities on a mobile device. For the purposes of these guidelines, the term “application” is 
platform agnostic; the authors use the term “application” to describe any mHealth tool, whe ther it is a 
smartphone app, interactive voice response,  or Short Message Service ( SMS). Having said that, the 
authors recognize that not all recommendations apply to SMS or interactive voice response -based 
programs.  The sensitivity of health data requires  that the developers of mobile apps for health should 
build systems that have a secure back -end database; keep minimal or no PHI data on the device ; and 
ensure that the hardware, software , and communication channels between the device and other systems 
are secure.  
Many  mHealth apps record , transmit , and store PHI, and thus the design and configuration of mHealth 
apps play a significant  role in determining the overall security of the data. This section outline s methods 
of securing data throughout the life  cycle of data in relation to the mHealth app.  
Questions Answered in This Section  
• What steps can I take throughout the data management life cycle to increase the security, 
privacy, and confidentiality of data stored within my mHealth application?  
• When are sensitive health data most vulnerable?   
A mobile app store is a repository of mobile applications. Examples include Google Play and Apple’s 
App Store. This repository provides a selection of approved applications that can be downloaded and 
installed on th e device, because m ost app stores offer safeguards to protect users from installing  apps that 
could steal sensitiv e information (Clark, 2009 ; Perakovic, Husnjak, & Remenar, 2012). From the 
developer perspective, data security can be strengthened within an app, particularly related to how 
16      mHealth Data Security, Privacy, and Confidentiality Guidelines  sensitive data are stored, accessed , and transferred (Majchrzycka & Poniszewska -Maranda, 2016 ; Arora, 
Yttri, & Nilson, 2014).  
Developers can take steps  to mitigate certain vulnerabilities at each stag e in the data life  cycle, although it 
is important to note that increased security usually correlates with increased cost. It is up to the program 
staff to determine the appropriate balance of data security and cost for their respective mHealth projects. 
Table 7 outlines steps to mitigate security risks . 
Table  7. Steps to mitigate security risks throughout the data life  cycle  
Data management stage  Best practice  
Data capture and storage  Limit the amount of data that are  collected and  stored on 
the device, including the internal memory or removable 
storage such as SD card  and SIM card (Arora, Yttri, & Nilson, 
2014).  
Encrypt sensitive data using Advanced Encryption Standard 
algorithm (Majchrzycka & Poniszewska -Maranda, 2016 ; Arora, 
Yttri, & Nilson, 2014).  
Back  up or archive data to prevent data loss.  
Be aware of the data management policies of each platform 
you use (i.e. , telecom provider, mobile application ). 
Access to data  Decide which users should have access to which data. 
Restrict access to sensitive data using passwords or two -factor 
authentication (Arora, Yttri, & Nilson, 2014).  
Use geolocation to track the location of the device when it 
requests access to sensitive information to flag potential 
misuse of sensitive data. Google Maps API is one way to 
implement this (Majchrzycka & Poniszewska -Maranda, 2016) . 
Use an application device identifier, which should be a 
randomly  generated 32 -bit string, to identify requests for 
sensitive information (Majchrzycka & Poniszewska -Maranda, 
2016).  
Train user s to log out after every session, or program the 
session to time out after a certain length of time (Majchrzycka 
& Poniszewska -Maranda, 2016).  
Ensure that mHealth apps do not request permissions that 
they  do not require, such as access to SMS, camera, 
contacts, etc.  
Data transfer  Use end -to-end data encryption when transferring sensitive 
data to hide the content of the message (Arora, Yttri, & 
Nilson, 2014 ; Majchrzycka & Poniszewska -Maranda, 2016).  
Use a digital signature to ensure that the message received is 
the same as the message that was sent (Majchrzycka & 
Poniszewska -Maranda, 2016).  
Ensure that security  keys contain at least 128 -bits to offer 
sufficient security (Arora, Yttri, & Nilson, 2014 ; Majchrzycka & 
Poniszewska -Maranda, 2016).  
If your mHealth program uses a web application, consider 
using secure transfers such as HTTPS . 
Data disposal  Outline how to dispose of sensitive data in a standard 
operating procedure  to ensure that there is no risk of 
exposure . 
Best practices for disposal of digital data include  “clearing 
(using software or hardware products to overwrite media with 
non-sensitive data), purging (degaussing or exposing media 
to a strong magnetic field in order to disrupt the recorded   mHealth Data Security, Privacy, and Co nfidentiality Guidelines       17 Data management stage  Best practice  
magnetic domains), or destroying the media (disintegration, 
pulveri zation, melting, incinerating or shredding)” (Office for 
Civil Rights, 2015) . 
Operating System  
A mobile operating system  (OS) is specifically designed to run on mobile devices , such as mobile phones, 
smartphones,  personal digital assistants , tablet computers and other handheld devices. Some mobile 
device OS are Apple iOS, Google Android, Research in Motion ’s BlackBerry OS, Nokia ’s Symbian , and 
Microsoft ’s Windows Phone OS. Some, such as Microsoft ’s Windows 8, function as both a traditional 
desktop OS and a mobile OS.  
Given the context of each project, the authors recognize that mHealth practitioners might not have the 
luxury to choose an operating system , because most  users may already have a cert ain type of phone . If 
they can choose, however, this section should be used to help mHealth practitioners choose an operating 
system  that is best suited for their program.   
Questions Answered in this Section  
• What security considerations should I be aware of when choosing a mobile operating system?  
• How do I balance functionality and security of different operating systems?  
Whichever operating system program managers choose,  they should make sure that users keep their 
device’s operating system  updated for t he best protection. Hackers search for vulnerabilities in systems, 
so keeping current with computer updates is important , because  these generally patch known 
vulnerabilities.  
Choosing an operating system for your mHealth program is a balance between funct ionality and security. 
Both Android and iOS are relatively secure. Although  Android provides more functionality for mHealth 
programs than iOS, it also gives more autonomy to the user, which can create vulnerabilities if the user is 
not properly trained. Table 8 outlines questions to consider when choosing operating systems.  
Table  8. Considerations related to the s ecurity of operating systems  
Security questions to consider when choosing  
an operating system  Rationale  
Is the OS open source, or is it a closed 
environment?  Open source allows for more contributions 
from the developer community, but it can 
leave some vulnerabilities , making it prone 
to security threats.  
Are there existing policies that protect users 
from downloading apps prone to 
insecurity?  Some OS and OS app stores protect users 
from down loading insecure apps , such as  
iOS and Android . 
Does the OS require user permission to 
install and update apps?  Asking for user permission is an additional 
layer of protection . 
Does the OS support restricted profiles?  Preventing users from accessing non -work -
related websites and apps can limit data 
breaches . 
Device  
Security  and privacy of data begin  with the mobile device. Many mHealth managers need to decide 
whether users will operate the mHealth application on their personal device or whether  the project will 18      mHealth Data Security, Privacy, and Confidentiality Guidelines  procure devices for the users. Each decision has  pros and cons . One factor to consider is that using a 
personal device can increase the vulnerability of  sensitive data, such as by getting viruses, leaving the 
phone around for others to find, or getting hacked.  Users  will also use their devices for personal reasons, 
and the project will not have control over how devices are used. A recent report, Mobile Technology in 
Support of Frontline Health Workers , outlined several pros and cons after surveying 70 mHealth experts 
(Agarwal, et al., 2016) .  
Whether health w orkers use their own device for  health service provision or data management  activities , 
or are provided with a device by the mHealth program, proper training can prevent many common data 
breaches. This section outlines common security risk s for mobile devices and possible solutions to 
counter those risks.  
Questions Answered in This Section  
• What are common security risks to mobile devices?  
• How do I care for my device so that it will serve the program longer?  
Common breaches occur when mobile devices are lost o r left unsecured, when password  rules are not 
enforced , and when mHealth users transfer or acces s sensitive information over an unsecured public 
network (Arora, Yttri, & Nilson, 2014). People who are responsible for mHealth devices  should also be 
trained to resist phishing attempts, where people inadvertently give away sensitive information to hackers 
(Perakovic, Hu snjak, & Remenar, 2012). Table 9 outlines common security risks and their corresponding 
solutions.  
Table 9. Common secu rity risks for mobile devices and possible solutions  
Risk Best practice  
Unauthorized users gain access  Lock device in a secure location when not in use . 
Enable remote wiping of data or device -locking protocols, 
also called geofencing.  
Hacked passwords  Use as many types of characters as possible (uppercase, 
lowercase, punctuation, etc.) If the device has provision for 
biometric security such as fingerprints, explore the feasibility 
of its use.  
Use at least 8 –12 characters  for a password.  
Change passwords regularly . 
Use a password management program; do not write the 
password down . 
Avoid easy -to-guess passwords, like “password,” “123456,” 
or a user’s  name . 
Phishing attempts  Train users to recognize and report phishing attempts . 
Viruses  Avoid adding removable storage to mHealth devices . 
Train users to recognize possible sources of viruses . 
Keep the application and operating system up to date . 
Ensure that devices undergo regular maintenance .  
Physical security  Whenever possible, use a case to protect the mobile 
device from damage, moisture, dust , and dirt . 
Keep mobile devices within optimal temperature range, 
according to the manufacturer . 
Make sure that the mobile phone has access to a charging 
station. If this is  not possible, users should continuously save 
data so that they are not lost if the power runs out.    mHealth Data Security, Privacy, and Co nfidentiality Guidelines       19 Network  
A network is a connection of mobile devices, servers , and computers  through communication media . A 
network is used to transmit information from one device to another. Networks  can be accessed  in many 
ways, each with its own level of security. This section outline s the risks and benefits of transmitting 
sensitive health information through each type of network, including public and private wireless 
networks, virtual private networks, mobile broadband networks , and Bluetooth.  
Questions Answered in This Section  
• What loopholes do hackers exploit to compromise data?  
• Which networks are the most secure for  transmitting sensitive data?  
• What can you do to keep your device safe in a network?  
Implementers of mHealth programs need to provide a secure network for data transfer as well as train 
users to recognize vulnerable  networks to ensure the security, privacy, and confidentiality of sensitive 
data. As Arora and colleagues observe, because wireless connections are “more susceptible to monitoring 
and interception than broadband (internet) networks…security protocols [are] the only barriers protecting 
data against a breach” (Arora, Yttri, & Nilson, 2014). Thus providing a secure connection, using 
encryption, training users in data security, and following a security protocol will help prevent data 
breaches.  In addition, it is critically important to put in p lace an intrusion detection system , that is, a 
network appliance that uses a set of heuristics to match known attack signatures against incoming 
network traffic and raises alerts when suspicious traffic is seen.  
When planning your mHealth program, you have  options —each with its own risks and benefits —for 
providing a secure net work for data transfer.  Table 10 outlines how to make the different types of 
wireless networks more secure for your mHealth program.  
Table 10. Networks and mHealth data security  
Network  Considerations  
Public wireless Internet  Sensitive data should never  be transferred over public 
wireless Internet  because data will be vulnerable to 
hacking by unauthorized users.  
All users of mHealth  programs must be trained on how to 
recognize vulnerable networks. These include slightly 
misspelled Wi -Fi network names or suspicious network 
names such as  Free PUBLIC WI -FI. Be especially careful not  
to connect to personal hotspots or to ad  hoc networks that 
you do not  know and trust.  
Programs should install and update firewall software 
regularly on their mobile devices to further safeguard data.  
Virtual Private Network (VPN)  If you must use public Wi-Fi, VPNs are the most secure 
option , because they use authentication and encryption to 
provide “’virtual private’ tunnels for your data through the 
public internet” (Goldsborough, 2013). A disadvantage of 
VPNs is that data transfer may be slow , which might burden 
users, and VPNs are comparat ively expensive (Arora, Yttri, & 
Nilson, 2014).  
Private wireless Internet  Private wireless networks are relatively secure but must be 
secured with a password (Goldsborough, 2013).  
Wi-Fi Protected Access 2 networks are the most secure, but 
they must be enabled on the device (Goldsborough, 2013 ; 
Arora, Yttri, & Nilson, 2014).  20      mHealth Data Security, Privacy, and Confidentiality Guidelines  Network  Considerations  
Mobile broadband network 
(3G/4G /LTE) 3G, 4G, and higher  networks are generally secure, using  
encryption and authentication . 
The network is not  always reliable , and transmission can be 
slow. 
Bluetooth  Organizations should change default settings of Bluetooth 
devices to reflect security policies (Padgette, et al., 2017).  
Bluetooth security features include authentication, 
confidentiality (preventing eavesdropping), and 
authorization (Padgette, et al., 2017).  
Bluetooth BR/EDR/HS mode 1 is not secure; mode 4 is the 
most secure (Padgette, et al., 2017).  
Bluetooth LE uses Advanced Encryption Standard -Counter. 
Bluetooth LE Security Mode 1 Level 3 is the most secure 
option (Padgette, et al., 2017).  
Bluetooth is susceptible to “denial of service attacks, 
eavesdropping, MITM attacks, message modification, and 
resource misappropriation” (Padgette, et al., 2017).  
Data Storage  
Once data are collected, mHealth programs need to decide where data should be stored. Different 
programs will have different storage needs, and some might choose to have either on -device or server 
backups to avoid data loss. mHealth programs often store sensitive health informatio n, so this section can 
be used to outline security risks and benefits of data storage options, including on -device storage, 
dedicated local s evers , and cloud -based servers.   
Questions Answered in This Section  
• What are our options for storing mHealth data?  
• What are the risks and benefits of storing data on the device?  
• What are the risks and benefits of storing data on a cloud or dedicated server?  
Storing sensitive data on a server, rather than on a device, can increase the security of the data, because  it 
minimizes the risk of human error ( e.g., accidentally deleting data, compromising data through personal 
Internet  usage) (Majchrzycka & Poniszewska -Maranda, 2016) . In resource -strained environments where 
security is a concern, t he emphasis should be on cloud -based services served through HTML5 apps  (Celi, 
et al., 2017), and  Table 11 outlines the risks and benefits of storing data on the device, local dedicated 
servers , and cloud -based servers.  The case study on pages 23 –24 outlin es how one mHealth program has 
taken steps to protect patient data in several of the components of the mobile technology ecosystem.    mHealth Data Security, Privacy, and Co nfidentiality Guidelines       21 Table 11. Risks and benefits of storage on devices, local dedicated servers , and cloud -based 
servers  
Data storage  Benefits  Risks  
On-device 
storage  Users can still a ccess the data if 
the network goes down . Unauthorized access to the 
device  
Users c an collect data in areas 
with limited or no access to the 
network.  Data loss and changes  
 
Storage is c onvenient and easy to 
use. Physically vulnerable 
(i.e., device could be 
damaged, lost , or stolen ) 
Users can more easily monitor their 
own data at a local level . Human error (i.e. , not saving, 
deleting data ) 
Local dedicated 
server  Users can more easily enforce and 
monitor data security practices . Need onsite technical team 
to set up and maintain server  
In some countries,  this option  is 
preferred . This preference is 
common  in countries with strict  
data ownership regulations . Physically vulnerable 
(i.e., server could be 
compromised ) 
Server resources are not shared 
with other projects or clients . Expensive to maintain  
Cloud -based 
server  Option is m ore cost -effective and 
scalable than local dedicated 
servers (Kalaiselvi, et al. , 2016) . Unauthorized access and use 
of data  if terms and 
conditions are not clearly 
stated and understood  
Renting space on a server buys 
more affordable security and 
maintenance than a local 
dedicated server . Political implications for 
sharing data across borders, 
particularly if countries have 
different regulatory 
frameworks  
Cloud servers follow internationally  
recognized quality standards, so 
users  can ensure that the  data are 
secure. Look for Information 
Technology Infrastructure Library 
or International Organization for 
Standardization (ISO20000) -
certified cloud providers.  Difficult to access data in 
cases of l imited or no network 
access   
  22      mHealth Data Security, Privacy, and Confidentiality Guidelines   
CASE STUDY : COMMUNITY -BASED  HYPERTENSION  
IMPROVEMENT PROJECT IN GHANA  
Fast Facts  
Who  FHI360 and Ghana Health Service  
What  Community -based Hypertension Improvement Project, which collects patient health 
data for diagnostics and treatment support  
Where   Lower Manya Krobo municipality, Ghana  
Primary users  109 health providers, including nurses, medical doctors, physician assistants, 
pharmacists, pharmacy assistants, and licensed chemical sellers   
Device  Samsung Tab 4  
Platform  CommCare  
A health worker uses the mVacciNation mobile app to record vaccination data in Nampula, Mozambique.  
Photo: © 2017 Arturo Sanabria, courtesy of Photoshare  
   mHealth Data Security, Privacy, and Co nfidentiality Guidelines       23 This case study examines how the Community -based Hypertension Improvement Project  handled some 
of the security considerations in the development of its mHealth application to help protect patient 
data confidentiality and privacy.  
Application Layer  
Because the health workers use the information on the device for ongoing diagnostics and treatment, 
a copy of patient information is stored on the device and another copy is transmitted to a cloud server. 
The application is protected by a unique username a nd password. The system generates a password 
for each authorized user that allows users to access the application during their work. This mechanism is 
useful because if a user is unable to log into the app, the system administrator can help the user access  
the app because he or she can trace the password from a log.  
Data Access within the A pplication  
The program is organized in groups , so that providers who are in close geographic proximity  work in the 
same group. To further protect  patient information in t he application, access is restricted to those in the 
group of providers with whom the cardiovascular nurse works.  
Device  
The project has a routine maintenance plan for the devices. The devices are inspected monthly to 
ensure that they function as they sho uld. Any unauthorized apps are removed , and damaged devices 
are either repaired or replaced as soon as possible. One lesson the project learned about managing 
the devices is to set policies about which applications could be installed on the device , because  the 
system administrator spends time during the monthly inspections deleting third -party applications.  
Data Storage  
The patient information is transmitted from the device to a cloud server , using GSM mobile services. 
Access to data on the server is restri cted to people cleared to access them.  As an added security 
feature, the server automatically records all transactions that occur in the database. The database 
administrator monitors this log to see whether an unauthorized person accesses the data. B y reviewing 
this log, the a dministrator can identify any unauthorized access or attempted access.  
Case study author : Reina Marie -Antoinette Mwinbang Der, M&E/mHealth Officer II, FHI360  
 
 
 
 
 
  24      mHealth Data Security, Privacy, and Confidentiality Guidelines  USER BEHAVIOR  
 
User behavior is inextricably linked to the 
security, privacy , and confidentiality  of an 
mHealth program. Although  the technology 
provides certain levels of security, people who 
operate and control the mobile devices are the 
first line of defense against breaches of security, 
privacy, and confidentiality. The Principles for 
Digital Development encourage implementers to  
train data users to minimize security risks 
(Principles for Digital Development, 2017) . This 
section explores user-behavior aspects of data 
security and where they overlap with privacy and 
confidentiality.  
 
 
  
A trainer in Ntcheu, Malawi, uses a mobile phone as part of a VillageReach two -way SMS project that allows community health workers 
to register pregnant women in their villages, log their estimated delivery dates, encourage them to continue attending antena tal care, 
and discuss where they will deliver.  
Photo: © 2015  Jodi-Ann Burey/VillageReach, courtesy of Photoshare  
 
 
  mHealth Data Security, Privacy, and Co nfidentiality Guidelines       25 Training and Technology Literacy  
Health workers often maintain the mobile devices, input and transmit health data, and sometimes use the 
mobile device for personal Internet  activities . Therefore they  must be trained to improve their  technolog y 
literacy and to minimize privacy and security risks (Arora, Yttri, & Nilson, 2014) . This section outlines the 
components of effective trainings to increase technology  literacy regarding  security.  
Questions Answered in This Section  
• What content should a training cover?  
• What is the optimal length of a training for an mHealth program?   
According to a recent systematic review of the literature, training health workers to use mobile phones 
can be an effective way to increase data quality (Agarwal, et al.,  2015). A training of trainers model could 
be a useful training model for scaling up an mHealth program (Agarwal, et al., 2015). Table 12 describes 
key components of an effective training aimed at increasing  the technology  literacy of users of mHealth 
programs. 
Table 12. Components of an effective training to increase technological literacy  
Training component  Best practice  
Training length  Continuous training over a minimum of a  six-month period  
with built -in refresher trainings  
Varied training length, d epending  on health workers’ level 
of technology literacy (Agarwal, et al. , 2015)  
Training content  User literacy/ how to use a mobile phone (Agarwal, et al. , 
2015)  
Understanding the importance of data security, privacy , 
and confidentiality  
How to use the mHealth application and avoid deleting the 
app (Agarwal, et al. , 2015 ; Wambugu & Villella, 2016)  
Roles and responsibilities of users related to data security, 
privacy , and confidentiality ; How to address technical 
difficulties and data breaches  (Agarwal, et al. , 2015)  
How to make passwords secure (Arora, Yttri, &  Nilson, 2014 ; 
Goldsborough , 2017)  
Standard operating procedure for maintaining data 
security , covering data management throughout the data 
life cycle , including how to dispose of data at the end of 
the project  
Common human errors that could affect data security, 
privacy , and confidentiality (For more information on 
training topics related to human error, see Table 9  in the 
Technology  section. ) 
How to transmit data to the server to reduce amount of 
data stored on the device (Arora, Yttri, & Nilson, 2014 ; 
Wambugu & Villella, 2016)  26      mHealth Data Security, Privacy, and Confidentiality Guidelines  Designing for and with the User  
Designing for , and with  the specific user s of your mHealth  program will help ensure that  the app is easy 
to use, which will minimize human errors. User -centered design puts the perspective of the end user at 
the center of the design process. This section outlines questions that should be answered during the 
design process to maximize the security of data from an mHealth program.  
Questions Answered in This  Section  
• What questions should we ask the developers during the design stage?  
• How can we incorporate “privacy by design” in the mHealth application?  
• Where can we learn more about human -centered design?  
Making sure that users are engaged throughout the design process is the best way to “make the user 
interface intuitive and easy to understand” (Agarwal, et al., 2015). The “privacy by design” approach is the 
idea that measures to protect the security, privacy , and confiden tiality of sensitive data can and should be 
incorporated in the design of the app (Information Commissioner's Office, 2017) . The Principles for 
Digit al Development  lists “design with the user” as the first principle, and the website provides a number 
of useful resources for implementers (Principles for Digital Development, 2017).  Table 13  lists resources 
for user-centered design.  
Table 13. User-centered design resources  
Field Guide f or Human -Centered Design  
IDEO’s Field Guide for Human Centered Design  explains the desig n process and provides examples 
of participatory activities that can be used to understand the user’s perspective.  
Principles f or Digital Development  
The Principles for Digital Development website also provides a list of toolkits  to help implementers 
design with the user.   
Putting the user at the center of the design process makes the app easier to use, and increasing health 
worker s’ underst anding of the app can also improve data security, privacy, and confidentiality (Agarwal, et 
al., 2015). Table 14 outlines questions that are related to data security, privacy , and confidentiality  that 
should be addressed during the design process.  The case  study on pages 29–30 outlines how one mHealth 
program addressed planning for security in its design and implementation and user training.  
    mHealth Data Security, Privacy, and Co nfidentiality Guidelines       27 Table 14. Data security, privacy , and confidentiality  questions to ask during the design 
process  
Category  Questions to consider  Security implication  
Audience  What is the technology 
literacy level of the users?  Are users aware of how to avoid 
threats to privacy, security , and 
confidentiality?  
Will users accidentally delete or share 
sensitive data?  
What training needs to be provided?  
What additional safety features 
should be added to the app?  
What language are users  
most comfortable speak ing, 
read ing, and writ ing?  Is the mHealth app in a language that 
is understood by the users ( e.g.,  to 
avoid mistakes that would jeopardize 
sensitive data )? 
Technology 
landscape  How much network 
connectivity will be 
available?  Will sensitive data be stored on the 
device? How much and for how 
long?  
What data storage or backup system 
will the program use?  
What type of connectivity will 
be available for users to 
transmit data ( e.g.,  2G, 3G, 
4G, wireless Internet)?  How will users transmit sensitive data?  
What other systems will the 
application need to integrate 
with?  How can the system be designed to  
maintain security, privacy , and 
confidentiality of sensitive data when 
it is integrated with another system?  
Functionality 
of mHealth 
app How will sensitive data be 
managed in the app and 
mHealth system?   Are there instructions and reminders 
to help health workers save and 
transmit data?  
Will health workers have the ability to 
review and edit data to ensure data 
quality?  
Will users use the device for 
personal Internet  browsing as 
well as the mHealth activity?  What safety features need to be 
added to  the device or app ( e.g.,  
restricted websites, passwords, 
antivirus software )? 
What training needs to be provided 
regarding safe Internet  browsing?  
What data do social media apps on 
the device collect?  
Do users share their device 
with anyone else?  What safety features need to be 
added to the app ( e.g.,  passwords, 
multi -factor authentication, multiple 
user profiles with restrictions )? 
How long will data be stored 
on the device?  How will data be backed  up in case 
of loss, theft , or damage to the 
device?  
What safety features need to be 
added to protect sensitive data on 
the device?  28      mHealth Data Security, Privacy, and Confidentiality Guidelines  CASE STUDY : mLAB IN KENYA  
Fast Facts  
Who  mHealth Kenya, in partnership with the Kenya Ministry of Health  
What  Mobile laboratory (mLab), which transmits laboratory results through a mobile 
application from laboratories to health facilities  
Where   Kenya, in more than 300 health facilities (in more than 20 counties)  
Primary users  Caregiver (clinician) at the Comprehensive Care Clinic and lab technician at the lab   
Device  Cell phone and tablet (Android -supported phone or tablet (version 5.0 and above)  
Platform  SMS-based Android app and web -based app  
Mobile laboratory (mLab), a CDC -funded project implemented by mHealth Kenya , in partnership with 
the Kenya Ministry of Health, aims to reduce the time taken from the dispatch of results from 
laboratories to the receipt of those results at the health facilities. The project is being implemented in 
more than 300 health facilities. T he goal of the program is to link people living with HIV to care in a 
reasonable time, thus contributing to the third 90 of the Joint United Nations Programme on HIV/AIDS 
09-90-90 targets.  
The mLab application has a web and an SMS -based mobile application.  Data from Early Infant 
Diagnosis and Viral Load lab results are sent from the lab through a web platform to the mobile app in 
the form of a n SMS. This SMS is sent to a facility -owned phone , where providers can check the results. 
Providers can also opt to receive messages in their own devices when the results are ready, but the 
results themselves are sent only to the facility -owned phone. mHealth Kenya took security, privacy, and 
confidentiality of lab results into account during both the planning and imple mentation stages in the 
following ways.  
Planning for Security  
During the design phase, mHealth Kenya was engaged in several mapping activities to identify risks 
and vulnerabilities in the system. They mapped all the potential system users and their use cas es. They 
then matched the different users with their roles and identified their access levels (see below). They also 
mapped all the access points to the system (e.g., APIs  and forms) and ensured that they were 
protected against unauthorized access by third -party applications. This mapping included a system 
analysis and a risk assessment of the existing systems and asked questions such as:  
• Does the facility have a secure place to  put the device?  
• What is the sensitivity of the data?  
• What is the bandwidth availability?  
Implementation  
Application Security  
As mentioned earlier, only the one facility -owned phone receives the SMS with the lab results. This SMS is 
encrypted and cannot  even be read until it is ingested in the application. Providers must use a login 
and password to access the application and view the test results. SMS are sent to other users’ phones , 
letting them know that results are ready to be viewed. But to view the results , they must go to the 
facility -owned phone and log  in to the application.    mHealth Data Security, Privacy, and Co nfidentiality Guidelines       29 Data Access Levels  
The mLab program has set up levels of users, and each level has the ability to see only certain 
information. The levels are as follows:  
• Health facilities:   
o Able to see individual patient data and test results  
• Service delivery partners:   
o Not able to see lab results  
o Able to add facilities, remove users, and view dashboards of results that have been 
sent to facilities  
• National level:  
o Able to view dashboards wit h high -level information such as how many facilities are 
receiving results  
Data Transmission  
Sensitive data sent using the mLab web application from the client to the server and vice versa are 
shielded , to avoid privacy leaks using Secure Sockets Layer. Th e SMS messages containing the test 
results that are sent to the facility tablet or phone are also encrypted , and can be read only when they 
are ingested in the mLab mobile application.  
User Training  
mHealth Kenya trains service delivery partners implementi ng mLab in the health facilities , so that the 
partners can then train their facility staff. This is a day -long training, as is the training for facility users. The 
training is done through a practical demonstration of the use of the platform. The program u ses random 
supportive supervision at the facilities to ensure that users are comfortable with the application. The 
program also sends notifications to users about emerging security trends , as they arise.   
Training topics are the following:  
• How to add facilities and counties  
• How to add facility administrators and notifiable persons for the facilities  
• How to use training tools  
• Password management  
• Emerging security trends  
Case study authors and team:  
Dr. Cathy Mwangi, CEO and Principal Investigator, mHeal th Kenya Limited  
Tychus Nyanga, Technical Director, mHealth Kenya Limited  
Harris Dindi, Lead Software Developer, mHealth Kenya Limited  
Collins Mudogo, Monitoring and Evaluation Officer, mHealth Kenya Limited  
  30      mHealth Data Security, Privacy, and Confidentiality Guidelines  CHECKLIST  
These guidelines  have a companion checklist that organizations can use to plan and assess  the ability of 
their mHealth system  to safeguard sensitive data. This may be used  as-is or adapt ed at different stages of 
program planning and implementation , and to put in action the practices the gu idelines recommend . 
Indeed , the guidelines should be a  living document that changes  to fit the ever -evolving digital 
environment. The checklist can be found here: 
https://www. measureevaluation.org/resources/publications/ms -17-125b  
 
 mHealth Data Security, Privacy, and Co nfidentiality Guidelines       31 REFERENCES  
African Union. (2014). African Union Convention on Cyber Security and Personal Data Protection.  Malabo: African 
Union. Retrieved from https://au.int/sites/default/files/treaties/29560 -treaty -0048_ -
_african_union_convention_on_cyber_se curity_and_personal_data_protection_e.pdf  
Agarwal, S., Perry, H., Long, L., & Labrique, A. (2015). Evidence on feasibility and effective use of 
mHealth strategies by frontline health workers in developing countries: Systemati c review. Tropica l Medicine 
and International Health, 20 (8), 1003 –1014. Retrieved from: 
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4692099/   
Agarwal, S., Rosenblum, L., Goldschmidt, T., Carras, M., Goe l, N., & Labrique, A. (2016). Mobile technology 
in support of frontline health workers : A comprehensive overview of the landscape, knowledge gaps and future directions . 
Baltimore, MD: Johns Hopkins University Global mHealth Initiative.  Retrieved from: 
http://www.chwcentral.org/sites/default/files/Mobile%20Technology%20in%20Support%20of%20Fro
ntline%20Health%20Workers.pdf  
Apple. (2017, March 28). Keeping iPhone, iPad, and iPod touch within acceptable operating 
temperatures . Retrieved from https://support.apple.com/en -us/HT201678   
Arora, S., Yttri, J., & Nilson, W. (2014). Privacy and security in mobile health (mHealth) research. Alcohol 
Research: Current Reviews, 36 (1), 143 –150. Retrieved from 
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4432854/  
Celi, L.A.G., Fraser , H., Nikore , V., Osorio , J., &  Paik, K. (E ds.). (2017). Global health informatics principles of 
eHealth and mHealth to improve quality of care.  Cambridge, MA , USA , London, UK: The MIT P ress. 
Centers for Disease Control and Prevention . (2011). Data security and confidentiality guidelin es for HIV, viral 
hepatitis, sexually transmitted disease, and tuberculosis programs:  Standards to facilitate sharing and use of Surveillance 
data for public health action. Atlanta , GA , USA : United States  Department for Health and Human Services, 
Centers for Disease Control and Prevention.  Retrieved from 
https://www.cdc.gov/nchhstp/programintegration/docs/pcs idatasecurityguidelines.pdf  
Clark, C. (2009, May 19). Mobile application security: Challenges and opportunities.  San Francisco, CA, 
USA:  iSEC Partners, Inc.   
Cohn, S. (2006). Privacy and confidentiality in the nationwide health information network . Washington, DC , USA : 
National Committee on Vital and Health Statistics. Retrieved from 
https://www.ncvhs.hhs.gov/recommendations -reports -presentations/june -22-2006-letter -to-the-
secretary -recommendations -regarding -privacy -and-confidentiality -in-the-nationwide -health -information -
network/  
Counci l of Europe. (1950). European Convention on Human Rights and Fundamental Freedoms, 
amended by Protocols No. 11 and 14 , Article 8. Retrieved from  
http://conventions.coe.int/t reaty/en/treaties/html/005.htm   
Gejibo, S.H. (2015). Towards a secure framework for mHealth : A case study in mobile data collection systems . Bergen, 
Norway: University of Bergen.  Retrieved from: https://bora.uib.no/handle/1956/10652  
Goldman, J. (2017). Healthcare industry suffers the most cyber attacks. Retrieved from:  
https://www.esecurityplanet.com/network -security/healthcare -industry -hit-most -frequently -by-cyber -
attacks.html   32      mHealth Data Security, Privacy, and Confidentiality Guidelines  Goldsborough, R. (2013). VPNs: When sniffing your data is rude. Teacher Librarian , 40(5), 64. Retrieved 
from http://connection.ebscohost.com/c/articles/88257509/vpns -when -sniffing -your-data-rude 
Goldsborough, R. (2017). Don 't take a pass on passwords. Teacher Librarian , 61. Retrieved from: 
https://www.highbeam.com/doc/1G1 -485167952.html  
Golstin , L., Hodge , J.G., Valentine , N.B., & Nygren -Krug , H. (2003). The domains of health 
responsiveness: A human rights analysis. Geneva, Switze rland: World Health Organization. Can't access 
this link Retrieved from http://www.who.int/helathinfor/paper53.pdf   
Grandison, T. (2017). Data Security for Mobile Health Care. In L. Celi, H. Fraser , V. Nikore, J. Osorio, & 
K. Paik, Global Health Informatics: Principles of eHealth and mHealth to Improve Quality of Care  (p. 168). 
Cambridge, MA , USA ; London, England: The MIT Press.  
Hastie, R. & Bolton, S. (2017). Responsible data management training pack. Re trieved from  
https://policy -practice.oxfam.org.uk/publications/responsible -data-management -training -pack-620235  
HealthIT.gov. (2016, May 2). Your health information privacy . Retrieved from 
https://www.healthit.gov/patients -families/your -health -information -privacy  
HIPAA Journal. Tips for reducing mobile device security risks. (2017).  HIPAA Journal. Retrieved from 
https://www.hipaajournal.com/mobile -device -security -risks/   
Information Commissioner's Office. (2017). The guide to data protection.  Wilmslow, Cheshire, UK: 
Information Commissioner's Office.  Retrieved from  https://ico.org.uk/for -organisations/guide -to-data-
protection/  
Institute for C ritical Infrastructure Technology. (2016). Hacking Healthcare IT in 201: Lessons the 
healthcare industry can learn from the OPM breach. Retrieved from http://icitech.org/wp -
content/uploads/2016/01/ICIT -Brief -Hacking -Healthcare -IT-in-2016.pdf  
International Telecommunications Union . (2016). Statistics.  Retrieved from http://www.itu.int/en/ITU -
D/Statistics/Documents/statistics/2016/ITU_Key_2005 -2016_ICT_data.xls  
Kalaiselvi, R., Kousalya, K., Varshaa, R., & Sugan ya, M. (2016). Enhanced secure sharing of personal 
health records in cloud computing. Gazi University Journal of Science , 583-591. Retrieved from: 
http://dergipark.gov.tr/download/article -file/225514  
Kumar, M. , & Wambugu, S. (2015). A primer on the security, privacy, and confidentiality of electronic 
health records. Chapel Hill, NC , USA : MEASURE Evaluation, University of North Carolina.  Retrieved 
from https://www.measureevaluation.org/resources/publications/sr -15-128-en 
Labrique, A., Vasudevan, L., Kochi, E., Fabricant, R., & Mehl, G. (2013). mHealth innovations as health 
system strengthening tools: 12 com mon applications and a visual framework. Global Health Science and 
Practice , 160-171. Retrieved from http://ghspjournal.org/content/1/2/160.full.pdf+html  
Majchrzycka, A., & Poniszewska -Maranda, A. (2016). Secure development model for mobile applications. 
Technical Sciences, 64 (3), 495 -503. Retrieved from  
https://www.degruyter.com/view/j/bpasts.2016.64.issue -3/bpasts -2016-0055/bpasts -2016-0055.xml  
 
 mHealth Data Security, Privacy, and Co nfidentiality Guidelines       33 MEASURE Evaluation. (2017). Improving data quality in mobile community -based health information s ystems—
Guidelines for d esign and implementation . Chapel Hill, N C, USA : MEASURE Evaluation, University of  North 
Carolina. Retrieved from  https://www.measureevaluation.org/resources/publications/tr -17-182 
MEASURE Evaluation -SIFSA . (2015a) . Good practices in issuing mobile devices to healthcare workers . Pretoria , 
South Africa : MEASURE Evaluation, University of  North Carolina. Retrieved from  
https://www.measur eevaluation.org/resources/publications/fs -15-148  
MEASURE Evaluation -SIFSA. (2015b). Interoperability considerations in the design, development, and 
implementation of mHealth p rojects.  Chapel Hill, NC , USA : MEASURE Evaluation, University of North 
Carolina.  Retrieved from  https://www.measureevaluation.org/resources/publications/fs -15-152-en    
Mehl, G., & Labrique, A. (2014). Prioritizing integrated mHealth strategies for u niversal health coverage.  
Science , 345(6202), 1284 –1287. Retrieved from http://science.sciencemag.org/content/345/6202/1284  
Merriam -Webster. (2017, May 11). Server . Retrieved from https://www.merriam -
webster.com/dictionary/server  
Morsy, M.A., Grundy, J., & Müller, I. (2010). An analysis of the cloud computing security problem. In 
Proceedings of the APSEC 2010 Cloud Workshop.  Sydney, Australia: Asia Pacific Software Engineering 
Conference  2010 Cloud Workshop. Retrieved from 
https://arxiv.org/ftp/arxiv/papers/1609/1609.01107.pdf  
National Department of Health , Republic of South Africa. (2015). mHealth Str ategy 2015 –2019. 
Retrieved from  http://static -a.net/cdn/ehna/i/mHealth%20Strategy%20South%20Africa%202015 -
2019.pdf   
Office for Civil Rights , U.S. Department of Health and Human Services . (2015, November 6). What do 
the HIPAA Privacy and Security Rules require of covered entities when they dispose of protected health 
information?  Retrieved from https://www.hhs.gov/hipaa/for -professionals/faq/575/what -does-hipaa -
require -of-covered -entities -when -they-dispose -inform ation/index.html  
Office for Civil Rights, U .S. Department of Health and Human Services. (2013, July 26). Breach 
Notification Rule . Retrieved from https://www.hhs.gov/hipaa/for -professionals/breach -
notification/index.html  
Padgette, J., Bahr, J., Batra, M., Holtmann, M., Smithbey, R., Chen, L., & Scarfone, K. (2017). Guide to 
bluetooth security.  Gaithersburg , MD : National Institute of Standards and Te chnology. Retrieved from 
http://nvlpubs.nist.gov/nistpubs/SpecialPublications/NIST.SP.800 -121r2.pdf  
Patrick, A (2014). The compl ete guide to healthcare p rivacy  and information security g overnance, Massachusetts , United 
States. Retrieved from  https://www.hcpro.com/   
Perakovic, D., Husnjak, S., & Remenar, V. (2012). Research of security threats in the use of modern 
terminal devices. In Proceedings of the 23rd International DAAAM Symposium  (pp. 0545 –0548). Vienna, 
Austria: Danube Adria Association for Automation & Manufacturing . Retrieved from 
https://bib.irb.hr/datoteka/600737.DAAAM_2012_Perakovic_Husnjak_Remenar.pdf  
Principles for Digital Development. (2017, May 22). About . Retrieved from 
http://digitalprinciples.org/about/  34      mHealth Data Security, Privacy, and Confidentiality Guidelines  Responsible  Data Forum . (2016). The hand-book of the modern development specialist: Being a complete illustrated 
guide to responsible data usage, manners & general d eportment . Retrieved from  
https://responsibledata.io/resources/handbook/assets/pdf/responsible -data-handbook.pdf  
Sacks, J., et al. (2015). Introduction of mobile health tools to sup port Ebola surveillance and contract 
tracing in Guinea. Global Health: Science and Practice, 3 (4), 646 -659. Retrieved from 
https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4682588/  
TrustLaw Connect, et al. (2013). Patient privacy in a mobile world: A framworkwork to address privacy law issues in 
mobile health.  London, UK: Thomson Reuters Foundation . Retrieved from 
http://www.mhealthknowledge.org/sites/default/files/10_trustlaw_connect_report.pdf  
United Nations Development Group  (UNDG) . (2017). Data privacy, ethics and protection guidance note on big 
data for achievement of the 2030 agenda. Geneva, Switzerland: UNDG . Retrieved from https://undg.org/wp -
content/uploads/2017/11/UNDG_BigData_final_web.pdf  
Wambugu, S., & Villella, C. (2016). mHealth for health systems in low- and middle-income countries: Challenges and 
opportunities in data quality, privacy , and security.  Chapel Hill, NC , USA : MEASURE Evaluation , University of 
North Carolina . Retrieved from https://www.measureevaluation.org/resources/publications/tr -16-140 
World Bank. (2016). World development report 2016: Digital dividends . Washington, DC , USA : World Bank. 
Retrieved from http://www.worldbank.org/en/publication/wdr2016  
World Health Organization. (2015). Global Observatory for eHealth: Third Global Survey on eHealth - 
2015. Retrieved from: http://www.who.int/goe /survey/2015survey/en/   
World Health Organization. (2017). eHealth at WHO . Retrieved from 
http://www.who.int/ehealth/about/en/  
World Health Organization and International Telecommunication Union. (2012).  National eHealth Strategy 
Toolkit: Overview.  Geneva: World Health Organization and International Telecommunication Union. 
Retrieved from http://www.who.int/ehealth/publications/overview. pdf 
World Health Organization , United Nations  Foundation,  Johns Hopkins University Global mHealth 
Initiative . (2015). The MAPS Toolkit: mHealth Assessment and Planning for Scale.  Geneva, Switzerland: World 
Health Organization.  Retrieved from http://who.int/reproductivehealth/topics/mhealth/maps -
toolkit/en/  
  mHealth Data Security, Privacy, and Co nfidentiality Guidelines       35 APPENDIX  A. RELATED MEASURE EVAL UATION RESOURCES  
Resource  Description  
Data Ethics: Harnessing the Power of 
Digital Health Information Systems  
(Wambugu, Thomas, Johnson , and 
Villella, 2017)  This report is based on an assessment in Kenya and 
Tanzania on the status and challenges of use of digital 
health systems.  
mHealth for  Low-and Middle -Income 
Countries, Challenges, Opportunities 
in Data Quality, Privacy and Security 
(Wambugu & Villella, 2016)  This report details emerging best practices and 
challenges related to how mobile technology is 
affecting health data quality, privacy , and s ecurity.  
A Primer on the Privacy, Security, and 
Confidentiality of Electronic Health 
Records  (Kumar & Wambugu, 2015)  This report d escribes the importance of cyber 
security in an increasingly connected health 
system, particularly in relation to PHI stored in 
electronic health records. The report discusses the 
key concepts of security, privacy,  and 
confidentiality; provides information  on global 
standards; and discusses key organizational 
processes to ensure security, privacy, and 
confidentiality of PHI. This report provides a firm 
grounding in the basics of security, privacy , and 
confidentiality.  
Information briefs  
(MEASURE Evaluation -SIFSA , 2015 a; 
MEASURE Evaluation –SIFSA, 2015 b) These information brief p rovide an overview of the 
legal and technological considerations for 
protecting personal information, including 
sensitive health data, in the South African context .  
Improving Data Quality in Mobile 
Community -Based Health 
Information Systems —Guidelines for 
Design and Implementation  
(MEASURE Evaluation, 2017)  These guidelines provide information on how to 
strengthen d ata quality in mobile 
community -based HIS. 
 
 
 
 
  36      mHealth Data Security, Privacy, and Confidentiality Guidelines  APPENDIX  B. COUNTRY PARTICIPANTS  
Guidelines development and review meeting participants  
Kenya  
Victor Ouma Achieng  UNICEF  
Christine Gichuhi  IntelliSOFT Consulting Limited  
Tony Kariri  Partners for Health and Development in Africa  
Amos Laboso  I-TECH Kenya  
Nancy Macharia  Jomo Kenyatta University of Agriculture and Technology  
Shem Mbandu  South Eastern Kenya University  
Caroline Mbindyo  Living Goods  
Stephen Mburu  University of Nairobi  
Samuel Mbuthia  Medic Mobile  
Brian Mecha  Muva Technologies  
Prachi Mehta  CDC Kenya  
Naomi Muinga  KEMRI -Wellcome Trust Research Programme  
Edwin M Mulwa  KEMRI Research Care and Training Program and Family AIDS Care 
and Education Services Program  
Alice Ndwiga  Afya Links  
Martin Njoroge  KEMRI -Wellcome Trust Research Programme  
Hon. Michael Onyango  Kisumu County  
Martin Osumba  RTI International  
George  O. Otieno  Kenyatta University  
Otieno Davies Ray  Global Programs for Research and Training   
Philomena Waruhari  Moi University —Institute of Biomedical Informatics  
Kennedy Sitali  mHealth Kenya  
 
Ghana  
Marcus K. G. Adomey  University of Ghana Computing Systems  
Kofi Adu -Koranteng  University of Professional Studies Accra  
Harvey Akafu  Ghana Health Service  
William Azietsi -Bokor  PharmAccess  
Stephen Bewong  National Health Insurance Authority  
Adams Bashiru  Palladium  
Selasie Brown  Universit y of Professional Studies Accra/ White Orange  
Reina Marie -Antoinette Der  FHI360  
Chase Freeman  Management Systems International  
Nii Lante Heward -Mills VOTO Mobile  
Marian Honu  FHI360  
David Hutchful  Hutchlabs Atelier  
Ernest Mensah  Data Protection Commission  
Anthony Ofosu  Ghana Health Service  
Nana Kwabena Owusu  Leti Arts  
Sarah Sackey  Christian Health Association of Ghana  mHealth Data Security, Privacy, and Co nfidentiality Guidelines       37 Ghana  
Frank Twu masi  Not specified  
Alex Israel Yao Attachey  PharmAccess  
Emmanuel Yartey  FHI360  
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
 
  38      mHealth Data Security, Privacy, and Confidentiality Guidelines  APPENDIX  C. HOW THE GUIDELINES W ERE DEVELOPED  
In developing  these guidelines , the authors reviewed the findings and recommendations from the mHealth 
for Health Information Systems in Low - and Middle -Income Countries: Challenges and Opportunities in Data Quality, 
Privacy and Security  report , and searched for additional literature t o update the information in that report.  
Guided by  the results from this literature review , MEASURE Evaluation developed an interview guide, 
which was used to  obtain feedback and insights from a total of 40 mHealth stakeholders in Kenya and 
Ghana during tw o half -day workshops.  
Kenya and Ghana  were selected for several reasons. First, b oth countries were in the process of enacting 
legal frameworks for data protection: the Data Protection Act , in Kenya , and the Personal Data 
Protection  Act, in Ghana. Both countries have a good mix of paper and electronic data management 
systems with associated guiding documents, including policies and frameworks. Both countries have 
recently launched their five -year strategic frameworks for eHealth  and digital health . MEASU RE 
Evaluation also h ad other related ongoing work in these countries, which made it easier to leverage  the 
resources needed . 
In Kenya, MEASURE Evaluation hosted a half -day workshop with a technically -diverse group of 
21 participants representing mHealth im plementers from the private sector, nongovernmental 
organizations , government , and donor organizations. Participants were divided in groups to review each 
section of these guidelines to provide feedback on what should be added, removed , or changed in each 
section. Follow -up conversations were conducted with some workshop participants to better understand 
their feedback.  
This process was repeated in Ghana two weeks later, where 19 stakeholders, representing the 
government, nongovernmental organizations , academia , and the private sector participated in the 
workshop. Changes were made to the document based on suggestions that came from workshop 
participants. See Appendix B  for country contributions.  
Finally, the document was sent to six digital health experts involved with MEASURE Evaluation, who 
were asked to review the guidelines and provide feedback. Five of the six reviewers accepted. They 
provided their reviews on the content, structure of the do cument , and applicability to low- and middle -
income countries.  The authors  discussed and incorporated the feedback in these guidelines as 
appropriate.   
 
This publication was produced with the support of the United States Agency for International Developm ent (USAID) under the terms of 
MEASURE Evaluation cooperative agreement AID -OAA -L-14-00004. MEASURE Evaluation is implemented by the Carolina Population Center, 
University of North Carolina at Chapel Hill, in partnership with ICF International; John Snow, Inc., Management Sciences for Health; Palladium; 
and Tulane University. Views expressed are not necessarily those of USAID or the United States government.   MS-17-125A    
 
 MEASURE  Evaluation  
University of North Carolina at Chapel Hill  
123 West Franklin Street, Suite 330  
Chapel Hill, NC, USA 27516  
Phone: +1 919 -445-9350  • measure@unc.edu  
www.measureevaluation.org  
 
 
MEASURE  Evaluation  
University of North Carolina at Chapel Hill  
400 Meadowmont Village Circle, 3rd Floor  
Chapel Hill, North Carolina 27517  
Phone: +1 -919-445-9359 • measure@unc.edu  
www.measureevaluation.org  "
DataPrivacy,3011.txt,"Privacy Perception in Location -Based Services for 
Mobile Devices in the University Community of 
the North Coast of Colombia * 
Percepción de privacidad en servicios basados e n localización para dispositivos móviles en la comunidad 
universitaria de la costa norte de Colombia 
Date received: 6 August 2018 | Date accepted: 3 December 2018 | Date published: 24 June 2019 
MARGARITA GAMARRA ACOSTAa 
Universidad Autónom a del Caribe, Colombia 
ORCI D: 0000- 0003-1834-2984 
INÉS MERIÑO FUENTES  
Universidad del Norte, Colombia 
ORCI D: 0000-0002-6859-3303 
JUAN CALABRIA SARMIENTO  
Universidad del Norte, Colombia 
ORCI D: 0000-0003-1004-5280 
OMAR GUTIERREZ ACOSTA  
Universidad del Norte, Colombia 
ORCI D: 0000-0003-0439-8450 
MAURICIO BARRIOS BARRIOS
Universidad Autónoma del Caribe,  Colombia 
ORCI D: 0000-0002-1933-8496 
NALLIG LEAL N ARVAEZ 
Universidad Autónom a del Caribe, Colombia 
ORCI D: 0000-0002-4913-8540 
PEDRO WIGHTMAN ROJAS
Universidad del Norte, Colombia 
ORCI D: 0000- 0002-7641-2090 
* Research article
a Corresponding author. E -mail: margarita.gamarra@uautonoma.edu.co 
DOI: https://doi.org/10.11144/Javeriana.iyu23-1.pplb    
How to cite this a rticle: 
M. Gamarra et al ., “Privacy perception in locatio n-based services for mobile devices in the university community of the 
north coast of Colombi a,” Ing. Univ.  vol. 23, no. 1, 2019 [Online]. https://doi.org/10.11144/Javeriana.iyu2 3-1.pplb
Privacy Perception in Location -Based Services for Mobile Devices in the University Community of the North Coast of Colombia  
 
INGENIERÍA Y UNIVERSIDAD: ENGINEERING FO R DEVELOPMENT | COLOMBIA | V. 23 | NO. 1 | 2019  | ISSN: 0123 -2126 /2011 -2769 (Online) | Pág. 2 
 Abstract  
Introduction:  The use of mobile applications has 
increased in the last years. Most of them require 
the knowledge of the user location, either for 
their core service or for marketing purposes. 
Location -based services (LBS) offer context -
based assistance to users based o n their location. 
Although these applications ask the user for 
permission to use their location and even 
explain in detail how this information will be 
used in its terms and conditions, most users are 
not aware or even interested in the fact that their 
location information is stored in databases and 
monetized by selling it to third -party companies. 
Regarding this situation, we developed a study 
with the aim to assess perception, concerns and 
awareness from users about their location 
information. Methods:  This work is based on an 
exploratory survey applied to the university 
community, mainly from the North Coast of 
Colombia, to measure the perception of location 
privacy of users with mobile devices. The 
questionnaire was applied using Google Forms. 
The sur vey has nineteen questions organized in 
three sections: personal information, 
identification of privacy and privacy 
management. These questions were designed to 
know the users ’ perceptions of privacy concerns 
in LBS and any actions they take to preserve it . 
Results:  The results show that, in general, the 
respondents do not have a real concern 
regarding the privacy of their geolocation data, 
and the majority is not willing to pay to protect 
their privacy. Conclusions:  This type of surveys 
can generate awaren ess among participants 
about the use of their private information. The 
results expose in this paper can be used to create 
government policies and regulations by 
technology companies about the privacy 
management . 
 
 
 
 
 
 
Keywords:  Location privacy , location -based 
services,  privacy perception.   Resumen  
Introducción:  El uso de aplicaciones móviles 
se ha incrementado en los últimos años. La 
mayoría de ellas requiere conocer la ubicación 
del usuario, ya sea para su servicio principal o 
para fines de marketing. Los servicios basados 
en localización (SBL) ofrecen asistenc ia 
contextual para los usuarios según su 
ubicación. Aunque estas aplicaciones le piden 
permiso al usuario para usar su ubicación e 
incluso explican en detalle cómo se usará esta 
información en sus términos y condiciones, la 
mayoría de los usuarios no están  conscientes 
ni incluso interesados en el hecho de que la 
información de su ubicación se almacene en 
bases de datos y se monetice, vendiéndolo a 
terceros. Con respecto a esta situación, 
desarrollamos un estudio con el objetivo de 
evaluar la percepción, las  preocupaciones y el 
conocimiento de los usuarios sobre la 
información de su ubicación. Métodos:  este 
trabajo se basa en una encuesta exploratoria 
aplicada a la comunidad universitaria, 
principalmente de la costa norte de Colombia, 
para medir la percepción  de la privacidad de 
ubicación de los usuarios con dispositivos 
móviles. El cuestionario se aplicó utilizando 
los formularios de Google. La encuesta tiene 
diecinueve preguntas organizadas en tres 
secciones: información personal, identificación 
de privacida d y gestión de la privacidad. Estas 
preguntas fueron diseñadas para conocer las 
percepciones de los usuarios sobre las 
preocupaciones de privacidad en SBL y 
cualquier acción que tomen para preservarla. 
Resultados:  los resultados muestran que, en 
general, l os encuestados no tienen una 
preocupación real con respecto a la privacidad 
de sus datos de geolocalización, y la mayoría 
no está dispuesta a pagar para proteger su 
privacidad. Conclusiones:  este tipo de 
encuestas puede generar conciencia entre los 
partici pantes sobre el uso de su información 
privada. Los resultados expuestos en este 
documento se pueden utilizar para crear 
políticas y regulaciones gubernamentales por 
parte de las compañías de tecnología sobre la 
administración de la privacidad.  
Palabras clave:  privacidad en la 
localización , servicios basados en localización , 
percepción de privacidad.  Privacy Perception in Location -Based Services for Mobile Devices in the University Community of the North Coast of Colombia  
 
INGENIERÍA Y UNIVERSIDAD: ENGINEERING FO R DEVELOPMENT | COLOMBIA | V. 23 | NO. 1 | 2019  | ISSN: 0123 -2126 /2011 -2769 (Online) | Pág. 3 
 Introduction  
 
Mobile technologies have increased recently and along with them, the flow of information. 
The access to knowledge and use of these data concerns many users. It has opened the 
debate about what kind of data should be public or private. In the study presente d in [1], 
when Americans are asked what comes to mind when they hear the word “privacy,” they 
give important weight to the idea that privacy applies to their “rights”. According to this 
study, the item details of physical location over time  is fifth out of  sixteen on the level of 
sensitivity of personal information. Location -based information systems (LBISs) are 
defined as “applications that provide users with information based on their geographical 
position, which could be obtained from the mobile device t hey are accessing the service, or 
using a manually defined location” [2]. For LBSs to provide the requested information 
properly, sensitive data about the subject’s location is required [3]. Location information 
privacy is an important topic in the context  of expanding mobile technologies and 
applications.  
 
The perception of privacy is a subjective concept that has changed with the advance in 
technology; for instance, from mail to e -mail or from telephones to cellphones. This variety 
of concepts hinders it s measurement and the setting of privacy options in the mobile 
applications that use LBS, so it is more suitable to have a multidimensional concept of 
privacy. For example, in [4], the authors distinguish four dimensions of privacy and 
defining it as ‘‘the  ability to control and limit physical, interactional, psychological and 
informational access to the self or one’s group’’. Likewise, the work in [5] defines privacy 
“as the claim of individuals, groups or institutions to determine for themselves when, how , 
and what information about them is communicated to others”, while in [6], “the privacy 
involves the policies, procedures, and other controls that determine which personal 
information is collected, how it is used, with whom it is shared, and how individua ls who 
are the subject of that information are informed and involved in this process”.  
 
The concept of privacy and its implications is of vital importance, and for this reason, the 
UN Human Rights Council defends the right to privacy in the digital age an d recognizes the 
global and open nature of the Internet and the rapid advancement in information and 
communication technologies as a driving force in accelerating progress towards 
development in its various forms. It also affirms that the same rights that people have 
offline must also be protected online, including the right to privacy [7]. In addition to these 
definitions, location awareness involves privacy issues that differ from others. For example, 
when users download some mobile applications, they are  not aware that they are also 
giving the application permission for this information (usually because the majority of user Privacy Perception in Location -Based Services for Mobile Devices in the University Community of the North Coast of Colombia  
 
INGENIERÍA Y UNIVERSIDAD: ENGINEERING FO R DEVELOPMENT | COLOMBIA | V. 23 | NO. 1 | 2019  | ISSN: 0123 -2126 /2011 -2769 (Online) | Pág. 4 
 do not read the terms and conditions). On the other hand, users are concerned with whether 
their location is stored and whether this information is sold to third parties, but they are 
willing to sacrifice their information in order to have access to the service.  
 
In a previous study, privacy has been evaluated in different contexts: management of data 
in the Internet [8], email, phone c onversations and clinical history. However, to the best of 
the knowledge of the authors, there has not been a previous study that measures privacy 
perception in location -based services in Colombia. This work tries to cover two dimensions 
for privacy in LBS : privacy awareness and privacy management. The results analyzed from 
the survey offer a general view on the areas of privacy in location, and useful insights to 
understand the user preferences. The main percentage of the surveyed population is the 
univers ity communities in the North Caribbean region, with a small number of participants 
from other communities.  
 
Related Work  
 
New modes of communication have created many concerns about user security and 
privacy: identity, location, routine, and opinions are m ostly public information via social 
networks. The question of “who knows this information” has arisen within mobile 
technologies. This is a real concern because there are reported cases where a government 
used LBIS to determine political views of individua ls based on the presence of keywords or 
expressions against official positions, turning these people into targets of exhaustive 
tracking. However, this is not the only risk. In [9], the authors also include other risks:  
 
 Being tracked by an operator to det ermine where a particular user's phone is 
located.  
 Listing all devices that are deployed in a certain area at a certain time.  
 Wi-Fi and Bluetooth tracking through the MAC address of the mobile device, even 
if this is not actively connected to a network.  
 Illegal malware installed on mobile devices that track the user's location and their 
conversations.  
 
The above risks have also been analyzed in a study by [10] at MIT, where it was 
demonstrated that in a dataset of 1 ,5 million people where the location of an  individual is 
specified hourly and with a spatial resolution equal to that given by the carrier's antennas, 
only four space -temporal points are required to uniquely identify 95  % of the individuals.  Privacy Perception in Location -Based Services for Mobile Devices in the University Community of the North Coast of Colombia  
 
INGENIERÍA Y UNIVERSIDAD: ENGINEERING FO R DEVELOPMENT | COLOMBIA | V. 23 | NO. 1 | 2019  | ISSN: 0123 -2126 /2011 -2769 (Online) | Pág. 5 
  
Privacy in other context has been analyzed by many other studies. For example, the work 
done by [11] proposes a categorization of factors that should be included when examining 
Internet user’s privacy perception. The results of the performed analyses indicate that 
respondents who were more concerned about information collected during their online 
activity were also more concerned about their online privacy. In addition, respondents with 
a higher level of dissatisfaction with the current privacy protection reported a higher level 
of online privacy concerns.  
 
Furthermore, other recent studies show that users are becoming more aware of the security 
risks associated with their location, finding that 19  % of cellphone owners have turned off 
the location tracking feature on their devices because they were concerne d that other 
individuals or companies could access that information [12].  
 
Regarding location privacy, some techniques based on obfuscation have been developed. In 
[3], the authors present a compendium of techniques to protect the location privacy of the 
users. In [13], the authors propose an approach based on entity resolution which enables 
users to disclose their mobility information without compromising their privacy, even the 
data are linked with external publicly available information (social networks) . In this field 
of study, there are other proposed methods to conceal the location of a user [14] [15] [16] 
[17].  
 
There are few existing studies regarding privacy perception surveys. in [18], results show 
that users have a variety of motivations for part icipating in location -based surveys and that 
these motivations depend on the type of the survey in question. This study concluded that 
users concerned with privacy are less likely to be motivated by a monetary benefit but by 
the importance of the topic and  that the intrinsic motivators that drive the users concerned 
with privacy differ from those that drive unconcerned users.  
 
In 2011, the National Institute of Information Technologies (INTECO) of Spain developed 
a series of investigations supporting privac y with mobile devices [19]. T he purpose of the 
study was to “ perform a diagnostic of the use that the evolutionary netizens made in the 
mobile devices and smartphones, as well as the safety measures used and the impact 
suffered"". Other study based on perce ption was developed by the University of Madeira, 
using the basis of location from social networks. It stated “that location is an idiosyncratic 
property of people’s social networking profiles, and sharing it does not conform to existing Privacy Perception in Location -Based Services for Mobile Devices in the University Community of the North Coast of Colombia  
 
INGENIERÍA Y UNIVERSIDAD: ENGINEERING FO R DEVELOPMENT | COLOMBIA | V. 23 | NO. 1 | 2019  | ISSN: 0123 -2126 /2011 -2769 (Online) | Pág. 6 
 social network pra ctices and norms, particularly when the sharing is done in real -time and 
through mobile devices that the user permanently carries around” [20].  
 
In Colombia, the Groupe Speciale Mobile As sociation (GSMA) published the “ Study of 
GSMA on attitudes related t o the privacy of the mobile users” [21], The aim was to 
understand the privacy concerns that users of mobile device have and to help the 
development of public policies. Although data management involves the concept of 
privacy, there are not many studies ab out this topic, especially regarding LBS. This work is 
a first step to evaluate privacy perception in LBS in Colombia, as mentioned in the 
introduction.  
 
Methodology  
 
This study is based on an exploratory survey to measure the perception of location priva cy 
of users with mobile devices. The questionnaire was applied using Google Forms, an 
application that is a part of the Google Drive suite, in which basic surveys can be designed 
and distributed electronically. We, with the help of external experts, used cons ensus 
techniques [22] and indications to generate and select the questions as it is suggested in 
[23]. Finally, the form of the survey instrument was designed to identify the perceptions of 
privacy concerns in LBS.  
 
The link to the form was distributed by  the academic programs in participating universities 
to all the enrolled students and faculty. In addition, other mechanisms, such as social 
networks and email mailing lists were used in order to increase the outreach of the survey. 
Once all the answers we re received, a filtering process was performed in order to eliminate 
incomplete or corrupt information. Next, the final dataset was used for statistical analysis: 
the qualitative variables were used as categorical variables to perform multiple 
correspon dence analysis (see  Results section ) and several descriptive graphics were used 
to interpret the results  
 
Materials  
 
The survey has nineteen questions organized in three sections: personal information, 
identification of privacy and privacy management. These questions were designed to know 
the users’ perceptions of privacy concerns in LBS and any actions they take to pres erve it. Privacy Perception in Location -Based Services for Mobile Devices in the University Community of the North Coast of Colombia  
 
INGENIERÍA Y UNIVERSIDAD: ENGINEERING FO R DEVELOPMENT | COLOMBIA | V. 23 | NO. 1 | 2019  | ISSN: 0123 -2126 /2011 -2769 (Online) | Pág. 7 
 The criteria used by the authors to generate the questions were determined by the following 
suggestions given by [23]:  
 
 Questions should be clear, simple, short and motivating.  
 To avoid ambiguities, the questions should include a single logical st atement.  
 The questions should be grouped by subject.  
 
The responses for the multiple choice questions were selected by consensus with the 
purpose of trying to cover the most important possibilities and according to other similar 
studies such as [24], [25].  
 
For the demographic and socioeconomic data categories, the following variables were 
included: age, gender, city of residence, level of study, economic sector and monthly 
income.  
 
In the categories of privacy awareness about location, the questions were made to ascertain 
the level of knowledge that the users of mobile devices have about the privacy of their 
location. The following questions were asked:  
 
1. Do you know if you have applications on your mobile phone that have access to 
your geographical locati on? (Options: Yes/No)  
2. How do you rate the fact that applications on your mobile can know and store your 
geographical location with or without your consent? (Options: scale 1 -bad, 5 -
Good)  
3. Which entities would you allow to have access to your geographical location? 
(Options, multiple responses: Relatives and friends, Advertising and marketing 
companies, Social networks, Coworkers, Financial Institutions or Insurers, 
Government, Telecommunica tions companies and None)  
4. Do you use on your mobile device applications that store your geographical 
location or that of your family? (Options: Yes/No)  
5. Do you know if a relative or a friend has had an incident in security or privacy 
associated with the kn owledge of the geographical location by third parties? 
(Options: Yes/No)  Privacy Perception in Location -Based Services for Mobile Devices in the University Community of the North Coast of Colombia  
 
INGENIERÍA Y UNIVERSIDAD: ENGINEERING FO R DEVELOPMENT | COLOMBIA | V. 23 | NO. 1 | 2019  | ISSN: 0123 -2126 /2011 -2769 (Online) | Pág. 8 
 6. Do you agree with creating laws that protect the location of people? (Options: 
Yes/No)  
7. Would you be willing to pay to protect access to your location? (Options: Yes/No)  
8. What is the re ason you share your location? (Options, multiple responses: Safety, 
Work. Business and Other)  
 
Finally, the last category presented questions to analyze the level of supervision and control 
of the users on the management of the privacy of their location. T he questions asked are as 
follows:  
 
1. Which of the following situations do you consider an invasion of the privacy of your 
location? (Options, multiple responses: Selling my location data to third parties, 
Give my location to use an App, Sending advertising using my location and not 
being informed about using my location.)  
2. How often would you prefer Apps to ask you about access to your location? 
(Options: Every time you use the application, only when you install the application 
and do not ask about permission  to access your location.)  
3. Which of the following cases do you think the location could be accessed without 
the user ’s permission? (Options, multiple responses: Know the location of your 
children, Know the location of your employees, In case of emergency, Sending 
advertising targeting your interests, Never and Other)  
 
Participants  
 
The survey was sent to a total of 5800 people, of whom 670 responded, a response rate of 
11,5 %. Responses were analyzed, cleaned and standardized in terms of age and city origin, 
because these questions were open -ended.  
 
The mean age of the sample was 23 years, and the range was 13 -63. The majority of the 
respondents could be classified as young adults. The respondents were from Colombia, 
especially from the Colombian Carib bean Region. In addition, 50 ,1 % of the population 
surveyed had a professional career, and 64 ,9 % were students.  
 
 Privacy Perception in Location -Based Services for Mobile Devices in the University Community of the North Coast of Colombia  
 
INGENIERÍA Y UNIVERSIDAD: ENGINEERING FO R DEVELOPMENT | COLOMBIA | V. 23 | NO. 1 | 2019  | ISSN: 0123 -2126 /2011 -2769 (Online) | Pág. 9 
 Results  
 
In the survey, all the questions were the same and mandatory. However if the response to 
the question about occupation was “ work ”, then the respondent was redirected to the 
section “ economic area ”, which had 2 questions about salary and about specific area.  
 
After the answers were submitted, a cleaning process was performed, removing unrelated 
and inappropriate responses. Then, each q uestion was entered into the statistical analysis 
software IBM -SPSS. Closed responses and multiple choice questions were quantified by 
assigning a value for the label.  
 
According to the kind of question (privacy awareness or privacy management), two 
diffe rent analyses were performed: a discriminant analysis for nominal variables, as in the 
case of privacy awareness [26], and for the section privacy management, a cross -tabulation 
analysis was performed for each variable.  
 
Demographic and Socioeconomic Data 
 
As mentioned in the Methodology section, the survey was conducted among 670 
participants, of whom 59 ,6 % were male and 40 ,4 % female. Most participants were in the 
age range of 18 to 25 years old, 66 ,62 %, followed by the age range of 26 to 40, 17 ,29 %. 
These ages coincided with the answers given for the activities reported by  the participants, 
as shown in f igures 1 and 3. 
 
Figure 1. Pe rcentage of age of participants  
 
Source : author ’s own elaboration  
Privacy Perception in Location -Based Services for Mobile Devices in the University Community of the North Coast of Colombia  
 
INGENIERÍA Y UNIVERSIDAD: ENGINEERING FO R DEVELOPMENT | COLOMBIA | V. 23 | NO. 1 | 2019  | ISSN: 0123 -2126 /2011 -2769 (Online) | Pág. 10 
  
The main cities of residence of the participants were  Santa Marta, Barranquilla and 
Monteria. The geographical distribution of the cities and the number s of participants are 
shown in f igure 2. 
 
Figure 2. Geographical distribution of the cities and amount of participants  
 
Source: Galvis [27] 
 
The level of education of the participants was mostly those who have obtained a 
professional degree, 50 ,15 %; followed by secondary education, 23 ,88 %; and third, those 
with technical and technological training, 17 ,91 %, as shown in f igure 3. 
  
 
 
 
 
 
 
 
Privacy Perception in Location -Based Services for Mobile Devices in the University Community of the North Coast of Colombia  
 
INGENIERÍA Y UNIVERSIDAD: ENGINEERING FO R DEVELOPMENT | COLOMBIA | V. 23 | NO. 1 | 2019  | ISSN: 0123 -2126 /2011 -2769 (Online) | Pág. 11 
 Figure 3. Level o f education of the participants  
 
Source:  author’s own elaboration  
 
With regards to the activities of the people who participated in this study, 64 ,92 % are 
students, while 27 ,37 % are either dependent or inde pendent employees, as shown in f igure 
4. 
 
Figure 4. Activities of the participants  
 
Source: author’s own elaboration  
 
Of those who are working, the majority work in the education sector, 29 ,6 %; professional 
services, 12 ,80 %; and health, 12  %, as shown in f igure 5. 
Privacy Perception in Location -Based Services for Mobile Devices in the University Community of the North Coast of Colombia  
 
INGENIERÍA Y UNIVERSIDAD: ENGINEERING FO R DEVELOPMENT | COLOMBIA | V. 23 | NO. 1 | 2019  | ISSN: 0123 -2126 /2011 -2769 (Online) | Pág. 12 
  
Figure 5. Wor king sector of the participants  
 
Source:  author’s own elaboration  
 
 
The participants ’ monthly income is between COL$ 1000001 and COL$ 3000000 in 
39,56 % of the cases, and it is less than COL$ 1000000 in 38 ,46 % of the cases, as shown in 
figure 6. 
 
Figure 6. Inc ome of the working participants  
 
Source: author’s own elaboration  
Privacy Perception in Location -Based Services for Mobile Devices in the University Community of the North Coast of Colombia  
 
INGENIERÍA Y UNIVERSIDAD: ENGINEERING FO R DEVELOPMENT | COLOMBIA | V. 23 | NO. 1 | 2019  | ISSN: 0123 -2126 /2011 -2769 (Online) | Pág. 13 
  
 
Respondents’ Privacy Awareness  
 
This section describes the percentage of respondents who know about apps that use their 
location. It considers the respondents reported age, gender, education level and occupation. 
Statistical tests are conducted to determine whether any differences observ ed are 
statistically significant.  
 
Do you Know if you Have Applications on your Cellphone that Have Access to your Geographical 
Location?  
 
The majority of respondents (92 ,54 %) stated that they know about apps that have ac cess to 
their geolocation (see f igure 7a). This behavior was common in all cases (age, gender, 
education level and occupation). A first discriminant analysis showed that the relationship 
between this question and age, education level and occupation of respondent was not 
statistically sig nificant. Nevertheless, gender has a p -value  < 0,05 in the test of equal means 
of groups. Then, a second analysis was performed, comparing the gender with the other 
variables.  
 
In the case of gender and age range, it can be seen that the knowledge about location 
differed on the predictor variable gender. This result was obtained using ANOVA. A single 
discriminant function was calculated. The value of this function was significantly different 
for knowledge and nonknowledge (χ^2  = 8,740, df  = 2, p-value  < 0,05). The correlations 
between the predictor variables and the discriminant function suggested that gender was the 
best predictor for this question, since gender was positively correlated with discriminant 
function value. This means that gender can affect the answer to this question.  
 
 
 
 
 
 
 
 Privacy Perception in Location -Based Services for Mobile Devices in the University Community of the North Coast of Colombia  
 
INGENIERÍA Y UNIVERSIDAD: ENGINEERING FO R DEVELOPMENT | COLOMBIA | V. 23 | NO. 1 | 2019  | ISSN: 0123 -2126 /2011 -2769 (Online) | Pág. 14 
 Figure 7. Percentage of participants que stioned about privacy awareness  
a)     b)  
Source: author’s own elaboration  
 
Would you Be Willing to Pay to Protect Access to your Location?  
 
The majority of respondents (67 ,61 %) stated that they would not pay to protect  access to 
their location (see f igure 7b). This behavior was common in all cases (age, gender, 
education level and occupation). A discriminant analysis showed that the relationship 
between this question and gender,  age, education level and occupation of respondents was 
not statistically significant.  
 
Other Questions Related with Privacy Awareness  
 
The respondents were also questioned about the entities that could have access to their 
location, and the majority (83 ,3 %) agreed that family members could access the 
geolocation. Regarding the reason to share the location, 75 ,2 % of respondents selected to 
be in touch with relatives , of which 49  % selected for safety . In f igure 8, the respondents 
rated the access to the l ocation of the mobile applications, being 1 -bad and 5 -good.  
 
 
 
 
 
 
 
Privacy Perception in Location -Based Services for Mobile Devices in the University Community of the North Coast of Colombia  
 
INGENIERÍA Y UNIVERSIDAD: ENGINEERING FO R DEVELOPMENT | COLOMBIA | V. 23 | NO. 1 | 2019  | ISSN: 0123 -2126 /2011 -2769 (Online) | Pág. 15 
  
Figure 8. How do you rate the fact that your mobile applications can know and store your geographical location 
with or without your agreement?  
 
Source:  author’s own elaboration  
 
Taking into account these results, the main reasons for respondents to share their location is 
to keep in touch with relatives or safety reasons. Additionally, more than a third of them 
have an indifferent position regarding other entities having access to their location, and a 
third of the surveyed individuals consider that it tends to be bad.  
 
Respondents Privacy Management  
 
This section describes the percentage of respondents who would take action facing the 
privacy management. It considers the respondents’ reported age, gender, education level 
and occupation. A cross -tabulation is created in order to analyze the relationship  between 
the independent variables and the response options.  
 
Respondents were split into four groups of their reported age (18 years or younger; 19 -25 
years; 26 -40 years; 40 years or older), corresponding to levels 1, 2, 3 and 4, respectively. 
The gender  was codified with 1 for female and 2 for male; the education level was codified 
1-specialization, 2 -master or Ph.D., 3 -primary school, 4 -professional, 5 -high school, 6 -
technique; the occupation was codified 1 -art, 2 -looking for a job, 3 -sport, 4 -studing, 5-
chores, 6 -internship and 7 -working.  
 
 
 
 
Privacy Perception in Location -Based Services for Mobile Devices in the University Community of the North Coast of Colombia  
 
INGENIERÍA Y UNIVERSIDAD: ENGINEERING FO R DEVELOPMENT | COLOMBIA | V. 23 | NO. 1 | 2019  | ISSN: 0123 -2126 /2011 -2769 (Online) | Pág. 16 
 Which of the Following Situations do you Consider an Invasion of the Privacy of your Location?  
 
In this question, the respondents had four options for responses available: 1) Selling my 
location data to third parties, 2) Give my location to use an App, 3) Sending advertising 
using my location and 4) Not being informed about using my location.  
 
In figure 9, it can be seen that the majority of respondents consider an invasion of the 
privacy in location selling the location data to third parties  and a large percentage (66 ,3 %) 
also consider not being informed about using the location  an invasion.  
 
Figure 9. Which of the following situations do you consider an invasion of the privacy of your location?   
 
Source: author’s own elaboration  
 
A crosstabulation was performed for each independent variable (age, gender, education 
level and occupation). Fig ure 10 shows the analysis between the age range and the four 
options of the question. Figures 11, 12 and 13 show the crosstabulation between the four 
options of the question and gender, education level and occupation, respectively.  
 
In this case, the majorit y of respondents in the group comprising 19 -25 years, male gender, 
bachelors and students considered an invasion of the privacy in the categories of selling the 
location data to third parties  and not being informed about using the location . This shows 
that not knowing what using the location information may be a concern for the user.  
 
 
Privacy Perception in Location -Based Services for Mobile Devices in the University Community of the North Coast of Colombia  
 
INGENIERÍA Y UNIVERSIDAD: ENGINEERING FO R DEVELOPMENT | COLOMBIA | V. 23 | NO. 1 | 2019  | ISSN: 0123 -2126 /2011 -2769 (Online) | Pág. 17 
 Figure 10. Crosstabulation for age_rang e and invasion question  
 
Source : author’s own elaboration  
 
Figure 11. Crosstabulation f or gender and invasion question  
 
Source: author’s own elaboration  
Privacy Perception in Location -Based Services for Mobile Devices in the University Community of the North Coast of Colombia  
 
INGENIERÍA Y UNIVERSIDAD: ENGINEERING FO R DEVELOPMENT | COLOMBIA | V. 23 | NO. 1 | 2019  | ISSN: 0123 -2126 /2011 -2769 (Online) | Pág. 18 
  
Figure 12. Crosstabulation for educat ion level and invasion question  
 
Source: author’s own elaboration  
 
Figure 13. Crosstabulation for o ccupation and invasion question  
 
Source: author’s own elaboration  
Privacy Perception in Location -Based Services for Mobile Devices in the University Community of the North Coast of Colombia  
 
INGENIERÍA Y UNIVERSIDAD: ENGINEERING FO R DEVELOPMENT | COLOMBIA | V. 23 | NO. 1 | 2019  | ISSN: 0123 -2126 /2011 -2769 (Online) | Pág. 19 
  
Which of the Following Cases do you Think the Location could Be Accessed without the User’s 
Permission?  
 
In this question, the respondents had five options, with multiple responses available: 1) 
know the location of your children, 2) know the location of your employees, 3) in cas e of 
emergency, 4) sending advertising targeting your interests, 5) never and 6) other.  
 
The f igure 14 shows that the majority of respondents consider that the location could be 
accessed without permission in case of emergency (76 ,6 %) and the 56 ,1 % consider they 
can access to the location of their children, even without their permission.  
  
Figure 14. Which of the following cases do you think the location could be accessed without the user ’s permission?   
 
Source: author’s own elaboration  
 
A crosstabulation was performed for each independent variable (age, gender, education 
level and occupation). Additionally, a crosstabulation for location variable was done, with 
the aim to compare their results in different regions of Colombia. Fig ure 15 sho ws the 
analysis between the age range and the five options of the question. Figures 16, 17 and 18 
show the analyses between the five options of the question and gender, education level and 
occupation, respectively. Option 6) Other was omitted in the graphi cs because of the low 
percentage of respondents.  
 
Privacy Perception in Location -Based Services for Mobile Devices in the University Community of the North Coast of Colombia  
 
INGENIERÍA Y UNIVERSIDAD: ENGINEERING FO R DEVELOPMENT | COLOMBIA | V. 23 | NO. 1 | 2019  | ISSN: 0123 -2126 /2011 -2769 (Online) | Pág. 20 
 In this case, the majority of respondents in the group 19 -25 years, male gender, bachelors 
and students consider that the location could be accessed without permission in case of 
emergency and they can acce ss to the location of their children , even without their 
permission.  
 
Figure 15. Graph with crosstabulation for ag e_range and permission question  
 
Source: author’s own elaboration  
 
Figure 16. Crosstabulation for  gender and permission question  
 
Source: author’s own elaboration  
Privacy Perception in Location -Based Services for Mobile Devices in the University Community of the North Coast of Colombia  
 
INGENIERÍA Y UNIVERSIDAD: ENGINEERING FO R DEVELOPMENT | COLOMBIA | V. 23 | NO. 1 | 2019  | ISSN: 0123 -2126 /2011 -2769 (Online) | Pág. 21 
  
Figure 17. Crosstabulation for educatio n level and permission question  
 
Source: author’s own elaboration  
 
Figure 18. Crosstabulation for occ upation and permission question  
 
Source: author’s own elaboration  
Privacy Perception in Location -Based Services for Mobile Devices in the University Community of the North Coast of Colombia  
 
INGENIERÍA Y UNIVERSIDAD: ENGINEERING FO R DEVELOPMENT | COLOMBIA | V. 23 | NO. 1 | 2019  | ISSN: 0123 -2126 /2011 -2769 (Online) | Pág. 22 
  
Figure 19. Crosstabulation for l ocation and permission question  
 
Source: author’s own elaboration  
 
According to the comp arative analysis by regions in f igure 19, it may be observed that the 
percentage of respondents who agree to access the location without the user's permission in 
cases of emergency and to know the location of their children  does not vary according to 
the region. In all cases, it tends to rema in constant with 56  % for the Caribbean cities and 
62 % for the rest of the country.  
 
We can state that the concerns about the privacy of the location data are the same across the 
country and are not affected by the region of origin of the respondents. The above is also 
evident in the disapproval of the persons to whom its location can be known by the 
employers , a response that gets a very low percentage of acceptance: 5  % in the Caribbean 
region and of 8 ,5 % on average for the rest of the country. Nevertheless, it is important to 
highlight that the size of the sample in regions that do not belong to the Caribbean coast is 
very low; thus, it is desirable in future studies to expand the sample of those regions in 
order to get better conclusions.  
 
Privacy Perception in Location -Based Services for Mobile Devices in the University Community of the North Coast of Colombia  
 
INGENIERÍA Y UNIVERSIDAD: ENGINEERING FO R DEVELOPMENT | COLOMBIA | V. 23 | NO. 1 | 2019  | ISSN: 0123 -2126 /2011 -2769 (Online) | Pág. 23 
 Multiple Correspondence Analysis  
 
A multiple correspondence analysis was performed to discern the attributes of the groups 
that are formed when matching the qualitative variables, discerning each one of the 
dimensions. All the variables were coded according to the definition in Results section, but 
the variable Occupation  (In what occupies your time ) was summarized in three categories: 
1) work, 2) studies and 3) leisure . The results are presented in f igure 20, where dimension 1 
is the variable Age and dimension 9 is the variable payment for protection in location : 
 
Figure 20. Dimension 9 v s. Dimension 1 of the variables  
 
Source: author’s own elaboration  
 
According to f igure 20, the group “young students or just graduated nonsalaried”, which 
corresponds to the groups adolescents and young adults, mostly students (age range: 1 
under 18 years and 2, 19 to 25 years), or young just graduates who do not yet earn salaries, 
regardle ss of gender, disapprove the use of their location with or without their consent. This 
may be mainly because they know about technology and have some degree of knowledge 
about the use that the telecommunications companies can give to this information. In t his 
group, there are a greater correspondence in levels 2 of “Gender ”, level 1 of the variable 
Privacy Perception in Location -Based Services for Mobile Devices in the University Community of the North Coast of Colombia  
 
INGENIERÍA Y UNIVERSIDAD: ENGINEERING FO R DEVELOPMENT | COLOMBIA | V. 23 | NO. 1 | 2019  | ISSN: 0123 -2126 /2011 -2769 (Online) | Pág. 24 
 “According to others use your location ” and level 1 of the variable “Pay to protect your 
location ”, which indicates that within this group, the men disapprove ev en more the use 
consented or not of its location, reason why even they would be willing to pay to protect 
this information.  
 
On the other hand, women in this group tend to not to pay for the protection of their 
location, and in fact they are more inclined  to use geo -localized applications than men. The 
line is used to visualize the aforementioned situation of disparity in preferences between 
men and women in this group.  
 
The group “Employees adults rank 3 who have leisure time and potentially unfamiliar w ith 
technology ” shows the opposite correspondence to the previous group. In this case, the 
people in the age range of 26 to 40 years who have a job that provides a good income and 
have sufficient leisure time mostly approve the use of their location with o r without their 
consent. This may be because they are people who do not have a good knowledge of 
technology and use it only to supply their needs in telecommunications.  
 
A third group, “Postgraduate Professionals from Different Employment Sectors ” is 
obser ved, which shows the correspondence of adults over 40 with a high level of education 
(Specialization, M.Sc. and Ph.D.), regardless of their work sector. In this case, there is not a 
well-defined correspondence related to the aspects of use of its location through its mobile 
phones. Furthermore, they are a potential group that would not bother sharing their location 
or give others access to it without their consent.  
 
There is a remnant group, “Low income with no leisure time ”, which shows that low -
income peo ple, in general terms, spend all their time at work, so they may not be related to 
the use of geo -localized applications (because their salary level does not allow it or because 
they are not inclined to use them) resulting in that they are not clearly incl ined by the 
approval or disapproval of the use co nsented or not of its location.  
 
Conclusions  
 
In this study, a survey was applied to assess how citizens from Colombia perceive privacy 
in LBS for mobile device users. A total of 670 answers were used in the analysis from 690 
original answers, after the filtering process. It is worth noting the demogr aphics of the 
respondents; the majority of them were university students because the contacted Privacy Perception in Location -Based Services for Mobile Devices in the University Community of the North Coast of Colombia  
 
INGENIERÍA Y UNIVERSIDAD: ENGINEERING FO R DEVELOPMENT | COLOMBIA | V. 23 | NO. 1 | 2019  | ISSN: 0123 -2126 /2011 -2769 (Online) | Pág. 25 
 population was several universities of Barranquilla, Santa Marta and Monteria, mainly. 
This can bias the results, but the questions and the methodology can be ap plied to the whole 
country, which would build a more assorted sample.  
 
Regarding the results, it can be stated that, in general, the respondents did not show a real 
concern about the privacy in their geo -location and the majority is not willing to pay to 
protect this privacy. However, they consider it an invasion to their geo -location privacy if 
location data is sold to third parties or if they are not informed about using the location. 
They agree with allowing access the location without permission in ord er to know the 
location of their children and in case of emergency.  
 
Furthermore, with the multiple correspondence analysis we find that mobile device users 
who would be willing to pay to protect their privacy are young men in the age range of 18 
to 25 yea rs, who also disapprove the use of their location information without consent.  
 
Responding to this type of surveys can generate awareness among participants about the 
use of their private information. In addition, these results can be used to create gover nment 
policies and regulations by technology companies about the privacy management.  
 
Finally, it is worth mentioning that Google Forms  facilitated the assessment of data, taking 
into account that the participants were not geographically located in the sam e area. 
Additionally, it was necessary to limit the number of questions in order to ensure that the 
motivation level of respondents was kept high and to avoid poor data quality [28]. The 
survey did not ask the identity of the respondent, ensuring anonymity  of the respondents.  
 
For future work, a new survey should be applied to a larger and more diverse population in 
order to capture not only university -related citizens but also other communities that may 
have different needs from their LBS. Additionally, it  could show the impact of the number 
of increasing scandals related to privacy in social networks, such as the ones about 
Facebook and the elections in the United States. In addition, the survey may evaluate the 
perception of the citizens about strategies to save the privacy in LBS for mobile device 
users.  
 
 
 Privacy Perception in Location -Based Services for Mobile Devices in the University Community of the North Coast of Colombia  
 
INGENIERÍA Y UNIVERSIDAD: ENGINEERING FO R DEVELOPMENT | COLOMBIA | V. 23 | NO. 1 | 2019  | ISSN: 0123 -2126 /2011 -2769 (Online) | Pág. 26 
 References  
 
[1] M. Madden, “Public perceptions of privacy and security in the post-Snowden Era,” Pew Research 
Center, 2014. [Online]. Available: https://pewrsr.ch/2JVAjQK  
[2] P. Kalnis, G. Ghinita, K. Mouratid is, and D. Papadias, “Preventing location -based identity 
inference in anonymous spatial queries,” IEEE Trans. Knowl. Data Eng. , vol. 19, no. 12, pp. 
1719 –1733, Dec. 2007.  doi: 10.1109/TKDE.2007.190662  
[3] M. Zurbaran, L. Gonzalez, P. Wightman Rojas, and M. Labrador, “A survey on privacy in 
location -based services,” Ing. Desarr. , vol. 32, no. 2, pp. 314 –343, 2014.  Available: 
http://bit.ly/2ENeA9i  
[4] J. K. Burgoon, R. Parrott, B. A. Le Poire, D. L. Kelley, J. B. Walther, and D. Perry, “Maintaining 
and restoring privacy through communication in different types of relationships,” J. Soc. Pers. 
Relat. , vol. 6, no. 2, pp. 131 –158, May 1989.  https://doi.org/10.1177/026540758900600201  
[5] A. F. Westin, “Washington and Lee Law Review Privacy and Freedom,” Lee L. Rev ., vol. 166,  
no. 1,  1968.  Available: http://bit.ly/2HU9dak  
[6] L. Steinfeld and K. Sutherland Archuleta, “Privacy protection and compliance in higher 
education: The role of the CPO,” EDUCAUSE Rev ., vol. 41, no. 5 , pp. 62 –71, Sept./Oct. 2006.  
Available: http://bit.ly/2Zb1J8R  
[7] UN High Commissioner for Human Rights, “The right to privacy in the digital age,” Human 
Rights Council 27th Session,  Jun. 30 2014. Available: http://bit.ly/2IfAZgl  
[8] C. Paine, U. -D. Reips, S. Stieger, A. Joinson, and T. Buchanan, “Internet users’ perceptions of 
‘privacy concerns’ and ‘privacy actions,’” Int. J. Hum. Comput. Stud. , vol. 65, no. 6, pp. 526 –
536, 2007.  https://doi.org/10.1016/j.ijhcs.2006.12.001  
[9] EFF, “The problem with mobile phones,” Surveillance Self -Defense - EFF, 2015. [Online]. 
Accessed on: Aug. 06 2018. Available: http://bit.ly/2wyrnrP   
[10] Y.-A. de Montjoye and C. A. Hidalgo, “Unique in the crowd: The privacy bounds of human 
mobility,” Sci. Rep. , vol. 3, pp. 193 –220, Mar. 2013.  Available: https://go.nature.com/2Wimanf  
[11] R. Mekovec and N. Vrþek, “Factors that influence internet users’ privacy perception ,” in 33rd 
Int. Conf. Inf. Technol. Interfaces , Cavtat, Croatia, Jun. 2011 , pp. 27–30. Available: 
http://bit.ly/2WGXc0a   
[12] J. L. Boyles, A. Smith, and M. Madden, “Privacy and data management on mobile devices ,” Pew 
Research Center  [Online]. Available: https://pewrsr.ch/2HRxrln  
[13] I. Boutsis and V. Kalogeraki, “A fast and efficient entity resolution approach for preserving 
privacy in mobile data,” in 2016 IEEE Int. Congr . Big Data , BigData Congr ., pp. 173 –180.  doi: 
10.1109/BigDataCongress.2016.29  
[14] L. Tang, S. Vrbsky, and X. Hong, “Collaborated camouflaging mobility for mobile privacy ,” in 
IEEE GLOBECOM 2008 , pp. 2154 –2158.  doi: 10.1109/GLOCOM.2008.ECP.415  
[15] R. Liu, J. Cao, L. Yang, and K. Zhang, “PriWe: Recommendation for privacy settings of mobile 
apps based on crowdsourced users’ expectations,” in 2015 IEEE Int. Conf . Mobile Services , pp. 
150–157. doi: 10.1109/MobServ.2015.30  
[16] Y. Gong, L. Wei, Y. Guo, C. Zhang, and Y. Fang, “Optimal task recommendation for mobile 
crowdsourcing with privacy control,” in IEEE Internet Things J. , vol. 3, no. 5, pp. 745 –756, Oct. 
2016.  doi: 10.1109/JIOT.2015.2512282  
[17] G. Font , J. Bustos , S. Blasco,  and A. Hevia , “Location privacy for a monitoring system of the 
quality of access to mobile internet,” in IEEE Lat. Am. Trans. , vol. 14, no. 6, pp. 2894 –2896, Jun. 
2016.  doi: 10.1109/CNS.2014.6997533  Privacy Perception in Location -Based Services for Mobile Devices in the University Community of the North Coast of Colombia  
 
INGENIERÍA Y UNIVERSIDAD: ENGINEERING FO R DEVELOPMENT | COLOMBIA | V. 23 | NO. 1 | 2019  | ISSN: 0123 -2126 /2011 -2769 (Online) | Pág. 27 
 [18] M. Poikela, R. Schmidt, I. Wechsung, and S. Moller, “About your smartphone usage : Privacy in 
location -based mobile participation,” in 2015 IEEE Int. Symp . Technol . Soc., ISTAS , pp. 1–6. doi: 
10.1109/ISTAS.2015.7439421  
[19] Instituto Nacional de Tecnologías de la Comunicación , INTECO, “Estudio sobre seguridad en 
dispositivos móviles y smartphones,” Observ. Seguridad Inform., 2011.  Available: 
http://bit.ly/2JTWJSk  
[20] D. Wagner et al., “Hide and seek,” in Proc . 12th Int. Conf. Human Comput . Interaction Mobile 
Devices Serv. - MobileHCI ’10 , 2010, p p. 55–58. 
[21] GSMA, “Mobile privacy: Consumer research insights and considerations for policymakers,” 
2014.  Availab le: http://bit.ly/2Xn2oUb  
[22] A. Fink, J. Kosecoff, M. Chassin, and R. H. Brook, “Consensus methods: Characteristics and 
guidelines for use,” Am. J. Public Health , vol. 74, no. 9, pp. 979 –83, Sep. 1984.  
[23] J. Casas Anguita, J. R. Repullo Labrador, and J. Donado Campos, “La encuesta como t écnica de 
investigación.  Elaboración de cuestionarios y tratamiento estadístico de los datos (I),” Aten. 
Primaria , vol. 31, no. 8, pp. 469 –558, 2003.  https://doi.org/10.1016/S0212 -6567(03)70728 -8 
[24] D. Christin, C. Buchner, and N. Leibecke, “What’s the value of your privacy? Exploring factors 
that influence privacy -sensitive contributions to participatory sensing applications,” in 38th Ann. 
IEEE Conf . Local Comput . Netw .-Workshops , 2013, pp. 918 –923. doi: 
10.1109/LCNW.2013.6758532  
[25] A. J. B. Brush, J. Krumm, and J. Scott, “Exploring end user preferences for location obfuscation, 
location -based services, and the value of location,” in Proc . 12th ACM Int. Conf . Ubiquitous 
Comput ., 2010, pp. 95 –104. Available:  http://bit.ly/2KpDyzl  
[26] M. Arriaza Balm ón, Guía pr áctica de an álisis de datos . S. l. : Instituto de Investigaci ón y 
Formaci ón Agraria y Pesquera, 2006.  Available: http://bit.ly/ 2JYnyEW  
[27] L. Galvis, “Geografía económica del Caribe continental ,” Doc. Trabajo Econom. Regional , no. 
119, 2009.  
[28] U.-D. Reips, “Standards for internet -based experimenting,” Exp. Psychol. , vol. 49, no. 4, pp. 
243–256, 2002.  Available: http://bit.ly/2Xk7e4m  "
DataPrivacy,3012.txt,"International Journal of Law and Management
Is data privacy a fundamental right in India? An analysis and recommendations from policy and legal
perspective
Sheshadri Chatterjee,
Article information:
To cite this document:
Sheshadri Chatterjee, ""Is data privacy a fundamental right in India? An analysis and recommendations from policy and legal
perspective"", International Journal of Law and Management, https://doi.org/10.1108/IJLMA-01-2018-0013
Permanent link to this document:
https://doi.org/10.1108/IJLMA-01-2018-0013
Downloaded on: 22 January 2019, At: 09:23 (PT)
References: this document contains references to 0 other documents.
To copy this document: permissions@emeraldinsight.com
Access to this document was granted through an Emerald subscription provided by emerald-srm:402646 []
For Authors
If you would like to write for this, or any other Emerald publication, then please use our Emerald for Authors service
information about how to choose which publication to write for and submission guidelines are available for all. Please
visit www.emeraldinsight.com/authors for more information.
About Emerald www.emeraldinsight.com
Emerald is a global publisher linking research and practice to the benefit of society. The company manages a portfolio of
more than 290 journals and over 2,350 books and book series volumes, as well as providing an extensive range of online
products and additional customer resources and services.
Emerald is both COUNTER 4 and TRANSFER compliant. The organization is a partner of the Committee on Publication
Ethics (COPE) and also works with Portico and the LOCKSS initiative for digital archive preservation.
*Related content and download information correct at time of download.
Downloaded by Tulane University At 09:23 22 January 2019 (PT)
Is data privacy a fundamental right in India? An analysis and 
recommendations from policy and legal perspective
1. Introduction
 
In recent days, collection of data of multipurpose nature has become ubiquitous. Data is being 
collected by Google for the users who are acquiring relevant knowledge by visiting the 
appropriate sites. Collection of data is also done by Facebook and it is left for sharing. Other 
organizations are also engaged in this affair. Data is used for marketing and to realize business 
behavior of the users which helps the companies to improve their business health. Of course, 
it is the duty of the companies to adopt Fair Information Principles and Policies. They are 
supposed to get the users informed before collection of their data (Baskerville 1973; Bellamy 
2000). Thus, though it is a fact that organizations adopt such principles for privacy protection 
policy, much is left for adoption of fair practices. The effect of privacy policy to protect privacy 
is growing globally (Casalo 2007). This practice of data protection system cropped up initially 
in Northern European countries.1 It spread over throughout the globe from early 1970s.2 Of 
course, initially this system started to protect non- sensitive data like data concerning to car 
ownership, number of children and so on.3 More the data flowing crossing the borders, more 
is the occurrence of breach of privacy including commission of financial fraud (Chang and Ho 
2006). For this, structuring proper policy in this context has become demand of the society. 
Very recently, huge number of debit and credit cards had been exposed by hackers who sold 
this stolen information of credit and debit cards data to people of different countries (Cranor et 
al. 2000). Social networking sites have become a fashion of the people around the globe for 
use connecting one man with other across the border even culminating chance of data leakage 
out of proportion. The data exchanged are stored on the servers and nobody has the knowledge 
as to where those servers are situated. Do we know where the main server of Facebook is 
situated? There has been occurrence of innumerable incidences where the intruders have 
accessed in some data of others resulting in unwanted compromise of privacy for millions of 
records. In this way, personal data are being hijacked. But, authorities initially did not focus 
attention to frame policy for protection of personal data. Rather, the authorities became sincere 
to fairness and reasonableness of data processing, it being a global scenario.4 Online culture 
has spread throughout the World and India is also a part of that. This has resulted in chance of 
data theft (Burkert and Herbert 1983). It helped for committing financial fraud. Information 
Technology is growing in India appreciably. Government sector is investing a lot for 
implementation of e-governance projects. Indian IT and services of IT industry are developing 
out of proportion. It is known that outsourcing industry currently is involved with $60 billion 
and is expected to reach $225 billion by 2020. This transformation is also increasing the threat 
of data leakage resulting in compromise in data privacy (Culnan and Pamele 1999). It is 
important to note that Indians are not individualistic with law. Individualism Index (IDV)5 is 
greater than Power Distance Index (PDI)6 as opined by Hofsted.7 Increase of commercial 
activities in India is depending on the reliance of personal information of the users. 
Governmental agencies depending on e-transaction are following standard guidelines for data 
Downloaded by Tulane University At 09:23 22 January 2019 (PT)
protection so that the consumers do not feel insecure to use this electronic platform. In cross 
border data flow, protection of data has become a much-discussed subject.8 Clients outside 
India are always expecting more security of their data (Costa, Luiz and Poullet 2012). They are 
wanting implementation of robust data protection policy to be adhered to by India so that they 
feel secured in outsourcing to India where there exist many reasons for getting advantages of 
outsourcing over other countries. This will be discussed later. 
By introduction of different enactments like Indian Telegraph Act, 1885; Indian Contract Act, 
1872; The Special Relief Act, 1963; The Public Financial Institutions Act, 1983; The Consumer 
Protection Act, 1986; The Credit Information Companies (Regulations) Act 2005 and I.T. Act, 
2000, attempts are made to address this situation. Very recently, IT (Amendment) Act, 2008 
has introduced the concept of “sensitive personal information” and has enacted to fix liability 
on “Body Corporate” for protecting the same. This policy has improved the protection 
mechanism a lot. We shall discuss this issue covering IT Act 2000 later. This amendment also 
has introduced procedures for protection of sensitive personal information. All these are needed 
for protection of privacy (Dammann et al. 1977). However, in this article we have taken a 
holistic attempt to discuss about meaning of privacy under different perspectives to have a 
vivid conception. We have also discussed how issues of privacy are affecting outsourcing 
business in Indian. Position of right to privacy in the Constitution of India has been discussed 
in different perspectives. It includes the power of Constitution for legislating data protection 
privacy laws under Article 246 including interpretation of entry 97 in List I. We have also 
discussed Article 248 of the Constitution of India (Gellert et al. 2013). We have also, in this 
article, discussed the Indian and Western Culture (Gonzalez et al. 2012) of conception of 
privacy including constitutional position of right to privacy as laid down in Part III of the 
Constitution of India. It includes explicit explanation of Article 21 of the Constitution of India. 
We have also touched judicial developments of privacy citing different case laws like cases of 
Gobinda, Malek Singh and so on. We have also discussed detail of I.T. Act, 2000 relating to 
issues of privacy. We have analyzed the full bench judgement of apex Court of India regarding 
issues connected with ‘right to privacy’ read with Articles of Part III of the Constitution of 
India. We have provided brief recommendations for extending and developing privacy 
protection policy mechanisms in India. At last, a meaningful and effective conclusion has been 
drawn.   
2. Privacy and Policy
There are many avenues in defining privacy. It depends on what approach one is following in 
defining privacy (Dlamini et al. 2009). The approaches are mainly structuralist approach, 
individualistic approach and integrative approach. Structuralist approach defines privacy in 
terms of social structure. It includes moral right or a legal right which helps one to restrict or 
to limit others in matters relating to access to some information or to some other persons. It has 
defined (Allen 1988) privacy as “Privacy denotes a degree of inaccessibility of persons, their 
mental status, and information about them to the senses and surveillance devices of others”. A 
researcher (Bok 1983) had the idea that privacy is related with secrecy. Bok defined privacy as 
“privacy is a condition of being protected from unwanted access by others”. It appears that 
structuralist approach argues that more we can keep individual information secret; more 
Downloaded by Tulane University At 09:23 22 January 2019 (PT)
privacy is ensured (Dutta and Mc Crohan 2002). Individualistic approach defines privacy as a 
measure to control ourselves as to how we can keep our personal information not divulged to 
others. Privacy has been defined (Westin 2003) as “claim of individuals, groups or institutions 
about them is communicated to others”. Privacy has also been defined (Decrew 1989) as 
“information control and control over decision-making”. Integrative approach defining privacy 
is associated with having right and interest. Privacy is defined in this context (Reiman 1976) as 
“an important interest in simply being able to restrict information about, and observation of 
myself regardless of what may be done with that information or the result of that observations”. 
From this definition given by Reiman, it is evident that this approach is a culmination of 
combination of structuralist approach and individualistic approach. From the knowledge of the 
above definitions in the light of integrative approach for defining privacy, it is seen that privacy 
may be construed to be an ingredient which contributes important value to the society (Eloff 
and Von Solms 2000). In India, the conception of privacy varies from individual to individual, 
from place to place according to system of the society. In some places of India, even, sense of 
privacy depends on gender. Women in some places of India do not like to appear before public 
keeping in seclusion observing ‘Purdah’.9 Purdah includes as opined by some judicial decisions 
as human morality and natural modesty. It should be confined in apartments only. So, holding 
‘Purdah’ is construed to be, in specific cases, legal in the sense of restricted privacy. 
3. Outsourcing, Data Protection and Privacy Policy
Many other developed countries like USA, EU and so on are now-a-days (Late 1990s) are 
found to have been engaged outsourcing their works to India and this has resulted in huge 
productions of jobs in India (Dietter 1994; Joseph et al. 2017). However, this outsourcing 
invariably is associated with operation of date transfer. Consequently, whenever by the other 
countries outsourcing is made to India; for protection of data of those countries against data 
theft, India must be very vigilant since otherwise the Indian people will lose jobs (Thomson et 
al. 2006; Featherman et al. 2010). In this context, consideration of the nature of data is to be 
properly attended. Not only that. What the purpose of the data is to be considered. How long, 
that is, with what duration the proposed operational procedure would last is also an important 
issue to be focused. Naturally, the legal perspective, in general and in specific zone, is required 
to be scrutinized (Zinser 2003). It is pertinent to mention here that when the countries who 
would outsource to India would focus attention on two things. They would focus on the issue 
as to how Indians are adhering to follow formal legal rules including procedural mechanisms 
regarding formal oversight associated with implementation of legislation. They would also 
want to be satisfied to see as to how professional rules, policies and measures embodied with 
security and privacy in protecting their so-supplied data are being dealt with by the Indians 
(Sequeira 2003; Solms and Solms 2004). If these are really and honestly complied with by the 
Indians to the respectable level of satisfaction of the countries who are outsourcing, they would 
unhesitatingly outsource to India and India would relish profound job-creation. This is very 
important and touchy issue for the Indians because Indians possess variety of regulatory culture 
so far as, systems, customs and other casual and information elements and policies are 
concerned circumscribing issues related with data protection (Foxman and Kileoyne 1993; 
Tryfonos et al. 2001). To gain the flavor of having matters outsourced to India by other 
Downloaded by Tulane University At 09:23 22 January 2019 (PT)
countries like USA and European Unions since these countries are concerned to cut cost by 
outsourcing their activities to third congenial countries where there exists cheaper workforce, 
India being a fittest choice should act in such a manner as outsiders do not feel that their data 
privacy would be at stake (Sloot 2014). Other developed countries choose India for outsourcing 
for many reasons. India has huge skilled, semi-skilled manpower. They are conversant in 
speaking, writing and understanding English in better way. India has a good educational system 
with general emphasis on mathematics and science helping to conceptualize the computer 
logic. Indians are comparatively capable of maintaining strong quality orientations. Indians are 
capable of working hands having, in addition, abilities to tactfully manage time zone 
differences for its unique geographical orientation. India has simple tax structure that has 
helped to place BPO/KPO industries in equal footings with IT enterprises. Judged from the 
above discussions, it is evident that India must have robust data protection policy for strict 
maintenance of data protection so that the countries outsourcing to India may not feel anxiety 
and risk regarding their data protection (Tripathi and Mishra 2000; Furnell 2006). Again, to 
discuss how data privacy is ensured in India, initially we are required to know the different 
aspects of Constitution of India regarding privacy right (Jackson et al. 2014) and the Indian 
policy for protection of data.
3.1 Preamble of the Constitution of India and Privacy
The Constitution of India in preamble defines India as a “Sovereign, Socialist, Secular, 
Democratic Republic”. The preamble was kept outside (exterior) of the Constitution of India. 
By the Supreme Court of India in Berubari Union case, in a unanimous decision Justice 
Gajendragadkar held inter alia that the preamble should be construed to be an important weapon 
and lever. It will open the original intention of the originator of constitution. Further it was held 
that the preamble can hardly impose any limitation. In Golak Nath case (Golak Nath v State of 
Punjab) the petitioner challenged the order of lower court where some lands of Golak Nath 
family were declared surplus under Punjab Security and Land Tenure Act, 1953. It was 
challenged ultimately in the Supreme Court of India under article 32 of the Constitution 
claiming such order denied the constitutional right to hold property under article 19(f) and 
19(g) and on the ground of violation of article 14 of the Constitution. Golak Nath also pleaded 
against inclusion of this State Act under Ninth Schedule. The Supreme Court of India left 
Parliament with no power to curtail fundamental right and hence ordered in favor of Golak 
Nath. Justice Wanchoo held inter alia that preamble can hardly control the unambiguous 
language of the recital of the Indian Constitution. But, the Kesavananda Bharati case 
(Kesavananda Bharati v State of Kerala) both these views of the justices have been rejected. 
Majority of the justices in this case held that preamble is a part and parcel of the Constitution 
of India. Question may crop up as to why in discussing data protection and privacy matters we 
are profoundly discussing about ‘Preamble’ of the Constitution. This is only because to inhale 
how our constitution is robust to uphold sense of values including privacy and its policy which 
cover entire part III of the Constitution of India, especially Article 21 of the Constitution of 
India. In the Kharak Singh case (Kharak Singh v State of UP) police officials used to have 
enquired the presence of the petitioner in his house even at night jeopardizing his privacy. The 
issue eventually went to the Supreme Court of India where it was held that privacy is to be 
Downloaded by Tulane University At 09:23 22 January 2019 (PT)
protected in terms of the Constitution of India but never at the cost of some greater issues and 
it was held that the case is not maintainable under article 32 of the Constitution of India. The 
privacy in that case was not given much importance with the help of the Constitution of India. 
However, recently the Supreme Court of India has given a historical judgement declaring right 
to privacy as a fundamental right under the Constitution of India. This will be discussed in later 
part when we shall discuss the full bench referral judgement, K.S. Puttaswamy and others v 
Union of India and others, of the apex court of India where full bench observed eventually 
privacy as a part of fundamental right. Though explicitly this has not been included in the 
constitution following procedure laid down in article 368 of the Constitution of India till date. 
To visualize the context of rigidity of the preamble of the constitution where the human rights 
are implicitly included, it is worth mentioning any unethical amendment made by process of 
Article 368 contravening very sense of feature of constitution declared void, Indira Gandhi v 
Raj Narain.
4. Data Protection and Competence for legalization 
4.1 Global Scenario for Data Protection Legislation 
In matters connected with the global scenario in Data Protection, it is seen that the different 
countries used to have followed two distinct models. They are European Model and American 
Marketplace Model. The former model is framed with the idea of right-based approach and the 
later follows sector-specific Data Protection laws. The European General Data Protection 
Regulation, 2016 (EU GDPR) is very stringent law. It gives safeguards to right to privacy and 
right to protection of personal data. For protection of personal data in US, there is no single 
National Law. In US Constitution, there is no provision for personal privacy explicitly. In US, 
different states used to have followed different Data Protection laws. In US, government and 
private approach covering Data Protection is different. Government follows Electronic 
Communication Privacy Act, 1986, The Right to Financial Privacy Act and so on whereas the 
affairs of private sector are controlled by Federal Trade Commission Act, The Finance Services 
Modernization Act. From both these EU and US laws, the Indian authorities got inputs and 
framed laws of Data Protection in Indian context.  
4.2 Data Protection and Indian Legislation 
We know in the Constitution of India, Part IX has given legislative power divided in the three 
lists, List I, List II and List III under article 246 of the Constitution of India. The lists are 
depicted in seventh schedule. The List I is called Union List, List II is called State List and List 
III is called concurrent list. The article 246 states “Subject matter of laws made by parliament 
and by the Legislatures of States:
(1) Notwithstanding anything in clauses (2) and (3), Parliament has exclusive power to make 
laws with respect to any of the matters enumerated in List I in the seventh schedule (in this 
constitution referred to as the Union List).  
(2) Notwithstanding in clause (3), parliament, and, subject to clause (1), the legislature of any 
state also, have power to make laws with respect to any of the matter enumerated in List III in 
the seventh schedule (in this Constitution referred to as “Concurrent List”). 
Downloaded by Tulane University At 09:23 22 January 2019 (PT)
(3) Subject to clause (1) and (2), the legislature of any State has exclusive power to make laws 
for such state or any part thereof with respect to any of the matters enumerated in List II in the 
seventh schedule (in the Constitution referred to as “State List”).
(4) Parliament has power to make laws with respect to any matter for any part of the territory 
of India not included [in a State] notwithstanding that such matter is a matter enumerated in 
the State List.”
The Article 248 states “Residuary powers of legislation 
(1) Parliament has exclusive power to make any law with respect to any matter not enumerated 
in the concurrent list or State List. 
(2) Such power shall include the power of making any law imposing a tax not mentioned in 
either of those Lists.”
Slight studies of List I, List II and List III show that the lists do not include powers of 
Parliament or States to legislate acts relating to data privacy and data protection. Then how 
those laws protecting data privacy can be framed?
The entries in the List I, List II & List III can be amended by the help of procedure envisaged 
in article 368 of the Constitution of India. But in that case such amendment is to be ratified by 
not less than one half of the state legislatives before it is placed before the President to get His 
assent. 
Again, if we focus our attention to entry No.97 of List I (Union List) it appears that it is written 
“any other matter not enumerated in List II or List III including any tax not mentioned in either 
of those Lists.” Such being the scenario, how it is possible by the Legislative authority to frame 
laws covering data protection? 
In H.S. Dhillon case, the Supreme Court showed the thoroughfare. It was held that if it is found 
that subject matter of a legislation hardly comes under the ambit of any entry depicted in List 
II or in List III, the Parliament can enact in terms of the residuary power laid down in article 
248 of the Constitution of India. It has also been explicitly confirmed in the case of Attorney 
General (for India) v Amrat Lal Prajivandas. In terms of the above discussions, it may be 
inferred that the Parliament is competent enough for framing legislation on the issues of privacy 
because this issue of privacy may be construed to be within other matter not contained in List 
II (State List) nor in List III (concurrent List) as is evident from the recital of entry 97 of List I 
and from the recital of article 248 of the Constitution of India.
5. Data Protection Policy and Legalization
To ascertain the features of data protection policy and to know the structure of regulations in 
matters relating with data protection issues for the Indians, a specific study is required to be 
conducted. There exists policies and regulations regarding data protection for the western 
countries. These may give an idea to the Indian authority to structure the policies and to frame 
regulations. However, a comparative study is needed to be conducted to conceptualize the 
reactions of Indian citizens and people of western countries as to how they behave in case when 
breaches of privacy occur. This will give food and inputs for the authorities to provide 
Downloaded by Tulane University At 09:23 22 January 2019 (PT)
comprehensive policies for data protection for Indian citizens and would give inputs 
appropriately for framing executable enactments to address protection of privacy for the Indian 
citizens.  
5.1 Background of Privacy of Indians and western people
Mellon University10 published a report on privacy perception by conducting surveys. This was 
conducted by School of Computer Science of that University. The repot highlighted that 
privacy of Indians usually covers conception of privacy in one’s private territory, that is, 
personal privacy.11 The survey found that a different perception of privacy is held by the people 
of non-western countries.12The survey shows that Indians are found to be more concerned in 
considering issue of privacy in a completely different dimension and they ascribe seriousness 
in protecting privacy sometimes even being unreasonably strict compared to the conception of 
western people.13 Thus, it can be inferred that a positive relationship exists among nationality, 
concern of people regarding privacy and naturally shape of regulations necessary through 
enactments.14Culture and privacy conception, rather variation of culture from country to 
country and extent of privacy concern count much in formulation of privacy protection policy 
to safeguard privacy. 15A new terminology is required to be imported for data protection in this 
modern cyberspace culture which is ‘technical culture’. This is because system of storying, 
exchanging data has undergone drastic change. It was never done before even in developing 
country like India.16 This introduction of new technology needed for framing policy and 
regulation in India for protecting privacy.17 It is also important to note that the policy, 
regulations, restrictions relating to privacy protection in US, Canada, Australia still target to 
cover affairs of privacy in public sector.18As I have already stated that Indians being under 
British rule for around long two hundred years had a fear always for framing new policies and 
enactments covering affairs of public concerning to privacy.19 The Indians gained from British 
rule procrastinating tendency to adopt for any system and this prolixity was found to have been 
removed by Indians in late 1990s and they focused more attention for reforms of policy and of 
legislation befitting with need of the society.20 Before we go deep into the core mechanism and 
analyze the regulations around Data Protection, it is very important to conceptualize the clear 
idea and perception relating to ‘Privacy in India’. The Indian culture is totally different from 
the customs and landmarks of Western World (Heringa and Kiiver 2012). Hence, to realize the 
sense of privacy in India, the culture and customs and practices of India are to be understood 
distinctly. These aspects play vital role when matters related with privacy in India are dealt 
with (Hofsted 1997; Budak and Rajh 2011). To realize this, we have here depended on two 
studies. These have been published by the ‘School of Computer Science of the Carnegie Mellon 
University’.  
The first survey includes on contacting for taking feedback from workforce with high-tech 
capabilities in India for having an idea of sense of data privacy by the people engaged in 
outsourcing paradigm (Kankanhalli et al. 2003; Geers 2010). 
The second survey conducted by P. Kumaraguru and L. Cranor has highlighted a 
comprehensive comparison between people of India and people of United States. It ventilates 
that there lies a huge gap regarding the sense of the word ‘Privacy’ between the people of India 
Downloaded by Tulane University At 09:23 22 January 2019 (PT)
and the people of United States. It is to note, despite need and pressure from society, the Indian 
authorities had shown a reluctance to adopt data privacy law.21
The Indians feel sense of privacy preliminary covering privacy of personal space and privacy 
of subjects. But the people of USA think of privacy surrounding privacy covering information, 
privacy associated with financial issues and privacy with theft of identity since it appears that 
48% of Indians are found concerned with privacy to home, privacy to physical space and 
privacy to living space. On the contrary, only 18% of USA people are concerned regarding 
privacy on these issues (Peltier 2002; Kolb and Abdullah 2009). 
It also appears that 89% of US people are concerned about data Security & Privacy but only 
21% of the Indian people are concerned regarding these aspects presumably for lack of 
awareness (Kruger and Kearney 2006). Thus, it is safely concluded that there exists a sharp 
difference in the matter of conception of privacy in India compared with the conception of 
privacy in western countries. 
6. Conception of Right to Privacy
Compared to western countries, it appears that Indians are found to be more privacy-cautious, 
but they’re not able to prioritize the issues. As a result, they attach more concern regarding 
infringement of privacy for flimsy issues (MaQingxiong and Pearson 2008). Besides, concept 
of privacy is individual-centric. What I think infringement of privacy, others might not think. 
Over-progressive persons sometimes say that they are free, they have no privacy as they have 
nothing to hide. It is an over exaggeration. The conception of privacy is associated with sense 
of information security culture (Martins and Eloff 2002). Owing to advancement of 
technologies, the coverage of infringement of privacy amplitude has been reduced because 
technicalities automatically protect privacy needing no human endeavor. Still, these technical 
challenges also sometimes threaten for privacy infringement (Osborne 1998; Ohkubo et al. 
2005). Thus, concept of right to privacy cannot be calibrated and measured as it has no standard 
being it to be dependent on the individual’s perception.   
6.1 Scenario referring to the Constitution of India
Before the Constitution of India was adopted having effect from 26th January 1950, the right to 
privacy of the citizens of India had not been guaranteed and legal status whatever meager might 
it be conferred to the citizens of India came into force after adoption of the Constitution. Prior 
to adoption of Constitution, the matter connected with protection of privacy was covered by 
criminal laws especially relating to property, dwelling house and imputation of un-chastity 
action to women. These were considered as punishable offence. At that time, the law of torts 
reportedly acted as a meaningful weapon to cover infringement of privacy concerning to 
property and so on. But, privacy, jeopardizing one’s name and fame, was hardly protected 
legally in the real sense of the term (Lynskey 2014). Even after adoption of the Constitution of 
India, there was no explicit guarantee of right to privacy in the Constitution of India. But, it 
contains protection of other rights implicitly mentioned in the text. The logic behind this 
formulation that the right explicitly mentioned in the text would have no value if other rights 
are not protected by implication.22 From Article 14 to Article 30, Part III of the Constitution of 
India embodied Fundamental Rights. However, several judicial decisions initially enticed the 
Downloaded by Tulane University At 09:23 22 January 2019 (PT)
right to privacy within the penumbra of the Constitution. The Article 21 of the Constitution of 
India envisages “no person shall be deprived of his life or personal liberty except according to 
procedures established by law”. The last portion of this Article 21 ‘except according to 
procedures established by law’ is very important. Based on this Article, observations of the 
Supreme Court highlights inter alia that if any one while discharging of what they perceive to 
be their duties appears to have deprived other persons relating to their personal liberty, the 
person doing so should strictly observe the rules as prescribed. Thus, in brief, it may be 
construed that “personal liberty” has a meaning with wide connotation. It (personal liberty) 
means life is required to be free without being encroached by something which law does not 
permit. The elucidation of Article 21 gives a scope so that any law framed must possess 
prescribed procedure. The prescribed procedure should echo fundamental right conferred in 
Article 19 relating to a given scenario. The procedure must come out successful so far as acid 
test of Article 21 is concerned as cited in M.P. Sharma v Satish Chandra.  
Article 21 encompasses and covers human rights which are conceivable. The Article 21 has 
given in broad spectrum negative injunction in respect of a positive mandate allowing to do all 
things which would transform life living with dignity as cited in Maneka Gandhi v Union of 
India. In the Article 21, the word ‘deprived’ is to be interpreted in a very cautious way. It 
(deprived) cannot be construed to be “total loss” but it tantamount to any restriction or 
impediment on liberty. Now if we focus our attention in U.S. Constitution, we will see that it 
has mentioned the expression ‘due process of law’ while the Constitution of India made it a 
point ‘procedure established by law’. 
The US Courts in different judgements observed inter alia that if it is found that the law framed 
is not reasonable or fair or just, it should be construed the essential requirements of “due 
process” as envisaged should not be considered to have been satisfied (Symonides 2003). US 
Court observed that it should not be thought or taken for granted that ‘due process of law’ does 
not only render procedural safeguards but also it honestly protects an individual both from 
legislature and from executive. But the Indian Constitution is very vivid in this respect. It 
enjoins that there must be a law which is valid as cited in A.K. Gopalan v State of Madras. 
Supreme Court of India has classified Article 21 of the Constitution by issuing valuable 
guidelines and information covering sense of privacy. They are in brief mentioned here.  
Capital punishment does not violate Article 21 of the Constitution of India, even, foreigners 
should be provided with life protection and protection of liberty as laid down in this Article 21. 
Hanging order by court should not be construed to be infringement of this Article 21 which 
also gives protection to sexually harassed women when they are working in their workplaces. 
The delinquents not to be handcuffed if they have no chance to escape. Delay in implementation 
of hanging order by court would convert the punishment to life imprisonment as per Article 21 
of Constitution of India. Denial of medical help by a Government hospital or by a private 
hospital should be construed to be violation of Article 21 of the Constitution of India. For 
upliftment of legal education, Government is to permit and to fund private Law Colleges. 
Persons incapable of repaying any debt for valid reasons should not be imprisoned. Right to 
commit suicide is not covered by Article 21 of the Constitution of India but it is an offence in 
terms of section 309 of Indian Penal Code. For better works, the employees of public and 
private industries should be properly attended by the authority for improvement of their health. 
Downloaded by Tulane University At 09:23 22 January 2019 (PT)
Unauthorized telephone tapping is required to be considered as violation of Article 21 of the 
Constitution of India.
These guidelines given by apex court of India give many subordinate courts to deal with similar 
issues to adjudicate in a proper and calibrated way. Be it mention here, once again though right 
to privacy in the Indian constitution has not given absolute right, but case studies in Indian 
context show that right to privacy is encircled with a specialty compared to other rights in 
Indian constitution.23
6.2 Judicial Developments Connecting Privacy Policy in India
There are many judicial decisions which have dealt with matters related with privacy. The 
decisions covering areas of privacy also have put balance about other rights. Some of the 
examples are given below. 
(a) People’s Union for Civil Liberties (PUCL) v Union of India
It was held by Supreme Court that telephone tapping by Government U/S.5(2) of Telegraph 
Act, 1985 violates Article 21 of the Constitution of India. Court held that right to privacy should 
be considered under Article 21 read with Article 17 of International Covenant on Civil and 
Political Rights, 1966. 
(b) Gobinda v State of M.P.
The Supreme Court of India observed that claims covering Privacy-Dignity are to be 
scrutinized with care and it would be denied when it will be seen that a countervailing interest 
appears to be superior. 
(c) Malak Singh v State of P & H
It was held by the court that in the name of surveillance over habitual offenders, bad characters 
by the Police, the privacy of a citizen should not be infringed U/A 21 & U/A 19(1)(d) of the 
Constitution of India.
(d) Puran Mal v Director of Inspection (Investigation) of Income Tax, New Delhi
It has been held that when evidence has been collected by the help of illegal search, the 
evidence cannot be discarded on the ground of infringement of privacy because it does not 
include any specific fundamental right to privacy. (It is submitted that this example appears to 
have weakened the force of right to privacy since evidence collected through a process of 
search which is not legal).  
(e) State of Punjab v Baldev Singh
In searching a person, provisions enjoined U/S.50 of Criminal Procedure Code is mandatory. 
But the Court did not hold that though the evidence collected through searching not following 
procedure U/S.50 of Criminal Procedure Code would be considered impermissible evidence. 
Downloaded by Tulane University At 09:23 22 January 2019 (PT)
(f) V. S. Kuttan Pillai v Ramkrishnan
The Court held that general warrant connecting searching and then seizing documents would 
not be construed to have violated right to privacy even it is found that the searching did not 
produce effective results because of counter availing interest of State. 
(g) People’s Union for Civil Liberties (PUCL) v Union of India
Through publication of details of criminal activities and assets of a candidate, one cannot say 
that it violates right to his privacy since right of the candidate to divulge his criminal 
antecedents and assets to the electorate is superior compared to the desire of secrecy of the 
candidate contesting in election.  
(h) Sharada v Dharmpal
In a divorce proceeding, medical examination was essential. The Court held that such 
examination is not violative of right to privacy because its necessity was a public policy which 
is superior over the private interest.
(i) District Registrar and Collector v Canara Bank
The Court struck down Sec.73 of Indian Stamp Act 1899 [Amendment by Andhra Pradesh Act 
(Act 17 of 1986)] which did put a bar for searching for documents in private premises and to 
seize those documents without disclosing the reasons for such search and seizure on the ground 
of privacy protection. The Court did it because protection of privacy is not allowable which 
would be part of public records and records of Courts.
(j) R. Rajagopal v State of Tamil Nadu
The petitioner published autobiography of Auto Shankar. State ordered to stop that on the 
ground of privacy violating Article 21 of the Constitution of India. But the Court held that 
petitioners have a right to do so. The Court held that if going beyond that the petitioners did 
publish his life story of that person, the petitioners would invade the privacy and liable for legal 
consequences. The Court stated – “A citizen has a right to safeguard the privacy of his own, 
his family, marriage, procreation, motherhood, child bearing and education among other 
matters. None can publish anything concerning the above matters without his consent – 
whether truthful or otherwise and whether laudatory or critical. If he does so, he would be 
violating the right to privacy of the person concerned and would be liable in an action for 
damages”. 
(k) A v Hospital B
A doctor disclosed an incurable physical ailment of a person to some persons who happen to 
be relative of another person with whom he was to be married. It was held by the Court that it 
does not amount to infringe privacy. In another case P v Hospital Q, the Court held that it is 
not a bar by a healthy person to marry one knowing her fact of illness.
Downloaded by Tulane University At 09:23 22 January 2019 (PT)
7. Privacy policy and the Information Technology Act, 2000
7.1 Background
In this section we will discuss and deal with provisions of Information Technology Act, 2000 
relating to that part which mainly deals with privacy in a broad spectrum with policy 
regulations covering the protection of privacy. This Information Technology Act, 2000 (I.T. 
Act) is presented usually as a Data Protection Act. This Act received assent of on 5th June 2000 
and has been acted upon with effect from 17th October 2000. Its target is to regulate I.T. 
activities in India covering all aspects. Framing of Information Technology Act was 
necessitated to accelerate regulations covering security for e-commerce and for this a legal 
infrastructure was needed to be framed. This may ensure security, safety, privacy in electronic 
transactions, for use of digital signature for authentication of electronic records and to place 
before the world regarding growing power in I.T. It also projects the role of government in 
matters relating to safeguard and to promote I.T. activities in India keeping pace with other 
countries.  
Thus, I.T. Act covers and controls e-commerce and e-governance activities with the help of 
introduction of series of rules concerning with digital signature, electronic-form information, 
computer related crimes and their protections, damages caused in the source of computers, 
legal thoroughfare to prevent breach of confidentiality and protecting unwanted viewing of 
pornography. Some experts highly have spoken in favor of this enactment since it has brought 
a regime in the data protection ambience in e-governance and e-commerce sectors including 
ceaseless use of on-line activities enriching e-commerce culture (Goodhue and Straub 1991). 
7.2 Concept of Personal Data
In I.T. Act, 2000, data has been defined U/S.2(o) of I.T. Act, 2000. But it is very interesting to 
note, this enactment appears to be significantly silent in defining “Personal Data”. Very recital 
of the definition of ‘Data’ would construe that the definition has been knitted focusing attention 
mainly on cybercrimes. However, it is opined by some experts in this line that this I.T. Act has 
taken a sincere attempt to provide safeguard in data protection. In the subsequent studies we 
will try to discuss the pros and cons of the provisions of this I.T. Act, 2000 keeping in mind 
always that this Act has hardly defined what is meant by “personal data” in Section 2 of the 
I.T. Act, 2000. It is pertinent to mention here the exact recital of the definition of ‘Data’ as 
enjoined in Section 2 of I.T. Act, 2000. It envisages “a representation of information, 
knowledge, facts, concepts or instructions which are being prepared or have been prepared in 
a formalized manner, and intended to be processed, is being processed or has been processed 
in a computer system or computer network and may be in any form (including computer 
printouts magnetic or optical storage media, punched cards, punched tapes) or stored in the 
memory of the computer”. It is worth mentioning here that no court in India has yet recognized 
general right to protection of personal data.24Its enforcement became tough though there has 
been self-regulation for the protection of data relating to software industry by NASSCOM.25 
However, how I.T. Act, 2000 has covered it, is discussed below. 
Downloaded by Tulane University At 09:23 22 January 2019 (PT)
7.3 Enactments covering Data Protection in I.T. Act, 2000
Cyber breach and contravention include unauthorized access to computer, computer network, 
computer resources of various nature and computer system; alteration in unauthorized way, 
deletion of data or otherwise, addition, alteration, medication, destruction of data in computer, 
duplication or transmission of date and so on. It jeopardizes interest of concerned person and 
for its protection, the I.T. Act, 2000 has dealt with in Chapters IX and XI and some of the 
salient sections of these chapters are considered as ‘backbone’ covering strict and meaningful 
data protection measures. 
The Sections 43 and 43(A) deals with imposition of penalty for causing damages to the 
computer or to the computer system. These sections practically deal with liability of civil nature 
when data theft takes place, when unauthorized digital copying takes place, when unauthorized 
downloading and extraction of data by unscrupulous person/s take place, when unauthorized 
use of cookies and so on occur. 
Section 66 helps to give protection to computer code sources. Besides, this Section deals with 
matters concerned with hacking and as such, this provision is commonly termed as the data 
protection section during legal mechanism for protection of data from the unscrupulous 
person/s. This provision has given much protection towards confidentiality of data. If anyone 
saves very important, confidential and personal data in computer in his personal e-mail and if 
any other person without having consent accesses in the said document/s, it tantamount that 
this access has breached the provision and the party is held liable under this section. 
Section 72 deals with provision of penalty for causing breach and violation of confidentiality 
as well as privacy. This section envisages 
“Any person who, in pursuance of any of the powers conferred under this Act, rules or 
regulations made thereunder, has secured access to any electronic record, book, register, 
correspondence, information, document or other material without the consent of the person 
concerned discloses such electronic record book, register, correspondence, information, 
document or other material to any other person shall be punishable with imprisonment for a 
term which may extend to two years, or with fine which may extend to one lakh rupees or with 
both”.
It appears that this section only provides a provision covering having consent of the concerned 
person though with limited scope. This section when read between the lines can be said that it 
is also, strictly speaking, has been able to provide adequate level of protection so far as 
protection of “personal data” is concerned. It is interesting to note from the literature of this 
section (Section 72) that it appears to have confined and covered itself relating to acts and 
omissions of those persons having conferred with powers envisaged under this Act. As this 
Section has given powers to some authorities, it can be said that number of controller of data 
is limited. 
It has been noted that provisions of I.T. Act, 2000 could not deliver full proof personal data 
protection policy and as such some of the relevant sections dealing with the data protection 
have been assessed on recommendation of the expert committee constituted in terms of 
notification No.9(16)/2004-EC dated 07.01.2005. It is worth-mentioning here that one of the 
considerations for amendment was to “consider and recommended suitable legislation for data 
Downloaded by Tulane University At 09:23 22 January 2019 (PT)
protection (privacy) in the Information Technology Act, 2000”. On such recommendation in 
this context Section 43, 66 & 72 were amended on the score of “data protection and privacy”. 
Insertions through amendments were made and new amended sub-sections and proviso were 
introduced which are Section 43(2), Explanation (v), Explanation (vi), Section 66(a), (i), (ii), 
(iii), 66(b), (i), (ii), (iii), (iv) & (v) with explanations. Slight glimpse over amendments 
highlights that amendment of section 66 has taken a holistic attempt to criminalize conducts 
with considerably wide amplitude. The amendments imported the idea to synchronize the 
provisions of Indian Penal Code. and Criminal Procedure Code. and it is seen that such 
amendments have been made inhaling sense of provisions of the European Convention 
covering Cyber Crime. 
The sense of privacy is associated with sense of security and security has been dealt within I.T. 
Act, 2000. However, slight scrutiny transpires that so far as security coverage is concerned as 
envisaged in I.T. Act, it is more aligned to focus appropriate attention on issues connected with 
transactions of commercial nature compared to those required for “personal data” protection. 
What is more that provisions of I.T. Act, 2000 is found lacking to ensure that personal data 
would be allowed to be processed only from the proper consent or instruction from the 
controller. 
It is, as such, seen from the above discussions that I.T. Act, 2000 is trying to cover full proof 
security for protection of privacy and framing of such provisions has full endorsement in terms 
of Article 248 of the Constitution of India which has been discussed in the earlier section of 
this article.  
8. Analysis of full bench judgement of apex court in India 
So far as judgement of nine judge referral bench of the Apex Court of India is concerned 
regarding constitutional status of right to privacy, it transpires that the judgement containing 
long 547 pages, delivered on 24 August 2017, has stressed values of dignity and liberty in 
dealing with the constitutionality of right to privacy. Covering spatial, decisional and 
informational peripheries, the bench very effectively mentioned the legal system prevalent in 
USA and in UK and other western countries, so far as issues of privacy are concerned. The 
judgement may be construed to be a vital and meaningful comprehensive document to 
synthesize how there exists needs of protection of privacy in the azure of Indian jurisprudence. 
The judgment also dealt with the issues of privacy corresponding to flow of data in this digital 
era. Jurisprudence of fundamental rights protected in Part III of the Constitution of India has 
been effectively classified in this valued judgement. It will be discussed here how Constitution 
guards the right to privacy in the light of this judgement as it has delivered an emphatic 
endorsement circumscribing constitutional right to privacy.
8.1 Data Privacy Policy of India and Source of Privacy
In the name of data privacy, it is very confusing to note what is meant by privacy protection? 
What do we want to protect? What is protected and what is not protected. Accurate answers to 
these questions are very difficult to extract. In this digital era, data flow is there, and its 
Downloaded by Tulane University At 09:23 22 January 2019 (PT)
management is needed so that they are not misused. The said judgement has also dealt with 
these issues. The protection of data is required to be ensured as it would otherwise infringe the 
privacy which, as opined by the referral bench, amounts to encroach Article 21 of the 
Constitution of India. In the judgement, the apex court dealt with the issue as to where this right 
to privacy can be appropriately placed in the literature of the Constitution of India. In this 
judicial proceeding in the apex court, the Attorney General mentioned that previously two 
benches in two separate cases did not observe that privacy is a fundamental right, that is, in the 
case of M.P. Sharma v Satish Chandra and Kharak Singh v State of UP. However, the full 
bench scanned the decisions of these two cases which helped them to decide the issue of privacy 
as these two cases acted as a source of thinking the issue of privacy in a larger bench as these 
two judgements were issued in smaller bench.  
8.2 Privacy, Dignity and Liberty
Article 21 of the Constitution of India deals with right to life and liberty. In dealing with this 
issue in the full bench, Chondrachud J referred the judgment of Kesovananda Bharati v State 
of Kerala and relying on that it was observed that since preamble is a part of the Constitution, 
dignity is an important feature which provides a calibrated scheme for protection to individuals 
since it was further observed that Article 21 of the Constitution envisages “right to live and 
liberty” impliedly includes “right to live with dignity” and the apex court further registered 
opinion that “privacy is a postulate of human dignity itself”. Moreover, the apex court also 
cited decision of U S Court where it was observed that life means ‘more than mere animal 
existence’, Munn v Illionoss, and basing on this, the apex court opined inter alia “… Liberty 
has a broader meaning of which privacy is a sub set…”. The apex court also cited another 
observation of Supreme Court, ADM, Jabalpur v Shivkant Sukla where it was observed that 
privacy is a natural right. Practically, as opined by Rurth Gavism (1980) that privacy consists 
of complex combination of three isolated features like- security, anonymity and solitude. 
9. Recommendations from privacy policy perspective 
Public and private authorities are required to work in tandem for ensuring protection of privacy 
as well as for safeguards of personal data. Now to execute such data protection mechanism, all 
the stakeholders including Government, policy and law-making authorities, industries and 
regulating bodies and organizations dealing with outsourced data are to work in close liaison. 
There should not be any obstruction which might inhibit the flow of process for ensuring 
privacy protection in all respects. The following recommendations might be of paramount 
importance for the concerned stakeholders. 
9.1 Government Initiatives 
Information Technology Act, 2000 should be made more specific having précised provision 
for punishment to the delinquents for commission of data infringement. In the definition of 
“Body Corporate” as envisaged in Information Technology (Amendment) Act, 2008, there is 
need to include all Governmental agencies responsible for ensuring data protection to bring 
transparency. Ceaseless efforts are supposed to be made by establishing national eco system 
for safeguard of data (Hone and Eloff 2002) with greater investment for framing appropriate 
Downloaded by Tulane University At 09:23 22 January 2019 (PT)
data policy (Hu and Dinev 2005). Arrangements of proper training are being ensured for the 
end users for getting appropriate results to achieve protection of privacy. 
9.2 Duties of the Policy and Law-making authorities
The projects adopted are to be processed sagaciously adhering proper privacy policy including 
enforcement of existing data protection regulations in force for collection, storage and 
processing of data. There is need to develop proper culture among the end users. This is 
instrumental for protection of data in the real sense of the term. The law-making authorities 
must be keeping themselves updated concerning to latest technological advancement connected 
with data flow mechanisms so that any untoward situation may be effectively combated to 
avoid any dire consequence of cybercrimes. 
9.3 Duties of Industries and Regulatory bodies
Practices of industries are to be controlled providing standard processes to ensure protection of 
data with prescriptions for proper remedies to the end-users in case they become victim of data 
privacy infringement. Close relation is to be made with the Governmental authorities to know 
about their privacy protection policies to follow those strictly. Then commission of 
infringement of privacy would not occur. The regulatory bodies are to be always on their vigil 
over the industries to watch if the industrial authorities are following the prevailing concerned 
regulations, strictly and honestly. 
9.4 Outsourcing Issues
The Indian enterprises dealing with imported data of foreign companies should follow 
acceptable privacy policies to bring confidence on the foreign companies who are outsourcing 
the works to their Indian counterparts. The Indian enterprises working with outsourcing issues 
should train their employees properly so that they are sincere in dealing with important and 
sensitive data so outsourced. The authorities of Indian companies should arrange to use 
automated devices for protecting outsourced data so that the foreign companies outsourcing 
their work may have confidence over the Indian companies which would help growth of job 
creation. 
10. Conclusion 
Throughout this article so far, we have discussed how in India privacy of data is protected 
including the issue of non-availability of specific definition of “personal data” in Indian 
jurisprudence causing more reliance on different judicial decisions for interpreting the causes 
when personal data breach alleged to have taken place. We have also discussed how 
fundamental rights envisaged in Part III of the Constitution of India explicitly did not nurture 
the right to privacy. We have discussed how different observations and orders of different 
constitutional benches, under such situation when there is absence of explicit birth of right to 
privacy as fundamental right in the Constitution of India, gave much food for extending 
protection of privacy. Besides, needing the demand of the society on the modern online 
environment; taking support from the observation of the Supreme Court of India in the case of 
Dhillon; the Information Technology Act, 2000 was framed in terms of having residuary power 
envisaged in the Article 248 of the Constitution of India for such enactment in absence of 
Downloaded by Tulane University At 09:23 22 January 2019 (PT)
explicit empowerment available from the recital of entry 97 of list I (Seventh Schedule of the 
Constitution of India), to address the menace of data theft in the cyber platform. Criticisms are 
also there owing to unavailability of definition of “personal data” in the recital of section 2 of 
I.T. Act, 2000 and to keep such data breaches in check, penal provisions have been enjoined 
by amending I.T. Act, 2000 in 2008 [I.T. (Amendment) Act 2008]. All these have been 
discussed in detail. Eventually by the interference of the apex court of the country, referral full 
bench was established to adjudicate if right to privacy may be construed to be fundamental 
right. The referral full bench in its long 547 page judgement explicitly discussed the provisions 
of the Constitution of India along with the privacy policy of India as well as those existed in 
US, UK and other western countries and fell back upon the Article 21 of Constitution of India 
wherein right to life and liberty was guaranteed as fundamental right including lending the 
recital of the preamble of the Constitution of India declared part of the Constitution in 
Kesavananda Bharati case and eventually declared the “right to privacy” as a fundamental right 
without conferring this right to be absolute. This Article also has provided some 
recommendations for adopting proper privacy policy in different sectors. However, in this 
society, where every citizen; so, to say; possessing a mobile, uses internet rendering the society 
as internet society, it is now not possible to go back to the traditional processes of conducting 
commercial activities and judged from this argument, it is inevitable that Big data flow can 
hardly be kept in check. This would, as already stated, enhance the occurrence of data breaches 
endangering the privacy of the concerned citizens and as such, it has become high time to 
formulate robust privacy policy to arrest and to reduce data theft. For this, it would have been 
better had the authorities focused appropriate attention to arrange to amend the Constitution of 
India appropriately by the help of mechanism enjoined in Article 368 of the Constitution of 
India to explicitly include “right to privacy” as an absolute fundamental right in Part III of the 
Constitution of India. It would have curtailed scope of judicial litigants to escape through the 
jargons of complex judicial contradictory words to hoodwink the authorities concerned for 
coming out successful even after commission of data-breach offences. Besides, the clear and 
specific definition of “personal data” is also required to be included in the relevant Acts 
including I.T. Act, 2000 so that none can get unnecessary chance and scope to interpret the 
meaning of “personal data” in their own thinking for having improper gain.  
Notes
1. It is based on: Bart van der Sloot, do data protection rules protect the individual, and should 
they? An assessment of the proposed General Data Protection Regulation, International 
Data Privacy Law, 4 (2014).
2. Frits W. Hondius, Emerging Data Protection in Europe (Amsterdam: North-Holland, 
1975) and Herbert Burkert, Freedom of Information and Data Protection (Bonn: 
Gesellschaft für Mathematik und Datenverarbeitung, 1983). 
3. Advisory Committee of Secretary on Automated Personal Data Systems, Records, 
Computers and the Rights of Citizens (1973).
4. See, Allan F. Westin & Michael A. Baker, Databanks in a Free Society: Computers, 
Record-keeping and Privacy. New York: The New York Times Book, 1972.
Downloaded by Tulane University At 09:23 22 January 2019 (PT)
5. The Individualism Index used to have measured the extent to which a society can 
emphasize individual rights compared to collective goals. (I.T. Act 2000).
6. “Power distance” can be interpreted as the way through which a culture advances and 
accepts inequality, prestige, wealth and power. (See Hofstead Geert, 2001).
7. Hofstede Geert developed many cultural values to assess difference in culture among 
societies identifying five isolated dimensions of human behavior which are: (a) 
individualism/collectivism (b) long-term or short-term orientation (c) power distance (d) 
uncertainty avoidance (e) masculinity/femininity. (See Hofstead Geert. (2001), Cultures’ 
consequences: Comparing values, behaviors, institutions, and organizations across 
nations).
8. Debate took place about this topic. Reference has been made to the reviewers regarding 
this contribution. First Reviewer opined in the European Union, protection of data is 
connected directly to the right to private life. The progressive acceptance of personal data 
protection is an issue of top most relevance. It took place primarily in European Union law 
through a connection to the right in respect for personnel life. Second Reviewer took a 
opposite view. The second reviewer argues that data protection right was gradually 
disconnected from the privacy right. Several statements on data protection also suggest 
that it was seen like something which could hardly be occupied by privacy right.
9. By “Purdah” we mean curtain. It is an established practice of dissuading women from 
being observed by outside men. This assumes two ramifications: (a) Segregation of the 
sexes physically (b) necessity for women to conceal their form. Purdah exists in various 
shapes in Islamic world. Among Hindu women, usually things are not so. (see Basaiv v 
Hasan Raza Khan [1963] AIR Allahabad 340 (India). 
10. Ponnurangam Kumaraguru and Lorrie F. Cranor, Privacy in India: Attitudes and 
Awareness, Workshop on privacy enhancing technologies, Dubrovnik, Croatia.
11.  Regarding cultural policy of privacy, Kumaraguru and Cranor referred to the deficiency 
of a privacy concern among family members involved in family business. There exists a 
specific amount of personal and general information traded between organizations 
involved in trading.
12. The concept of privacy in Thailand appears to be ‘collective’ and not individual. Buddhism 
which is obeyed by people of Thailand hardly recognize human beings possessing the right 
to privacy. The word privacy has separate cultural realization. From a Chinese perspective, 
privacy is hardly seen and construed as an ‘intrinsic good’ but taken as instrumental good. 
The Chinese do not think privacy as necessary but, consider it important. (see Krishna 
Kitiyadisai. 2005. Privacy Rights and protection: Foreign Values in Modern Thai 
Context).
13. Process of handling and collecting data has been controlled by the Government agencies 
through organization of debates, regarding policies and by legislation though there is less 
involvement of private sectors. It has been mostly done in political angle. 
14. Milberg and Westin noted that countries with ‘no privacy regulations,’ were associated 
with lower information privacy concerns, and countries having moderate regulatory 
structures were concerned with higher levels of concern. (see Alan Westin, Privacy and 
freedom, 1967)
15. Different culture is supposed to receive and explain information differently irrespective of 
the universal concepts that all people would share. It is also a fact that one should not 
expect that identical information may not give rise to same realization.
16. A fundamental list of technologies which have impact on privacy are: RFID, location 
identification technologies with GPS, smart cards, technologies associating data mining 
and surveillance including biometrics. 
Downloaded by Tulane University At 09:23 22 January 2019 (PT)
17. Outsourcing industries of India are expected to earn considerable revenues to the tune of 
$ 50 billion within 2018. It is also expected for providing employment covering 2 million 
workers with in 2018. In India the industry involved it outsourcing has developed to the 
tune more than 30% every year for 5 years with effect from 2016. (see Financial Express, 
29 Jan 2008). 
18. The privacy framework of United States consists of special laws including constitutional, 
regulatory, common law and statutory laws. Besides, there are sectoral laws which control 
and govern personal data which are most sensitive. 
19. In India, ironically there is no system that a provision being obsolete would automatically 
be done away with. It would appear in the law books for ever unless they are categorically 
declared as repealed. It appears that based on reports of Law Commission of India, some 
of the legislations have been declared repealed which have become defuncted. It is 
observed also still in the book of laws there is existence of some laws coming from the 
British Regime though they are not in function.
20. The new economic policy of 1991 containing sense of liberalization brought in an 
electrified reconfiguration in the scenario of Indian economy. Emphasis has been given to 
focus much stress on contribution of private sector. Now, intervention of the Government 
was found limited only to that extent which was justified. (see Jagdish N. Bhagwati. 1993. 
India in transition: Freeing the Economy).
21. India still has not adopted a law which clearly governs protection of personal data and 
during late 1990s, the sense of data protection came on the surface culminating thereby 
eventually introduction of I.T. Act, 2000 though it did not categorically interpret the 
meaning of “personal data”. 
22. The judiciary system of India is thoroughly independent. The Supreme Court in India is at 
its apex. It has been a key feature that the Supreme Court possesses in the system of India’s 
democracy highest judicial place right from its existence. This apex court always plays a 
vital role for protection of individual rights enshrined under part III of the Constitution of 
India where basic individual rights have been given protection as fundamental rights. (see 
Constitution of India, Articles 19-35).
23. The argument is supported by two important cases where infringement of privacy occurred 
in India. The first case is concerned with scandal case of DPS MMS where in the CEO of 
Baazee.com and Avnish Bajaj was imprisoned for six days as ordered by Delhi court. The 
case was focused not on the breach of privacy but on the alleged illegal distribution of 
MMS chip. The second case was involved taking video footage of an actor snapped by an 
agency of media compromising privacy in taking that video footage. The actor was 
aggrieved as he thought that his privacy was infringed but ironically, he could not 
substantiate a case in the court of law.  
24. The debate involving data protection as well as data privacy in India considerably grew 
up consequent upon offshore outsourcing by the oversees organizations to the Indian 
counterparts. Had such outsourcing cases not taken place, the India would perhaps never 
have been worried much about protection of data.  
25. India’s software service industry is supervised by NASSCOM for establishing the 
prevailing regulation established Data Security Council of India (DSCI) for ensuring 
maintenance of data privacy and data protection for the outsourcing industries. 
Downloaded by Tulane University At 09:23 22 January 2019 (PT)
References
Alexander Zinser. 2003. “International data transfer out of European Union:The adequate level 
of Data Protection,” Journal of Computer & Information Law 21(4): 547-550. 
Allen, A. 1988. “Uneasy Access: Privacy for women in a Free Society,” Rowman & Littlefield 
Totowa, 30:226.
Baskerville, R. 1993. “Information Systems Security Design Methods: Implications for 
Information System Development,” ACM Computing Surveys, 25(4): 375-414. 
Bellamy, C. 2000. The politics of public information systems. In G. D. Garson. Handbook of 
Public Information Systems. New York: Marcel Dekker. 3: 85-98.
Bok, S. 1983. Secrets: On the ethics of Concealment and Revilation. Panthcon Books, New 
York, NJ. 
Burkert, Herbert. 1983. Freedom of Information and Data Protection. Bonn: Gesellschaft für 
Mathematik und Datenverarbeitung. 
Casaló, L.V., Flavián, C. and Guinaliíu, M. 2007. “The role of security, privacy, usability and 
reputation in the development of online banking,” Online Information Review 31(5): 583-
603.
Chang, E.C., and Ho, C.B. 2006. “Organizational factors to the effectiveness of implementing 
information security management,” Industrial Management and Data Systems 106 (3): 
345-361. 
Costa, Luiz and Yves Poullet. 2012. “Privacy and the regulation of 2012,” Computer Law & 
Security Review, 28(3): 254-262. 
Cranor, Lorrie Faith and Paul Resnick. 2000. “Protocol for automated negotiations with buyer 
anonymity and seller reputation,” Netnomics 2 (1): 1-23.
Culnan, Mary, J., and Pamcle, K. Armstrong. 1999. “Information Privacy Concern, Procedural 
Fairness and Impersonal Trust: An Empirical Investigation,” Organization Science 10(1): 
104-115.
Dammann, Ulrich, Otto Mallmann and Spiros Simitis. 1977. Data Protection Legislation: An 
International Documentation: Engl.–German: eine internationale Dokumentation = Die 
Gesetzgebung zum Datenschutz. Frankfurt am Main: Metzner.
Decrew, J. 1989. “The Scope of Privacy in Law and Ethics,” Law and Philosophy 5(2):145-
173. 
Dieter, F. 1994. “A Security Framework for Information System Outsourcing,” Information 
Management and Computer Security 2(4): 3-8. 
Dlamini, M.T., Eloff, J.H.P. and Eloff, M.M. 2009. “Information Security: the moving target,” 
Computer & Security 28 (3/4): 189-198. 
Dutta, A. and McCrohan, K. 2002. “Management's role in information security in a cyber 
economy, California Management Review,” 45 (1): 67-87.
Eloff, M. M. and Von Solms, S. H. 2000. “Information security management: an approach to 
combine process certification and product evaluation,” Computers & Security 19(8): 
698–709.
Featherman, M.S., Miyazaki, A.D., and Sprott, D.E. 2010. “Reducing online privacy risk to 
facilitate e-service adoption: the influence of perceived ease of use and corporate 
credibility,” The Journal of Services Marketing 24(3): 219-229. 
Foxman, E.R. and Kilcoyne, P. 1993. “Information technology, marketing practice, and 
consumer privacy: ethical issue,” Journal of Public & Marketing 12: 106-119.
Furnell S. 2006. “Malicious or misinformed? Exploring a contributor to the insider threat,” 
Computer Fraud and Security 9(1): 8 –12.
Downloaded by Tulane University At 09:23 22 January 2019 (PT)
Geers, K. 2010. “The challenge of cyberattack difference,” Computer Law and Security 
Review, 26(3): 298-303.
Gellert, Raphael and Serge Gutwirth. 2013. “The legal construction of privacy and data 
protection,” Computer Law & Security Review 29(5): 522-530.
Gonzalez Fuster, Gloria and Raphael Gellert. 2012. “The fundamental right of data protection 
in the European Union: in search of an uncharted right,” International Review of Law, 
Computers & Technology 26(1): 73-82.
Goodhue, D. L. and Straub, D. W. 1991. “Security concerns of system users: A study of 
perceptions of the adequacy of security,” Information & Management, 20(1): 13-27.
Heebe, N.L. and Cark, J.G. 2007. “A model for predicting Haecker Behaviour,” Journal of 
Information System Security 3(3): 3-20. 
Heringa, A. W. and Philipp Kiiver. 2012. Constitutions compared: an introduction to 
comparative constitutional law. Cambridge: Portland.
Hofsted, G. 1997. Culture and Organizations: Software of the mind. USA: McGraw-Hill 
Education, 3rd edition.
Hone, K. and Eloff, J. H. P. 2002. “Information security policy—What do international security 
standards say,” Computers & Security 21(5): 402–409.
Hu, Q., and Dinev, T. 2005. “Is spyware and Internet nuisance or public menace?”, 
Communications of ACM 48(8): 61-67. 
Jackson, Vicki C. and Mark Tushnet. 2014. Comparative constitutional law. St. Paul: 
Foundation Press.
Jelena Budak and Edo Rajh. 2016. “Citizens’ Privacy Concern: Does National Culture 
Matter?”, Joint Conference of SurPRISE, PRISMS and PACT, Australia Academy of 
Science, Vienna 11-12. 
Joseph N. and Abubaker H. 2017. “Exploring the impact of outsourcing on organizational 
performance,” Journal of Global Operations and Strategic Sourcing 10(3): 362-387. 
Kankanhalli, A., Teo, H. K., Tan, B.C.Y., and Wei, K.K. 2003. “An integrated study of 
information system security effectiveness,” International Journal of Information 
Management 23(2):139-154. 
Kolb, N. and Abdullah, F. 2009. “Developing an information security awareness program for 
a non-profit organization,” International Management Review 5(2): 103-107.
Kruger, H.A. and Kearney, W.D. 2006. “A prototype for assessing information security 
awareness,” Computers and Security 25(4): 289-296. 
Lynskey, Orla. 2014. “Deconstructing data protection: the ‘added-value’ of a right to data 
protection in the EU legal order,” International and Comparative Law Quarterly 63(3):569-
597.
Ma Qingxiong, Johnston, A. C. and Pearson, J. M. 2008. “Information security management 
objectives and practices: A parsimonious framework,” Information Management and 
Computer Security 16(3): 251-270.
Martins, A. and Eloff, J.H. P. 2002. Information security culture, IFIP/SEC2002. In: Security 
in the information society. Boston: Kluwer Academic 203–214.
Ohkubo, M., Suzuki, K. and Kinoshita, S. 2005. “RFID privacy issues and technical 
challenges,” Communications of the ACM 48(9): 66-71.
Osborne, K. 1998. “Auditing the IT security function,” Computers & Security 17(1): 34-41.
Peltier, T. 2002. “How to build a comprehensive security awareness program,” Computer 
Security Journal 16(2): 23-32.
Reiman, J. 1976. “Privacy, intimacy and personhood,” Philosophy and Public Affairs 6(1): 26-
44.
Rurth Gavison. 1980. “Privacy and the Limits of Law,” Law Journal 89: 421-437.
Downloaded by Tulane University At 09:23 22 January 2019 (PT)
Sequeira, D. 2003. “Intrusion Protection Systems: Security: Silver Bullet?”, Business 
Communication Review 36-41.
Sloot, Bart van der. 2014. “Do data protection rules protect the individual and should they? An 
assessment of the proposed General Data Protection Regulation,” International Data 
Privacy Law 4(4): 307-325.
Solms, B. Von and Solms, R. Von. 2004. “The ten deadly sins of information security 
management,” Computers & Security, 23(5): 371–376.
Symonides, Janusz. 2003. Human rights: international protection, monitoring, enforcement 
Paris: UNESCO Publication. 
Thomson, K.L., Von Solms, R., and Lauw, L. 2006. “Cultivating an organizational information 
security culture,” Computer Fraud and Security 6(10): 7-11. 
Tripathi, B., & Mishra, J. 2000. “Protective Measures in E-commerce to deal with security 
threats arising out of social issues – A framework,” International Journal of Computer 
Engineering & Technology, 4(1): 46-53. 
Tryfonas, T., Kiountouzis, E., and Poulymenakou, A. 2001. “Embedding security practices in 
contemporary information systems development approaches,” Information Management 
and Computer Security 9(4):183-197. 
Westin, A. 2003. “Social and Political dimension of privacy,” Journal of Social Issues 59(2): 
431-453. 
Cases Cited
A K Gopalan v State of Madras [1950] SCR 88.
A v Hospital B [1999] AIR SC 495.
A.D.M, Jabalpur v Shivkant Sukla [1976] SCR 172.
Attorney General (for India) v Amrat Lal Prajivandas [1994] AIR SC 2179. 
Berubari Union Case and Others v Unknown [1960] AIR SC 845.
District Registrar and Collector v Canara Bank [2005] 1 SCC 496.
Gobinda v State of Madhya Pradesh [1975] 2 SCC 148.
Golak Nath v State of Punjab [1967] AIR SC 1643.
Indira Gandhi v Rajnarayan [1975] AIR SC 2299.
Kesavananda Bharati v State of Kerala [1973] AIR SC 1461.
Kharak Singh v State of UP [1963] AIR SC 1295.
K.S. Puttaswamy and others v Union of India and others [2012] SC WP (Civil) No. 494. 
M. P. Sharma v Satish Chandra, District Magistrate, Delhi [1954] AIR SC 300.
Malak Singh v State of Punjab and Haryana [1981] AIR SC 760.
Maneka Gandhi v Union of India [1978] AIR SC 597.
Munn v Illionoss [1877] 94 US 113.
P v Hospital Q [2003] AIR SC 664.
P. Sharma v Satish Chandra [1954] AIR SC 30.
People’s Union for Civil Liberties (PUCL) v Union of India [2003] AIR SC 2363.
People’s Union for Civil Liberties PUCI v Union of India [1997] 1 SCC 301.
Puran Mal v Director of Inspector (Investigation) of Income Tax, Delhi [1974] AIR SC 348.
R. Rajagopal v State of Tamil Nadu [1995] AIR SC 254.
Sharada v Dharmpal [2003] AIR SC 3450.
Downloaded by Tulane University At 09:23 22 January 2019 (PT)
State of Punjab v Baldev Singh [1999] AIR SC 2378.
The Union of India v H.S. Dhillon [1972] AIR SC 1061.
V.S. Kuttan Pellai v Ramkrishnan [1980] AIR SC 185.
About the author
Sheshadri Chatterjee is a PhD Research scholar at Indian Institute of Technology Delhi, India 
and acted as a management consultant practitioner in several Multinational Companies. 
Academically he is from engineering, management and legal background. He has completed 
Master of Technology from Jadavpur University, Kolkata, and Master of Business 
Administration from Indian Institute of Technology, Delhi and master’s in law from National 
Law University Delhi. Sheshadri is also a certified project management professional, PMP 
from Project Management Institute (PMI), USA, Certified Professional of Cyber Laws from 
Indian Law Institute Delhi, and have completed PRINCE2, OGC, UK and ITIL v3 UK.
Downloaded by Tulane University At 09:23 22 January 2019 (PT)
"
DataPrivacy,3013.txt,"https://doi.org/10.1177/1367877919842575International Journal of Cultural Studies 
 1 –19
© The Author(s) 2019
Article reuse guidelines:  
sagepub.com/journals-permissions
DOI: 10.1177/1367877919842575
journals.sagepub.com/home/ics
The digital carceral: Media 
infrastructure, digital cultures and state surveillance in post-Arab Spring Morocco
Annemarie Iddins
Fairfield University, USA
Abstract
This paper analyzes Moroccan discourses around media infrastructures and their intersection 
with carceral culture, taking up Mamfakinch’s responses to state-sponsored spyware attacks and judicial harassment as symbolic of shifting imaginaries of the digital. This work is situated within 
a growing subset of the media and communication literature on media infrastructures, which 
works to connect the materiality of media systems with everyday media cultures, practices and power. Mamfakinch’s experience with spyware and subsequent evolution into a digital rights organization are indicative of attempts to transfer a lingering carceral culture into digital spaces 
and a shift in state and activist internet imaginaries. In a global era and as part of a hypersurveillant 
state, Mamfakinch demonstrates how the digital becomes an increasingly important site for the surveillance and policing of dissent while presenting new modes of publicness and activism that directly challenge those endeavors.
Keywords
activism, carceral culture, digital culture, global media, media infrastructure, Morocco, surveillance
In July 2012 a spyware attack targeted the activist blogging collective Mamfakinch, 
which had emerged out of the Arab Spring moment in Morocco as a key site for circulat-ing information about protests and larger social change in Morocco. The site’s editorial team received an email suggesting it would reveal a major scandal, but which turned out to be bait for the installation of spyware on the collective’s personal devices. Earlier the site had been subject to DDoS (distributed denial of service) attacks that made it difficult 
Corresponding author:
Annemarie Iddins, PhD, Fairfield University, 1073 North Benson Road, Fairfield, CT 06824, USA. Email: aiddins@fairfield.edu842575 ICS0010.1177/1367877919842575International Journal of Cultural StudiesIddins
research-article 2019
Special Issue: Arab Media
2 International Journal of Cultural Studies 00(0)
to access, but the spyware signified a new level of surveillance and significant invest-
ment in monitoring activists.
Mamfakinch, meaning ‘no concessions’ in Moroccan Arabic, emerged out of the 
February 20th movement in Morocco and its preparations for initial protests on that date. In the midst of a media blackout and misinformation about the nascent movement, Mamfakinch became a site to aggregate and share information about Arab Spring protests in Morocco. Operated by a collective of activists, bloggers and journalists in Morocco and the diaspora, the site operated on the logic of continuous circulation of content, often acting as an intermediary among the February 20th movement, Moroccan publics, local media, global media, transnational activist networks and international observers. Translating con-tent across Arabic, French and English Mamfakinch brought together diverse publics around discussions about political and social change in Morocco during the Arab Spring and its aftermath. In some ways Mamfakinch came to act as a sort of networked activist infrastructure, or a set of people, practices and platforms that could be mobilized in moments of contention For a more detailed analysis of Mafakinch, see Iddins, 2018).
Mamfakinch sent the spyware from the 2012 attack to Citizen Lab
1 at the University 
of Toronto, which identified it as ‘a variant of a commercial backdoor sold by the Italian company “Hacking Team”’ in an October 2012 report (Marquis-Boire, 2012). Hacking Team claims only to sell its technologies to ‘worldwide law enforcement and intelligence communities’, billing itself as ‘The Hacking Suite for Governmental Interception’ (Hacking Team, 2016). Hacking Team’s spyware is both technologically sophisticated and expensive, meaning that cost would make it prohibitive to most individuals.
2 Hacking 
Team’s Remote Control System software allowed for remote access to and tracking of infected devices, logging emails, passwords, social media accounts, webcams and more (Gallagher, 2012). As part of a global information economy, surveillance technologies have become a multi-billion dollar a year commercial industry,
3 raising concerns about 
larger historical trends of solidarity among carceral states – including Western democra-cies – and their industry partners in building repressive logics into everyday media infrastructures.
Media infrastructure and the carceral
This paper analyzes Moroccan discourses around media infrastructures and their inter -
section with carceral culture, taking up Mamfakinch’s responses to state-sponsored spy-ware attacks and judicial harassment as symbolic of shifting imaginaries of the digital. This work is situated within a growing subset of media and communication literature on media infrastructures, which works to connect the materiality of media systems with everyday media cultures, practices and power (Aouragh and Chakravartty, 2016; Graham and Marvin, 2001; Larkin, 2008; Parks and Starosielski, 2015; Plantin et al., 2016; Punathambekar and Mohan, 2018; Sandvig, 2013). Using the technological affordances of the digital, Mamfakinch was a response to a media system restrained by red lines, economic imperatives and an insufficient online presence. Mamfakinch’s experience with spyware and subsequent evolution into a digital rights organization are indicative of attempts to transfer a lingering carceral culture into digital spaces and recognition of the limitations of utopian internet imaginaries. Mamfakinch demonstrates how the digital, in Iddins 3
a global era and as part of a hypersurveillant state, becomes an increasingly important 
site for the surveillance and policing of dissent while presenting new modes of public-ness and activism that challenge those endeavors.
Through Mamfakinch this article connects longer histories of carceral culture with 
media infrastructure and digital cultures in Morocco. Fundamentally it argues that Mamfakinch’s transformation into a digital rights association represents a shift in imagi-nations tied to the transfer of carceral culture and logics into the digital realm, and the ways in which the democratic promises and potential of media infrastructures declined as a result of greater awareness of its popular political potential. Mamfakinch constitutes a transitional moment in the Moroccan media landscape in which divergent imaginaries of the digital by the state and civil society came into sharp relief. Similar to many Global South states, the Moroccan state imagined telecommunications and the digital as the key to economic growth and prosperity as part of the global knowledge economy while approaching public use as a steam valve rather than a credible threat to the status quo. Meanwhile, the digital became a refuge for independent journalists at a time immediately preceding the Arab Spring when independent media in Morocco were largely in recoil as a result of judicial harassment and economic censorship. For these journalists, and for civil society more broadly, imaginaries of the internet as a radically democratic and unregulated space beyond the gatekeeping power of the state were central at a time when media infrastructures were emergent and access rapidly expanding.
The prevalence of utopian narratives of technological determinism during and in the 
immediate aftermath of Arab Spring uprisings necessitates reconsidering through a lens of continuity rather than rupture. If many scholars have focused on the exceptionalism or particular dynamics of the Arab Spring moment, fewer have attempted to historicize that moment. In looking at the Arab Spring and its aftermath through the lens of continuity rather than narratives of democratization and change, a very different picture emerges of this moment’s enduring legacy. This is not to negate the important interventions and significant changes produced by Arab Spring protesters and popular democracy move-ments, but rather to highlight the moment’s significance in setting in place repressive approaches to digital culture that include a backlash aimed at reining in online political culture which has received relatively little scholarly attention. Many key dynamics emerging from that moment remain unexplored, including investments in surveillance technologies and press code reforms that attempt to transfer restrictions on legacy media into the digital realm. With this in mind, I work to historicize and connect carceral and digital cultures in Morocco, placing them in relation to broader media infrastructure, ask-ing how imaginations of media infrastructures have shifted over time and what role broader histories of carceral culture play in shaping current conceptions of digital rights? Methodologically, this work is based in a discourse analysis of digital development that took on two dimensions: state discourse and civil society discourse. For the state, I looked at public discourse from authorities, development plans, loan applications and regulatory reports in order to construct a genealogy of the digital in its early days. For civil society, I analyzed discourse from local and global non-governmental organizations (NGOs), activists and media around the same genealogy and its potential and pitfalls for their purposes, including an interview with Mamfakinch co-founder and spokesman Hisham Almiraat.4 International Journal of Cultural Studies 00(0)
The changes described above are particularly significant in semi-authoritarian socie-
ties, where government accountability is reduced by lack of real competition for power 
(Ottaway, 2003). In media studies, much attention is given to extreme authoritarianism in media–state relations – cases like China’s ‘great firewall’ or Egypt’s internet shutdown in 2011. In comparison to dynamics elsewhere in the region, Morocco was applauded for taking a hands-off approach to protesters and online spaces after violence in Tunisia and Egypt captured global attention. This article attempts to make sense of the subtle, insidious nature of semi-authoritarianism online constituted by strategic forms of repres-sion and their institutionalization. How do we explain a moment when a monarchy is applauded for ostensibly prohibiting police from physically harming protesters and allowing critical speech, yet carries on reinscribing its power above the political system, investing in increased surveillance of dissidents and constructing bureaucratic barriers to their on- and offline operations through more mundane tools of governance like regula-tion and the courts? This frequently pairs with strategic repression out of the public eye, when global attention has waned, and in geographically and economically marginalized areas. At the same time, collaborative projects like Mamfakinch are indicative of the extent to which peer production of knowledge and information is expanding the array of relevant actors, realms of collaboration and opportunities to learn from and work with other initiatives – all associated with more democratic understandings of culture and a plane of politics that extends beyond the nation. These contradictions come into sharper focus in the longer term aftermath of the Arab Spring.
Surveillance
The Oxford English Dictionary defines surveillance as the ‘close observation, especially 
of a suspected spy or criminal’, From the French sur (over) and veiller (to watch), the 
term implies top-down watching (sur) from a position of power and carries a negative connotation generally reserved for criminals, spies and terrorists. And yet surveillance has been used to describe increasingly banal forms of data collection and watching that characterize everyday life activities in a hypermediated age. In doing so it carries into the realm of everyday life connotations of assumed suspicion for any who question the implications for privacy rights or civil liberties, with the hegemonic refrain: ‘What do you have to hide?’ From a Foucauldian perspective, surveillance is a means of enacting power, instrumental in disciplining and domination (Foucault, 1977). Scholars of the expansive (and expanding) scope of surveillance in the digital era follow similar defini-tions based around the collection of information for purposes of order, classification and control (Fuchs, 2011; Lyon, 2015; etc.).
The control inherent in surveillance directly correlates with the carceral nature of a 
society. In his work on the origins of modern incarceration, Foucault (1977: 299) invokes ‘the “carceral” with its many diffuse or compact forms, its institutions of supervision or constraint, of discreet surveillance and insistent coercion’. In other words, power is dif-fuse, operating on a continuum of external and internalized discipline and constraints. It is difficult to read this description of the carceral or Foucault’s analysis of Bentham’s panopticon without evoking similarities to contemporary forms of digital surveillance and social control. For my purposes, what Foucault fails to do is locate the power through Iddins 5
which the carceral operates, particularly in relation to institutions. To the question ‘who 
is doing what and how does it come about?’ Foucault provides few answers. In this case, media infrastructure provides a means to locate power by connecting materiality with digital cultures and imaginaries.
I take an expansive view of infrastructure that follows Larkin’s (2008: 6) definition as 
the ‘totality of both technical and cultural systems that create institutionalized structures whereby goods of all sorts circulate’. Including both material systems and the platforms and practices that configure their use (often referred to as ‘soft’ infrastructure) allows for greater elucidation of the way the material and cultural components are layered onto media objects and our imaginaries about them in ways that are often difficult to deline-ate. While defining media infrastructure as ‘situated sociotechnical systems’, Parks and Starosielski (2015: 4–5) point to the ways media infrastructures are both materially and discursively constructed. It is impossible to separate imaginations about who and what connectivity is for from the way those infrastructures are constructed and implemented. Clearly in capital-intensive infrastructure construction the state is a locus of power; how-ever Mamfakinch also makes clear that users – and activists in particular – layer plat-forms and practices onto systems of wires and cables that allow them to work within broader political economic structures to reimagine who and what media infrastructures are for. Generally, infrastructure operating properly is defined by its invisibility (Parks and Starosielski, 2015: 6). But there is another level of visibility linked to media infra-structures, and that is the ability to make visible the everyday lives of people as they interact with this increasingly essential mediated world, dependent on devices, wires, towers and signals. The transformation of bodies into mobile sites of data and metadata production by media infrastructure represent an intensification of the body as a site for the production of data, building on colonial logics of quantification and categorization as described in Anderson’s (1983) work on the census. Recent work on surveillance capital-ism (Zuboff, 2019) indicates the extent to which surveillance is becoming the bedrock of information economies, implicating the neoliberal state in the process.
Approaching mediated activism and surveillance from the perspective of media infra-
structure constitutes a refusal to fetishize bodily emancipation and an insistence on mak-ing visible the logics, practices, materialities and bodies of carceral states, of situating them in local, historical context rather than obscuring the institutions, platforms and practices that make surveillance of activist bodies banal. Media infrastructures offer affordances for activist anonymity, solidarity and action at a distance that shouldn’t be discounted, but neither should we discount their affordances for solidarity among car -
ceral states as part of a neoliberal world order. Fundamentally, activists are increasingly visible while the conditions of watching are increasingly obscured, both by distance and increasingly complex, highly specialized technological systems. The uncertainty inscribed in this dynamic – in terms of who is watching and when and what type of reper -
cussions result – can ultimately be paralyzing for a networked public sphere.
Moroccan carceral culture
Modern Morocco has a long history of carceral culture, particularly related to politi-cal dissidents. Much of the 38-year reign of Hassan II became known as the Years of 6 International Journal of Cultural Studies 00(0)
Lead, which is generally characterized as beginning with the state of exception in 
1965 when the king assumed all legislative and executive powers, and ending with a wave of amnesties in the early 1990s. During this time, police surveillance, sudden disappearances and secret prisons gave rise to widespread fear and paranoia as part of Moroccan public culture, a genre of Moroccan literature referred to as la littéra-ture carcérale (carceral literature) and Morocco’s abysmal human rights record among the international community. Several sites came to occupy an infamous place in carceral literature and public memory: Dar el-Mokri, the Rabat residence used to torture dissidents; Derb Moulay Cherif, known as Casablanca’s ‘house of torture’; Kenitra Central Prison, where many political prisoners were held long-term; and finally, Tazmamart, the secret military prison on the edge of the Sahara where some of the regime’s most threatening challengers were ‘disappeared’. Communications about this period largely emerged only after Mohammed VI succeeded his father in 1999, with his transition to power coming at a time of economic liberalization and cultural globalization that led the state to take a (more) hands-off approach to media and new communication technologies. The late 1990s and early 2000s saw a wealth of testimonies from former prisoners and accounts written in prison finally being published and available to the public in Morocco.
4 The lingering importance of this 
era is evident in the reworking of these memories in post-Arab Spring popular cul-ture through such films as Hicham Lasri’s C’est eux les chiens (2013), Abdelkader 
Lagtaa’s La Moitié du ciel (2015) and Mohamed Mouftakir’s L’Orchestre des aveu-
gles (2015).
An emergent civil society inscribed human rights in Moroccan public discourse as a 
central issue for the left. This had strong implications for a group of independent news magazines that emerged during the Alternance era
5 (Le Journal, Demain, Telquel) as 
platforms for and participants in this public discourse. Slyomovics (2005) argues that Morocco’s human rights movement, largely built on the testimonials of former prisoners, was both a result of and instrumental in making the unspeakable realities of torture speakable and part of Moroccan discourse. The human rights movement used media as its mechanism for publicizing torture, disappearances and imprisonment. According to Waltz (1995: 224): ‘Most Moroccans who learned of the release of Tazmamart prisoners in 1991, for example, received the news via Paris contacts and the French press’, with the Moroccan press often refusing to carry communiqués from human rights groups. Under Mohammed VI, the new independent news magazines became the province of a new, more liberal generation of the secular left that closely covered activities of Moroccan human rights groups. In 2001, Le Journal, in collaboration with Le Monde, published a two-part series based on revelations of former secret agent Ahmed Boukhari about the 1965 disappearance/murder of leftist opposition leader Mehdi Ben Barka in Paris, includ-ing allegations of French and American complicity (Smith et al., 2001a, 2001b).
6 
Independent media adopted a try-and-see ethos, testing the boundaries of speech under Mohammed VI, entering into a dynamic of media–state maneuvering dependent on com-plementary or contradictory aims at any given moment. Progressively it became clear that Mohammed VI’s policies served as a continuum of similarly repressive governance through different mechanisms. After Casablanca’s 16 May 2003 suicide bombings,
7 anti-
terrorism legislation became one such mechanism and economic censorship another, Iddins 7
leading journalists and civil society networks to take advantage of relatively unrestricted 
online spaces associated with emergent media infrastructure.
Digital infrastructures and imaginaries
In the 1980s and 1990s, telecommunications were at the vanguard of liberalization in Morocco and indicative of a broader trajectory of neoliberal market reforms. The Moroccan state was an early advocate of the strategic importance of telecommunica-tions, and its turn-of-the-century regulatory framework and liberalization policies were lauded as an example for the Global South, as well as a reason Morocco managed to rank with countries with far more resources in terms of telecom penetration and usage.
8 The 
major regulatory shift associated with law 24-96 and its aims to liberalize and privatize the Moroccan telecommunications sector in 1998 was preceded by several state initia-tives to modernize and expand telecommunications, beginning in 1987 with a US$125 mil-
lion loan from the World Bank (World Bank, 1995: 5) and followed by a 1993–7 project receiving UA743.56 million
9 from various donors, including the African Development 
Bank (ADB, 1999). After law 24-96, a five-year plan was established promulgating ICT (information and communication technology) development as national priority (Morocco, 2007). In his 1997 Throne Speech, King Hassan II discussed telecommunications pri-vatization being imperative because the sector’s investment needs went beyond the means of the state, saying: ‘In the field of communications, a reform of the institutional framework governing this sector is inevitable due to demands from the increased liber -
alization of the world economy on the one hand and rapid technological developments on the other’ (El Kabsi, 2003: 62). For the Moroccan state, telecommunications was seen as a profitable industry in and of itself that the country was uniquely positioned to benefit from, but also a tool for economic development and access to other industries associated with the global knowledge economy.
In 2001, telecommunications accounted for 4% of Morocco’s gross domestic product 
and between 1998 and 2001 the sector accounted for 66% of foreign investment in the country (ANRT, 2001). The government adopted several successive strategies to capital-ize on this growth. A national strategy under the name ‘E- Morocco’ was adopted in 2001, focusing on closing the digital divide and positioning Morocco as a key ICT player globally. According to a 2007 plan update, Morocco’s emphasis on ICTs is based on the industry’s ‘crucial role in the effort undertaken to elaborate and materialize a specific model of sustainable social and economic development for Morocco’ and cites Mohammed VI as saying: ‘This is because we strive to ensure for our great people a global and integrated development, allowing Morocco to occupy the position that it deserves in a world transformed by the digital revolution underway’(Morocco, 2007: 9). Subsequent plan updates, such as Digital Morocco 2013, shift to an external focus, with increased emphasis on offshoring, export of information technology services and estab-lishment of Morocco as a regional telecommunications hub.
At the same time, logics of surveillance and control were being built into emergent 
media infrastructures. Despite privatization, the state has maintained a key 30% stake in dominant telecommunications provider Maroc Telecom, which owns the internet back-bone, requiring other providers to lease access through Maroc Telecom or other public 8 International Journal of Cultural Studies 00(0)
entities controlled by the state. Maroc Telecom’s investments in two new fiber optic 
submarine cables since 2007 creates a similar centralizing effect for international con-nectivity, increasing filtering and surveillance capabilities.
10 At a time when people’s 
communities and concerns are less likely to be local, they are often reliant on media infrastructure to communicate at a distance and should be able to expect privacy and access to anonymizers. As early as 1999, Human Rights Watch (HRW) acknowledged privacy online to be fundamental to extending freedom of expression into a global era. ‘By guaranteeing privacy of communications and authenticating the identity of commu-nicators, encryption also enables free association between individuals in cyberspace, an important extension of a traditional right in the new circumstances of globalization’ (HRW, 1999: 10). The resources and power of governments and corporations often mean that they are able to use these infrastructures to advance their own interests in ways that do not necessarily align with the rights of individual users and are increasingly invisible to those users, as demonstrated by Edward Snowden’s 2013 revelations about American citizens being swept up in the National Security Agency’s (NSA’s) expansive surveil-lance net. A 2003 Privacy International and GreenNet Educational Trust report warns about the widespread use of anti-terror legislation to crack down on civil rights in a post-9/11 world, pointing out:
Governments of developing nations rely on Western countries to supply them with the necessary 
technologies of surveillance and control, such as digital wiretapping equipment, deciphering equipment, scanners, bugs, tracking equipment and computer intercept systems. The transfer of 
surveillance technology from first to third world is now a lucrative sideline for the arms 
industry. Without the aid of this technology transfer, it is unlikely that non-democratic regimes could impose the current levels of control over Internet activity. (Privacy International and GreenNet Educational Trust, 2003: 7)
This includes not just technology transfer, but practices and policies that allow states and 
corporations high degrees of discretion in the application of these capabilities. As the internet’s affordances for openness and connectivity challenged the gatekeeping power of the state, Morocco has imposed few restrictions compared to regional neighbors and yet it has invested heavily in surveillance systems and made high-profile examples out of prosecuting uses of media infrastructure that challenge official discourse. For example, in 2011 French newspaper Le Canard Enchaîné (Canard, 2011) and cyber-surveillance monitoring site Reflets published invoices showing that Morocco had invested more than US$2 million in computers and hard drives associated with a surveillance system named Eagle sold by the company Bull through its subsidiary Amesys (Reflets, 2011). Eagle allows for Deep Packet Inspection (DPI), which allows for the detailed inspection and filtering of data packets as they pass through cables and switching stations.
11 Further, it 
is telling that in a 2009–10 case, bloggers who publicized student protests in the south of the country were prosecuted, but it was the owner of the cyber cafe they posted from that received the harshest sentence.
12 Such a prosecution indicates both active surveillance of 
protest movements and that providing access to infrastructure for purposes that contra-dict state discourse and imaginaries is punishable, effectively distributing policing responsibilities throughout that infrastructure.Iddins 9
After the Arab Spring: from Mamfakinch to ADN
Mamfakinch ceased publication in February 2014, and one of the reasons cited for its 
discontinuation was a backlash from authorities that threatened the anonymity on which many of the site’s members depended. According to Almiraat, the Hacking Team spy-ware attack discouraged members until the decrease in the number of participants made the project simply unsustainable (Almiraat, personal communication, 8 May 2015). Several members of Mamfakinch subsequently organized the Digital Rights Association (Association des Droits Numériques – ADN), an NGO dedicated to establishing and protecting rights such as freedom of expression online, access to digital technologies, protection of personal data and participative governance of the internet, among others. However, ADN was not allowed to register as an NGO, meaning the association lacked government approval and was de facto illegal. In its Facebook profile, the organization outlined its mission to:
raise awareness, promote and undertake research on digital rights; Defend human rights in the 
digital space; Advocate, mobilize and foster the convergence of stakeholder initiatives – government, private sector, civil society, the media; Constitute a force of proposal in terms of 
Internet governance. (ADN, 2014a)
In September 2014 ADN released a communiqué on Facebook decrying intimidation 
from authorities that caused a Rabat hotel to decline to host a joint workshop of the Association Marocaine des Droits de l’Homme (AMDH – Moroccan Association for Human Rights) and Privacy International that was to end with a press conference announcing ADN’s formation (ADN, 2014b). The workshop, ‘Freedom of expression and human rights in the digital age’, took place at AMDH headquarters, but ADN’s com-muniqué issues a warning that the organization has sought legal counsel and reminds authorities of existing legal protections for freedom of assembly.
ADN hosted its first ‘Rencontres Raqmiya’ (Digital Meetings) on 13–14 December 
2014, in Rabat, under the theme ‘Internet: Between private life and freedom of expres-sion’. As with the earlier workshop, authorities blocked the scheduled venue, but the conference took place elsewhere, with many former Mamfakinch bloggers attending. Several months later, in February 2015, an ADN communiqué indicated that police raided their headquarters, to which the association responded by denouncing:
this new violation of civil liberties, a new episode in a black series of systematic attacks carried 
out by the State since the summer of 2014 against human rights associations and for which our association has also paid the price. (ADN, 2015)
Amid this generalized campaign of dissuasion, ADN continued its campaigns to educate 
and mobilize publics in opposition to reforms of the penal and press codes that increased legal provisions aimed at cracking down on digital dissent in a post-Arab Spring environment.
ADN collaborated with activists at Privacy International to produce a report titled 
Their Eyes on Me: Stories of Surveillance in Morocco, which aims to give names and 10 International Journal of Cultural Studies 00(0)
faces to the often invisible targets of cyber-surveillance (Privacy International, 2015). 
Four Moroccans, including three former Mamfakinch contributors, participated in the report: Hisham Almiraat, Samia Errazzouki, Yassir Kazar and independent journalist Ali Anouzla. The report highlights increasing threats to privacy, freedom of association and freedom of speech online in the aftermath of the Arab Spring, especially for activists associated with the February 20th movement, highlighting threats posed by government investments in surveillance technologies and nationalist hacking groups. The report makes clear that Moroccan authorities combined cyber-surveillance with more tradi-tional means of intimidation, such as phone tapping and police questioning. ‘State agents’ visited the family of one Mamfakinch blogger located abroad multiple times, an act she described as ‘not [about having] answers or information. It is just to intimidate me so I can get the message. It is just about “We want you to know that we’re watching you, not just online but in person”’ (Privacy International, 2015: 25). These accounts are indica-tive of the ways in which media infrastructure has offered new means of intimidation and enabled transfer of restrictions on legacy media into the digital realm.
While the internet had been heralded as a new terrain for liberty in Morocco, govern-
ment use of surveillance technologies and post-Arab Spring crackdowns on dissidents meant that for Almiraat ‘they have turned the internet into something dangerous for those who want to take part in the public debate but have something to lose’ (Privacy International, 2015: 20). This chilling effect was especially potent for those who were not well-versed in digital technologies to begin with. According to Mamfakinch con-tributor Yassir Kazar, people react differently to the trauma of having their privacy invaded. ‘Some people manage to transcend it and to turn into a positive experience and others remain traumatised, especially if technology was not their cup of tea in the first place’ (Privacy International, 2015: 30). For Kazar, the experience was an affirming one in that the government felt sufficiently threatened by Mamfakinch to put significant effort into its surveillance and intimidation.
I spoke with Almiraat a few days after the press conference meant to be the official 
release of the Privacy International report in Morocco. ‘Very early in the morning there were four police vans blocking access to the venue’, he said, ‘and ADN is not the only one [experiencing this]’ (Almiraat, personal communication, 8 May 2015). For Almiraat, there is a general feeling that civil society has become the environment where ideas and like-minded people can come together to build alternative views of the future, a situation that makes it threatening to the status quo. And the authorities don’t have to do much to instill fear in people. ‘We still have memories of 70s/80s/early 90s’, said Almiraat ‘so it’s not a big leap’ (Almiraat, personal communication, 8 May 2015). In many ways, ADN is a response to a paradigm shift coming out of the Arab Spring in regard to official atti-tudes toward the internet, but also activist attitudes. The democratic utopian vision of the internet as a tool capable of confronting authoritarianism gave way to an emphasis on building institutional frameworks to support more traditional forms of political struggle, as seen in the heavy involvement of AMDH and more traditional civil society organiza-tions in ADN efforts. According to Almiraat ‘There was this realization that the internet is not enough’ (Almiraat, personal communication, 8 May 2015).
ADN substitutes depth for the breadth of a collective like Mamfakinch that was more 
oriented toward public engagement than institution building. But getting publics to see Iddins 11
the pressing nature of privacy policies and regulatory frameworks is a more difficult task 
than harnessing the energy of a popular movement. The Privacy International report intended to put a human face on that struggle, and was meant to be the first in a series of three reports produced in collaboration with ADN. ‘It’s hard to make people realize that privacy doesn’t only concern people who have something to hide,’ said Almiraat, ‘People don’t see the direct link between privacy and freedom of expression.’ Almiraat says ADN targets law students and young technologists who are familiar with things like WikiLeaks and the NSA. ‘In that way we’re elitist or selective and even then it’s a hard sell’, he said, ‘but journalists seem to understand that privacy is fundamentally linked to freedom of access and freedom of expression.’ While Mamfakinch opened up conversations that had been happening in private to broader publics, ADN brings targeted conversations to spe-cific publics. In many ways these publics are the ones that were brought to the fore by the Arab Spring moment. At a time when few Moroccan news organizations had a signifi-cant online presence and electronic journalism was in its infancy, Mamfakinch found its niche in curating online information, mobilizing around human rights and privileging citizen voices, especially those associated with civil society and the independent press. For Almiraat, Mamfakinch’s legacy as part of that moment is wrapped up in its combina-tion of on- and offline efforts, and of using technology to pursue public conversations about topics and issues that are off limits in mainstream media.
It was the epitome of what the internet can do best, that is bring together like-minded people to 
work together regardless of distance or geography. In 3–6 months, we had something like a million unique visitors, which shows that people want to talk about sex, religion, the monarchy. 
Mamfakinch showed that the internet can achieve some sort of change and be a motor in some 
circles, provided that people also organize and do things offline. (Almiraat, personal 
communication, 8 May 2015)
Judicial harassment
On 9 May 2015, Morocco’s official state news agency, MAP, published a short news bul-
letin under the title ‘Accused of digital espionage, the Interior files complaint’ (MAP, 2015). Barely two paragraphs, the bulletin states that the Interior Minister filed a lawsuit with the public prosecutor’s office against ‘certain persons who have prepared and dis-tributed a report containing serious accusations’ of espionage by its services, citing an unnamed source from the Ministry of the Interior (MAP, 2015). MAP gives no names, but the news bulletin appeared three days after Privacy International and ADN presented their cyber-surveillance report. Almiraat apparently learned about the lawsuit indirectly as he told Reuters: ‘We suppose they are talking about us’ (El Yaakoubi, 2015). A state-ment from Privacy International confirms: ‘Since the MAP dispatch, the Ministry of Interior has not disclosed further information. Neither Privacy International nor our local Moroccan partner – the Association des Droits Numériques (ADN) – has been contacted by the Moroccan government’ (Privacy International, 2015). While the report didn’t reveal anything that hadn’t been circulating on the internet for years in regard to the Moroccan government’s purchases of spyware or intimidation of journalists and activ-ists, publicizing those allegations in conjunction with international partners and in offline forums in Morocco was apparently a step too far.12 International Journal of Cultural Studies 00(0)
Almiraat’s odyssey with lawsuits and court cases was just beginning. In September 
Almiraat and Karima Nadir (ADN VP) were interrogated by the Moroccan Judicial 
Police (BNPJ) in conjunction with the defamation suit filed by the Ministry of the Interior (Alami, 2015; FIDH, 2015). Besides the initial defamation charges associated with the Privacy International report, Almiraat was also charged in conjunction with six other journalists and human rights activists: historian and journalist Maati Monjib, journalists Hicham Mansouri, Abdessamad Iyach/Iach, Maria Moukrim, and Rashid Tarik, plus Mohamed Saber/Essabr president of the Moroccan Association of Youth Education (Association Marocaine pour l’Education de la Jeunesse – AMEJ). The five journalists were affiliated with the Association Marocaine du Journalism d’Investigation (AMJI – the Moroccan Association of Investigative Journalism) and all seven had worked with Dutch NGO Free Press Unlimited on training Moroccans to use the open-source smart-phone app StoryMaker, which allows for producing and publishing media from Android devices. Their work earned Almiraat, Monjib, Mansouri, Iyach and Saber accusations of ‘threatening the internal security of the state’, while Moukrim and Tarik were charged with accepting foreign funding without notifying the government (CIHRS, 2015). The charges carry maximum penalties of five years in prison and hefty fines. The trial, ini-tially set for 19 November 2015, has been postponed ten times over almost three years (FPU, 2016). The charges and ongoing trial indicate use of surveillance and subsequent judicial harassment as part of a broader strategy of digital enclosure being used to con-strain networked publics and their political potential.
The digital carceral
In 2015 Hacking Team itself got hacked, with the hacker tweeting from the company’s account ‘Since we have nothing to hide, we’re publishing all our e-mails, files and source code’, and linking to 400GB of company data (Ghattas, 2015). Leaked files showed that after Mexico and Italy, Morocco was the third highest revenue source, spending more than US$3 million in recent years (Gallagher, 2015).
13 A presentation prepared for a 
surveillance conference references regulations, activists (specifically Citizen Lab, HRW, Privacy International and Anonymous) and the Wassenaar Agreement as having a ‘chill-ing effect’ on the use of technology to fight crime (Gallagher, 2015). Internal emails revealed in the hack show a disregard for concerns about human rights abuses, with one Financial Times article titled ‘Spectre of ISIS used to erode rights in Morocco’ circulated by Vincenzetti with [NOT REALLY] added to the email subject line. Vincenzetti’s email states:
The ISIS menace looms on Moroccan horizon. The King of Morocco is a benevolent monarch, 
he never overcame any Moroccan Parliament decision, he is very different from his father, Morocco is actually the most pro-western Arab country, national security initiatives are solely 
needed in order to tighten stability and defend the country from extremists. Casablanca has 
been bombed by extreme Islamist a few years ago, FYI. (Currier and Marquis-Boire, 2015)
We see here Mohammed VI constructed in opposition to his authoritarian father and 
cooperation with Western interests superseding domestic repression. For Mamfakinch Iddins 13
the hack was accompanied by a sense of vindication as one contributor tweeted ‘There 
you go. The Makhzen actually used surveillance tools from Hacking Team against citi-zens’, linking to analysis from The Intercept (Belmkaddem, 2015).
Surveillance technologies and their international trade are not new. Use of Hacking 
Team spyware against Mamfakinch is merely a recent manifestation of longer histories of solidarity among carceral states, whereby collaboration between repressive state appa-ratuses fulfill a variety of interests and aims. Allegations of CIA training and assistance for secret police and security forces in Morocco and other Global South countries during the Cold War were followed with formal military assistance programs that included tools for domestic use (Klare, 1979), as well as sales from industries supported and enabled by governments. While sales to the most egregiously authoritarian regimes are widely decried, their use by semi-authoritarian regimes raises questions about a willingness to overlook their use against activists and dissidents in favor of broader strategic interests and provides a model for enacting repression without eliciting the ire of the international community. Morocco continues to be a Hacking Team client, a state of affairs the com-pany justifies by the government’s cooperation in the global war on terror.
Vincenzetti says his company lawfully engaged with a government that, he notes in an email, 
‘is an ally of the U.S. and a partner in the fight against terrorism. Morocco is also an ally of most European nations, and Moroccan intelligence agencies recently provided France with 
essential information to locate the terrorists in Paris and in Bruxelles.’ (Kushner, 2016)
The U.S. State Department reports that Morocco has indeed participated in counterter -
rorism efforts with international partners and dismantled domestic cells (U.S. Department 
of State, 2018). Yet the state’s discretionary application of increasingly diffuse and per -
vasive surveillance capabilities elides distinctions between violent extremism and disa-greement with authorities. In 2016 internal documents from New Zealand-based surveillance company Endace revealed Morocco’s domestic intelligence agency, the DGST, to be among its clients (Gallagher and Hager, 2016). According to Amnesty’s North Africa researcher:
In Morocco, digital surveillance is intimately linked with repression of peaceful dissent – 
people who are peacefully protesting or criticizing the authorities face intimidation, arrest, unfair trials, and sometimes imprisonment. We fear that the more that these surveillance tools 
are sold [to Moroccan agencies], the more we will see human rights abuses, especially in 
relation to freedom of expression and information. (Gallagher and Hager, 2016)
A 2015 Amnesty International report on torture in Morocco between 2010 and 2014 
alleges the DGST participated in torture on multiple occasions and links the organization to secret detentions at a prison in Temara (Amnesty, 2015). In any case, it is clear that the institutionalization of surveillance as part of media infrastructure constitutes a continua-tion of carceral culture and its re-inscription in a hypermediated world.
Engaging with the infrastructural turn in media studies, ADN demonstrates the sig-
nificance of shifting imaginations surrounding media infrastructure and its contradic-tory affordances for increasing openness and diffuse constraints. Analysis of activist 14 International Journal of Cultural Studies 00(0)
engagements with and use of media infrastructures, as in the case of Mamfakinch and 
ADN, responds to Parks and Starosielski’s (2015) call for ‘detailed investigation of the varied ways that infrastructures intersect with cultures of everyday life’, which are often missing from infrastructural accounts. The Privacy International/ADN report on cyber-surveillance and working groups on press and penal code reform are indicative of the ways activists challenge state and corporate infrastructural imaginations while attempt-ing to institutionalize alternatives. The distributed technological affordances and net-worked practices associated with smartphones and microblogging allow publics to challenge the gatekeeping power of the state through everyday communicative practices, while transnational civil society networks oriented toward freedom of expression have developed practices for reporting and publicizing abuses. All of this constitutes media infrastructure constructed by users as part of digital popular and political cultures. And yet, as part of a networked public sphere, media infrastructure also makes possible the ability to circumscribe publics in everyday life. This diffuse carcerality permeates rou-tine communicative practices, resulting in complex interactions between the ephemeral and material.
Ultimately, ADN represents a transitional moment in internet imaginaries, both for 
the state and civil society. From Mamfakinch to ADN there is a shift from expectations of economic restructuring and democratic culture to increasingly authoritarian evoca-tions of media infrastructure as a mechanism of visibility and control. In the process we see the mediation of everyday life transformed into a process for the production of social control that is based both in the actual technological capability of watching, but also in activists’ expectations and feelings of being watched. Imaginations of media infrastructure as space for strategizing and solidarity among activists took on the same function for carceral states, ultimately offering new means to constrain infrastructural imaginations.
Funding
This research received no specific grant from any funding agency in the public, commercial, or 
not-for-profit sectors.
Notes
 1. Citizen Lab does research ‘that monitors, analyzes, and impacts the exercise of political power in cyberspace’. See: https://citizenlab.org/about/
 2. Privacy International (2015: 10) reports that the software itself costs about US$200,000.
 3. It is difficult to get precise numbers because surveillance technologies are inherently secretive and often seen as a sub-category of the defense sector produced by subsidiaries of defense contractors (as was the case with Boeing’s Narus until it was sold to Symantec in 2014). However, a 2010 estimate put global homeland security spending at US$178 billion (Hayes, 
2012). The Spade Defense Index (DXS), which bills itself as ‘an investment benchmark for 
the defense, homeland security and space sectors’, claims the index has outperformed the Standard & Poor’s 500 by nearly 350% over the last 15 years, while identifying ‘emergence of spending on cybersecurity and other advanced technologies’ as an industry trend driving growth (Spade, 2019). For more on these trends see Hayes (2012), Lyon (2017), Mattelart 
(2010) and Privacy International (2016).Iddins 15
 4. These cases have been well documented by scholars of literature, anthropology, and sociol-
ogy, among others. For more information see: Benalil (2011), Daure-Serfaty (1992), Hachad (2017), Menin (2014), Redouane (2011) and Zekri (2006).
 5. L’Alternance, literally ‘alternating’ or a change in power, refers to a period at the end of Hassan II’s reign, when he integrated opposition parties, notably those of the left, into government.
 6. Boukhari alleged that the CIA played a key role in training Moroccan secret police in tactics used against dissidents during the Years of Lead.
 7. On 16 May 2003, young men from the Sidi Moumen shantytown blew themselves up at sites across Casablanca, killing 33 victims and 12 suicide bombers.
 8. ‘Similarly to mobile telephony, four out of five countries with the highest level of Internet user penetration are Gulf Cooperation Countries (GCC) countries (chart 1.13). The only exception in the top five list is Morocco, with a penetration of 33.0 per cent, as of 2008’ (ITU, 
2009: 10).
 9. The UA (Unit of Account) is the official currency of the African Development Bank, which is 
intended to provide standard reporting against the volatility of the currencies in which many of the projects occur and the non-convertibility of some currencies.
10. It is also significant to note that Morocco’s Interior Minister (currently Abdelouafi Laftit) sits on Maroc Telecom’s supervisory board.
11. Amesys was the subject of a judicial inquiry in France (Gallagher, 2012) after the Wall Street Journal reported finding its Eagle software being used by Gaddafi’s regime against its own citizens, along with other surveillance technologies purchased from foreign com-
panies after the UN ended international sanctions against Libya in 2003 (Sonne and Coker, 
2011). Reporters Sans Frontières (RSF) labeled both Amesys and Hacking Team as top five ‘Corporate Enemies of the Internet’ in its 2013 report on cyber-surveillance in the ‘era of digi-tal mercenaries’ (RSF, 2013). Realization of these uses post-Arab Spring led to revisions in 
the Wassenaar Agreement on Arms control to include surveillance software and the European 
Union’s revision of its dual-use policy.
12. In December 2009 and January 2010 two members of the Association of Moroccan Bloggers, Bashir Hazzam and Boubakr Al-Yadib, were imprisoned in conjunction with student protests in Tarhijict: Hazzam for posting a statement calling for the release of student protesters and 
linking to a video from the protest, and Al-Yadib for participating in an illegal protest and 
covering it on his blog. Abdullah Boukfou, the owner of the cybercafé from which Hazzam blogged received one year in prison, the harshest sentence of the three.
13. In addition to Morocco, the leaked files also revealed that Hacking Team had sold to a ‘brutal 
paramilitary security agency’ in Bangladesh, to the U.S. Drug Enforcement Agency for use in 
Columbia and to an authoritarian government in Belarus, among others (Gallagher, 2015).
References
ADB (African Development Bank) (1999) Projet de développement des télécommunications: 
rapport d’achèvement. Abidjan, Côte d’Ivoire: ADB. Available at: https://www.afdb.org /fileadmin/uploads/afdb/Documents/Project-and-Operations/ADB-BD-IF-99-323-FR-PCR  
-MAROC-DOC-COMPLET.PDF (accessed January 2017).
ADN (Association des Droits Numériques – Digital Rights Association) (2014a) À propos. 
Facebook.
ADN (2014b) Communiqué, 5 September. Facebook.
ADN (2015) Communiqué, 16 February. Facebook.
Alami A (2015) Moroccan government cracks down on journalists and activists. New York Times, 
11 October: A6.16 International Journal of Cultural Studies 00(0)
Amnesty (2015) Shadow of Impunity: Torture in Morocco and Western Sahara. London: Amnesty 
International. Available at: http://www.amnestyusa.org/research/reports/shadow-of-impunity 
-torture-in-morocco-and-western-sahara
Anderson B (1983) Imagined Communities. London: Verso.ANRT (Agence Nationale de Réglementation des Télécommunications) (2001) Rapport Annuel 
2001. Rabat, Morocco: ANRT. Available at: https://www.anrt.ma/sites/default/files/rappor tannuel/Rapport%20Annuel%202001.pdf (accessed January 2017).
Aouragh M and Chakravartty P (2016) Infrastructures of empire: Towards a critical geopolitics of 
media and information studies. Media, Culture & Society 38(4): 559–575.
Belmkaddem Z (2015) Et voilà: Makhzen a effectivement utilisé outils de surveillance par 
Hacking Team contre citoyens. Twitter, 8 July. Available at: https://twitter.com/Onlyzineb /status/618869582795800576 (accessed January 2017).
Benalil M (2011) Écritures carcérales dans les littératures maghrébines. Expressions maghrébines: 
Revue de la coordination internationale des chercheurs sur les littératures du Maghreb 10(2).
Canard J ( (2011) La haute technologie française fait le bonheur des tyrans. Le Canard Enchaîné, 
12 July. Available at: http://reflets.info/wp-content/uploads/2012/02/Le-Canard-enchai%CC
%82ne%CC%81.11–12–07.jpg (accessed January 2017).
CIHRS (2015) Morocco should drop charges against freedom of expression advocates and halt all 
restrictions on rights and freedoms. Cairo Institute for Human Rights Studies, 19 November. 
Available at: http://www.cihrs.org/?p=17614&lang=en (accessed 25 March 2019).
Currier C and Marquis-Boire M (2015) A detailed look at Hacking Team’s emails about its 
repressive clients. The Intercept, 7 July. Available at: https://theintercept.com/2015/07/07  
/leaked-documents-confirm-hacking-team-sells-spyware-repressive-countries/ (accessed January 2017).
Daure-Serfaty C (1992) Tazmamart: Une prison de la mort au Maroc. Paris: Stock.
El Kabsi F (2003) L’implantation d’internet au Maroc: Enjeux et perspectives. Rabat: Royaume 
du Maroc, Centre national de documentation du Maroc. Available at: http://www.abhatoo.net 
.ma/maalama-textuelle/developpement-economique-et-social/developpement-economique  
/nouvelle-economie/secteur-des-ntic/l-implantation-d-internet-au-maroc-enjeux-perspectives (accessed January 2017).
El Yaakoubi A (2015) Moroccan government sues authors of report accusing it of spying – state 
news agency. Reuters, 9 May. Available at: http://uk.reuters.com/article/uk-morocco-cyber security-rights-idUKKBN0NU0EW20150509 (accessed January 2017).
FIDH (International Federation for Human Rights) (2015) Maroc: Poursuite du harcèlement 
judiciaire de M. Maâti Monjib, M. Hisham Almiraat, M. Hicham Mansouri, M. Mohamed Sber, M. Abdessamad Ait Aicha, M. Rachid Tarek et Mme Maria Moukrim. 13 November. Available at: https://www.fidh.org/fr/themes/defenseurs-des-droits-humains/maroc-pour  
suite-du-harcelement-judiciaire-de-m-maati-monjib-m-hisham (accessed January 2017).
Foucault M (1977) Discipline and Punish: The Birth of the Prison. New York: Vintage Books.
FPU (Free Press Unlimited) (2016) These seven Moroccan human rights defenders are on trial. 26 
October. Available at: https://www.freepressunlimited.org/en/news/these-seven-moroccan -human-rights-defenders-are-on-trial (accessed January 2017).
Fuchs C (2011) New media, Web 2.0 and surveillance. Sociology Compass 5(2): 134–147.
Gallagher R (2012) How government-grade spy tech used a fake scandal to dupe journalists. Slate, 
20 August. Available at: http://www.slate.com/blogs/future_tense/2012/08/20/moroccan _website_mamfakinch_targeted_by_government_grade_spyware_from_hacking_team_.html (accessed January 2017).
Gallagher R (2015) Hacking Team emails expose proposed death squad deal, secret U.K. sales 
push and much more. The Intercept, 8 July. Available at: https://theintercept.com/2015/07/08 /hacking-team-emails-exposed-death-squad-uk-spying/ (accessed January 2017).Iddins 17
Gallagher R and Hager N (2016) Private eyes: the little-known company that enables world-
wide mass surveillance. The Intercept, 23 October. Available at: https://theintercept  
.com/2016/10/23/endace-mass-surveillance-gchq-governments/ (accessed January 2017).
Ghattas A (2015) For Arab human rights defenders, hacking team files confirm suspicions of state 
surveillance. Global Voices, 8 July. Available at: https://advox.globalvoices.org/2015/07/08 
/for-arab-human-rights-defenders-hacking-team-files-confirm-suspicions-of-state-surveil  
lance/# (accessed January 2017).
Graham S and Marvin S (2001) Splintering Urbanism: Networked Infrastructures, Technological 
Mobilities and the Urban Condition. London: Routledge.
Hachad N (2017) Narrating Tazmamart: Visceral contestations of Morocco’s transitional justice 
and democracy. Journal of North African Studies 23(1): 1–17.
Hacking Team (2016) Hacking Team website. Available at: http://www.hackingteam.it/ (accessed 
January 2017).
Hayes B (2012) The surveillance-industrial complex. In Ball K, Haggerty K and Lyon D (eds) 
Routledge Handbook of Surveillance Studies. London: Routledge, pp. 167–175.
HRW (Human Rights Watch) (1999) The Internet in the Mideast and North Africa: Free 
Expression and Censorship. New York: HRW, Middle East and North Africa Division. 
Available at: https://www.hrw.org/sites/default/files/reports/midintnt996.PDF (accessed January 2017).
Iddins A (2018) Mamfakinch: From protest slogan to mediated activism. International Journal of 
Communication 12: 3580–3599.
ITU (International Telecommunications Union) (2009) Information Society Statistical Profiles 
2009: Arab States. Available at: http://unpan1.un.org/intradoc/groups/public/documents/un -dpadm/unpan039529.pdf (accessed January 2017).
Klare M (1979) The international repression trade. Bulletin of the Atomic Scientists 35: 22–27.
Kushner D (2016) Fear this man. Foreign Policy, 26 April. Available at: http://foreignpolicy 
.com/2016/04/26/fear-this-man-cyber-warfare-hacking-team-david-vincenzetti/ (accessed 
January 2017).
Larkin B (2008) Signal and Noise: Media, Infrastructure, and Urban Culture in Nigeria. Durham, 
NC: Duke University Press.
Lyon D (2015) Surveillance after Snowden. Cambridge: Polity Press.MAP (Agence Maghreb Arabe Presse) (2015) Accusé d’espionnage de courriels, l’Intérieur porte. 
10 May. Available at: https://www.yawatani.com/societe-rss/8425-accuse-d-espionnage-de -courriels-l-interieur-porte-plainte (accessed 25 March 2019).
Marquis-Boire M (2012) Backdoors are Forever: Hacking Team and the Targeting of Dissent. 
The Citizen Lab, Toronto. Available at: https://citizenlab.org/2012/10/backdoors-are-for ever-hacking-team-and-the-targeting-of-dissent/ (accessed January 2017).
Mattelart A (2010) The Globalization of Surveillance. Cambridge: Polity.Menin L (2014) Rewriting the world: Gendered violence, the political imagination and memoirs 
from the ‘Years of Lead’ in Morocco. International Journal of Conflict and Violence 8(1): 
45–60.
Morocco (2007) E-Morocco 2010 Strategy: Accomplishments, Perspectives and Action Plans 
– Towards Building Our Information and Knowledge Society. Ministry of Industry, Trade 
and New Technologies, Department of Post, Telecommunications, and Information 
Technologies. Available at: http://www.abhatoo.net.ma/maalama-textuelle/developpement -economique-et-social/developpement-social/information-et-communication/politique-d  
-information/e-morocco-2010-strategy-accomplishments-perspectives-action-plans-towards  
-building-our-information-and-knowledge-society (accessed April 2019).
Ottaway M (2003) Democracy Challenged: The Rise of Semi-Authoritarianism. Washington, DC: 
Carnegie Endowment for International Peace.18 International Journal of Cultural Studies 00(0)
Parks L and Starosielski N (2015) Signal Traffic: Critical Studies of Media Infrastructure. Urbana, 
IL: University of Illinois Press.
Plantin J-C, Lagoze C, Edwards PN and Sandvig C (2016) Infrastructure studies meet platform 
studies in the age of Google and Facebook. New Media & Society 20(1): 293–310.
Privacy International (2015) Their Eyes on Me: Stories of Surveillance in Morocco. London: 
Privacy International. Available at: https://privacyinternational.org/sites/default/files/Their 
%20Eyes%20on%20Me%20-%20English_0.pdf (accessed January 2017).
Privacy International (2016) The Global Surveillance Industry. London: Privacy International. 
Available at: https://privacyinternational.org/explainer/1632/global-surveillance-industry (accessed January 2017).
Privacy International and GreenNet Educational Trust (2003) Silenced: An International Report on 
Censorship and Control of the Internet. Available at: https://citizenlab.org/2003/09/silenced 
-censorship-and-control-of-the-internet/ (accessed January 2017).
Punathambekar A and Mohan S (2018) Global Digital Cultures: Perspectives from South Asia. 
Ann Arbor, MI: University of Michigan Press.
Redouane N (2011) De l’autre côté de la souffrance. Journal of Research in Gender Studies 1(1): 
103–121.
Reflets (2011) Maroc : Le meilleur ami de la France se met au DPI grâce à Amesys, la filiale 
de Bull. Reflets, 30 November. Available at: https://reflets.info/maroc-le-meilleur-ami-de-la 
-france-se-met-au-dpi-grace-a-amesys-la-filiale-de-bull/ (accessed 25 March 2019).
RSF (Reporters Sans Frontières) (2013) The Enemies of Internet Special Edition: Surveillance. 
Available at: http://surveillance.rsf.org/en/ (accessed January 2017).
Sandvig C (2013) Internet as infrastructure. In: Dutton WH, Oxford Handbook of Internet Studies. 
Oxford: Oxford University Press, pp. 86–106.
Slyomovics S (2005) The Performance of Human Rights in Morocco. Philadelphia, PA: University 
of Pennsylvania Press.
Smith S, Jamaï A and Amar A (2001a) La vérité sur l’assassinat en France de Mehdi Ben Barka. 
Le Monde, 29 June. Available at: https://www.lemonde.fr/archives/article/2001/06/29/la 
-verite-sur-l-assassinat-en-france-de-mehdi-ben-barka_203565_1819218.html?xtmc=ben  
_barka&xtcr=16 (accessed January 2017).
Smith S, Jamaï A and Amar A (2001b) La vérité sur la ‘disparition’ au Maroc de Mehdi Ben 
Barka. Le Monde, 30 June. Available at: https://www.lemonde.fr/archives/article/2001/06/30 
/la-verite-sur-la-disparition-au-maroc-de-mehdi-ben-barka_203728_1819218.html?xtmc  
=ben_barka&xtcr=7 (accessed January 2017).
Sonne P and Coker M (2011) Firms aided Libyan spies. Wall Street Journal, 30 August.Spade Defense Index (2019) Spade Defence Index website. Available at: http://spadeindex.com/ 
(accessed February 2019).
U.S. Department of State (2018) Country Reports on Terrorism 2017 – Morocco. Available at: 
https://www.refworld.org/docid/5bcf1f91a.html (accessed February 2019).
Waltz S (1995) Human Rights and Reform: Changing the Face of North African Politics. Berkeley, 
CA: University of California Press.
World Bank (1995) First Telecommunications Project (Loan 2798-MOR): Implementation Completion 
Report (ICR). Available at: http://documents.worldbank.org/curated/en/390421468053361367  
/Morocco-First-Telecommunications-Project (accessed January 2017).
Zekri K (2006) Fictions du réel: Modernité romanesque et écriture du réel au Maroc – 1990–2006. 
Paris: L’Harmattan.
Zuboff S (2019) The Age of Surveillance Capitalism: The Fight for a Human Future at the Frontier 
of Power. New York: Hachette Book Group.Iddins 19
Author biography
Annemarie Iddins is an Assistant Professor in the Department of Communication at Fairfield 
University. Her research is situated within the global media studies subfield of communication, focusing on transnational media industries and cultural politics in the Maghreb and its diaspora."
DataPrivacy,3014.txt,"Vol. 10, 2020
A new decade 
for social changes
www .techniumscience.com9772668 779000ISSN 2668-7798 
 
 
 
 
 
Investigating the nexus between Mobile Apps Adoption and 
Privacy Concerns among Users: An Empirical Analysis from 
Ghana  
 
Acheampong Owusu1, Ivy Hawah Taana2, Akeem Soladoye B akare3, Daha Tijjani 
Abdurrahaman4, Frederick Edem Broni Jr.5 
1 5Operation s and Management Information Systems Department, University of Ghana 
Business School, Legon, Accra , 2 3 4Limkokwing University of Creative Technology, 
Cyberjaya, Malaysia ,  
aowusu@ug.edu.gh  
 
Abstract . There has been a proliferation of Mobile Phones Appl ications commonly referred to as 
Mobile Apps lately due to the advancement of technology. These Mobile Apps are used for a myriad 
of task s ranging from  Mobile Commerce  (m-commerce ), healthcare, learning, social media , among 
others. However, with th is sprea d of the Mobile Ap ps come s the issues of privacy concerns. This 
study, therefore, seeks to investigate why users continue to use the Mobile Apps despite the privacy 
concerns  and what measures they have put in place to mitigate this menace . Through the 
Ante cedents, Privacy, Concerns, and Outcome ( APCO ) model, the study developed a research model 
that was hypothesized and evaluated with 316 respondents from  a tertiary institution. The data was 
analyzed through the partial least squares structural equation  model (PLS -SEM). The results indicate 
that whilst antecedent variable Gender influences  Privacy Concern , Experience on the other hand 
does not . Also, Enjoyment, Privacy Risk, and Trust influence Usage Contin uance  Intention. Also, 
Application Popularity  influences Change Privacy Settings. Again , Privacy Concern influence 
Privacy Risk but does not influence  Trust. Moreover, Enjoyment, Privacy Risk, and Trust do not 
influence Change Privacy Settings. This study has given the researchers  insights as to why use rs 
continue to use Mobile Apps despite the Privacy Concerns that have been raised in the literature. 
Other implications for theory, policy , and practice are also discussed.  
Keywords . Mobile Apps, Privacy Concerns, Ghana  
 
1. Introduction  
In recent time s, technology has advanced which has made mobile devices parsimonious. Thus, 
there is a proliferation of Mobile devices such as smartphones, tablets among others globally and 
this is progressing steadily in most developing economies such as G hana. The stat istics show that 
currently,  Ghana has “the highest mobile penetration in West Africa and already outperforms many 
of its regional peers ”. As at the  “end of 2019, mobile adoption stood at 55 percent, higher than the 
regional average which is a t 44.8 percent ”. Thus, “a huge number of people can be served through 
digital services, positively impacting the growth of the digital economy ” (Omondi , 2020).  Omondi 
(2020) further stated that “a s of the third quarter of 2019, Ghana counted 16.7 million u nique 
mobile s ubscribers, 15.1 million smartphone devices , and 10.7 million mobile internet users in the 
country (as of Q3 2019). 3G coverage accounts for 60 percent of the total connections, 4G has 
167
Technium Social Sciences Journal
Vol. 10, 167-188, August 2020
ISSN: 2668-7798
www.techniumscience.com 
 
 
 
 
 
started gaining traction and will overtake 2G services by  2023. 3G and 4G will account for nearly 
95 percent of total connections by 2025 ” (Omondi , 2020).  
A common phenomenon lately in the mobile devices’ ecosystem is the development of mobile 
phones applications (commonly referred to as apps) for the main thre e mobile device s platforms 
i.e. iOS, android and windows (Hoehle & Venkatesh, 2015)  where users can run various types of 
these “apps for a broad range of purposes, incl uding searching for online information, playing 
games, making purchases, and staying connected with others ” (Byun, Chiu, & Bae, 2018) . As a 
result, users are downloading, installing, and using all sorts of these apps ranging from those used 
for m-commerce/ electronic commerce ( e-ecommerce ) (Sun, Fang, & Hwang, 2019;  Zhang, Chen, 
& Lee, 2013) , social media /entertainment  (Benam ati, Ozdemir, & Smith, 2017; Lankton & Tripp, 
2013; Ozdemir, Jeff Smith, & Benamati, 2017) , health  (Conroy, Yang, & Maher, 2014; Martínez -
Pérez, De La Torre -Díez, López -Coronado, & Herreros -González, 2013) , religion  (Campbell, 
Altenhofen, Bellar, & Cho, 2014; Díez Bosch, Micó Sanz , & Sabaté Gauxachs, 2017; Richardson 
et al., 2020) , learning and research  (Pindeh, Suki, & Suki, 2016; Zydney & Warner, 2016) , 
language learning (Godwin -Jones, 2011; Pindeh et al., 2016) , sports (Byun et al., 2018; Guo et al., 
2017; Hing, Russell, Li, & Vitartas, 2018; Modave et al., 2015) , betting  (Hing et al., 2018; Lopez -
Gonzalez & Griffiths, 2018) , gaming  (Balakrishnan & Griffiths, 2018; Christensen & Prax, 2012; 
Lopez -Gonzalez & Griffiths, 2018; Merikivi, Tuunainen, & Nguyen, 2017) , among others. Whilst 
most of these apps are free, others demand a token fee from the users. The free ones are  sometimes 
overwhelmed with adverts, in -app purchases and also seek to exploit user’s privacy by asking for 
access to contacts list, pictures, documents, email, among others. Naive users who consent to these 
sometimes find their privacy being violated.  
Although, these apps are handy and have been making life more convenient for users as services 
that hitherto would have required users to travel to offices , etc. to  transact  business , can be done 
in one’s place of convenience   (Pagoto, Schneider, Jojic, Debiasse, & Mann, 2013) . In Ghana for 
instance, lately, the government of Ghana is in a digitalization drive where many se rvices such as 
acquiring Passports, Driving License, National Insurance Card, Ghana Lottery, P ayment of Utility 
Bills ( electricity, water etc. ) (http://www.eservices.gov.gh/) , among others are all having apps that 
users install and can use at their conveni ence. Due to the sensitivity of the data and the transactions 
involved, some of these apps are highly secured to prevent fraudsters from  duping users. However, 
when it comes t o the generic apps for social media, entertainment (games , etc.),  among others, 
security may not be that strong. Thus, a user’s privacy can be compromise d. Some of these generic 
apps in some instances, demand access to the user’s Phone Contact Book, Locat ion, Pictures, 
Videos, Music , etc. which can highly be a violation of the user’s privacy.  
Thus, this study seeks to  investigate why users continue to use Mobile Apps despite the privacy 
concerns  and what measures they have put in place to mitigate this m enace . Knowing these will 
help inform policymakers and Apps developers as to what measures they should put in place to 
mitigate some of the issues of privacy concern s arising from the use of Mobile Apps.  
The rest of the paper is as follows. The next secti on introduces readers to the Literature review 
which discusses the concept of Mobil e Apps, its applications , and benefits  derived from  their 
usage. The section further discusses the underpinning theory, related studies about Mobile Apps , 
and the conceptual  development with the formulation of hypotheses. The next section talks about 
the M ethodology with the survey questionnaire, sampling, and the data collection method. This is 
followed by the data analysis technique used and the discussions of the findings.  The c onclusion 
then follows with implications and suggestions for future studies.  
 
 
 
 
 
168
Technium Social Sciences Journal
Vol. 10, 167-188, August 2020
ISSN: 2668-7798
www.techniumscience.com 
 
 
 
 
 
2. Literature Review  
2.1 The concept of Mobile Apps  
Mobile Apps are pervasive lately and have permeated every aspect of our lives as a result of the 
proliferation of Smartphones which has made it easier for the development of these Apps ( Statista, 
2016). 
The Vodafone Group  (2015, p. 211 ) and Byun et al. (2018)  defined Mobile Apps as software 
programs “designed to run on smartphones or tablet devices and provide a convenient  means for 
the user to perform certain tasks” . Also, Hoehle and Venkatesh (2015)  in conceptualizing a Mobile 
Apps refers to it as “ an IT software artifact that is specifically developed for mobile operating 
systems insta lled on handheld devices, such as smartphones or tablet computers” . They further 
averred that Mobile Apps  either come  “preinstalled on mobile devices or can be downloaded from 
various mobile application stores ” (e.g., Apple’s iTunes store , Android Playstor e, Microsoft store).  
The benefits of Mobile Apps are numerous and usually depends on the purpose in which the y are 
developed for. In most ins tances, the use of Mobile Apps provides convenience to end -users which 
can help in reducing costs. Some of the high lighted benefits in the literature include the 
applications of Mobile Apps for weight loss (Pagoto et al., 2013) , learning and research (Zydney 
& Warner, 2016) , getting connected (Benamati et al., 2017; Lankton & Tripp, 2013)  among others.  
 
2.2 Underpinning theory  
This study is underpinned by the Antecedents, Privacy, Concerns, and Outcome (APCO) model 
propounded by ( Smith et al., 2011).  The APCO model  (Smith et al., 2011)  suggests antecedents 
variables Privacy Experience and Demographic Privacy Di fferences  (e.g. Gender, Age, among 
others) will influence Privacy Concerns. They continue to declare that Privacy Concerns will 
influence  Trust and Privacy Risk /Costs . Also, Trust, Privacy Risk, Benefits , and Perceived 
Application Popularity will influence  user’s Usage Continuance Intention and the Behavioral 
Reactions  (e.g., self -disclosure, risks, and regulation)  of the application.  
Although the APCO model has been used extensively to study different phenomenon such as e -
commerce/m -commerce (Sun et al., 2019; Zhang et al., 2013) , social media site s (e.g. Facebook) 
(Benamati et al., 2017; Ozdemir et al., 2017) , sharing ec onomy  (Li, Lee, & Yang, 2019)  among 
others, little is k nown about it when it comes to that of Mobile Apps which have seen extensive 
growth lately. This study thus aims to contribute theoretically in this regard to fill the gaps left in 
the literature.  
 
2.3 Related studies about Mobile Apps  
In this section, w e pre sent the related studies about Mobile Apps privacy concerns in the literature.  
 
Table 1: Related studies  
Article  Theory  Methodology  Context  Findings  
Alashoor, Han, and  
Joseph  (2017)  Protection 
motivation 
theory  
The theory 
of 
planned 
behavior  
APCO 
Model  Survey  
Questionnaires  
with SEM  USA  The findings show “ awareness 
of big data implications had a 
positive impact on privacy 
concerns. In turn,  
privacy concerns impacted 
self-disclosure concerns 
positively and self -disclosure 
accuracy negatively ”.  
Li, Lee, Chang, and 
Yang (2020)  APCO 
Model  Conceptual  USA   
Sun et al.  (2019)  APCO 
Model  Survey  
Questionnaires  China  The results “indicate that hot 
topic interactivity and group 
buying experience have 
169
Technium Social Sciences Journal
Vol. 10, 167-188, August 2020
ISSN: 2668-7798
www.techniumscience.com 
 
 
 
 
 
significant negative impacts 
on 
privacy concerns and 
significant positive impacts on 
perceived benefits. Privacy 
concerns negatively  
influence the behavior of 
information disclosure while 
perceived benefi ts positively 
influence the  
behavior of information 
disclosure ”. 
Dinev, Mcc onnell, 
and Smith  (2015)  APCO 
Model  Conceptual    
Ozdemir et al. 
(2017)  APCO 
Model  Survey  
Questionnaires  
with SEM  USA  The findings show that “ both 
privacy  experiences and 
privacy awareness are quite 
significant predictors of 
privacy  concerns. ” The results 
also show that “trust, risk, 
benefits, and privacy concerns 
work  
together to explain a large 
amount (37%) of the variance 
in disclosure  
behaviors ”. 
Benamati et al. 
(2017)  APCO 
Model  Survey  
Questionnaires  USA  The results suggest that “ PA 
and gender are important 
explanators for CFIP, which in 
turn explains  privacy -
protecting behaviors ”. The 
results also show that 
“perceived risk affects trust, 
which in turn affects 
behaviors in the studied 
context ”. 
Zhang et al. (2013 ) APCO 
Model  Survey  
Questionnaires  USA  The results show that “ the 
consumers’ demographic  
differences have varying 
degrees of impact on their  
concerns for  information 
privacy in the context of m -
commerce ”. 
Buck  (2017)  APCO 
Model  Survey  
Questionn aires  German  The results show “significant 
correlations to the concerns 
users have about their privacy 
– an increasing future self -
continuity is related with 
higher concerns ”. 
Lankton  and Tripp 
(2013)  APCO 
Model  Survey  
Questionnaires  USA  The results indicate 
“Experience doe s not 
influence privacy concern but 
gender does . Privacy concern 
has positive significant effects 
on privacy risk and change 
privacy settings but  has no 
significant effects on trust, 
limit number friends, 
170
Technium Social Sciences Journal
Vol. 10, 167-188, August 2020
ISSN: 2668-7798
www.techniumscience.com 
 
 
 
 
 
differentiate friends, and 
continuance intention ”. 
The results also reveal ed that 
“neither trust nor privacy risk 
have  significant influences on 
the privacy behaviors but do 
significantly influence 
continuance intention ”.  
Also , “enjoyment influences 
continuance intention ”. 
 
 As indicated in Table 1, most  of the studies about the APCO model have focused on 
Social Media, e -commerce, m -commerce, sharing economy among others. Also, in terms of the 
context, the focus has been on developed economies leaving the developing economies behind. 
Thus, this study will  add up to the scanty literature in the developing country context.  
 
2.4 Conceptual Model and Hypotheses Development  
The formulation of hypotheses for the various relationships unearthed in the APCO macro model  
is done in this section . Table 2 presents  the meanings of the constructs used in this research and 
illustrates how  they correlate to the APCO constructs. The research model is presented in Figure 
1 which  was developed based on the APCO model as discussed above and extant literature.  
 
Table 2: Constru cts, their definitions and supporting literature  
Construct  
(In this study)  APCO Model Variable  Definition  Supporting 
Literature  
Experience  Privacy Experiences  Length and frequency of 
prior apps’ use  (Dinev et al., 2015; 
Lankton & Tripp, 
2013; Li et al., 
2020, 2019; 
Metzger, 2007)  
Gender  Demographic Differences  
Privacy  Gender  (Benamati et al., 
2017;  Lankton & 
Tripp, 2013; 
Metzger, 2007; 
Ozdemir et al., 
2017)  
Privacy Concerns  Privacy Concerns  Concerns about 
opportunistic behavior 
related to personal 
information that is 
disclosed by the 
respondent in particular  (Alashoor e t al., 
2017; Benamati et 
al., 2017; Dinev et 
al., 2015; Lankton 
& Tripp, 2013; Li 
et al., 2020, 2019; 
Metzger, 2007; 
Ozdemir et al., 
2017)  
Privacy Risk  Risks/Costs  Concerns about 
opportunistic behavior 
“related to personal 
information that is 
disclos ed by the 
respondent in general ” (Benamati et al., 
2017; Lankton & 
Tripp,  2013; 
Ozdemir et al., 
2017; Zimmer, 
Arsal, Al -Marzouq, 
& Grover, 2010)  
Enjoyment  Benefits  The extent to which using 
the apps are “perceived to 
be enjoyable in its own 
right, apart from any (Buck, 2017; Dinev 
et al., 2015; 
Lankton & Tripp, 
2013; Ozdemir et 
171
Technium Social Sciences Journal
Vol. 10, 167-188, August 2020
ISSN: 2668-7798
www.techniumscience.com 
 
 
 
 
 
performance consequences 
that may be incurred ” al., 2017; Sun et al., 
2019)  
Trust  Trust  This is “ being willing to 
depend on the website, or 
a volitional preparedness 
to make one self 
vulnerable ” (Alashoor et al., 
2017; Benamati et 
al., 2017; Dinev et 
al., 2015; Lankton 
& Tripp, 2013; 
Ozdemir et al., 
2017; Zimmer et 
al., 2010)  
Application Popularity   refers to the fact that many 
people like something or 
someone ""  (Cambridge, 2016 ) 
Change Privacy Settings  Behavioral Reactions  Refers to “w hether the 
vendor -provided privacy 
settings have been 
changed ” (Lankton & Tripp, 
2013)  
Usage Continuance 
Intention  Intentions to continue 
using free apps  (Lankton & Tri pp, 
2013; Li et al., 
2020, 2019; Rezaei, 
Shahijan, Amin, & 
Ismail, 2016)  
 
 
Figure 1: Conceptual model  
 
Experience – The mobile apps “model includes privacy experience as an antecedent to privacy 
concern because individuals who have bee n exposed to or been the victim of personal information 
abuse should have stronger p rivacy concerns ” (Smith et al., 2011). Additional ly, Tuqfekci (2008)  
“finds that nonusers of a social network site had higher privacy concerns than users ”. We include 
“the prior experience to capture these effects, as users with more prior experience are m ore likely 
to have encountered privacy abuses ”. Besides, Sun et al. (2019)  in their study find out that “the 
consumers’ demographic differences have varying degrees of impact on their concerns for 
information privacy in the context of m -commerce”.  In ano ther study, Ozdemir et al.  (2017)  
established that “ both privacy experiences and pri vacy awareness are quite significant predictors 
Experience  
Privacy Risk  
Privacy Concern  
Trust  
 
Enjoyment  
Usage 
Continuance 
Intention  
 
Gender  
Change Privacy 
Settings  
Reactions  
Perceived App 
Popularity  
172
Technium Social Sciences Journal
Vol. 10, 167-188, August 2020
ISSN: 2668-7798
www.techniumscience.com 
 
 
 
 
 
of privacy  concerns” . Also, Lankton and Tripp (2013)  find out that “ Experience does not influence 
privacy concern ”.  
 
Based on the above discussions, i t is there hypothesized that:  
H1: Experience will positively influence Priva cy Concern  
 
Gender  – We use gender in our model to address population disparities as suggested by mobile 
apps. Researchers have found that females are more interested in privacy th an males in general. 
Sheehan (1999)  reports, for example, that females are more interested than males when an app 
claims its publicly identifying information is accessed by certain companies. We say that it is less 
relaxed for females to be in touch with u nknown individuals than for males t o be worried about 
the opportunism of those in mobile apps. Furthermore, Fogul and Nehmad (2009)  identified 
females with much more mobile apps privacy issues than males, suggesting that females have less 
control over the security of their privacy than male s by implicit social contracts.  Also, Zhang et al. 
(2013)  find that “the consumers’ demographic d ifferences have varying degrees of impact on their 
concerns for information privacy in the context of m -commerce”.  In another study, Lankton and 
Tripp (2013)  find out that “Gender  does not influence privacy concern ”. 
 
Based on the above discussions, it is there hypothesized that: 
H2: Females will have greater  Privacy Concerns than Males 
 
Privacy Concerns  – Privacy concern negatively affects Trust since larger concerns may make 
“one less likely to feel they can rely or depend on the technology ”. Whilst Privacy Concern “relates  
to the likelihood of not having desirable results, Trust refers to the likelihood that one can depend 
and rely on the trustee to perform desirable actions ” (Fogel & Nehmad , 2009 ; Owusu, Br oni J nr, 
& Akakpo, 2019) . This implies a negative relation amongst the variables. Empirical research 
demonstrates that “Privacy Concern negatively influences Trust” (Eastlick et al., 2006 ). In another 
study, Lankton and Tripp (2013)  established that “ Privacy Concern has positive  significant effects 
on Privacy Risk”. 
 
Based on the above discussions, it is there hypothesized that:  
H3: Privacy Concern will positively influence Privacy Risk 
 
Mobile Apps “depicts privacy risk, trust, and behavioral reactions as out comes of Privacy 
Concerns” (Fogel & Nehmad , 2009) . Both Privacy Concern and Privacy Risk signify “the costs of 
disclosing information ” (Dinev & Hart, 2006 ). The two variables are “highly related because 
perceptions that one’s personal information might be used opportunistical ly can influence one’s 
perceptions that personal information , in general,  might be used opportunistically ”. In a study by 
Lankton and  Tripp (2013) , they found that “ Privacy concern has no significant effects on trust”.  
 
Based on the  above discussions, it is there hypothesi zed that:  
H4: Privacy Concern will negatively influence Trust  
 
Mobile apps also predict that privacy concern s will affect behavioral reactions. The more mobile 
app privacy concerns “the more one is likely to exerci se privacy behaviors to control their pers onal 
information and prevent bad things from happening ”. Privacy concern s can make mobile apps 
“users more likely to take action to protect their privacy ”. In mobile apps, “privacy behaviors  can 
include changing vendor privacy settings ” (change privacy set tings) ( Lankton et al., 2012 ), 
“limiting the number of friends in one’s friends ' list (limit number friends) and allowing only 
friends one has interacted with a lot in one’s friends list (di fferentiate friends) ” (Debatin et al., 
173
Technium Social Sciences Journal
Vol. 10, 167-188, August 2020
ISSN: 2668-7798
www.techniumscience.com 
 
 
 
 
 
2009; Stutzman & Kramer -Duffield, 2010 ). These “responses to high concerns are consistent with 
expectancy theory’s explanation that individuals are motivated to minimize negative outcomes ” 
(Dinev & Hart, 2006 ). We als o “include usage continuance intention in our model as a behaviora l 
reaction to privacy concerns ” because some mobile apps “users might discontinue use if their 
concerns are too high ”. A study by Lankton and Tripp (2013)  established that “Privacy concern 
has no significant e ffects  on continuance intention”.  
 
Based on the above discussions,  it is there hypothesized that:  
H5a: Privacy Concern will positively influence  Usage Continuance Intention.  
H5b: Privacy Concern will negatively influence Change Privacy Settings .  
 
Trust  - Mobile Apps also predict that “Trust and the Privacy Risks and ben efits involved in the 
privacy calculus decision will influence behavioral reactions ” (Fogel & Nehmad , 2009) . Trust 
“plays an important role in predicting privacy behaviors ” (Dinev & Hart, 2006). Trust lowers “the 
perceived concerns about revealing informat ion making one feel that disclosure is a safe activity ” 
(Metzger, 2004 ). If some one decides an app “is dependable and reliable one will be less likely to 
take steps to keep informati on private ”. Trust “also predicts usage continuance intentions because 
when one is willing to depend, one makes a conscious choice to put aside doubts and move forward 
with the relationship ” (Holmes, 1991 ). Benamati et al. (2010)  find that “Trust influences intention 
to use a bookseller website ”. Lankton and Tripp (2013)  in their study established  that “ Trust ha s 
no significant influence on Change Privacy Settings but do significantly influence Continuance 
Usage I ntention”.  
 
Based on the above discussions, it is ther e hypothesized that:  
H6a: Trust will negatively influence Usage Continuance Intenti on. 
H6b: Trust will positively influence Change Privacy Settings.  
 
Privacy Risk  - The final relationships that deal with mobile apps “are the effects of Privacy Risks 
and benefits on behavioral reactions ” (Fogel & Nehmad , 2009) . Similar to “Privacy Concern s, 
Privacy Risks should increase the likelihood of engaging in privacy behaviors t o protect the 
opportunistic use of personal information ” (Wu et al., 2012 ). These risks “should also make one 
less likely to want to continue ” using mobile apps.  In another s tudy, Lankton and Tripp (2013)  
established that “Privacy Risk ha s no significant influence on change privacy settings  but do 
significantly influence Usage C ontinuance Intention ”.  
 
Based on the above discussions, it is there hypothesized that:  
H7a: Privacy Risk will positive ly influence Usage Continuance Intention.  
H7b: Privacy Risk will positively influence Change Privacy Settings.  
 
According to previous stud ies that inspire the mobile apps model, “benefits from using technology 
should decrease one’s privacy behaviors and in crease continued use ”. We contemplate Enjoyment 
as a gain of mobile apps use because “Enjoyment is a major reason people use social networking 
apps” (Xu et al., 2008 ). The more “enjoyable using the mobile apps, the less likely one will engage 
in privacy be haviors because this might stifle one’s ability to  make use ” of the app.  Enjoyment is 
“part of the extended unified theory of acceptance and use of technology for consumers as a 
predictor of continuance intention ” (Venkatesh et al., 2012 ). People will “want to continue using 
a technology they find enjoyable ”. Prior research finds “that enjoyment significantly influences 
the intention to continue using mobile apps ” (Sledgianowski & Kulviwat, 2009 ). In another study, 
Lankton and Tripp (2013)  established that “ enjoyment influenc es continuance intention”.  
 
174
Technium Social Sciences Journal
Vol. 10, 167-188, August 2020
ISSN: 2668-7798
www.techniumscience.com 
 
 
 
 
 
Based on the ab ove discussions, it is there hypothesized that:  
H8a: Enjoyment will posit ively influence Usage Continuance Intention.  
H8b: Enjoyment will negatively  influence Change Privacy Settings.  
Popularity - The popularity o f apps has a positive effect on the behavior of people  to continue 
using the app (Ferdous, Osman i, & Mayora, 2015a) . Ferdous, Osmani, and Mayora (2015b)  
contend that “s martphone app usage behavior is driven by factors such as the popularity of apps 
(e.g., how many categories of apps a user visited and how is his/her attention spread) , frequency 
and duration of app usage (e.g., how often do es a user use an y application and how long is the app 
used), frequency of unique apps used (e.g., if the user keeps using only a few apps regularly or the 
list of his/her visited apps is large) ”. In a study by Ferdous et al. (2015a) , they found out that 
“perceived app popularity make s them less concerned ” and also “ a positive effect of perceived 
App Popularity on download intention ”. Also, in another study, Shen (2015)  find out that “Apps' 
Populari ty is more effective when  the app carries low perceived risk”  which leads to Usage 
Continuance Intention .  
 
Based on the above discussions, it is there hypothesized that:  
H9: App popularity will positively influence U sage Continuance Intention.  
H9: App pop ularity will positively influence Change Privacy Settings . 
 
3. Methodology  
The quantitative approach with the survey method was used for this study. Data was collected 
through a survey method with stratified and convenience sampling from 316 students from a 
tertiary institution in Ghana. The questionnaire comprises three sections  with section one asking 
questions about the respondent’s demographics. Section two dealt with the antecedent variables 
and the independent variables whilst the last section asks qu estions about the dependent variables. 
The Constructs were operationalized as follows: Experience has 2 items, Application Popularity, 
Trust, and Enjoyment all have 3 items each, Privacy Concerns and Privacy Risk each has 4 items. 
All these constructs were  measured on a 5 -point Likert Scale with “1” representing “Strongly 
Disagre e” and “5” representing “Strongly Agree”.  The items were adapted from (Smith et al., 
2011; Fuller, Serva, & Benamati, 2007; LaRose & Rifon, 2007; Smith, Milberg, & Burke, 1996) ). 
The data was collected through a hand -deliverin g questionnaire where the students were given 
ample time to fill the questi onnaire in their respective halls of residence and return them on a given 
date. It took almost three months to collect the data.  
 
4. Analysis and Findings  
The demographics were anal yzed through SPSS version 22 and the inferential statistics done 
through Partial Least Squares Structural Equation Modeling (PLS -SEM) specifically with 
SmartPLS 3.3.2  
 
4.1 Respondents Demographics  
Table 3: Respondents Demographics  
Variable  Item  Frequency  Percentage  
Gender  Male  
Female  153 
163 48.4 
51.6 
Age Below 20  
20-29 
30-39 
40-49 
50-59 
60+ 0 
66 
122 
74 
54 
0 0 
20.9 
38.6 
23.4 
17.1 
0 
175
Technium Social Sciences Journal
Vol. 10, 167-188, August 2020
ISSN: 2668-7798
www.techniumscience.com 
 
 
 
 
 
Use SmartPhone?  Yes 
No 316 
0 100 
0 
 
Table 3 shows the demographic profile of the respondents. 48.4 % of the respondents wer e males 
whilst 51.6% were females. Thus, the majority of the respondents were females. In terms of the 
Age distribution of the respondents, 38.6% of the respondents were found in the 30 -39 age 
categories followed by 23.4% who were f ound in the 40 -49 age ca tegories. The greater percentage 
of the age distribution is above 30 years because most of the respondents were students doing their 
Master s's degree. Regarding the respondents ' usage of Smart Phones, all of them declared they 
used smartphones. This is an indication of the proliferation of Smartphones in the Ghanaian 
economy.  
 
4.2 Inferential statistics  
The survey data collected were subjected to partial least squares structural equation modeling 
(PLS -SEM) where the 2 -step approach of  analyzing data in SEM was employed. The measurement 
model was assessed firstly, which was then followed by the evaluation of the structural model 
(Hair et al. 2017). SmartPLS 3.3.2 (Ringle et al., 2015) was used to assess both the measurement 
and structur al models.  The use of P LS-SEM is justified by its popularity lately in terms of handling 
of data that is not normally distributed and handling of complex research models like our 
conceptual model in this study ( Hair et al., 2014, p. 19 ). Thus, PLS -SEM has been used in a lot of 
recent studies which includes (Abdurrahaman, Owusu, & Bakare, 2020; Owusu, 2017, 2019; 
Owusu, Agbemabiese, Abdurrahaman, & Soladoye, 2017; Owusu, Ghanbari -Baghestan, & 
Kalantari, 2017)  in Information Systems and (Abdurraham, Owus u, Soladoye, & Kalimuthu, 2018; 
Bakare, Ow usu, & Abdurrahaman, 2017)  in Advertising and Marketing.  
 
4.2.1 Assessing the Measurement Model  
4.2.1.1 Reliability and Validity Test  
Creswell (2012, p.159)  defines Reliability as “scores from an instrument are stable and consistent.” 
Also, Saunders et al. (2009, p.156)  defined reliability as “the extent to which your data collection 
techniques or analysis procedures will yield consistent findings”. Furthermore, Creswell (2014,  p. 
200) asserted that the “vali dity and reliability of scores on instruments are prime to mean ingful 
interpretations of data”.  
 Reliability comes in different forms. One form is “internal consistency” which is 
computed by calculating the Cronbach’s alpha (Saunders et al. 2009, p.374; Hair et al. 2014, 
p.101) . Also,  “Cronbach’s alpha measures the degree to which” the items used are “internally 
reliable with other items” including the construct. This “takes values ranging” from “1 (which 
indicates the item s correlate perfectly) and 0 (which denotes the items are total ly inconsistent)”. 
An “alpha score of above 0.70 indicated internal consistency and was considered reliable” 
(Nunnally, 1978).  
 
Table 4: Results of Construct Reliability and Validity Test  
Const ructs   
Indicators  Outer 
Loadin
gs Cronbach
's Alpha  Composite 
Reliability  Average 
Variance  
Extracted 
(AVE)  
Change_Pri
vacy_Setting
s Change_Priv
acy_Settings   
1.00 1.00 1.00 1.00 
Enjoyment  Enjoyment1  
Enjoyment2  0.848  
0.880  0.832  0.899  0.748  
176
Technium Social Sciences Journal
Vol. 10, 167-188, August 2020
ISSN: 2668-7798
www.techniumscience.com 
 
 
 
 
 
Enjoyment3  0.868  
Exp Exp1  
Exp2  -0.553  
0.694  0.712  0.716  0.693  
Gender  Gender  1.00 1.000  1.00 1.00 
Perc_App_P
op Perc_App_Po
p1 
Perc_App_Po
p2 0.493  
0.832  0.702  0.805  0.678  
PrivCon  Priv_Con1  
Priv_Con2  
Priv_Con3  
Priv_Con4  0.817  
0.869  
0.915  
0.890  0.878  0.917  0.734  
Priv_Risk  Priv_Risk1  
Priv_Risk2  
Priv_Risk3  
Priv_Risk4  0.862  
0.896  
0.887  
0.778  0.896  0.928  0.763  
Trust  Trust1  
Trust2  
Trust3  0.793  
0.869  
0.888  0.812  0.887  0.724  
Usage_Cont
_Int Usage_Cont_
Int1 
Usage_Cont_
Int2 
Usage_Cont_
Int3 0.938  
0.949  
0.903  0.923  0.951  0.866  
 
The reliability test for this study’s instrument was assessed through both Cronbach’s alpha value 
as well as the composite reliability (Hair et al. 2014, p.98)  as shown in Table 4. The data was also 
subjected to confirmatory factor analysis (CFA) to get insights as to the multidimensionality of the 
items using SmartPLS 3. 3.2 (Ringle et al., 2015) and discriminant validity test. Cronbach alpha 
value “provides an estimate of the reliability based on the intercorrelations of the observed 
indicator variables” (Hair et al. 2014, p.101) . Cronbach’s alpha has a weakness of assuming all the 
indicators are equally reliable (that is “all the indicators have equal outer loadings on the 
construct”) and it is also “sensitive to the number of items in the scale and generally tends to 
underestimate the internal consistency reliability” (Hair et al. 2014, p.101) . However, composite 
reliability “takes into account the diff erent outer loadings of the indicator variables” (Hair et al. 
2014, p.10 1) and thus ensures more reliability on each construct.  
 Table 4 illustrates the summary measurement model with the “Cronbach Alpha, 
Composite Reliability, and the Average Variance Extracted (AVE) values” of all the constructs. 
As shown in Table 4, the tes t results revealed that all the latent “variables meet the Cronbach alpha 
value of 0.7 or higher” (Nunally, 1978), which “is the acceptable value for reliability”. Also, the 
results indicate that all the “measures are robust in terms of their inte rnal cons istency reliability” 
which is shown by “the composite reliability”. The composite reliabilities values of the different 
measures in the model range from 0.716 to 0.951 which well exceeds “the recommended value of 
0.7” as shown in Table 4. Also, fr om Table 4, all the AVE values are well above 0.5. This proves 
that all the indicators for each construct converge very well to form the constructs (Hair et al. 2014, 
p.107) . However,  Change_Privacy_Settings and Gender each have only one indicator. As a result, 
the outer loadings, Cronbach alpha, Composite Reliability, and AVE all s how 1. This is acceptable 
as indicated by Hair et al. (2014, p.) . 
177
Technium Social Sciences Journal
Vol. 10, 167-188, August 2020
ISSN: 2668-7798
www.techniumscience.com 
 
 
 
 
 
 Again, from Table 4, the confirmatory factor analysis illustrates acceptable factor loading 
on the hypothesized measurements. The outer loadings of almost all the indicators meet t he 
accept able value of 0.7 or higher which is recommended for indicator reliability with very few 
falling below 0.7. However, these indicators were maintained as (Wong, 2013 ) has reco mmended 
values of 0.4 or higher for exploratory research.  The indicators below 0.7 have their values ranging 
from 0.553 (Exp1) to 0.694 (Exp2) and hence they were maintained. However, the outer loadings 
“Per_App_Pop3” has very low outer loading o f 0.205. As a result, it was deleted.  
 Construct  validity confirms if the items measure hypothetical constructs as planned 
(Creswell , 2014, p.206 ) . This has become very helpful in recent studies and has “focused on 
whether the scores serve a use ful purpose and have positive consequences when they are used in 
practice.” Also, Creswell (2014, p.206) emphasized that “establishing the validity of the scores in 
a survey helps to identify whether an instrument might be a good one to use in survey resea rch.”  
 After the reliability test, the next analysis was to check the validity of the measures. This 
was done by the disc riminant validity via the Fornell -Larcker criteria.  
 
Table 5: Discriminant Validity - Fornell -Larcker Criterion  
 Constructs  Enjoy
ment  Ex
p Gen
der Perc_App
_Pop  PrivR
isk Priv_
Con Tr
ust Usage_Co
nt_Int  
Change_Privacy
_Settings  1.00        
Enjoyment  0.865         
Exp 0.009  0.6
27       
Gender  -0.028  -
0.0
84 1.00
0      
Perc_App_Pop  0.276  0.0
17 0.01
4 0.571      
PrivRisk  0.180  -
0.0
59 0.10
2 0.064  0.857     
Priv_Con  0.197  0.0
45 0.11
9 0.100  0.607  0.873    
Trust  0.106  -
0.0
63 0.08
9 0.135  0.050  0.034  0.8
51  
Usage_Cont_Int  0.287  0.0
40 0.03
8 0.155  0.226  0.119  0.3
61 0.931  
 
From Table 5, all the constructs meet the satisfactory values for “discriminant v alidity based on 
the Fornell -Larcker Criterion” (Hair et al. 2014, p.105; Fornell & Larcker, 1981) , as “the square 
root of each construct's AVE is greater than its highest correlation with any other construct”.  
 
4.2.1.2 Assessing the Structural Model  
The structural model is assessed in the second st age only after determining that the measurement 
model satisfies all the acceptable values.  
 But before evaluating the structural model, the researchers assessed if collinearity  exist s 
among the constructs. Collinearity measures if the constructs are highly  correlated and it s refer to 
as multicollinearity if it involves more than two constructs ((Hair et al. , 2014, p.218). 
 Collinearity issues are calculated by the computation of  tolerance levels and the variance 
178
Technium Social Sciences Journal
Vol. 10, 167-188, August 2020
ISSN: 2668-7798
www.techniumscience.com 
 
 
 
 
 
inflation factor (VIF) values. The VIF is def ined as the reciprocal of the tolerance (Hair et al. , 
2014, p.124). For PLS -SEM, Hair et al. (2014, p.125) and Hair, Ringle, and Sarstedt (2011) 
confirmed that “a tolerance val ue of 0.20 or lower and a VIF value of 5 and higher ” respectively 
among the pred ictive constructs indicate a possible collinearity problem.  
Table 6: Collinearity values for the predictive constructs  
 Constructs  Enjoy
ment  Ex
p Gen
der Perc_  
App_
Pop PrivR
isk Pri
v_ 
Co
n Tru
st Usage
_ 
Cont_
Int Collinea
rity 
Proble
m 
(VIF>5)
? 
Change_Privacy_
Settings          No 
Enjoyment         1.120  No 
Exp      1.0
07   No 
Gender       1.0
07   No 
Perc_App_Pop         1.096  No 
PrivRisk         1.035  No 
Priv_Con      1.000   1.0
00  No 
Trust         1.025  No 
Usage_Cont_Int          No 
 From Table 6, all the predic tor constructs have their VIF values below 5, which implies 
that multicollinearity issues are not found among them. Therefore, the other test such as the 
significance of path coefficients, effect sizes, and so on can be carried on using the model.  
 
4.2.1.3  Hypotheses testing  
The bootstrapping technique was used for the testing of the hypotheses. SmartPLS 3.3. 2 generates 
the t-value which “provides the statistical significance of the causal path between the constructs in 
the hypothesized model”. Hair et al. (2014, p.164) declared that “for a two -tailed test, the popular 
critical t -values in PLS -SEM are 1.65 (α =  0.10), 1.96 (α = 0.05), or 2.57 (α = 0.01)”.  
 
179
Technium Social Sciences Journal
Vol. 10, 167-188, August 2020
ISSN: 2668-7798
www.techniumscience.com 
 
 
 
 
 
 
 
Figure 2: Structural Model  
 
Table 7: Results of Hypotheses Testing  
 Paths  Hypothesi
s β-value  t-
values P Values  Supported
? 
Enjoyment -> 
Change_Privacy_Settings  H8b -0.080  1.321  0.186  No 
Enjoyment -> 
Usage_Cont_Int  H8a 0.237  4.423  0.000  Yes 
Exp -> Priv_Con  H1 0.056  0.594  0.552  No 
Gender -> Priv_Con  H2 0.124  2.225  0.026  Yes 
Perc_App_Pop -> 
Change_Priv acy_Settings  H9b 0.147  2.668  0.008  Yes 
Perc_App_Pop -> 
Usage_Cont_Int  H9a -0.030  0.472  0.637  No 
PrivRisk -> 
Change_Privacy_Settings  H7b 0.016  0.230  0.818  No 
PrivRisk -> 
Usage_Cont_Int  H7a 0.205  2.643  0.008  Yes 
Priv_Con -> 
Change_Privacy_Settings  H5b 0.004 0.052  0.959  No 
Priv_Con -> PrivRisk  H3 0.607  13.264  0.000  Yes 
Priv_Con -> Trust  H4 0.034  0.554  0.580  No 
Priv_Con -> 
Usage_Cont_Int  H5a -0.058  0.831  0.406  No 
180
Technium Social Sciences Journal
Vol. 10, 167-188, August 2020
ISSN: 2668-7798
www.techniumscience.com 
 
 
 
 
 
Trust -> 
Change_Privacy_Settings  H6b -0.012  0.217  0.829  No 
Trust -> Usage_Cont_Int  H6a 0.332 6.743  0.000  Yes 
 
Results from “the bootstrapping technique for the Structural Model signifying the t -values causal” 
links amongst the latent variables are shown in Figure 2  and Table 7, respectively. At “5% 
significance level (t -value s ≥ 1.96)”,  Enjo yment -> Usage_Cont_Int (t = 4.423, p = 0.000), Gender 
-> Priv_Con (t=2.2 25, p=0.02 6), Perc_App_Pop -> Change_Privacy_Settings  (t=2.668, p=0.008),  
PrivRisk -> Usage_Cont_Int (t=2. 643, p=0.00 8), Priv_Con -> PrivRisk (t=13. 264, p=0.000),  and  
Trust -> Usage_Cont_Int (t=6. 743, p=0.000) emerged significant. Therefore, hypotheses H8a, H2, 
H9b, H7a, H3, and H6a  are accepted. However, Enjoyment -> Change_Privacy_Settings  (t=1.321, 
p=0.186), Exp -> Priv_Con (t=0.5 94, p=0.55 2),  Perc_App_Pop -> Usage _Cont_Int (t=0 .472, 
p=0.6 37), PrivRisk -> Change_Privacy_Settings  (t=0.230, p=0.818),  Priv_Con -> 
Change_Privacy_Settings  (t=0.052, p=0.959), Priv_Con -> Trust  (t=0.554, p=0.580), Priv_Con -
> Usage_Cont_Int  (t=0.831, p=0.406), and Trust -> Change_Privacy_ Settings  (t=0.217, p=0.829) 
are insignificant. Therefore, hypotheses H8b, H1, H9a, H7b, H5b, H4, H5a, and H6b are all 
rejected.  
 
Table 8: R2, R2 adjusted, f2 and Q2 values  
Constructs  R2 R2 
Adjuste
d f2 
Usage_Cont_I
nt f2 
Change_Privacy_Settin
gs Q2 
Change_P rivacy_Settin
gs 0.02
0 0.004    0.00
4 
Enjoyment  - - 0.051  0.006  - 
Exp - - 0.003   - 
Gender  - - 0.015   - 
Perc_App_Pop  - -  0.019  - 
PrivRisk  0.38
9 0.367    0.26
7 
Priv_Con  0.01
7 0.011  0.584   0.00
8 
Trust  0.00
1 -0.002  0.133   0.00
2 
Usage_Cont_Int  0.22
3 0.213    0.18
4 
 
4.2.1.4 The coefficient of determination - R square  
Denoted as the R2, value, the coefficient of determination measures the model’s predictive 
accuracy in PLS -SEM. The R2 value is computed as “the squared correlation between a specific 
endogenou s construct’s actual and predicted value”. The R2 value “is the representation of  all the 
exogenous latent variables” collective “effects on the endogenous construct”. It is “also the amount 
of the variance in the endogenous constructs which is explained b y all of the exogenous constructs 
linked to it” (Hair et al., 2014, pp. 174 –175).  The R2 value has “a rule of thumb with values ranging 
from 0 to 1”. Values of “0.75, 0.50, or 0.25 for endogenous latent variables have been specified as 
substantial, moderat e, or weak, respectively” (Hair et al., 2014, p. 175).  
From Table 8, the R2 values of 0.389, 0.017, 0.001 , and 0.223 indicate that the combined effects 
of exogenous latent variables Trust, Privacy Risk, Enjoyment , and Perceived Application 
Popularity expla in 22.3% of “the variance of the endogenous construct” Usage Continuance 
Intenti on. Similarly, the exogenous latent variable Privacy Concern explain s 38.9% on Privacy 
181
Technium Social Sciences Journal
Vol. 10, 167-188, August 2020
ISSN: 2668-7798
www.techniumscience.com 
 
 
 
 
 
Risk and 0.1% of Trust respectively. Also , antecedent variables Experience and Gender exp lain 
1.7% of exogenous latent variable Privacy Concern s. 
 
4.2.1.5 Effect sizes  
Denoted as f2, the effect size is “used to measure an exogenous latent variable’s influence on an 
endogenous construct’s” R2 value. The f2 value is used by researchers to measur e the relevance  of 
individual “latent variable’s contribution in explaining the v ariance of the endogenous constructs”. 
The rule of thumb for f2 values ranges from “0.02, 0.15 and 0.35 indicating an exogenous 
construct’s small, medium or large effect, resp ectively, on an endogenous construct” (Hair et al., 
2014, p. 186).  
From Table 8, with an f2 value of 0.584, it emerged that Privacy Concern has the highest effect 
size on Privacy Risk. Also, with an f2 value of 0.133, Trust has a medium effect on Usage 
Continuance Intention. Also , with an f2 value of 0.051, Enjoyment has a small effec t on Usage 
Continuance Intention. The rest have insignificant effect sizes on the Usage Continuance Intention.  
 
4.2.1.6 Model’s Predictive Relevance  
The predictive relevance of  the model was “assessed through the Stone –Geisser’s Q2 value” 
(Geisser, 1974; Stone, 1974). This “study followed the blindfolding procedure” via “the cross -
validated redundancy approach as recommended by ” (Hair et al., 2014, p. 183), “to calculate the 
Q2 value”. As shown in Table 8, the Q2 values of 0.267, 0.008, 0.002 , and 0. 184 indicate the 
structural path model has predictive relevance as they are all above 0.  
 
5. Discussions  
This study set forth to investigate why users continue to use Mobile Apps desp ite the privacy 
concerns it brings. The findings from the analysis  indicat e that whilst antecedent variable Gender 
influences  Privacy Concern s, antecedent variable Experience on the other hand does not influence 
Privacy Concern s. Also, Enjoyment, Privacy R isk, and Trust influence Usage Contin uance  
Intention. Also, Application Po pularity  influences Change Privacy Settings. Again , Privacy 
Concern influence Privacy Risk but does not influence  Trust. However, Enjoyment, Privacy Risk, 
and Trust do not influence Change Privacy Settings.  
Antecedent variable Experience has been established to influence Privacy Concerns (Ozdemir et 
al., 2017) . However, contrary to the hypothesized model, in this study, Experience does not 
influence Privacy Concerns. This finding is in support of the findings of (Lankton & Tripp, 2013) . 
This may be due to the fact that the respondents sampled in th is study are students who  are 
relatively young and have not gotten much of life experience hence, in adopting mobile Apps they 
do not hinge their privacy concern on experience.  
Research has proven that females have much more mobile apps privacy issues than  males, 
suggesting that females have less control over the security of their privacy than males by implicit 
social contracts  (Fogul & Nehmad, 2009) . Thus, it was hypothesized that the antecedent variable 
(Gender) in  terms of Females will have greater priva cy concerns than males . Our results indeed 
have established the fact that females are more concerned in terms of their privacy issues in support 
of the findings of (Zhang et al., 2013) . From this finding, it is apparent that the privacy concerns 
of the females who use mobile Apps should be given priority attention. This is to assure them of 
the protection of their information as such t hey will be more willing to continue usin g the Apps.  
Privacy Concerns  have been established to be a strong variable when it comes to the adoption of 
mobile apps. Thus, as hypothesized that Privacy Concern s will positively influence Privacy Risk, 
it was revealed that indeed Privacy Concerns influe nce Privacy Risk in this study thus confirming 
the findings of (Lankton & Tripp, 2013) . Also, as hypothesized that Privacy Concern will 
negatively influence Trust , it was found out that Privacy Concern does not influence Trust in  this 
study. This finding is in support of (Lankton & Tripp, 2013) . Similarly , we hypothesized that 
182
Technium Social Sciences Journal
Vol. 10, 167-188, August 2020
ISSN: 2668-7798
www.techniumscience.com 
 
 
 
 
 
Privacy Concern will positively influence  Usage Continuan ce Intention  as well as Privacy Concern 
negatively influenc ing Change Privacy Settings . The findings from the analysis revealed that both 
hypotheses are not support ed as Privacy Concerns do not influence both Usage Continuance 
Intention and Change Privacy Settings. Thus, the findings are in support of (Lankton & Tripp, 
2013). Given these findings, it can be submitted that organizations cannot rely on Privacy 
Concerns to achieve Trust, Usage Continuance Intention , and Change Privac y Setting about their 
mobile Apps.  
It has been established that Trust “plays an important ro le in predicting privacy behaviors ” (Dinev 
& Hart, 2006).  Thus, we hypothesized that Trust will negatively influence Usage Continuance 
Intention  but will positively influence Change Privacy Settings.  Our findings revealed that Trust 
has an influence on Usa ge Continuance Intention but not on Change Privacy Settings.  Thus, this 
finding is in support of (Lankton & Tripp, 2013) . This impl ies that any organization that wants to 
sustain the usage of its Apps by its cust omers should make sure that the customers are able to 
develop trust in the App. Organizations can achieve th is by making sure that the information of 
the customers is highly protected.  
Privacy Risks are predicted to  “increase the likelihood of engaging in  privacy behaviors to protect 
the opportunistic use of personal information ” (Wu et al., 2012).  These “risks should also make 
one less likely to want to continue using ” mobile apps.  Thus, we hypothesized that  Privacy Risk 
will both positively influence Usa ge Continuance Intention  and Change Privacy Settings.  The 
findings show that Privacy Risk has an influence on  Usage Continuance Intention but not on 
Change Privacy Settings. This finding is in support of (Lankton & Tripp, 2013) . Privacy risk is the 
“potential loss of control over personal information. it is clear from these findings that having 
gotten the consent of customers to use their personal information, organizations th at have such 
information must safeguard it adequately. This is what can guarantee customers’ usage 
continuance intention.  
Based on “p revious  research that motivates ” the mobile apps “model, benefits from using 
technology should decrease one’s privacy behav iors and increase continued use ”. We believe 
Enjoyment as a gain of Mobile Apps use because “Enjoyment is a major reason people use social 
networking apps ” (Xu et al., 2008) . The more enjoyable using mobile apps, “the less likely one 
will engage in privacy  behaviors because this might stifle one’s ability to make use of the app ”. 
Thus, we hypothesized that Enjoyment will positively  influence U sage Continuance Intention  but 
negatively  influence Change Privacy Settings.  The analysis indicates that Enjoyment h as an 
influence on Usage Continuance Intention but not on Change Privacy Setting. The findings are in 
support of (Lankton & Tripp, 2013 ; Sledgianowski & Kulviwat, 2009 ). Enjoyment as far as 
technology is concerned refers to the intrinsic reward  derived through the use of the technology.  
Therefore, going by these findings it is suggested that mobile Apps should be designed in such a 
way that it contains features that will enable the users to derive enjoyment while using it. In other 
words, featu res that offer hedonic value to the customers should be embedded in the mobile Apps 
in order to sustain their usage continuance intention.  
Prior research has established that the  popularity of apps has a positive effect on the behavior of 
people  to continu e using the app (Ferdous et al., 2015a) . Thus, in thi s study, we hypothesized that 
App Popularity will both positively influence  Usage Continuance Intention  and Change Privacy 
Settings . Our findings revealed that App Popularity influence Change Privacy Settings but not 
Usage Continuance Intention. This findi ng is in contrast to that of (Ferdous et al., 2015a; Shen, 
2015) . Given these fin dings, it means that App Popularity is one of the factors that can be leveraged 
in order to influence the Change Privacy Settings of the App users.  It, therefore,  implies that 
organ izations should strive to make their App popular.  
 
 
 
183
Technium Social Sciences Journal
Vol. 10, 167-188, August 2020
ISSN: 2668-7798
www.techniumscience.com 
 
 
 
 
 
6. Conclusion  and Reco mmendations  
The primary aim of this study was to investigate why users continue to use Mobile Apps despite 
the privacy concerns it brings. Our findings have revealed that whilst Antecedent Variable Gender 
influences Privacy Concern, Experience does not. A lso, Privacy Concern influences Privacy Risk 
but not Trust, Usage Continuance Intention, and Change Privacy Settings. Also, it was established 
that Enjoyment, Privacy Risk,  and Trust have an influence on Usage Continuance Intention but 
not on Change Privac y Settings. Moreover, App Popularity has an influence on Change Privacy 
Settings but not on Usage Continuance Intention.  
 In terms of contributions to theory, this study de veloped a conceptual model based on the 
APCO model which was tested with empirical d ata to establish the hypothesized relationships 
through an SEM. Thus, this study has contributed to the body of knowledge from a developing 
country context where there is a  dearth of literature regarding the phenomenon (Mobile Apps 
Privacy Concerns) being discussed.  
 In terms of contribution to policy and  practice, our findings have established that Mobile 
Apps users are more concerned with the Enjoyment they derived from u sing the Apps, yet they 
are more afraid of their Privacy Risk. Therefore, their cont inuance usage of the Apps depends 
largely on the Trust of the App. So, policymakers should make legislations that will guide Apps 
developers to make sure that the Apps bein g developed are highly protected of users’ privacy.  
 This study like any other has s ome limitations. First, although the sample data was enough 
to be generalized, yet the data collection was done from only one tertiary institution which is based 
in Accra. Thus, it becomes difficult to generalize the findings. Future studies can stratify a nd 
collect data from not only students but the working class, as well as rural dwellers. Also, not all 
the variables in the APCO model was studied. Future, studies can intr oduce other variables 
especially more demographics as antecedents’ variables.  
 
References  
[1]. Abdurraham, D. T., Owusu, A., Soladoye, B. A., & Kalimuthu, K. R. (2018). Celebrity -
Brand Endorsement: A Study on its Impacts on Generation Y -ers in Nigeria. Asian Journal 
of Scientific Research , 11(3), 415 –427. https://doi.org/10.3923/ajsr.2018.415.427  
[2]. Abdurrahaman, D. T., Owusu, A., & Bakare, A. S. (2020). Evaluating Factors Affecting 
User Satisfaction in University Enterprise Content Management ( ECM ) Systems. The 
Electronic Journal of Information S ystems Evaluation , 23(1), 1 –16. 
https://doi.org/10.3419 0/EJISE.20.23.1.001  
[3]. Alashoor, T., Han, S., & Joseph, R. C. (2017). Familiarity with Big Data, Privacy 
Concerns, and Self -disclosure Accuracy in Aocial Networking Websites: An APCO 
Model. Communications  of the Association for Information Systems , 41(October  2018), 
62–96. https://doi.org/10.17705/1cais.04104  
[4]. Bakare, A. S., Owusu, A., & Abdurrahaman, D. T. (2017). The behavior response of the 
Nigerian youths toward mobile advertising: An examination of the  influence of values, 
attitudes and culture. Cogent Bus iness and Management , 4(1), 1 –18. 
https://doi.org/10.1080/23311975.2017.1353231  
[5]. Balakrishnan, J., & Griffiths, M. D. (2018). Loyalty towards online games, gaming 
addiction, and purchase intention towar ds online mobile in -game features. Computers in 
Human B ehavior , 87(February), 238 –246. https://doi.org/10.1016/j.chb.2018.06.002  
[6]. Benamati, J. H., Ozdemir, Z. D., & Smith, H. J. (2017). An empirical test of an Antecedents 
- Privacy Concerns - Outcomes model . Journal of Information Science , 43(5), 583 –600. 
https ://doi.org/10.1177/0165551516653590  
[7]. Buck, C. (2017). Stop Disclosing Personal Data about Your Future Self. In Twenty -third 
Americas Conference on Information Systems  (pp. 1 –10). 
[8]. Byun, H., Chiu, W., & B ae, J. S. (2018). Exploring the adoption of sports bran d apps: An 
184
Technium Social Sciences Journal
Vol. 10, 167-188, August 2020
ISSN: 2668-7798
www.techniumscience.com 
 
 
 
 
 
application of the modified technology acceptance model. International Journal of Asian 
Business and Information Management , 9(1), 52 –65. 
https://doi.org/10.4018/IJABIM.2018010105  
[9]. Campbell, H. A., Altenhofen, B., Bellar, W., & Cho, K. J. (2014).  There’s a religious app 
for that! A framework for studying religious mobile applications. Mobile Media and 
Communication , 2(2), 154 –172. https://doi.org/10.1177/2050157914520846  
[10]. Christensen, C., & Pra x, P. (2012). Assemblage, adaptation and apps: Smartpho nes and 
mobile gaming. Continuum: Journal of Media & Cultural Studies , 26(5), 731 –739. 
https://doi.org/10.1080/10304312.2012.706461  
[11]. Conroy, D. E., Yang, C. H., & Maher, J. P. (2014). Behavior change te chniques in top -
ranked mobile apps for physical activit y. American Journal of Preventive Medicine , 46(6), 
649–652. https://doi.org/10.1016/j.amepre.2014.01.010  
[12]. Creswell, J. W. (2014). Research Design: Qualitative, Quantitative, and Mixed Method 
Approaches . Research Design: Qualitative, Quantitative, and Mixed Method Approaches . 
https://doi.org/10.1007/s13398 -014-0173 -7.2 
[13]. Debatin, B., Lovejoy, J. P., Horn, A. K., & Hughes, B. N. (2009). Facebook and online 
privacy: Attitudes, behaviors, and unintended conseq uences. Journal of computer -
mediated communication, 15 (1), 83 -108. 
[14]. Díez Bosch, M., Micó Sanz, J. L., & Sabaté Gauxachs , A. (2017). Typing my Religion. 
Digital use of religious webs and apps by adolescents and youth for religious and 
interreligious dialogue . Church, Communication and Culture , 2(2), 121 –143. 
https://doi.org/10.1080/23753234.2017.1347800  
[15]. Dinev, T., & Hart, P . (2006). An extended privacy calculus model for e -commerce 
transactions. Information systems research, 17 (1), 61 -80. 
[16]. Dinev, T., Mcconnell , A. R., & Smith, H. J. (2015). Informing Privacy Research Through 
Information Systems, Psychology, and Behavioral Eco nomics: Thinking Outside the 
“APCO” Box. Information Systems Research , 26(4), 639 –655. 
[17]. Eastlick, M. A., Lotz, S. L., & Warrington, P. (200 6). Understanding online B -to-C 
relationships: An integrated model of privacy concerns, trust, and commitment. Journal  of 
Business Research, 59 (8), 877 -886. 
[18]. Ferdous, R., Osmani, V., & Mayora, O. (2015a). Smartphone app usage as a predictor of 
perceived str ess levels at workplace. Proceedings of the 2015 9th International Conference 
on Pervasive Computing Technologies for Healthcare, PervasiveHealth 2015 , 225 –228. 
https://doi.org/10.4108/icst.pervasivehealth.2015.260192  
[19]. Ferdous, R., Osmani, V., & Mayora, O. (2015b). Smartphone app usage as a predictor of 
perceived stress levels at workplace. Proceedings of the 2015 9th Inte rnational Conference 
on Pervasive Computing Technologies for Healthcare, PervasiveHealth 2015 , 225 –228. 
https://doi.org/10.4108/icst.perva sivehealth.2015.260192  
[20]. Fogel, J., & Nehmad, E. (2009). Internet social network communities: Risk taking, trust, 
and pr ivacy concerns. Computers in human behavior, 25 (1), 153 -160. 
[21]. Fuller, M. A., Serva, M. A., & Benamati, J. (2007). Seeing is believing: The transitory 
influence of reputation information on E -Commerce trust and decision making: Research 
Note. Decision Scienc es, 38(4), 675 –699. https://doi.org/10.1111/j.1540 -
5915.2007.00174.x  
[22]. Godwin -Jones, R. (2011). Emerging Technologies: Mobile APPs for Langu age Learning. 
Language Learning and Technology , 15(2), 2 –11. 
[23]. Guo, Y., Bian, J., Leavitt, T., Vincent, H. K., Zalm, L. Vander, Teurlings, T. L., … 
Modave, F. (2017). Assessing the quality of mobile exercise apps based on the American 
college of sports medic ine guidelines: A reliable and valid scoring instrument. Journal of 
Medical Internet Research , 19(3), 1 –10. https://do i.org/10.2196/jmir.6976  
185
Technium Social Sciences Journal
Vol. 10, 167-188, August 2020
ISSN: 2668-7798
www.techniumscience.com 
 
 
 
 
 
[24]. Hair, J. F., Hult, G. T. M., Ringle, C. M., & Sarstedt, M. (2014). A PRIMER ON PARTIAL 
LEAST SQUARES  STRUCTURAL EQ UATION MODELING (PLS -SEM) . SAGE 
Publications Inc.  
[25]. Hing, N., Russell, A. M. T., Li, E., & Vitartas, P. (2018). Does the  uptake of wagering 
inducements predict impulse betting on sport? Journal of Behavioral Addictions , 7(1), 
146–157. https://doi.org/10.1556 /2006.7.2018.17  
[26]. Hoehle, H., & Venkatesh, V. (2015). Research article Mobile application Usability : 
Conceptualization. MIS Quarterly , 39(2), 435 –472. Retrieved from 
https://pdfs.semanticscholar.org/8171/405b2c1538c6b2eff0eb7fb87b7b2c68eeba.pdf  
[27]. Holmes, J. G. (1991). Trust and the appraisal process in close relationships.  
[28]. Lankton, N., & Tripp, J. (2013). A quantitative and qualitative study of Facebook privacy 
using the Antecedent -Privacy Concern -Outcome Macro Model. In 19th Americas 
Conference on Information Systems, AMCIS 2013 - Hyperconnected World: Anything, 
Anywhere, Anytime  (Vol. 1, pp. 180 –191).  
[29]. LaRose, R., & Rifon, N. J. (2007). Promoting i -safety: Effects of privacy warnings and 
privacy seals on risk assessment and online privacy behavior. Journal of Consumer Affairs , 
41(1), 127 –149. https://doi.org/10.1111/j.1745 -6606.2006.00071.x  
[30]. Li, L., Lee, K. Y., Chang, Y., & Yang, S. -B. (2020). Linking Privacy Concerns for 
Traceable Information and Information Privacy Protective Responses on Electric Scooter 
Sharing Platforms. I n Proceedings of the 53rd Hawaii International Conference on System 
Sciences  (pp. 851 –857). https://doi.org/10.24251/hicss.2020.105  
[31]. Li, L., Lee, K. Y., & Yang, S. -B. (2019). Do Micro -Mobility Services Take Away Our 
Privacy? Focusing on the Privacy Paradox in E-Scooter Sharing Platforms Research -in-
Progress. In Twenty -Third Pacific Asia Conference on Information Systems  (pp. 1 –8). 
[32]. Lopez -Gonzalez, H., & Griffiths, M. D. (2018). Understanding the convergence of 
markets in online sports betting. International R eview for the Sociology of Sport , 53(7), 
807–823. https://doi.org/10.1177/1012690216680602  
[33]. Martínez -Pérez, B., De La Torre -Díez, I., López -Coronado, M., & Herreros -González, J. 
(2013). Mobile apps in cardiology: Review. Journal of Medical Internet Research , 15(7), 
1–15. https://doi.org/10.2196/mhealth.2737  
[34]. Merikivi, J., Tuunainen, V., & Nguyen, D. (2017). What makes continued mobile gaming 
enjoyable? Computers in Human Behavior , 68, 411 –421. 
https://doi.org/10.1016/j.chb.2016.11.070  
[35]. Metzger, M. J. (2004). P rivacy, trust, and disclosure: Exploring barriers to electronic 
commerce. Journal of computer -mediated communication, 9 (4), JCMC942.  
[36]. Metzger, M. J. (2007). Communication privacy management in electronic commerce. 
Journal of Computer -Mediated Communication , 12(2), 335 –361. 
https://doi.org/10.1111/j.1083 -6101.2007.00328.x  
[37]. Modave, F., Bian, J., Leavitt, T., Bromwell, J., Harris III, C., & Vincent, H. (2015). Low 
Quality of Free Coaching Apps With Respect to the American College of Sports Medicine 
Guidelines: A  Review of Current Mobile Apps. JMIR MHealth and UHealth , 3(3), 1 –12. 
https://doi.org/10.2196/mhealth.4669  
[38]. Omondi, G. (2020). The sta te of mobile in Ghana’s tech ecosystem. The Ecosystem 
Accelerator programme is supported by the UK Department for Internati onal Development 
(DFID), the Australian Government, the GSMA and its members. Retrieved from: 
https://www.g sma.com/mobilefordevelopment/blog/the -state-of-mobile -in-ghanas -tech-
ecosystem/#:~:text=Ghana%20has%20the%20highest%20mobile ,is%20at%2044.8%20p
er%20cent.&text=There%20are%2013.1%20million%20active,million%20registered)%2
0mobile%20money%20accounts.  
[39]. Owusu, A . (2017). Business intelligence systems and bank performance in Ghana : The 
186
Technium Social Sciences Journal
Vol. 10, 167-188, August 2020
ISSN: 2668-7798
www.techniumscience.com 
 
 
 
 
 
balanced scorecard approach. Cogent Business & Management , 4(1364056), 1 –22. 
https://doi.org/10.1080/23311975.2017.1364056  
[40]. Owusu, A. (2019). Examining the Moderating Effects of Tim e-Since -Adoption on the 
Nexus Between Business Intelligence Systems and Organisational Performance. 
International Journal o f Technology Diffusion , 10(3), 49 –68. 
https://doi.org/10.4018/ijtd.2019070104  
[41]. Owusu, A., Agbemabiese, G. C., Abdurrahaman, D. T., & Soladoye, B. A. (2017). 
DETERMINANTS OF BUSINESS INTELLIGENCE SYSTEMS ADOPTION IN 
DEVELOPING COUNTRIES: AN EMPIRICAL ANALYSIS FROM GHANAIAN 
BANKS. Journal of Internet Banking and Commerce , 22(S8), 1 –25. Retrieved from 
http://www.icommercecentral.com  
[42]. Owusu, A., Broni Jnr, F. E., & Akakpo, P. K. (2019). PRELIMINARY INSIGHTS INTO 
THE CONCERNS OF ONLINE PRIVACY AND SECURITY AMONG MILLENNIALS 
IN A DEVELOPING ECONOMY. Journal of Theoretical and Applied Informa tion 
Technology , 15(11), 3063 –3076. Retrieved from www.jatit.org  
[43]. Owusu, A., Ghanbari -Baghestan, A., & Kalantari, A. (2017). Investigating the Factors 
Affecting Business Intelligence Systems Adoption. International Journal of Technology 
Diffusion , 8(2), 1 –25. https://doi.org/10.4018/ijtd.2017040101  
[44]. Ozdemir, Z. D., Jeff S mith, H., & Benamati, J. H. (2017). Antecedents and outcomes of 
information privacy concerns in a peer context: An exploratory study. European Journal 
of Information Systems , 26(6), 642 –660. https://doi.org/10.1057/s41303 -017-0056 -z 
[45]. Pagoto, S., Schneider, K., Jojic, M., Debiasse, M., & Mann, D. (2013). Evidence -based 
strategies in weight -loss mobile apps. American Journal of Preventive Medicine , 45(5), 
576–582. https://doi.org/10.1016/j.amepre .2013.04.025  
[46]. Pindeh, N., Suki, N. M., & Suki, N. M. (2016). User Acceptance on Mobile Apps as an 
Effective Medium to Learn Kadazandusun Language. Procedia Economics and Finance , 
37(16), 372 –378. https://doi.org/10.1016/s2212 -5671(16)30139 -3 
[47]. Rezaei, S., Sha hijan, M. K., Amin, M., & Ismail, W. K. W. (2016). Determinants o f App 
Stores Continuance Behavior: A PLS Path Modelling Approach. Journal of Internet 
Commerce , 15(4), 408 –440. https://doi.org/10.1080/15332861.2016.1256749  
[48]. Richardson, M., Cannon, S., Teich ert, L., Vance, A., Kramer, I., Barter, M., … Callahan, 
C. (2020) . Religion -focused dating apps: A Q methodology study on the uses of mutual. 
Telematics and Informatics , (June), 1 –10. https://doi.org/10.1016/j.tele.2020.101448  
[49]. Ringle, C. M., Wende, S., & Becker, J. -M. (2015 ). SmartPLS 3. Boenningstedt: 
SmartPLS GmbH , http://www.smartpls.com.  
[50]. Saunders, M., Lewis, P., & Thornhill, A. (2009). Research Methods for Business 
Students . Pearson Education  (Fifth). Pearson Education, Prentice Hall.  
[51]. Sheehan, K. B. (19 99). An investigation of gender differences in on -line privacy concerns 
and resultant behaviors. Journal of Interactive Marketing, 13 (4), 24 -38. 
[52]. Shen, G. C. C. (2015). Users’ adoption of mobile applications: Product type and message 
framing’s moderating ef fect. Journal of Business Research , 68(11), 2317 –2321. 
https:// doi.org/10.1016/j.jbusres.2015.06.018  
[53]. Sledgianowski, D., & Kulviwat, S. (2009). Using social network sites: The effects of 
playfulness, critical mass and trust in a hedonic context. Journal of computer information 
systems, 49( 4), 74 -83. 
[54]. Smith, H. J., Dinev , T., & Xu, H. (2011). Information privacy research: an 
interdisciplinary review. MIS quarterly , 989 -1015.  
[55]. Smith, H. J., Milberg, S. J., & Burke, S. J. (1996). Information privacy: Measuring 
individuals’ concerns about organizational practices. MIS Quarter ly: Management 
Information Systems , 20(2), 167 –195. https://doi.org/10.2307/249477  
187
Technium Social Sciences Journal
Vol. 10, 167-188, August 2020
ISSN: 2668-7798
www.techniumscience.com 
 
 
 
 
 
[56]. Statista. (2016). Mobile app usage. Retrieved from 
https://www.statista.com/study/11559/mobile -app-usagestatista -dossier/  
[57]. Stutzman, F., & Kramer -Duffield, J. (2010, April). Friends only: examining a privacy -
enhancing behav ior in facebook. In Proceedings of the SIGCHI conference on human 
factors in computing systems  (pp. 1553 -1562).  
[58]. Sun, Y., Fang, S., & Hwang, Y. (2019). Investigating Privacy and Information Disclosure 
Behavior in Social Electronic Commerce. Sustainability , 11(12). 
https://doi.org/10.3390/su10023311  
[59]. Venkatesh, V.,  Thong, J. Y., & Xu, X. (2012). Consumer acceptance and use of 
information technology: extending the unified theory of acceptance and use of technology. 
MIS quarterly , 157 -178. 
[60]. Vodafone Group. (2015) . Unifying communication, Annual Report 2015. Retrieved f rom 
http://www.vodafone.com/content/annualreport/annualreport15/assets/pdf/full_annual_re
port_2015.pdf  
[61]. Wong, K. K. (2013). Partial Least Squares Structural Equation Modeling (PLS -SEM) 
Techniques Usin g SmartPLS. Marketing Bulletin , 24(1), 1 –32. 
https://doi. org/10.1108/EBR -10-2013 -0128  
[62]. Wu, K. W., Huang, S. Y., Yen, D. C., & Popova, I. (2012). The effect of online privacy 
policy on consumer privacy concern and trust. Computers in human behavior, 28 (3), 8 89-
897. 
[63]. Xu, H., Dinev, T., Smith, H. J., & Hart, P. (2008 ). Examining the formation of individual's 
privacy concerns: Toward an integrative view. ICIS 2008 proceedings , 6. 
[64]. Zhang, R., Chen, J. Q., & Lee, J. C. A. (2013). Mobile commerce and consumer privacy 
concerns. Journal of Computer Information Systems , 53(4), 31 –38. 
https://doi.org/10.1080/08874417.2013.11645648  
[65]. Zimmer, J. C., Arsal, R. E., Al -Marzouq, M., & Grover, V. ( 2010). Investigating online 
information disclosure: Effects of information relevance, trust and risk. Information and 
Management , 47(2), 115 –123. https://doi.org/10.1016/j.im.2009.12.003  
[66]. Zydney, J. M., & Warner, Z. (2016). Mobile apps for science learning:  Review of 
research. Computers and Education , 94, 1 –17. 
https://doi.org/10.1016/j.compedu.2015.11.001  
 
 
188
Technium Social Sciences Journal
Vol. 10, 167-188, August 2020
ISSN: 2668-7798
www.techniumscience.com"
DataPrivacy,3015.txt,"1 
 2  
 
DATA PROTECTION CHALLENGES IN THE INTERNET OF 
THINGS ERA: AN ASSESSMENT OF PROTECTION 
OFFERED BY PDPA 2010  
 
Sidi Mohamed Sidi Ahmed1 
Ahmad Ibrahim Kulliyyah of Laws (AIKOL), International Islamic University Malaysia (IIUM) . 
(Email: kaldbkar@yahoo.com)  
Sonny Zulhuda2 
Ahmad Ibrahim Kulliyyah of Laws (AIKOL), International Islamic University Malaysia (IIUM) . 
(Email: sonny@iium.edu.my)  
 
Received  date:  04-08-2019  
Revised  date:  01-10-2019  
Accepted date : 04-10-2019    
Published date : 15-12-2019  
  
To cite this document: Ahmed, S. M. S., & Zulhuda, S. (2019).  Data Protection Challenges 
in The Internet of Things Era: An Assessment of Protection Offered by PDPA 2010.  
International Journal of Law, Government and Communication , 4(17), 01-12.  
DOI: 10.35631/ijlgc.417001  
___________________________________________ _______________________________________________  
 
Abstract : The Internet of Things (IoT) is an emerging technology of the 21st century.  It is 
described as the first real evolution of the Internet that could positively or negatively affect 
all aspects of life. The basic idea of the IoT revolves arou nd connecting things and objects 
(persons, animals, cars, trees, etc.) to the Internet and enabling them to communicate and 
then process (generate, receive, send, etc.) data about themselves and the environment 
surrounding them. Without a doubt, the IoT wi ll bring countless benefits and provide timely -
data and information about places and objects. However, the IoT, like other technologies, 
has disadvantages especially in terms of privacy and security of data.  Particularly, the IoT 
might challenge personal data protection law and misgive its ability to effectively stand in the 
rapid successive technology waves.  As the most important law relating to the protection of 
personal data in Malaysia, the Personal Data Protection Act (PDPA) 2010 could be used as 
a benchmark for assessing the adequacy of data protection law in the country.  Thus, this 
paper attempts to shed light on data protection challenges in the IoT era and then assess the 
adequacy of this Act in dealing with those challenges. The paper employs a legal doctrinal 
method to analyze the legal frameworks relevant to personal data protection. It may also use 
a comparative method to compare the PDPA with its counterparts in other countries. A study 
such as this is arguably useful and timely as Malaysia i s already embarked in the IoT 
caravan with the vision of being “the Premier Regional IoT Development Hub.”  
 
Keywords : IoT Era, Data Protection, Assessment  of PDPA  
 
 
Volume: 4 Issues: 17 [December, 2019] pp.01 -12] 
International Journal of Law, Government and Communication  
eISSN: 0128 -1763  
Journal website: www. ijlgc .com  
 
Electronic copy available at: https://ssrn.com/abstract=35135282 
  
Introduction  
The Internet of Things (IoT) is an emerging technology that plays an essential role in the 
modern age. At present time, the IoT technology can be found almost in all things 
surrounding people such as cars, houses, wearable devices  and such like.  The basic  function 
of IoT revolves around connecting things (persons, animals, cars, trees, etc.) to the Internet 
and enabling them to communicate and then process (send, receive, generate, etc.) 
information about themselves and the things they are attached to.  Wi thout any doubt, IoT 
brings countless benefits to humans because data and information generated by IoT 
technology can be used to enhance existing services and create new ones.  However, IoT, like 
other technologies, has disadvantages especially in terms of  privacy and security of data 
streaming through it.  For example, in its Report 2015 about IoT, the Federal Trade 
Commission (2015) mentioned that IoT could enable unauthorized access and misuse of 
personal information, facilitate attacks on other systems and most dangerously create safety 
risks.   Particularly, IoT could challenge personal data protection law and misgive its ability to 
effectively stand in the rapid successive technology waves.  As the most important law 
relating to protection of personal d ata in Malaysia, Personal Data Protection Act (PDPA) 
2010 could be used as a benchmark for assessing the adequacy of data protection law in the 
country. This is the objective that this paper attempts to achieve by shedding light on data 
protection challeng es in the IoT era and then assessing the efficiency of the PDPA in terms of 
dealing with those challenges .  In order to achieve this, the second section of this paper 
provides an overview of the IoT including its definition and main features.  Moreover, th e 
discussion will be extended to challenges that IoT could pose on data flow through it. The 
third section will concentrate on PDPA and its relevance to data flow in IoT devices and 
systems.  This includes discussing the applicability of the Act to IoT dat a and examining 
some important terms used in the Act. The fourth section deals with principles of data 
protection established by the Act and how they can be complied with in the IoT environment. 
Lastly, the fifth final and final section provides concluding  points that summarize the 
outcome of the discussion as well as suggestions or recommendations to strengthen and 
improve data protection law in Malaysia.  
 
Methodology   
This research is a doctrinal research depending on both primary and secondary related 
sources.  
 
It uses the doctrinal legal method to analyse  and assess data protection principles provided in 
the PDPA 2010. The aim is to assess the efficiency of the PDPA in the IoT environment. It 
also employs a comparative method to compare the pro tection offered by the PDPA to 
personal data in the IoT era with its counterparts  in other countries especially in the European 
Union (EU) region where such comparison is relevant.  
 
It is argued and believed that a study such this will positively contribu te to the field of legal  
studies as it concerns one of the most penetrated technologies that could affect everyone in 
the digital era. In Malaysian particularly, protecting personal data in the IoT environment is 
an essential matter because the country is already joined the IoT caravan with the aim of 
being “as the Regional Development Hub for IoT ” (Ministry of Science, Technology and 
Innovation (MOATI), 2014) .  
  
IoT and Data Challenges  
IoT is one of the terms used to describe the evolving technology that enables connection of 
Electronic copy available at: https://ssrn.com/abstract=35135283 
 things to the Internet. Other names include, inter alia, Internet of Anything (IoA), Internet of 
Data (IoD), Internet of People (IoP) and Internet of Everything (Io E) (Edewede Oriwoh, 
2015) . The term IoT was firstly coined in 1999 with the vision that computers would be able 
to collect data and render it into useful information without human intervention  (Jorge E. 
Ibarra -Esquer, 2017) .  At the present time, IoT is a hyped term that is pre occupying time and 
agenda of many stakeholders including some governments around the globe  (Johanna Virkki, 
2013) . Before highlighting challenges pose by IoT on data protection law, it  is important, 
especially in the view of those who are not familiar with the subject, to mention what IoT is. 
IoT encompasses three words: internet, of and things. The Internet can be defined as, “a 
world -wide broadcasting capability, a mechanism for information dissemination, and a 
medium for collaboration and interaction between individuals and their computers without 
regard for geographic location ” (Barry  M. Leiner, 2009, p. 22) . On the other hand, the term 
""thing"" in the context of IoT is defined by the Concise Oxford English Dictionary as “an 
object that one need not, cannot, or does not wish to give a specific name to ” (Catherine 
Soanes, 2003) .  It i ncludes both inanimate and animate creations. In the context of IoT, 
‘things ’ can virtually include anything such as computers, electronic devices, clothing, trees, 
houses, fridges, people, animals, trees, house  (Somayya Madakam, 201 5) (Stephan Haller) , 
cars, and so forth.  As for the IoT definition, it can be approached from two aspects, namely 
time and description. In the time phase, IoT is a term used to refer to “the point in time when 
more “things o r objects ” were connected to the Internet than people ” and that point of time 
was “sometimes between 2008 and 2009”  (Evans, 2011, p. 2) .  The other approach defines 
IoT by describing its components and functions. In this re gard, IoT is defined in  the 
Malaysian National IoT Strategic Roadmap as “Intelligent interactivity between human and 
things to exchange information and knowledge for new value creation ” (Ministry of Science, 
Technology and Innov ation (MOATI), 2014, p. 4) .  It seems that the Roadmap ’s definition 
restricts interaction in the IoT environment to humans and things. However, interaction in IoT 
is not restricted to human and things only, but there are also interactions between peop le and 
people and between things and things  (Spyros G. Tzafestas, 2018) .   Moreover, IoT also is 
described as “the general idea of things, especially everyday objects, that are readable, 
recognizable, locatable, addressable through information sensing device and/ or controllable 
via the Internet, irrespective of the communica tion means ” (Patel, 2016, p. 6122) .  Probably, 
the best description of IoT  (Leloglu, 2017)  is the one provided by the International 
Telecommunication Union (ITU) which considers IoT as “a glo bal infrastructure for the 
information society, enabling advanced services by interconnecting (physical and virtual) 
things based on existing and evolving interoperable information and communication 
technologies ” (The Internatio nal Telecommunication Union (ITU), 2013, p. 1) .  The above 
definitions clearly indicate that the idea of IoT revolves around connecting things to the 
Internet in a way that enables such things to generate and process data about themselves and 
their environment . 
 
Simply put IoT aims to connect all things in all places during all times in order to create 
smart environments where cities, transport, energy, etc., become more intelligent  (Patel, 
2016)  and (River Publishers , 2014) .  IoT devices and applications include, inter alia, health 
and fitness devices, smartphone sensors, smart grids, home, banking  (Spyros G. Tzafestas, 
2018)  and (Pepper, 2014)   and other applicable things.  The wide usage of IoT is expected to 
bring a positive impact on aspects of economy and society (education, health care, energy, 
consumers, cities infrastructure and cities), but at the same time there are some challenges 
including privacy, security and technical issues that could act as a hindrance to the IoT 
growth  (Fischer, 2015)  and implications.  As an illustration, the Malaysian Roadmap counted 
some potential adva ntages that IoT could bring to the country in terms of economy, creating 
Electronic copy available at: https://ssrn.com/abstract=35135284 
 employment opportunities, serving the research community, etc., but at the same time the 
Roadmap acknowledged the existence of some potential challenges most especially in 
security an d privacy  (Ministry of Science, Technology and Innovation (MOATI), 2014) . 
 
Regarding challenges pose by IoT on data protection, there is no doubt that billions of 
unsecured objects connecting to the Internet and generating data  including personal data will 
have an impact on the safety of data flowing in IoT devices and systems.  This can be 
supported by the fact that most of IoT applications (63% of IoT applications -5.2 billion units) 
are found to be consumer applications  (Meulen, 2017) .  The European Commission counted 
some features of IoT that could have an effect on data -these features include the followings. 
Firstly, communication between objects to objects and person. Secondly, huge amount of 
personal data coming from various sources and automated communications  (European 
Commission, n.d.) .  These features could generate some challenges ranging from unease 
compliance with data protection principles to deciding the applicable law  (European 
Commission, n.d.) .  Moreover, others asserted that IoT could affect data protection through 
facilitating identification of persons (by linking objects to specific persons), profiling (by 
combining data collected by various objects linked to specific persons) and geolocation (by 
locating  places of persons through smartphone etc.)  (Fabiano, 2017) . These manifestations or 
examples of IoT challenges to the existing data protectio n laws illustrate or support the view 
that IoT “will be a legal tsunami, the intensity and magnitude of which are unknown to date ” 
(BARBRY , 2012) . The next paragraph will discuss these challenges in light of protection 
provided  by the Malaysian PDPA 2010.  
 
An Overview of PDPA 2010  
Data or information is the essence of the digital age because the function and operation of 
every modern activities depends on it. It becomes a valuable object that many people are 
yearning to possess and use for legitimate and illegitimate purposes. The importance of data 
in the information age resulted in the enactment of data protection laws which aim to protect 
interests of data subjects and at the same time encourage and facilitate the free  movement of 
data. The efforts of protecting data in the electronic environment have started in the fourth 
quarter of the 20th century and  placed on the agenda of national and regional organisations 
such as the United Nations (UN)  (United Nations -Economic and Social Council, 1990)   the 
Council of  European  (Council of Europe, 1981)   and the Organisation for Economic Co -
Operation and Development (OECD)  (Organisation for Economic  Co-Operation and 
Development (OECD), 2013) . At this time,  there are more than 100 countries that have data 
protection laws  (United Nations Conference on Trade and Development (UNCTAD), 2016) .  
In Malaysia, personal data is protected by Personal Data Protection Act (PDPA) 2010 (Act no 
709) which came after a long wait “to regulate the processing of personal data in commercial 
transactions. ”  This Act consists of 146 sections and 11 parts and rema ins the most relevant 
law in the country that deals with protection of personal data. However, relevance of this law 
to IoT does not guarantee applicability of the Act to all types of data flow in the IoT 
environment. Thus, discussing the scope of PDPA in light of data flow in IoT devices and 
systems is necessary.  
 
According to section two of the Act, PDPA applies to any person who processes or controls 
personal data for the purpose of commercial transactions established in Malaysia or used 
equipment for pu rposes other than transition of the data through the country.  Moreover, PDPA 
has a specific set of material and territorial scopes.  The territorial scope includes two 
scenarios: processing personal data by someone who is inside or outside the country. In the 
first scenario, the Act applies to data users who are established in Malaysia in all 
Electronic copy available at: https://ssrn.com/abstract=35135285 
 circumstances (processing done by themselves their agents, etc.).  In the case of data 
processed by someone who is not established in Malaysia, the Act will apply to a ll processing 
accept “for the purposes of transit through ” the country  (S 2 (2 -b).  This could arguably have 
the power to extend the territorial scope of the Act to some data processed in the IoT 
environment by data users who are established outside the co untry.  For example, personal 
data collected by some IoT devices (wearable devices, etc.) and share with outside -data users 
such as manufacturers could come into the scope of this section because they are engaging in 
processing data for purposes other than  transiting through the country.  As a comparison, IoT 
device manufacturers “qualify as data controllers ” under the EU law because they do more 
than selling the devices and most of them engage in collecting and processing data generated 
by these devices  (Article 29 Data Protection Working Party (WP29), 2014) .  Moreover, the 
EU GDPR applies to the processing of personal data by data controllers or data processors  
who establish outside the Union region in case of  they are offering goods or services to data 
subjects in the Union or monitoring their behaviour thereto (Art. 3 (2, a & b) of the EU 
General Data Protection Regulation (GDPR) 2016/679). Nevertheless, it is necessary to 
extend the scope of local legislation to inclu de those who operate from other jurisdictions in 
the modern borderless world. In this regard, it would be advised to interpret the term “uses 
equipment in Malaysia for processing the personal data ” in a way that covers those who offer 
goods or services or control behaviour of data subjects in Malaysia, as GDPR does (Art. 3 (2, 
a & b).  This, if done, could arguably enable PDPA to cover a variety of data processed in the 
IoT environment by data users who establish outside the country.  
 
However, the Act on th e material scope applies to personal data processed in “commercial 
transactions. ”  According to this, the terms processing of personal data and commercial 
transactions play a vital role in defining the material scope of the Act.  Processing includes 
most d ealing with personal data such as collecting, recording, storing and so forth (s 4 of 
PDPA 2010).  The broadening definition of processing “is deliberately done to cover all the 
activities of a data user ” (Abu Bakar Munir, 2012)  and this could be useful in the IoT 
environment where data is being processed in new ways.  As for personal data, the Act (s 4) 
defines it as any information “relates directly or indirectly to a data subject, who is identified 
or identifiable from that in formation or from that information and other information in the 
possession of the data user, ” including sensitive data (data related to physical or mental 
health, political, religions, etc.). Moreover, the term “commercial transactions ” covers “any 
transac tion of commercial nature, whether contractual or not, which includes any matter 
relating to supply or exchange of goods or services …” (s 4 of PDPA). Currently, there are 
various classes of data users under PDPA including communications, banking and financial 
institutions, private health and education, insurance, transportation, etc.  (S 2 of the Personal 
Data Protection (class of users) Orders 2013). It is fact that IoT technology penetrates in most 
sectors including commercial sectors and thus there is no doubt that PDPA could apply to 
some IoT data processed in its scope. As an example,  mobile banking applications installed in 
smart phones  (Maybank2u Mobile, 2018)  which are part of IoT like smart home  (Spyros G. 
Tzafestas, 2018)   could be taken as an example of IoT personal data protected by PDPA.  
 
More than anything else, the term “commercial transactions ” is one of the important terms 
that leaves its impression on the Act.  On one hand, the term could cause confusion because 
distinction between commercial and non -commercial activities could be uneasy in some 
circumstances  (Abu Bakar Munir, 2012) . On the other hand, it has the power to exclude 
variety of personal d ata from the scope of the Act. In the context of IoT, “commercial 
transactions ” is arguably the most important term that could affect or lessen the ability of 
PDPA to comprehensively protect personal data flow in the IoT environment.  This is because 
Electronic copy available at: https://ssrn.com/abstract=35135286 
 some IoT consumer -applications used for personal purposes may not be covered by the 
phrase “commercial transactions ” and such applications constitute a great part of the IoT 
applications  (Meulen, 2017) . Experts have opined that PDPA  should be amended to cover 
personal data in both non -commercial and commercial transactions to effectively protect 
personal data in Malaysia.   
 
Also, non -applicability of the Act to data processed by governmental bodies (s 3 of PDPA) is 
another important  issue that could lessen the efficiency and capability of PDPA to adequately 
coexist with waves of new technology such as IoT. Hence, the wide range exemption and 
extent of application of the data protection law on government agencies is an important issue  
to be addressed here. Regarding applicability of data protection law to the government, the 
approach taken by the EU law could be better to be followed by the Malaysian government.  
For example, GDPR clearly mentions that it “applies to the processing of personal data ” 
including data processed by governmental and non -governmental bodies. However, the 
GDPR exempts personal data processed by competent authorities, data processed for personal 
or household purposes among others. As for the PDPA, section 45 exe mpts some types of 
personal data from provisions of the Act or some of them. Moreover, Section 46 authorizes 
the Minister to make new exemption. As an illustration, the exemption includes data 
processed for personal or family use purposes, taxation and cri minal purposes, statistics or 
research purposes, order or judgments, regulatory functions, journalism, literary or artistic 
purposes and data related to physical or mental health (s 45 of PDPA).  Unlike the various 
exemptions made by PDPA, the non -applicab ility of the Act to the Federal and States 
Governments could have far -reaching on data protection because it will exclude the biggest 
data user (the Governments) from the scope of the Act.  The Act does not define the term 
government but it usually refers to “the whole class or body of office -holders or functionaries 
considered in the aggregate, upon whom devolves the executive, judicial, legislative, and 
administrative business of the state”  (Henry Campbell Black, 1968, p. 824)  This means that 
the term government in the Act could include ministries, public hospital, schools or 
universities, and such like. In fact, the Personal Data Protection (Class of Data Users) Orders 
2013 subject private healthcare, schools and hig her educational institutions to registration (s 2 
(4 &7), but the Orders are silent about their public counterparts. This means that Act 
considers them as governmental bodies.  For the sake of personal data protection, PDPA 
could be extended to include per sonal data processed by the government, but at the same time 
necessary exemption, shall be made as the EU Regulation does (Art. 2 (1 &2) of GDPR).     
 
As the discussion revealed, it can be argued that the Act may not apply to a variety of 
personal data pr ocessed in the IoT environment because such data might be processed out of 
the commercial atmosphere. For example, individuals who buy IoT wearable devices which 
are used to track some aspects of health or fitness data, etc.,  (Pepper, 2014)  from the market 
and voluntary use them to generate and store data about themselves.  When those individuals 
use these IoT devices and upload their data to the Internet or mobile applications, there is 
apparently no commercial transactio ns between those individuals and the manufacturers of 
these devices who can be in other jurisdictions.  In this regard, protection offered by PDPA 
can be considered as insufficient because it could not provide protection to personal data in 
scenarios like the above one.   
 
IoT and Data Protection  Principles  
Data protection laws around the world establish some principles to be followed in the course 
of processing personal data.  These principles mention rights and duties of parties involved in 
the processing. For example, section 5 of PDPA sets out 7 principle s including the general, 
Electronic copy available at: https://ssrn.com/abstract=35135287 
 notice and choice, disclosure, security, retention, data integrity and access principles (s 6 -12). 
These principles are backed by a system of punishments including fine (up to three hundred 
thousand ringgit) or imprisonment (up to 2 years) or both (s 5 (2) of PDPA). In the following 
subsections, these principles will briefly be discussed in order to highlight the difficulty of 
implementing them in an automated environment such as the IoT one.   
 
  General Principle  
As mentioned in s ection 6, this principle obliges the data user to take the consent of the data 
subject before processing personal data related to the latter. This section also restricts the 
processing of personal data to lawful, necessary and non -excessive activities.  Ad ditionally, 
this Principle makes a distinction between processing personal data and sensitive personal 
data which is subjected to more restrictions mentioned in section 40 of the Act. Taking 
consent of the data subject is a central issue in data protection  law.  Apart from mentioning 
that consent can be withdrawn (s 38), PDPA refers to data subject consent in various matters 
including disclosure of personal data, processing of sensitive personal data and transferring 
data outside the country  (Ss 8, 40 (1, a ) and 129 (3, a). The Act does not define the term of  
consent, but the subordinate regulations mention that consent can be obtained “in any form 
that such consent can be recorded and maintained properly by the data user” ( S 3 (1) of the 
Personal Data Prot ection Regulations (PDPR) 2013).   This means that consent can “be oral 
and implied”  (Abu Bakar Munir, 2012)   In the IoT context, using IoT wearable devices such 
as health and fitness devices (Pepper, 2014)   by individuals could be considered as consent in 
the meaning of  PDPA.   
 
The Notice and Choice  
The second principle in the Ac t is the principle of Notice and Choice (s 7).  This Principle 
obliges the data user to inform the data subject, as soon as possible, through written notice 
about the description of the data, the purposes of collection and processing of the data and the 
third party whom the data being or might be shared with, in addition to matters related to 
correction, restriction etc., and the means used therein.  It is important to note that the 
principle of notice and choice is an important principle especially in the online world.  
Linguistically, the term notice and choice mean “a written statement giving information or 
news” and “an act of choosing between two or more…things”  (Oxford Wordpower, 1999, p. 
507 & 123)  respectively.  In data protection atmosphere, while the term notice consists of “a 
presentation of terms,” choice is “an action signifying acceptance of” the terms presented in 
the notice  (Robert H. Sloan, 2013, p. 3) .  Complying with this principle in the IoT 
environment could be a challenging ed task. As an illustration, this principle makes it 
compulsory for the data user to provide written notice to the data subject including 
information about the data and the direction  that it will take in addition to contact of the data 
user( S 7 of PDPA and s 4 of the Data Protection Regulations 2013 ). In the IoT environment, 
however, identifying the data user could be a challenging task as in some cases (wearable 
devices, etc.) the da ta subject is dealing with devices that connected to the virtual world and 
has no connection with the manufacturers or suppliers of these devices.  In such scenarios, 
who shall provide the written notice and who is be contacted: the devices, manufacturers or 
suppliers? These and other challenges imposed by the IoT on data protection laws need a 
suitable solution.  
 
Disclosure Principle  
Section 8 of the Act spells out the context of disclosure Principle which prohibits revealing 
personal data in cases other than the agreed cases without the consent of the data subject 
especially in cases mentioned in section 39 of the Act.   
Electronic copy available at: https://ssrn.com/abstract=35135288 
  
 
Security Principle  
The Principle of Security is another crucial principle in the Act. It obliges the data user to 
“take practical steps to protect the personal data from any loss, misuse, modification, 
unauthorized or accidental access or disclosure” (s 9 (1, a) of PDPA).  A dditionally, this 
Principle mentions that the security measures should consider the nature of the personal data, 
its places or locations, storage, persons accessed to it, and the consequences of any security 
breach and such like. Both the PDPR 2013 (s 6) a nd the Personal Data Protection Standard 
(PDPS) 2015 (s 4) establish practical steps to be implemented in securing personal data. 
Without delving deeply into details of these rules, it  is argued that the security measures 
required by the security principle  are not easy to be achieved in the IoT environment because 
of countless flow of data from objects. Technically, IoT devices have a limited physical, low 
powers and computational capacity which in turn make adding security measures to them are 
not easy  (Pepper, 2014) . Another element of security that must be observed by the data user 
is “the place or location where the personal data is stored” (s 9 of PDPA). Practically 
speaking, complying with these could be a challenging matte r because some IoT devices will 
be put in outdoor environments that are subject to physical attacks  (Mohamed Abomhara, 
2015)  such as theft, vandalism and so forth.  Accordingly, the IoT challenges to security of 
personal data i s a major concern because the data users who are obliged to take those 
measures are not easy to be identified. Needless to say, that both physical and technical 
security are important in the IoT context because the consequences of any security breach 
may n ot only result in revealing personal data, but more dangerously IoT could be used to 
shut down transportation systems, alter medical devices, destroy industrial components, etc.,  
(Oltsik, 2014)  which in turn could have an effec t on people lives . 
 
Retention Principle  
This principle deals with the necessity of deleting personal data after the collection and 
usage.  The retention principle is an obligation on the data user to destroyed or permanently 
delated the data. The execution of data retention is done in accordance  with guidelines spelt 
out in the PDPS 2015 (s 6) constituting 7 steps to be taken by the data user in that regard.  In 
the case where PDPA applies to personal data processed in commercial transactions executed 
in the IoT environment, the data user shall c omply with all rules of Retention Principle either 
in the PDPA or its subsidiary regulations.     
 
Data Integrity & Access Principles  
Data Integrity & Access Principles deal  with the accuracy of data and the right of the data 
subject to access his personal data. For example, the Data Integrity obliges the data user “to 
take reasonable steps to ensure” the accuracy of the personal data in fulfilment of its purpose, 
etc. (s 11 of PDPA).  The reasonable steps required by this Principle are explained in  the 
PDPS 2013 (s 7).  In general, the Access Principle establishes the right of the data subject to 
access his personal data held by the data user in accordance with principles and guidelines set 
by PDPA and its subsidiary regulation  (Ss 12, 30 -37 of PDPA  and 9 -11 of PDPR 2013).  
 
Like the other data protection principles, the Access and Integrity principles oblige the data 
user to take specific steps to ensure the integrity and accessibility of the personal data. 
However, the implementation of these princi ples requires identifying data users and 
processors and communicating with them.  In the IoT environment, however, data users and 
processors in some scenarios (wearable devices, etc.) may neither be identified nor 
communicated with. That is, because, unlik e the EU perspective where the IoT device 
Electronic copy available at: https://ssrn.com/abstract=35135289 
 manufacturers is considered as data controllers on the ground that they are engaging in 
processing data generated by the devices  (Article 29 Data Protection Working Party (WP29), 
2014) , the Malaysian regulatory body does not deal with novel issues such as these.  
 
From the above discussion, it seems that current protection provided by PDPA is insufficient 
in the IoT environment. The inadequacy of the Act to provide enough protection to data flow 
in IoT could arguably be attributed to two reasons. The first one is the narrow scope of the 
Act which only applies to personal data processed in commercial transactions. The second 
reason is inability of the Act in its current version to cope wi th the rapid technology used in 
processing personal data. For example, PDPA applies primarily to data users who collect 
personal data in commercial transactions environment.  However, the concept of data users is 
extended by the IoT to new stakeholders who  collect personal data in various ways. In the EU 
region, device manufacturers, social platforms, application developers, etc., are considering 
data users  (Article 29 Data Protection Working Party (WP29), 2014) . By contrast, PDPA is 
not explicitly extended to cover those newcomers.  
 
Conclusion and Policy Recommendations  
This paragraph attempts to summarise the above discussion and then provide some 
recommendations that could improve protection of data or at lea st open the door for further 
discussion and suggestions.  
 
Conclusion  
The IoT brought challenges to data protection law and such challenges should be recognized 
in order to take proactive reforms as steps towards solutions. This paper was dedicated to the 
assessment of protection offered by PDPA to data flow in the IoT envir onment. It started with 
a general introduction highlighting the role that the IoT currently plays in today life and its 
challenges to the legal systems especially those provisions relating to data protection. It then 
defined the main terms such as the Inte rnet of things. As the objective of this paper is to 
assess protection provided by this Act to personal data processed in the IoT environment, the 
applicability of the PDPA to the IoT was discussed. In this regard, it was found that the term 
“commercial tr ansactions ” will affect and lesson the ability of the Act to adequately protect 
all data processed in the IoT environment as some IoT devices used for personal or family 
purposes may not be covered by such term. It is also mentioned that, the Act excluded data 
processed by governmental bodies (Both the Federal Government and the States 
Governments) from its applications and exempted various types of personal data processed 
by non -governmental entities. Apart from discussing the applicability of the Act to d ata flow 
in the IoT environment, the paper also discussed the difficulty of complying with data 
protection principles mention in the Act in the IoT age where data continuously flows. It 
concludes that there is a need for reforming and updating PDPA to enab le it to provide 
suitable protection to personal data in the IoT environment. It is mentioned that the approach 
taken by EU to protect personal data could be flowed by the Malaysian legislators to improve 
data protection in the country.   The next section will provide some suggestions that is 
believed to be useful and necessary for the interests of both data subjects and data users or in 
other words, the whole society.      
 
 Recommendations  
As evidently seen above on the analysis of the Act, some personal data processed in the IoT 
environment may not be covered by the Act which is considered the most important Act 
dealing with personal data protection in the country. This could have an effect  on individuals 
whom personal data are not protected by this supreme Act or probably on the interests of the 
Electronic copy available at: https://ssrn.com/abstract=351352810 
 country because protection of personal data is important for its free movement or exchange 
with other countries.  The above discussion revealed tha t the term “commercial transactions ” 
have the power to exclude all personal processed outside the commercial atmosphere.  For 
the sake of data protection, this paper recommends extending protection to personal data 
processed in both commercial and non -comm ercial transactions. Another important thing that 
this paper recommends is the extension of the scope of PDPA to cover personal data 
processed by the Federal and States Governments of the country.   This extension, if done, 
will grant some rights to data s ubjects and help prevent misuse of personal data processed by 
data users (here the Government employers) and also harmonize data protection law in the 
country in both public and private sectors.  The extension of the scope of the Act to data 
processed by th e Governments shall consider the nature of the Governments and their 
functions  and therefore making necessary exemption. Moreover, this paper also recommends 
extending the concept of the data user and data processor to cover new stakeholders in the 
IoT env ironment such as device manufacturers, application developers and such like.  
 
References  
Abu Bakar Munir, S. H. (2012). Personal Data Protection Act: Doing Well by Doing Good. 
Malayan Law Journal, 1 , lxxxvi.  
Article 29 Data Protection Workin g Party (WP29). (2014). Opinion 8/2014 on the Recent 
Development on the Internet of Things. ec.europa.eu. Retrieved 9 7, 2018, from 
Article 29 Data Protection Working Party (WP29), “Opinion 8/2014 on the Recent 
Development http://ec.europa.eu/justice/artic le-29/documentation/opinion -
recommendation/files/2014/wp223_en.pdf  
BARBRY, E. (2012, 3rd Q,). The Internet of Things, Legal Aspects What Will Change 
(Everything)…”. Digiworld Economic Journal, 87 , p. 83.  
Barry M. Leiner, R. E. (2009). A Brief History of t he Internet. ACM SIGCOMM Computer 
Communication Review, 39 (5), 22.  
Catherine Soanes, a. s. (Ed.). (2003). The Concise Oxford English Dictionary (11 ed.). 
Oxford University Press.  
Council of Europe. (1981). Convention for the Protection of Individual with regard to 
Automated Processing of Personal Data. Strasbourg: European Treaties Series -No. 
108. 
Edewede Oriwoh, M. C. (2015). Things in the Internet of Things: Towards a Definition. 
International Journal of Internet of Things, 4 (1), 1 -5. 
European Commission. (n.d.). IoT Privacy, Data Protection, Information Security . Retrieved 
January 30, 2019, from semanticscholar.org/paper: 
https://www.semanticscholar.org/paper/IoT -Privacy -%2C -Data -Protection -%2C -
Information/e4f35abaff323159dd66997dbea6d72d 5c0a73cc  
Evans, D. (2011). The Internet of Things – How the Next Evolution of the Internet is 
Changing Everything . (Cisco Internet Business Solutions Group (IBSG)) Retrieved 
January 29, 2019, from 
https://www.cisco.com/c/dam/en_us/about/ac79/docs/innov/IoT _IBSG_0411FINAL.p
df 
Fabiano, N. (2017). Internet of Things and the Legal Issues related to the Data Protection 
Law according to the new European General Data Protection Regulation. Athens 
Journal of Law, 3 (3), pp. 203 -204. 
Fischer, E. A. (2015, October 17) . The Internet of Things: Frequently Asked Questions . 
Retrieved 9 7, 2018, from GOVLAB: http://thegovlab.org/the -internet -of-things -
frequently -asked -questions/  
Electronic copy available at: https://ssrn.com/abstract=351352811 
 Henry Campbell Black, M. A. (1968). Black's Law Dictionary  (4 ed.). (1968, Ed.) (West 
Publishing  Co. 
Johanna Virkki, L. C. (2013). Personal Perspectives: Individual Privacy in the IOT. Advances 
in Internet of Things, 3 (2), 21. doi:10.4236/ait.2013.32003  
Jorge E. Ibarra -Esquer, F. F. -N.-R.-V. (2017). Tracking the Evolution of the Internet of 
Things Co ncept Across Different Application Domains. Sensors, 17 , 1379.  
Leloglu, E. (2017). A Review of Security Concerns in Internet of Things. Journal of 
Computer and Communications, 5 (1). doi:http://dx.doi.org/10.4236/jcc.2017.51010  
Maybank2u Mobile. (2018, Nove mber 25). Maybank2u.com is now on your mobile phone!  
Retrieved from maybank2u.com: 
https://www.maybank2u.com.my/mbb_info/m2u/public/personalDetail04.do?channelI
d=&cntTypeId=0&cntKey=ACC08.03.06&programId=ACC08.03 -
MobileBanking&chCatId=/mbb/Personal/ACC -Accounts  
Meulen, R. v. (2017, February 7). Gartner Says 8.4 Billion Connected ""Things"" Will Be in 
Use in 2017, Up 31 Percent From 2016 . Retrieved January 31 , 2019, from 
gartner.com: https://www.gartner.com/en/newsroom/press -releases/2017 -02-07-
gartner -says-8-billion -connected -things -will-be-in-use-in-2017 -up-31-percent -from -
2016  
Ministry of Science, Technology and Innovation (MOATI). (2014). National Internet of 
Things (IoT) Strategic Roadmap  (1st publication ed.). Kuala Lumpur: MIMOS 
Berhad.  
Mohamed Abomhara , G. M. (2015). Cyber Security and the Internet of Things: 
Vulnerabilities, Threats, Intruders and Attacks. Journal of Cyber Security, 4 , 65–88. 
Oltsik, J. (2014). White Paper. The Internet of Things: A CISO and Network Security 
Perspective . The Enterprise  Strategy Group, Inc.  
Organisation for Economic Co -Operation and Development (OECD). (2013). Privacy 
Framework”: OECD Guidelines Governing the Protection of Privacy and 
Transborder Flows of Personal Data (2013).  Retrieved 5 3, 2018, from cecd.org: 
https:// www.oecd.org/sti/ieconomy/2013 -oecd -privacy -guidelines.pdf.  
Oxford Wordpower.  (1999). Oxford University Press.  
Patel, K. K. (2016). , “Internet of Things -IOT: Definition, Characteristics, Architecture, 
Enabling Technologies, Application & Future Challenges. International Journal of 
Engineering Science and Computing (IJESC), 51 (5). 
Pepper, S. R. (2014). Regulating t he Internet of Things: First Steps Toward Managing 
Discrimination, Privacy, Security, and Consent. Texas Law Review, 93 , 95-117. 
River Publishers . (2014). River Publishers Series in Communication Internet of Things - 
From Research and Innovation to Market Deployment.  (O. V. Friess, Ed.) Aalborg, 
Denmark: River Publishers.  
Robert H. Sloan, R. W. (2013). Beyond Notice and Choice: Privacy, Norms, and Consent. 
Journal of High Technology Law . 
Somayya Madakam, R. R. (2015). “Internet of Things (IoT): A Literature  Review. Journal of 
Computer and Communications,, 3 , 164 -173. 
Spyros G. Tzafestas. (2018). “Ethics and Law in the Internet of Things World. Smart Citie, 
1(1), 99. doi: https://doi.org/10.3390/smartcities1010006  
Stephan Haller. (n.d.). The Things in the Internet of Things (A paper presented as a poster at 
the Internet of Things Conference 2010, Tokyo, Japan).  Retrieved January 29, 2019, 
from Researchgate: 
https://www.researchgate.net/publication/228488111_The_Thin gs_in_the_Internet_of
_Things  
Electronic copy available at: https://ssrn.com/abstract=351352812 
 The Federal Trade Commission (FTC) Staff Report. (2015). Internet of Things: Privacy & 
Security in a Connected World.  Retrieved AFebruary 17, 2019, from 
https://www.ftc.gov/system/files/documents/reports/federal -trade -commissio n-staff-
report -november -2013 -workshop -entitled -internet -things -privacy/150127iotrpt.pdf  
The International Telecommunication Union (ITU). (2013). Recommendation ITU -T 
Y.2060,Overview of the Internet of things,.  Geneva: International Telecommunication 
Union  (ITU) .  
United Nations Conference on Trade and Development (UNCTAD). (2016). United Nations 
Conference on Trade Data Protection Regulations and International data flows: 
Implications for Trade and Development.  United Nations Publication.  
United Nations -Economic and Social Council. (1990). Revised Version of the Guidelines for 
the Regulation of Computerized Personal Data Files” (E/CN.4/1990/72.). UN Digital 
Library. Retrieved 9 7, 2018, from 
https://digitallibrary.un.org/record/85149/files/E_CN.4_1990_72 -EN.pdf 
 
 
Electronic copy available at: https://ssrn.com/abstract=3513528"
DataPrivacy,3016.txt,"The Hidden Shadow of Coronavirus on Education in Developing Countries 
Online Learning Journal – Volume 25 Issue 1 – March 2021                    5 269   
The Hidden Shadow of Coronavirus on  
1 ountriesC eveloping D ducation in E 
 
Zuheir N Khlaif and Soheil Salha 
Faculty of Educational Sciences and Teachers ’ Training, An Najah National University,  
Nablus, Palestine 
 
Shahid Fareed 
Department of Computer Science, Bahauddin Zakariya University, Multan, Pakistan 
 
Hadi Rashed 
Business Communication Department, Kabul University, Kabul, Afghanistan 
 
Abstract 
The aim of this paper is to investigate the challenges associated with emergency remote teaching 
in the developing countries of Palestine, Libya, and Afghanistan, as reported by middle-school 
students, their parents, and teachers. These countries have been struggling with an unstable and 
violent situation for decades. Semi-structured interviews were conducted with 60 participants from 
the three countries and 60 online classes were observed. Findings revealed that COVID-19 
widened the digital gap among students and families , which created challenges in terms of online 
class attendance. In addition, violation of students’ and parents’ digital privacy emerged as another 
key challenge to emergency remote teaching. However, teacher presence and timely feedback in 
synchronous online sessions strengthened students’ engagement within the emergency remote 
teaching environment. Overall, emergency remote teaching during the COVID-19 crisis deepened 
inequities across students and infringed upon the digital ethics of students, teachers, and parents.   Keywords: COVID-19, Coronavirus, digital inequity, digital privacy  
 Khlaif, Z.N., Salha, S., Fareed, S., & Rashed, H. (2021). The hidden shadow of the coronavirus 
on education in developing countries. Online Learning, 25(1), 269-285. https://doi.org/10.24059/olj.v25i1.2287 
  
 
 
1 The motivation for the current study was the findings of Khalif and Salha’s (2020) brief report  regarding the  
unanticipated challenges of education during the COVID -19 pandemic. For this paper, the  researchers used the same 
data from the same participants , but developed different research questions on digital  privacy, gender inequity, and 
the specific impact of t he digital  divide.  The Hidden Shadow of Coronavirus on Education in Developing Countries 
Online Learning Journal – Volume 25 Issue 1 – March 2021                    5 270  The Hidden Shadow of Coronavirus on Education in Developing Countries 
Emergency Remote Teaching (ERT) is an unplanned and sudden shift from traditional in-
person education to remote teaching, using technology to deliver teaching and learning materials 
and activities, due to natural disasters, economic crises, violence, and p andemics. In the past, 
individual countries have used technology to teach online for a limited time in response to crises 
and school shutdowns (Czerniewicz et al., 2019; Holzweiss et al., 2020; Khlaif & Salha, 2020; 
Shraim & Khlaif, 2010; Traxler et al., 2019). This year, ERT was implemented in countries across 
the world to fight the outbreak of Coronavirus (Johnson et al. , 2020; Khlaif et al., 2020). ERT is 
unlike normal online learning: while typical online courses take 3–6 months to design, develop, 
and deliver to students, ERT materials or lectures were originally designed for face-to-face 
delivery, and their quality in the remote context can be low (Khlaif & Salha, 2020). In developing 
countries in particular, students may also struggle with the digital equity and accessibility of 
educational resources delivered in the context of a crisis (Czerniewicz et al., 2019; Jones & 
Bridges, 2016; Khlaif et al., 2020).  
In this study, we investigate the challenges associated with emergency remote teaching in 
the developing countries of Palestine, Libya and Afghanistan, as reported by middle-school 
students, their parents, and teachers. In the introductory sections below, we first describe our 
conceptual framework, which is centered in digital inequity and digital privacy. Next, we describe 
the context of the three countries under study and lay out our key research questions.  
 Conceptual Framework 
The dimensions and determinants of the impact of COVID-19 on educational systems 
within developing countries are wide and varied. In the context of digital learning, however, two 
issues may represent key challenges to students and their families in developing c ountries: digital 
inequity and digital privacy.  Below, we review these two issues in more detail, with particular 
reference to the three developing countries  under study. 
Digital Inequity 
Digital equity is conceptualized in the framework of resource distribution, technical 
knowledge and skills, and the impact of  Information and Communication Technology  (ICT) use 
(Yuen et al., 2018). Willems  et al.  (2019) defined digital equity as access to hardware and software 
and connectivity to the internet to create, share, and exchange high-quality content in a suitable 
language. Resta et al. (2018) suggested that there are five dimensions of digital equity: hardware , 
software and connectivity to the internet, high-quality digital content in local languages, creation 
and sharing of digital content, educators who know technology, and high-quality research and 
application. 
In recent years, with the increasing use of ICT in education, digital inequity has begun to 
receive more attention from researchers  (Kurea, 2015; Yuen et al., 2016). With the shift to remote 
learning during the COVID-19 crisis, digital inequity has become an even more critical topic (Hall 
et al., 2020; Khlaif et al., 2020). For example, a recent U.S. study reported that school closures due 
to the spread of COVID-19 prevented children with disabilities from continuing their learning. 
These children were from the poorest families who did not have access to technology  (Collis & 
Vegas, 2020).  
In developing counties, the digital divide may be even more stark. Shraim and Khlaif 
(2010) highlighted that a key challenge of implementing e-learning in the Palestinian educational The Hidden Shadow of Coronavirus on Education in Developing Countries 
Online Learning Journal – Volume 25 Issue 1 – March 2021                    5 271  system was students’ differential access to devices and high-speed internet. A significant aspect of 
the digital divide in developing countries is the difference between men ’s and women’s access to 
and use of technology (Mariscal et al., 2019; Singh, 2017). For example, Elaiess (2017) found a 
gender divide in Libya, with women and girls having less access to technology, such as the internet, 
compared with men because of cultural and social issues. Internet cafes for only men are not 
accepted.. The digital gap in gender is more evident for countries in the global south (such as the 
three countries under study) than for countries in the global north (such as the U.S.). 
Digital Privacy 
Digital privacy refers to the protection of an individual ’s information and communication 
that is used or created while using the internet on a computer or personal device (Robertson  et al., 
2019). It is worth noting that the protection of digital privacy requires the knowledge, the technical 
practices of online security, institutional privacy procedures , and the skill to conduct several 
processes for protecting online data (Li, 2018). Digital privacy is often understood as  an individual 
right that must be respected and secured even when people have different objectives and desires 
(Ahmed & Booth, 2016). With advancements in technology, individuals can communicate and 
leverage data more easily , but are also at increased risk of hackers violating their privacy and data      
confidentiality (Lustgarten et al., 2020). 
Despite the importance of digital privacy, students are often unaware of its meaning or 
value. For example, Lorenz et al. (2012) conducted a survey of sixty-one participants from Estonia, 
which examined the relationship  between students’ awareness of privacy and their level of 
participation in the online learning environment. The findings of the study revealed that students 
in the online learning environment did not distinguish between educational social interaction and 
their privacy. The researchers considered student lack of awareness to be a threat to the student ’s 
privacy (Lorenz et al., 2012). Similarly, Siemens et al. (2013) conducted research on Canadian 
students in a master’s degree program to explore their perceptions of privacy and its impact on 
their engagement within an e-learning environment. The findings of the study revealed that 
students are not aware of their privacy in the online forums.  
Students may be unaware of digital privacy issues because they have not been formally 
taught about them. Kumar et al. (2019) conducted nine focus group sessions with 25 teachers in 
elementary school settings across three regions in the north of the U .S. to ascertain the meaning of 
digital privacy and security. The findings of their study revealed that teachers rarely teach their 
students about digital privacy and security while using technology . However, teachers and parents 
themselves may have little understanding about digital privacy. Citizens’ concerns and behaviors 
in terms of digital privacy may also be influenced by political and cultural factors. For example, 
Fahey and Hino (2020) point out that during the COVID-19 pandemic, some countries (such as 
France, South Korea, Iran, and Qatar) have taken a “data first” approach to contact tracing and 
other public health measures, while other countries (such as Germany, Italy, Japan, and the U.S.) 
have taken a “privacy first” approach.  
Studies of digital privacy have not focused on developing countries. However, a unique set 
of political and cultural factors impact education in developing countries. For example, in 
Afghanistan, the education system has long struggled with issues revolving around  culture, lack 
of security, and distance; these challenges continue today (Arooje & Burridge, 2020; Rasmussen 
& Kelly , 2016). Many of these challenges are further affected by transparency and quality The Hidden Shadow of Coronavirus on Education in Developing Countries 
Online Learning Journal – Volume 25 Issue 1 – March 2021                    5 272  assurance. Thus, levels of awareness and concern about digital privacy in education could be quite 
different from those found in prior studies. 
Context of the Three Countries Under Study 
The three developing counties under study are Palestine, Libya, and Afghanistan. All three 
countries have been struggling with unstable situations and continuous violence for decades, and 
thus have implemented ERT many times before the COVID-19 pandemic. Although the three 
countries are quite distinct, all three share a similar security situation, have experience in education 
under crisis, used online learning before the COVID-19 pandemic, transformed education into 
ERT during the COVID-19 pandemic, and have the technological initiatives to integrate 
technology into teaching and learning. Below, we provide context regarding the educational 
system of each country. 
Libya 
Education policy in Libya is managed from the top down and is influenced by the overall 
political system of the country. The education system has changed frequently since the Arab Spring 
in 2011 (Aloreibi & Carey, 2017). The World Bank (2013) reported significant success in 
enhancing the literacy rate in Libya by increasing the budget for education and making it free and 
accessible for all Libyans and implementing new programs to improve education. Conversely, 
Gadour (2011) argued that the Libyan educational system is still not capable of producing 
graduates with the necessary skills to compete in a world where knowledge and technology are 
essential for development in different aspects of the society. Hbaci et al. (2020) found that Libyan 
educators believe that using technology is still a difficult task, and that schools need computers, 
internet access, training, and financial support to improve the teachers ’ technological competencies 
in teaching and learning. Libyan students may also be underrepresented in terms of their 
participation in global learning platforms: a recent study of Arab users of  the MITx online learning 
platform found that only 111 per million Libyans used the platform, compared to 1,552 per million 
in the United Arab Emirates and 383 per million in Egypt ( Ruipérez-Valiente et al., 2020). 
Palestine 
As an occupied territory, Palestine is subject to a variety of ongoing restrictions (such as 
checkpoints and other travel restrictions), as well as periodic “security spikes” characterized by 
unrest, violence, and travel lockdowns, which negatively impact its educational system. For 
example, students and teachers may be unable to pass through checkpoints in order to reach school 
in time, or school trips may be canceled due to travel restrictions (Traxler et al., 2019). A recent 
study conducted by Bruck et al. (2019) confirmed the continued conflict in Palestine decreased the 
probability to pass the final exam, and the opportunities to get an admission from the university to 
pursue higher education. To mitigate the negative impact of restriction of mobility o n their 
students’ education, many Palestinian teachers have turned to digital technologies (Traxler et al., 
2019). Furthermore, the Ministry of Education (hereafter MoE), which is responsible for the 
administration and development of the Palestinian education system at the levels of preschool, 
basic education, and secondary education  has taken different procedures on the ground to reduce 
the negative impact of conflict on educati on such as locating teachers close to their living place 
and opening new schools in rural areas  (Shraim, 2018; Shraim & Khlaif, 2010). Moreover, the 
MoE has pursued a variety of initiatives to integrate technology in public schools, such as the 
Professional Development program (supported by the U.S. through USAID), the Belgium e-
Learning project (supported by the Kingdom of Belgium), and Smart Learning (supported by the The Hidden Shadow of Coronavirus on Education in Developing Countries 
Online Learning Journal – Volume 25 Issue 1 – March 2021                    5 273  local community in Palestine) (Khlaif & Farid, 2018). As a consequence, Palestinian education is 
highly digitized at both the managerial and instructional levels  (Khlaif et al., 2019). In general, the 
Palestinian education system is thought to be strong, with an adult literacy rate at more than 90% 
(Itmazi. 2020). 
Afghanistan  
The education system in Afghanistan is influenced by the Taliban, and therefore there were 
several changes in the Afghani education system from the Taliban period to the post-Taliban 
period (Dandawate & Dhanamjaya, 2019). The decades of conflict almost destroyed the education 
system in Afghanistan. During the Taliban era, most girls’ schools were closed; girls represented 
only about 6% of school enrollment, and women were barred from the few Teacher Training 
Colleges that remained open (Shayan, 2015). For students who did attend school, the curriculum 
included a religious subject that took up to 50% of the teaching time (Guistozzi, 2010). Starting in 
2002, girls’ enrollment began to rebuild; by 2013, girls represented 38% of school enrollment, and 
32% of teachers were women (Shayan, 2015). However, enrollment is inequitably distributed 
across the country; in 2008, 80% of girls attended school in the Hazara region, while only 10% did 
so in several southern provinces (Larson, 2008). 
Noorajan (2020) described Afghani online learning as very poor, stating that access to  the 
internet and digital learning resources are still not available in most educational institutions. The 
digital divide is greater than expected because  of the digital inequity to access to technology and 
the social gap among the Afghani society  (Khan et al., 2012). Saay and Norta (2018) claimed that 
higher education institutions have moved forward in terms of creating a shared, reliable, high -
speed, interoperable e-learning structure, a similar system for K -12 is not yet a reality. 
Contribution of this Study 
Although much research has been conducted on distance education, there is a lack of 
research on ERT in developing countries, particularly since the COVID-19 emergency forced 
schools to close and transfer their education to online classes. This study examines a “hidden 
shadow” on education across these three countries: the unrevealed impact of ERT during the 
COVID-19 pandemic on education, including on students, parents, teachers, and quality of content. 
The study aims to answer the following questions: 
• How was Emergency Remote Teaching implemented in middle school settings  by 
instructors in these three countries? 
• To what extent did digital inequity create challenges for students and their families?  
• To what extent did digital privacy concerns create chall enges for students and their 
families?  
 
Methods 
A qualitative approach was adopted for the current study. The qualitative approach is 
generally used to describe an intervention or phenomenon in the real -life context in which it 
occurred (Yin, 2003). The qualitative approach used in the current study w as individual semi-
structured interviews and observations of online learning sessions.  
 The Hidden Shadow of Coronavirus on Education in Developing Countries 
Online Learning Journal – Volume 25 Issue 1 – March 2021                    5 274  Participants 
The participants in the study were parents, students, and teachers in middle school settings 
in Libya, Palestine, and Afghanistan. Sixty participants (20 participants from each country , 
including 15 teachers, 15 parents, and 30 students) were interviewed to discuss their lived 
experience with ERT in middle school settings during the COVID-19 pandemic. Furthermore, 60 
online classes (4 for each participating teacher) were observed.  
Purposeful sampling was used to recruit participants. The criteria for recruiting the 
participants in the study were having experience in online teaching during the COVID-19 
pandemic; living in Libya, Palestine, and Afghanistan; and allowing the online classes to be 
observed. The participants were recruited through the professional network of the first author, who 
was working with nonprofit organizations to train teachers on the use of ICT tools in education in 
these countries. Table 1 presents demographic information about t he participants. 
 
Table 1 
Participants’ Demographic Characteristics 
Participants Palestine Libya Afghanistan 
 Male Female Male Female Male Female 
Teachers 3 2 2 3 2 3 
Parents 2 3 2 3 3 2 
Middle school 
students 4 6 3 7 4 6 
Total 9 11 7 13 9 11 
 
Data Collection  
Semi -structured interviews with 60 participants on Zoom were the main data source of this 
research. Individual interviews were 20–30 minutes in length and were audio-recorded after 
obtaining permission from each participant. The interviews with participants from Libya and 
Palestine were in the Arabic language. The interviews with participants from Afghanistan were in 
the Dari and Arabic languages.  
The secondary data collection approach was observations of online classes. Researchers 
observed four online classes for each teacher. The main criterion for teachers to participate in the 
study was allowing their different online classes  to be observed. The researchers observed 60 
online classes from the three countries , which took around 50 hours. Forty online classes were in 
the Arabic language and 20 were in the Dari language (the Afghani official language). The classes 
comprised students from different backgrounds and cultures. All the students, parents, and teachers 
participated in the study on a voluntary basis.   
Data Analysis 
The data analysis process started with the manual transcription of the interview audio files. 
The transcribed files in the Dari language were translated into the Arabic language to facilitate the The Hidden Shadow of Coronavirus on Education in Developing Countries 
Online Learning Journal – Volume 25 Issue 1 – March 2021                    5 275  data analysis process. The researchers individually coded the data into themes and subthemes from 
all the data sources by using thematic analysis  (Creswell & Creswell, 2017). Through thematic 
analysis, researchers analyze, categorize , and report patterns in the data. After constructing the 
themes that emerged in the analysis of the interview data, the researchers used these themes to 
analyze the observations of online classes.  
In terms of the analysis of the observations of online classes , the researchers wrote notes 
on the structure of the classes  and audio-recordings of the sessions to answer the first research 
question. Furthermore, the data from the observations of online classes were used to support 
themes and subthemes that emerged in the individual interviews.  
Trustworthiness 
After transcribing the interviews, the researchers sent all the written transcripts to 
participants for checking, asking them to  verify the content for accuracy. Moreover, participants 
were asked to write notes, rewrite content, and add new information on the transcripts. 
Participants’ transcripts were not altered by the researchers. When the final themes had been 
constructed, a professor in educational technology was asked to select  a small portion of the 
transcription and analyze it based on the constructed themes to check the accuracy of the themes. 
In the Results section below, transcript excerpts are labeled according to the participant ’s role 
(Parent, Teacher, or Student), country (A = Afghanistan, L = Libya, or P = Palestine) and a 
personal identification number. 
 
Results 
Research question #1: How was Emergency Remote Teaching implemented in middle school 
settings by instructors in these three countries? 
Based on the analysis of the observations of online classes, the researchers categorized the 
implementation of ERT by schools and teachers in the three countries in to three themes: type of 
online learning classes, students ’ engagement, and the design and delivery of online instruction.  
Type of Online Learning Classes 
Most of the teachers in the three countries used both synchronous and asynchronous tools 
to deliver instruction to the learners in the middle school settings. Typically, teachers created their 
asynchronous class using Edmodo or Google Classroom, and posted links to synchronous Zoom 
sessions on the class wall. Most of these teachers also recorded the synchronous sessions and 
posted the recordings for students who may have missed class. Students needed to log into the 
virtual classes to access the recorded sessions. All the teachers used free versions of the Zoom 
application, Google Classroom, and Edmodo. 
However, some teachers used other asynchronous platforms to deliver instruction and to 
communicate with their students, and they rarely used synchronous sessions. A follow-up question 
was sent to these teachers about the reasons for not using synchronous session s. Typically, teachers 
responded that most of their students did not have access to the internet because of the electricity 
in Gaza in Palestine and in Afghanistan being cut off.  
Despite differences in the type of online instruction among the teachers from the three 
countries, all teachers’ instruction shared the following common features: it was free, it was easy The Hidden Shadow of Coronavirus on Education in Developing Countries 
Online Learning Journal – Volume 25 Issue 1 – March 2021                    5 276  to use, students had to enroll in the platform to access content, and technology tools were 
integrated.  
Types of Content Delivered  
Various types of content were used by teachers to instruct students, including interactive 
activities designed in open-source platforms, educational videos from YouTube, PowerPoint slides 
designed by teachers and students, pictures, infographic s, and whiteboard animation. Teachers 
from Palestine and Afghanistan focus ed on using simple technological tools to design and deliver 
the instruction, most of which were from open-source platforms. They integrated simple tools to 
design and deliver interactive content to the students. However, the Libyan teachers depended only 
on one tool to design and develop instruction, which was the Edmodo platform.  
Few teachers assigned online tasks or activities to their students in the synchronous 
sessions, as they reported they did not want to distract students in the online sessions . Instead, they 
assigned activities asynchronously using Edmodo or Google Classroom.  
In synchronous online sessions, teachers shared their screens, used different features of the 
online platform such as the whiteboard and polls, and used breakout rooms to allow students to 
work on assignments in small groups. Depending on the type of instruction, teachers allowed 
students to share their screen.  
Students’ Interaction 
Teachers tried to support students’ engagement and interaction with each other in the 
synchronous and asynchronous sessions by designing and delivering activities. A few teachers 
gave students the opportunity to interact with the instructor or with their peers. These teachers 
delivered online individual and group activities. Teachers asked students to work on these 
activities online by forming groups and giving them the option to share their screens to present 
their work to their peers. Other students were able to interact with their peers by providing text 
and verbal comments by using chat and audio features.  
Many of the students engaged and interacted with their teachers thr ough text questions 
written on the chat tab. In addition, some students engaged in answering their peers ’ questions on 
the chat tab.  
Some female students from the three countries were reluctant to participate verbally in the 
online discussion. However, we noticed that these students were active in the asynchronous 
discussion on the platform.  The researchers asked the female students a follow-up question about 
their engagement with the platform and discussions. Feedback from participants indicated that 
students had various reasons for choosing to participate in asynchronous rather than synchronous 
interaction, including personal reasons and cultural and religious reasons. (We discuss these 
reasons in more detail under Research Question 3.)   
Moreover, the researchers noticed that the presence of teachers on the platform and timely 
feedback encouraged students to complete their assignments and interact with their peers. Students 
from Afghanistan were more engaged in asynchronous activities than Palestinian st udents. 
Observations suggested that the reason for this was that unlike the Afghani teachers, Palestinian 
teachers did not interact with their students in asynchronous platform s. Libyan students engaged 
and interacted in both synchronous and asynchronous platforms less than the Afghani and 
Palestinian students.  The Hidden Shadow of Coronavirus on Education in Developing Countries 
Online Learning Journal – Volume 25 Issue 1 – March 2021                    5 277  Challenges of ERT  
All the participants from the three countries reported that ERT was a big challenge for them 
since it was unplanned, and students, teachers and parents were unprepared to shift to online 
learning. Most of the teachers in the three countries agreed that “this was not the first time we 
moved to online teaching during a crisis” (Teacher P2) and that previous transformation had been 
for “specific schools in specific districts due to violence” (Teacher A4). Implementing ERT on a 
large scale in the three countries caused pressure and anxiety for families in these countries. A 
parent from Afghanistan said: “Using online learning during the COVID-19 crisis put me and my 
wife under pressure. We have three kids in school… We do not have enough devices and enough 
knowledge about online teaching” (Parent A1). 
Research question #2: To what extent did digital inequity create challenges for students and 
their families? 
All the participants from the three countries reported that m oving to ERT suddenly 
increased the digital gap in the learning process. In addition, they all reported that digital inequity 
put them under high pressure and caused anxiety—particularly since they did not know when this 
state of emergency would end and allow them to return to face-to-face learning. Overall, the 
researchers identified three main themes: inequity in internet and hardware, quality of digital 
content, and inequity based on gender.  
Inequity in Internet  and Hardware 
Internet Broadband Connectivity 
Teachers from Libya, Afghanistan, and Palestine noted the weakness of internet bandwidth 
as an element that hindered their ability to provide timely instruction to students , ultimately 
affecting student engagement.  Although teachers in all three countries mentioned the issue, it was 
reported less in the Palestinian context than in the      Afghani and Libyan contexts. Teachers and 
students who referred to the inequity in internet reported that access to internet broadband was one 
of the major barriers to implementing ERT during the COVID -19 pandemic.  
For example, a teacher from Afghanistan reported that “[the] internet nowadays is weak 
because a lot of people were trying to attend online classes … It cannot help me prepare instruction 
and deliver it to my students” (Teacher A3). Researchers observed that students from rural areas 
in all three countries were at more of a disadvantage in terms of the availability and reliability of 
the internet than their peers from large cities. 
I live in a small village north of Gaza, Palestine. I did not attend online classes… I missed 
most of them because the electricity is cut off all the time… Even if we have electricity, 
the internet is weak. I asked my relative who lives in Gaza city to send me the recorded 
files to view them when electricity is available (Student P3).  
A female student from a rural area in Afghanistan stated: “I usually go to my uncle’s house 
to attend online classes because we do not have the internet at home” (Student A3).  
The participants from Libya reported that families’ financial situations influenced their 
connectivity to the internet.  
I am a teacher in Tripoli. I have not received my salary for the last year, so how can I pay 
for the internet? My priority is to pay for food and living expenses… Moving to emergency 
remote teaching was sudden, and it is difficult to adopt it in a short time (Parent L3).  The Hidden Shadow of Coronavirus on Education in Developing Countries 
Online Learning Journal – Volume 25 Issue 1 – March 2021                    5 278  Inequit y in Hardware 
Families from all three countries reported that some households have too few technological 
devices based on the number of children enrolled in schools and universities , especially in families 
in which the father or mother or both are enrolled in a graduate program at a univ ersity. A female 
student from Libya reported, “I cannot attend synchronous classes because my sister is using the 
device [laptop] when it is time for the class” (Student L2). A female student from Palestine stated 
that “before this crisis [COVID-19], I used to use my mom’s iPhone to finish my assignments, but 
now I cannot do this because she uses it to teach her students ” (Student P4). 
A parent from Libya reported:  
Because of the stay at home rule, I lost my job, and it is difficult to buy a new device for 
my kids to attend online learning. I asked their teacher to provide me the recorded teaching 
materials to use them by my children (Parent L3).  
A male student from Afghanistan stated: “I have two sisters, one in high school and the 
older one at the university. All of us are using the same desktop… My dad cannot buy an additional 
computer for us” (Student A9). A parent from Palestine said, “I cannot buy more devices because 
I have not received my salary in the last six months because of Coronavirus” (Parent P3).  
Quality of Digital Content 
Parents, teachers, and students agreed that the quality of digital content delivered by 
teachers in both synchronous and asynchronous sessions was low. For example, Libyan parents 
and students mentioned the low quality of course content for math and English, while Palestinian 
and Afghani students and parents reported the low quality of learning materials and content in 
science. Teachers believed the low quality of course content was a result of the sudden transition 
to remote learning contexts.  
We moved to online teaching within 24 hours . I do not have time to plan and design suitable 
content for my students… The teaching online is completely different from teaching face  
to face… It needs specific teaching strategies, specific content , which takes a lot of time to 
prepare (Teacher A3). 
Other teachers from Libya and Palestine reported that they were “unprepared” to design interactive 
content, stating that they “do not have a team to design” and “do not know the tools to use in the 
design process” (Teacher L1, Teacher L3, Teacher L4; Teacher P4, Teacher P5).  
Students from Afghanistan complained about the quality of the assignments and the 
grading system.  
The content is related to the lesson in the book, but when I went to do the assignment, it 
was unclear. I do not feel there was a connection between the assignment and the material. 
In addition, I do not know how my teacher will grade me (Student A4).  
Some parents from Palestine and Afghanistan were struggling to teach and assist their 
children because they do not have experience with the design of digital content. A Palestinian 
parent said, “I usually teach my children and help them finish their tasks and activities but now I 
do not know how to start with them… I asked my brother, who is a teac her, to help them” (Parent 
P2). A parent from Afghanistan stated: “I am not educated in using technology . It is difficult for 
me to teach my kids this type of content” (Parent A5). The Hidden Shadow of Coronavirus on Education in Developing Countries 
Online Learning Journal – Volume 25 Issue 1 – March 2021                    5 279  Inequity Based on Gender 
Most students and parents, and some teachers, believed that online course design and 
delivery was biased based on gender. For example, female students reported that course materials 
and activities tended to focus on males by showing photos of boys or using boys’ names.  
Our observational data supported female students ’ perceptions that teachers from all three 
countries privileged male students by using gendered language, referring mostly to boy s’ names, 
allowing male students to speak more frequently, and often interrupting female students . 
My teachers are biased toward males in online classes. Since the beginning of teaching 
online two months ago, I have only been allowed to use the mic twice for two or three 
minutes, but some males talk more than all the females (Student P4) . 
A female student from Libya said: “In the presentation of our project, my teacher did not 
give us an opportunity to present in the same way as males and in mixed teams . He asked if I prefer 
males to present the project… I like to discuss my ideas, but I am not allowed to” (Student L3).  
The inequity based on gender was not only due to teachers, as some families did not allow 
their female children to use the microphone. A female student from Libya stated: “My dad put 
restrictions on using the microphone and camera while using his laptop for online learning” 
(Student L5). A Palestinian female student mentioned that her mo ther allowed her to use the 
internet for learning without opening the mic and the camera (Student P2).  
Research question #3: To what extent did digital privacy concerns create challenges for 
students and their families?  
According to the data analysis, a new theme related to the ERT during the COVID-19 crisis 
was mentioned by the participants , which was digital privacy. Most of the participants in the 
current study stated that teaching and learning in the ERT environment should be managed 
professionally and ethically by teachers and students.  
All participants expressed concerns about digital privacy , and most of the students reported that 
their digital privacy was violated by on e of their family members or by their teachers. Furthermore, 
families stated that sharing devices such as smartphones, laptops, or desktops is considered as 
violating individual privacy. In general, perspectives on privacy seemed re lated to two key themes: 
gender differences in privacy and sharing devices or internet access.  
Gender Differences in Privacy  
As mentioned earlier, many female students mentioned that their families did not allow 
them to use their camera or microphone during class. A follow-up question was sent to the students 
asking why their families were concerned about the camera and microphone. Below, we draw from 
these responses as well as students’ original interviews.  
Due to security concerns, cultural traditions, or rel igious strictures, many female students  
and their families  felt their personal images should remain private. For example, one parent said, 
“We are living in a bad situation because of violence and security. I am concerned about her 
[daughter] privacy in online learning. I do not know how to protect her” (Parent A3). In some 
districts in Afghanistan, Libya, and Palestine, it is considered inappropriate for families to allow 
their children to share photos , videos, or even to appear talking in a video clip . Other families from 
these countries consider themselves open-minded and allow their children to share videos and The Hidden Shadow of Coronavirus on Education in Developing Countries 
Online Learning Journal – Volume 25 Issue 1 – March 2021                    5 280  photos and use a webcam on the internet; however, these families were a minority. Overall, many 
female students felt their privacy was threatened by teachers as well as other students.  
First, some teachers asked female students to engage in practices that the students or 
families felt were inappropriate. For example , the researchers noticed that some male teachers 
asked students to open their camera at the beginning of the session. Some of the girls refused to 
open it, with one girl saying, “I do not like to open my camera for personal reasons ” (Student P1), 
and the teacher respected their privacy. During the online sessions, some teachers asked students 
to share their desktop; some students agreed to share it while others refused to share it, with one 
saying, “I have personal pictures in the background on my computer ” (Student A2). In interviews, 
a few other female students reported that their teachers asked them to open the webcam, which 
they considered a violation of their digital privacy  and reported to their families . One student 
asked, “Personal information on social media and in the online classes  is private information. I do 
not like to use the camera, so why did my teacher asked me to open the camera? ” (Student L4). 
Other families did not allow their girls to attend online classes if they were asked to share their 
information on social media or share their laptop screen. 
Second, female students felt that many of their male peers did not know the “boundaries” 
or work “professionally” (Student A5) in the new online learning environment. A female student 
from Afghanistan expressed anger about some of her male friends who looked for her social media 
profile and sent her a friend request.      Some female teachers from Libya and Afghanistan also 
reported that many girls complained about the bad conduct of their peers, which included taking 
screen shots of the online sessions and posting them on social media pages, trying to add them as 
friends on social media, and sharing their emails on the chat tab in the online sessions to contact 
them.  
In contrast, most of the male students did not care about their privacy. They were willing 
to open their camera and to share their desktop and shared no concerns about these practices in 
their interviews.  
Sharing Devices or Internet  
Parents from all three countries reported they were forced to share their own smartphones, 
laptops, or desktops with their children for schooling purposes, which they felt was a violation of 
individual privacy. A parent from Palestine sta ted:  
Because we do not have extra devices, somet imes I share my smartphone with my son and 
allow him to use it… My accounts on social media are open,  and I share my thoughts with 
my close friends… Allowing a family member to share my device puts my digital privacy 
at risk (Parent P3).  
A Libyan parent reported that she does not like to share her laptop with her children. She 
said, “I am lawyer, and all of my clients’ files are stored on my computer… Honestly, when I let 
my kids use my laptop, I observed them” (Parent L4). We asked her a follow-up question about 
restricting access to her files by using a password. The answer was: “I do not have high skills in 
computers to protect myself… I would appreciate it if you could help me” (Parent L4). A woman 
from Afghanistan reported that her husband asked her if he could share her smartphone in his 
online session. “My husband is a science teacher and he asked if he could use my phone… At the 
beginning, I hesitated because I have my sister ’s wedding pictures on it” (Parent A3).  The Hidden Shadow of Coronavirus on Education in Developing Countries 
Online Learning Journal – Volume 25 Issue 1 – March 2021                    5 281  In addition to sharing devices, many families shared their internet access points with 
neighbors. This practice was common in Palestine and Afghanistan but was less common in Libya 
since buying an internet package is cheaper than in Palestine and Afghanistan. “No, I do not like 
to share internet with others. It is cheap… I have high internet speed for less than $10” (Parent 
L1). 
When families shared internet with neighbors, parents worried that their children could be 
stalk ed by others on their network. A parent from Afghanistan reported: “We share internet with 
one of our neighbors because it is expensive to buy an internet package alone… My daughter told 
me that our neighbors’ son knows everything about her online account and sen t her a friend 
request” (Parent A1). Students from Libya and Palestine reported that they do not feel safe and 
secure in the online environment when they share internet with their neighbors. For example, a 
female student from Palestine said : “When I post anything on social media, our neighbors ’ son 
was the first to like or comment. I feel he is waiting for me to write a post” (Student P8).  
 
Discussion 
The aim of the study was to explore the challenges of ERT during the COVID-19 crisis in 
three developing countries , Libya, Palestine, and Afghanistan. A qualitative approach was adopted 
to achieve this aim. Moving suddenly from face-to-face teaching to ERT complicated and 
disrupted the educational system in these countries , which had already been affected by violence 
and unstable policy, as well as the weaknesses of the infrastructure and the bad economic situation. 
Although teachers tried their best to foster students ’ engagement in online learning sessions , they 
sometimes failed because of the digital inequity among students and families.  
Teachers and students in the developing countries used synchronous and asynchronous free 
tools to continue teaching and learning in the ERT, and teachers on both asynchronous and 
synchronous platforms provided support and encouraged students to engag e in online activities . 
Teachers relied on their own technology solutions because there was no well-designed and secure 
online system or platform for teaching and learning . As a result, learners’ digital privacy was 
threatened, and consistent with previous studies, the quality of content in the emergency context 
was low compared with content delivered in a well -planned online learning context (Hodges et al., 
2020; Khlaif et al., 2020; Mohmmed et al., 2020;Whittle et al. (2020).  
Our results found a number of inequities between students. Some parents could not provide 
their children with technological devices and families in rural and remote places encountered 
difficulties in getting connected to internet broadband, which reduced students’ engagement in 
online synchronous sessions compared with online asynchronous sessions.  Girls were also less 
active in synchronous online sessions than boys, due in large part to their hesitancy to share their 
personal images, but they were more active and productive in the asynchronous platforms. These 
results align with previous studies suggesting that digital inequity negatively influences the online 
learning process (Khidhir et al., 2020; Khlaif & Salha, 2020; Khlaif et al., 2020). 
We also found that digital privacy was a major concern for both parents and students. Most 
families were concerned about daughters opening their webcams or sharing their screens . Parents 
were reluctant to share their own devices for their children’s schooling, and worried that their 
children’s activities could be monitored by neighbors via shared internet access points. In general, The Hidden Shadow of Coronavirus on Education in Developing Countries 
Online Learning Journal – Volume 25 Issue 1 – March 2021                    5 282  violation of students’ and parents’ digital privacy increased during the coronavirus ERT, which 
reduced students’ engagement in online classes. 
The concepts of digital inequity and digital ethics are still very limited, underrated , and 
unclear in developing countries such as Palestine, Afghanistan, and Libya. Both were eroded in 
the coronavirus ERT setting, which had a negative influence on students’ engagement in online 
classes. The absence of these concepts and related processes of instructional design among 
teachers in ERT settings  was obvious, which deepened digital inequity and damaged digital 
privacy. To address these issues, the Ministries of Education in these countries  may wish to 
consider three key actions. First, they could provide students and teachers with devices and reduce 
internet fees to make the internet available to a wide range of citizens . Second, they could develop 
a clear vision about digital privacy . Finally, develop a shared technology platform across the K -12 
system that will benefit learners and train teachers on it.   
 
 
 
 The Hidden Shadow of Coronavirus on Education in Developing Countries 
Online Learning Journal – Volume 25 Issue 1 – March 2021                    5 283  References 
Affouneh, S., Salha, S., & Khlaif, Z. N. (2020). Designing quality e-learning environments for 
emergency remote teaching in Coronavirus crisis. Interdisciplinary Journal of Virtual 
Learning in Medical Sciences , 11(2), 1–3. 
Ahmed, A. & Booth, D. (2016). The digital privacy laws and practices in the Jersey island. 
Procedia Computer Science, 98, 163–168. https://doi.org/10.1016/j.procs.2016.09.026 
 Aloreibi, A., & Carey, M. D. (2017). English language teaching in Libya after Gaddafi. 
In English language education policy in the Middle East and North Africa (pp. 93–114). 
Cham, Springer. https://doi.org/10.1007/978-3-319-46778-8_7 
Arooje , R., & Burridge N. (2020) . School education in Afghanistan. In P. Sarangapani  & R. 
Pappu (Eds .), Handbook of education systems in South Asia . Springer . 
https://doi.org/10.1007/978 -981- 13-3309- 5_10- 1  
Brück, T., Di Maio, M., & Miaari, S. H. (2019). Learning the hard way: The effect of violent 
conflict on student academic achievement.  Journal of the European Economic 
Association, 17(5), 1502–1537. 
Creswell, J. W., & Creswell, J. D. (2017). Research design: Qualitative, quantitative, and mixed 
methods approaches. Sage. 
 Dandawate, V. S., & Dhanamjaya, M. (2019). A review of the open-access scenario in 
Afghanistan. Library Hi Tech, 37(4), 660–668. https://doi.org/10.1108/LHT-01-2019-
0020  
Elaiess, R. (2017). Digital divide in Libya: A general assessment. International Research: 
Journal of Library and Information Science, 7(3), 1–9. 
Fahey, R. A., & Hino, A. (2020). COVID-19, digital privacy, and the social limits on data-
focused public health responses. International Journal of Information Management, 55, 
102181.Gadour , A. (2011) . Power and struggle for education in Libya. In R. G. 
Sultana ( Ed.), Educators of the Mediterranean… … up close and personal. 
Comparative and international education ( a diversity of voices)  (vol. 9, pp. 121– 131) . 
Sense  Publishers. https://doi.org/10.1007/978 -94-6091- 681- 6_11  
Guistozzi, A. (2010). Nation-building is not for all: The politics of education in Afghanistan . 
Retrieved from https://www.afghanistan-analysts.org/en/special-reports/nation-building-
is-not-for-all-the-politics-of-education/ 
 Hbaci, I., Ku, H. Y., & Abdunabi, R. (2020). Evaluating higher education educators ’ computer 
technology competencies in Libya.  Journal of Computing in Higher Education, 1–18. 
https://doi.org/10.1007/s12528-020-09261-z 
Hodges, C., Moore, S., Lockee, B., Trust, T., & Bond, A. (2020). The difference between 
emergency remote teaching and online learning. Educause Review, 27(1), 1–12. 
Holzweiss, P. C., Walker, D. W., Chisum, R., & Sosebee, T. (2020). Crisis planning for online 
students: Lessons learned from a major disruption. Online Learning, 24(2), 22–37. 
 The Hidden Shadow of Coronavirus on Education in Developing Countries 
Online Learning Journal – Volume 25 Issue 1 – March 2021                    5 284  Itmazi, J. (2020). Open educational resources in Palestine. In R. Huang, D. Liu, A. Tlili, Y. Gao 
& R. Koper (Eds.), Current state of open educational resources in the “belt and road” 
countries. Lecture notes in educational technology. Springer. https://doi.org/10.1007/978-981-15-3040-1_8 
Johnson, N., Veletsianos, G., & Seaman, J. (2020). US faculty and administrators’ experiences 
and approaches in the early weeks of the COVID-19 pandemic. Online Learning, 24(2), 
6–21. 
Jones, M., & Bridges, R. (2016). Equity, access, and the digital divide in learning 
technologies. Wiley Handbook of Learning Technology, 327–347. 
Khan, G. F., Moon, J., Swar, B., Zo, H., & Rho, J. J. (2012). E-government service use intentions 
in Afghanistan: Technology adoption and the digital divide in a war -torn 
country. Information Development, 28(4), 281–299. 
https://doi.org/10.1177/0266666912438879  
Khlaif, Z. N., & Farid, S. (2018). Transforming learning for the smart learning paradigm: 
Lessons learned from the Palestinian initiative.  Smart Learning Environments, 5(1), 12. 
Khlaif, Z. N., & Salha, S. (2020). The unanticipated educational challenges of developing 
countries in Covid-19 crisis: A brief report. Interdisciplinary Journal of Virtual Learning 
in Medical Sciences , 11(2), 130–134. 
Khlaif, Z. N., Salha, S., Affouneh, S., Rashed, H., & ElKimishy, L. A. (2020). The Covid-19 
epidemic: teachers’ responses to school closure in developing countries.  Technology, 
Pedagogy and Education, 1-15. 
Kurea, G. M. (2015). The implementation of the two third gender rule in the devolved 
government as stipulated in the Kenyan Constitution: The case study of Meru 
County (Doctoral dissertation, University of Nairobi). Retrieved from: 
http://erepository.uonbi.ac.ke/bitstream/handle/11295/94524/Kurea_The%20Impl ementat
ion%20of%20the%20Two%20Third%20Gender%20Rule%20in%20the%20Devolved%2
0Government%20as%20Stipulated%20in%20the%20Kenyan%20Constitution.pdf?seque
nce=3&isAllowed=y  
Li, X. (2018). Understanding eHealth literacy from a privacy perspective: eHealth literac y and 
digital privacy skills in American disadvantaged communities.  American Behavioral 
Scientist, 62(10), 1431–1449. https://doi.org/10.1177/0002764218787019  
Lorenz, B., Sousa, S., & Tomberg, V. (2013). Privacy awareness of students and its impact on 
online learning participation —A case study. In T. Ley, M. Ruohonen, M. Laanpere, & A. 
Tatnall (Eds.), OST 2012. IFIP AICT (vol. 395, pp. 189–192). Springer.  
Mariscal, J., Mayne, G., Aneja, U., & Sorgner, A. (2019). Bridging the gender digital 
gap. Economics: The Open-Access, Open-Assessment E-Journal, 13(9), 1–12.  
Mohmmed, A. O., Khidhir, B. A., Nazeer, A., & Vijayan, V. J. (2020). Emergency remote 
teaching during Coronavirus pandemic: The current trend and future directive at Middle 
East College Oman. Innovative Infrastructure Solutions, 5(3), 1–11. The Hidden Shadow of Coronavirus on Education in Developing Countries 
Online Learning Journal – Volume 25 Issue 1 – March 2021                    5 285  Noorajan A. (2020). A critical reflection of teacher education policies and programs in 
Afghanistan. In K. Pushpanadham (Ed.), Teacher education in the global era. Springer. 
https://doi.org/10.1007/978-981-15-4008-0_14  
Rasmussen, P. E., & Kelly, A. (2016). Education sector analysis Afghanistan . Pouras Consult. 
https://www.globalpartnership.org/sites/default/files/education -sector -analysis -
afghanistan.pdf . 
Resta, P., Laferrière, T., McLaughlin, R., & Kouraogo, A. (2018). Issues and challenges related 
to digital equity: An overview. Second handbook of information technology in primary 
and secondary education (pp. 1–18). https://doi.org/10.1007/978-0-387-73315-9_44  
Ruipérez-Valiente, J. A., Halawa, S., Slama, R., & Reich, J. (2020). Using multiplatform 
learning analytics to compare regional and global MOOC learning in the Arab world. 
Computers & Education, 146, 103776. https://doi.org/10.1016/j. compedu.2019.103776  
Shraim, K., & Khlaif, Z. (2010). An e-learning approach to secondary education in Palestine: 
Opportunities and challenges. Information Technology for Development, 16(3), 159–173. 
https://doi.org/10.1080/02681102.2010.501782  
Traxler, J., Khaif, Z., Nevill, A., Affouneh, S., Salha, S., Zuhd, A., & Trayek, F. (2019). Living 
under occupation: Palestinian teachers ’ experiences and their digital responses.  Research 
in Learning Technology, 27, 1–18. https://doi.org/10.25304/rlt.v27.2263  
United Nations. (2015). Sustainable development goals . Retrieved January 18, 2021, from 
https://sustainabledevelopment.un.org 
UNESCO. (2020). COVID-19 education response. Retrieved January 18, 2021, from 
https://en.unesco.org/covid19/educationresponse/globalcoalition  
Whittle, C., Tiwari, S., Yan, S., & Williams, J. (2020). Emergency remote teaching environment: 
A conceptual framework for responsive online teaching in crises.  Information and 
Learning Sciences, 121(5/6), 311–319. https://doi.org/10.1108/ILS-04-2020-0099  
Willems, J., Farley, H., & Campbell, C. (2019). The increasing significance of digital      equity 
in higher education. Australasian Journal of Educational Technology, 35(6), 1–8. 
https://doi.org/10.14742/ajet.5996  
World Bank. (2013). Libya: World development indicators . Retrieved January 18, 2021, from 
http://data.worldbank.org/country/libya#cp_wdi  
Yuen, A., Park, J., Chen, L., & Cheng, M. (2018). The significance of cultural capital and 
parental mediation for digital inequity. New Media & Society, 20(2), 599–617. 
https://doi.org/10.1177/1461444816667084  
 "
DataPrivacy,3017.txt,"1
WHY DATA PROTECTION MATTERS FOR DEVELOPMENTCGD Note
DECEMBER 2021
Why Data Protection Matters 
for Developmen t: The Case 
for Strengthening Inclusion 
and Regulatory Capacity 
Michael Pisa , Pam Dixon, and Ugonma Nwankwo
As data and digital tools assume an ever-larger role in all aspects of our lives, it is increasingly import -
ant to have clear and effective rules that govern how different actors can use personal data through its 
life cycle and across different data ecosystems. A key challenge for governments is establishing rules 
that protect citizens from harm while supporting useful innovation from both the public and private 
sectors.
Over the course of the Governing Data for Development project, we asked more than 100 experts from 
government, civil society, the private sector, development organizations, and the data governance 
and privacy communities for their views on the most significant challenges governments face in 
using and regulating the use of data to meet their development aims. Almost every expert we spoke 
with cited at least one of the three following obstacles: 
1.A lack of funding and political impetus needed to strengthen systems to manage data;
2.A shortage of people with the technical expertise needed to create and work within those systems; 
3.Uncertainty about how to comply with (often new) national laws governing the use of data.
This note summarizes two years of research under the CGD project, “Governing Data for Devel -
opment,” led by Michael Pisa and Ugonma Nwankwo, along with project co-chairs Pam Dixon 
and Benno Ndulu (in memoriam).* The project was funded by the Hewlett Foundation and 
guided by a working group of 15 experts. For more information, please visit cgdev.org/governing-
data
* We continue to mourn the loss of Benno Ndulu, who passed away in February 2021. You can read CGD President Masood 
Ahmed’s post remembering his life and contributions here.2
WHY DATA PROTECTION MATTERS FOR DEVELOPMENTUntil recently, the development community has paid substantially more attention to the first two chal -
lenges than the third.1 This has changed over the last several years, however, as the sector has begun to 
grapple with how to promote responsible data use, in line with a broader shift in societal views about 
the risks of data misuse and mounting concerns about digital surveillance and the growing role of AI.2 
The COVID-19 pandemic has underscored the importance of using data responsibly by forcing a con -
versation on the limits that should be placed on governments seeking to use personal data to support 
public health efforts.
Although data protection policies are just one element of a broader nexus of laws, regulations, and 
norms that determine how countries govern the use of data, they play an outsized role. For many gov -
ernments, establishing a data protection regime is a foundational step in developing a broader ap -
proach to modern digital governance. The choices that policymakers make when creating and imple -
menting data protection laws set a trajectory for how a government and its citizens will engage with 
digital ecosystems and data. These choices, therefore, have direct and long-lasting consequences for 
economic development. 
While the experts we spoke to for this project welcomed the growing number of countries that have 
enacted data protection regimes in recent years, they also raised concerns about the effectiveness of 
these regimes in practice, the challenges resource-constrained governments face in implementing 
them, and the potential negative consequences of poor implementation. 
In this note, we review lessons learned through our research over the last two years and offer sug -
gestions for policymakers seeking to regulate data use while keeping up with rapidly evolving digital 
practices and recommendations for how the international development community and high-income 
countries can promote a more inclusive and level playing field. 
WHY DATA PROTECTION MATTERS FOR ECONOMIC DEVELOPMENT 
Effective data protection laws and regulations help build trust in digital tools and systems by estab -
lishing rights  that protect citizens against the misuse of their personal data and obligations  that require 
organizations to use data in a fair, transparent, and accountable manner. In theory, this greater trust 
should translate to greater acceptance of services that rely on data sharing and data use, leading to 
more investment in the resources and expertise needed to fuel a country’s digital transformation and 
support evidence-informed policymaking (World Bank, 2021; Bhaskar and Chaturvedi, 2017; World 
Economic Forum, 2019). 
To meet these aims, however, data laws and regulations must be well-designed, tailored to local real -
ities, and effectively and consistently enforced. Unfortunately, early evidence suggests that in many 
countries that have enacted data protection laws, enforcement is weak, regulatory authorities lack 
independence, and policies are not designed in line with existing resource constraints. 
1  For more on efforts to strengthen funding and technical expertise see the work of the Global Partnership on Sustainable Devel -
opment Data, Open Data Watch, PARIS21 and the World Bank’s Global Data Facility.
2  The development community’s growing interest in how data is governed is perhaps best exemplified by the World Bank’s 2021 
World Development Report “Data for Better Lives,” which calls for a new social contract in which the use and re-use of personal 
data is governed primarily by a rights-based framework.3
WHY DATA PROTECTION MATTERS FOR DEVELOPMENTData protection rules that are poorly designed or inadequately enforced can hinder economic devel -
opment through different channels that can be roughly categorized as under-regulation, over-regula -
tion, and regulating the wrong things in the wrong way. 
 •Under-regulation:  Even when data protection laws exist “on the books,” they often fail to translate 
into “law on the ground” (Pisa, Dixon, Ndulu and Nwankwo, 2020). This weakens the level of 
protection provided and undermines the trust in data use and sharing that data protection laws 
are meant to instill. It also contributes to regulatory uncertainty, which can hinder useful data 
innovation by both the public and private sectors (Mungan, 2019).
 •Over-regulation: As in other sectors, over-regulation in the form of high compliance costs that bear 
little relation to improvements in desired policy outcomes have the potential to slow innovation 
by creating an unnecessary disincentive to investment. These costs are especially damaging to 
small- and medium-sized enterprises, which typically lack the well-resourced legal teams need -
ed to navigate complex compliance requirements. (UK Digital Competition Expert Panel, 2019; 
Voss, 2021). 
 •Regulating the wrong things in the wrong way: Several theorists have argued that current approaches to 
data protection place too much emphasis on protecting against individual harms and not enough 
on collective harms, putting it at odds with the growing reliance on machine learning algorithms 
that extract insights from collective data. (Tisné and Schaake, 2020; Moerel and Prins, 2016). 
Overemphasis on protecting against individual harms is mirrored by overreliance on informed 
consent as the primary basis for data processing, which often places an unreasonable burden on 
individuals and is meaningless in situations where they lack a basic understanding of how their 
data will be used (Medine, 2020; Selinger and Hartzog, 2020). 
By undermining people’s trust in how their data is used and raising hurdles to responsible innova -
tion, each of these conditions seems likely to lead to less investment in digital tools and data-driven 
services. But empirical evidence is lacking. Developing a better understanding of the causal pathways 
through which data regulations can affect a country’s digital and economic development is crucial to 
designing effective policies. 
THE GLOBAL DATA PROTECTION LANDSCAPE IS MARKED BY DISPARATE 
RESOURCES AND AN UNEVEN PLAYING FIELD 
Modern approaches to data protection can be traced back to the establishment of the Fair Information 
Practices  in the United States in the 1970s and the subsequent codification of and expansion on those 
principles by the OECD in the Guidelines on the Protection of Privacy and Transborder Flows of Personal Data  
published in 1980. The years that followed were characterized by a slow and steady diffusion of na -
tional data protection frameworks, mostly in wealthier countries, based and building on these prin -
ciples (Gellman, 2021). 
Over the last two decades, however, the number of countries that have adopted data protection legis -
lation has significantly increased. Just since 2010, 64 countries—most of which are in Africa, Asia and 
Latin America and over 70 percent of which are categorized as LMICs—have enacted new data protec -
tion laws, bringing the total with such laws in place up to 146 (Figure 1).4
WHY DATA PROTECTION MATTERS FOR DEVELOPMENTSeveral factors are driving the recent rapid adoption of national data protection frameworks, among 
them growing awareness of the risks of data misuse; the desire to create an enabling framework for 
responsible data use and sharing; the need to meet requirements of international development part -
ners; and the catalytic effect of the European Union (EU) General Data Protection Regulation (GDPR), 
which was enacted in 2016 and came into effect in 2018. Of the more than 60 countries that have en -
acted new data protections laws over the last decade, almost all modelled their approach in full or in 
part on the GDPR and its predecessor, the 1995 EU Data Protection Directive (DPD).
The GDPR altered the global data protection landscape by providing a more rigorous model for pro -
tecting the privacy of individual data than had previously existed and established the EU as the global 
leader in regulating data. The regulations provided mechanisms that strengthened individual con -
trol over how data is used, increased the accountability of data controllers, and raised the stakes for 
non-compliance through greater fines and penalties. In contrast, the United States, home to the 
world’s largest tech firms, has taken a sectoral and relatively hands-off approach to regulating the 
use of personal data at the national level, with exceptions around certain categories of data, including 
health data and data pertaining to children. 
The influence of the GDPR and DPD also reflects the extraterritorial scope of EU adequacy frameworks, 
which call on the European Commission to determine whether non-EU countries “offer guarantees 
ensuring an adequate level of protection essentially equivalent to that ensured within the Union, in 
particular where personal data are processed in one or several specific sectors” as a basis for transfer -
ring data (EU General Data Protection Regulation—Recital 104, 2016). Note: Data from Greenleaf’s Global Tables of Data Privacy Laws and Bills (6th Ed January 2019) ; Greenleaf and Cottier’s 2020 Ends a Decade 
of 62 New Data Privacy Laws ; and UNCTAD’s Data Protection and Privacy Legislation  mapping. Additional research conducted by World 
Privacy Forum and Center for Global Development.Figure 1. Data protection and privacy legislation around the world
Data protection legislation introduced and passed between 2010-Present Data protection legislation introduced and passed before 20105
WHY DATA PROTECTION MATTERS FOR DEVELOPMENTBecause companies based in countries that receive a favorable adequacy determination face lower 
barriers to doing business with EU citizens, achieving adequacy confers a significant competitive ad -
vantage in the global digital economy. For example, a report issued before the UK achieved GDPR ade -
quacy estimated that not receiving it would cost UK firms between £1 billion and £1.6 billion due to the 
additional compliance obligations (McCann, Patel and Ruiz, 2020). The risk is that failure to achieve 
adequacy will set countries that are already behind on their digital transformation even further back. 
Although a growing number of countries have incorporated elements of the GDPR into law, early and 
anecdotal evidence suggests that most of them struggle to implement it effectively due to its breadth 
and complexity (Davis, 2021; Dixon 2019). Even EU member states, which had roughly 25 years of 
practice implementing a similar framework under the DPD, have struggled to implement the updated 
law (European Commission, 2020; Voss, 2021). The challenge is much greater for countries that face 
severe resource constraints, have a smaller pool of experts to draw from, and have less experience 
implementing a comprehensive data protection framework.
The scale of the challenge is illustrated by wide regional disparities in the level of human and finan -
cial resources available to data protection authorities (DPAs), which are the institutions responsible 
for interpreting and enforcing data protection laws in most countries that have comprehensive data 
protection frameworks (Table 1). Not only do DPAs in lower income countries face severe resource con -
straints, they also are more likely to lack functional independence from the executive branch or other 
ministries, which makes it difficult for them to resist political influence or to hold other government 
actors accountable (Davis, 2021).
Acknowledging the difficulties of implementing the GDPR framework is not an endorsement of wa -
tering down existing rules or taking an entirely different approach. In fact, the experts we spoke to 
were nearly unanimous in their support of the principles that underlie the GDPR and in their belief 
that countries should take a comprehensive and rights-based approach to personal data protection (as 
opposed to a sectoral approach or one that seeks to achieve an economic balance of interests) (Pisa and 
Nwankwo, 2021).
Median per-country  
DPA budgetMedian per-country  
DPA staff
Region
North America $58 million 647
Asia/Oceania $6.9 million 77
Europe $2.2 million 34
Africa/Middle East $500,000 14
Central And South America $400,000 13
OECD
Member $6 million 50
Non-member $500,000 17Table 1. Regional differences in staffing and budget among data protection authorities
Source: Fazlioglu, 20186
WHY DATA PROTECTION MATTERS FOR DEVELOPMENTMany of the same experts expressed frustration, however, with how current arrangements for gov -
erning cross-border data flows have, in their view, unduly restricted domestic policy choices. This 
includes the GDPR adequacy process, which they regarded as excessively opaque and often driven by 
political and economic considerations, rather than the fitness of a country’s data protection regime. 
(Pisa and Nwankwo, 2021). For example, although the European Commission granted Japan adequacy 
in 2019 despite key differences between the GDPR and Japan’s model of cooperative data privacy, it 
has not granted adequacy to any country in Africa, despite several countries having data protection 
frameworks that are closely modelled on the GPDR. 
Lack of coordination on data regulations at the global and regional levels further disadvantages low- 
and middle-income countries who, on their own, lack the economic leverage needed to influence both 
the practices of big tech companies that dominate global data flows and the terms on which cross-bor -
der data flows are governed in bilateral agreements with wealthier countries (for an example of the 
latter, see Rutenberg and Omino: Why the US-Kenya Free Trade Agreement Negotiations Set a Bad 
Precedent for Data Policy ).
FUTURE-PROOFING DATA REGULATION 
Although lower income countries face unique challenges in getting data regulation “right” due to their 
greater resource constraints and more limited economic leverage, the problems they face are uni -
versal. Our research suggests the following lessons for policymakers trying to regulate data use while 
keeping up with fast evolving digital practices: 
 •Think Local… While the GDPR’s global influence has resulted in similarities across national data 
protection frameworks, beliefs about data policy remain highly contextual, reflecting differenc -
es in local norms about data use, resharing, and privacy. One way for regulators to mitigate this 
tension is by working with different sectors to co-create guidance and codes of practice that can 
simultaneously help companies better understand their compliance duties and help regulators 
better tailor regulation to local conditions (for an example, see Data Protection Code of Practice 
for Digital Identity Schemes in Africa ).
 •…But Don’t “Localize” Data Without Good Reason.  It is increasingly common for data protection re -
gimes to include localization measures that require firms that collect data about a country’s cit -
izens to store or process that data within the same jurisdiction. While national security and law 
enforcement concerns may provide valid reasons to limit cross-border data sharing in certain 
limited instances, these measures can harm local companies by preventing them from using 
foreign cloud service providers that often deliver cheaper, higher quality, and more secure data 
storage options than domestic providers (UNCTAD, 2013; Levite and Kalwani, 2020).
 •Invest in improving knowledge and capacity.  Low levels of digital literacy in the general population 
and among policymakers present a major hurdle to effectively implementing data governance 
and protection laws. Additionally, citizens must know their data rights and be equipped with the 
skills needed to question why their data is being collected and how it will be used. Experts noted 
that while DPAs have an important role to play in educating the public about their digital rights, 
they are often too resource-constrained to do so in practice.7
WHY DATA PROTECTION MATTERS FOR DEVELOPMENT •Foster approaches that move beyond consent as the primary basis for protecting personal data. Relying on 
individual consent places an unreasonable and unworkable burden on individuals. Addition -
ally, in complex data ecosystems, obtaining consent is not always possible. Policymakers should 
therefore consider ways to support testing and measuring the effectiveness of different models 
of personal data protection and enforcement, including, for example, legitimate purposes tests, 
data fiduciaries and trusts, and participatory data stewardship approaches (Medine and Murthy, 
2020; Ada Lovelace Institute, 2021; Hardinges, Wells, Blandford, Tennison, & Scott, 2019; Wylie 
and McDonald, 2018; Moerel and Prins, 2016).
WHAT CAN THE INTERNATIONAL COMMUNITY DO TO SUPPORT  
A MORE INCLUSIVE AND LEVEL PLAYING FIELD? 
Our research suggests several ways the international development community and high-income 
countries can promote a more level playing field for data protection policies and help LMICs advance 
on their path of digital transformation:
1. Devote more resources to strengthening domestic data governance and protection regimes, in line with the coun -
tries’ needs and capacities.  Development organizations should work with client countries to make 
sure their data governance and protection frameworks can support digital transformation. Im -
proving how these frameworks are implemented and enforced should be a key focus of funding 
vehicles to support more and better data use such as the World Bank’s recently announced Global 
Data Facility (Hammer, Kaushik, Song and Rickets, 2021). Funding vehicles aimed at support -
ing data and statistics priorities like the World Bank’s recently announced Global Data Facility 
should make strengthening and measuring the effectiveness of data governance a key focus of 
their lending (Hammer et al., 2021). 
2. Promote a common, transparent, and flexible approach to establishing the legality of cross-border data flows.  
The GDPR adequacy process is opaque and easily politicized. As more countries establish their 
own mechanisms for determining the legality of cross-border data flows, there is a danger that 
a proliferation of national data protection adequacy regimes could further fragment the global 
digital economy, unless they are anchored to a similar set of standards and adequacy assess -
ments are conducted transparently. 
As a first step, jurisdictions should be transparent about how they reach adequacy decisions, in -
cluding publicly stating why decisions are denied or delayed for certain countries. Beyond this, 
countries should agree to a set of standards to govern cross-border data flows that are strong 
enough to ensure high-quality data protection but flexible enough to allow governments to design 
frameworks that meet their own needs, priorities, and capacities. The Council of Europe’s Con -
vention 108+, which is the only legally binding multilateral instrument on the protection of pri -
vacy and personal data, provides a model of such an outcomes-based yet flexible arrangement, but 
governments may be more likely to ratify a framework whose design they have provided input on. 
3. Foster global and regional initiatives to harmonize national data policies that give LMICs a seat at the table.  To 
date, LMICs have had little input into global debates on data policy standards, leaving them in 
the role of “standards-takers.” Giving LMICs a meaningful voice in shaping the standards they 
are expected to meet makes effective implementation more likely. This will likely require the 
creation of new institutions as “existing institutional frameworks at the international level are 
not fit for purpose to address the specific characteristics and needs of global data governance” 
(UNCTAD Digital Economy Report 2021).8
WHY DATA PROTECTION MATTERS FOR DEVELOPMENT4. Identify and develop better data policy metrics. Currently, most cross-country measures related to data 
protection focus solely on legislation (Greenleaf, 2019; Chen, 2020; UNCTAD, 2020). New metrics 
are needed to better understand the relationship between data protection policies and economic 
outcomes, including on how well or poorly data protection measures are implemented, the effect 
of these measures on protection and investment outcomes, and the value created by key data 
ecosystems, cross-border data flows and data-driven innovation more broadly. The lack of such 
metrics is an obstacle to understanding which policies are working and which need reform. 
REFERENCES 
Ada Lovelace Institute. (2021).  Participatory Data Stewardship: A Framework for Involving People in the Use of Data . Retrieved 11 
03, 2021, from https://www.adalovelaceinstitute.org/report/participatory-data-stewardship/
Chakravorti, B., & Chaturvedi, R. (2017).  Digital Planet 2017: How Competitiveness and Trust in Digital Economies Vary Across 
the World . The Fletcher School, Tufts University, Medford, MA. Retrieved 11 03, 2021, from https://sites.tufts.edu/
digitalplanet/files/2020/03/Digital_Planet_2017_FINAL.pdf
Chen, R. (2020). Mapping data governance legal frameworks around the world: Findings from the Global Data Regu -
lation Diagnostic. In  Policy Research Working Paper.  World Bank, Washington, DC. Retrieved 09 13, 2021, from https://
openknowledge.worldbank.org/handle/10986/35410
Davis, T. (2021).  Data Protection in Africa: A Look at OGP Member Progress.  Open Government Partnership , Washington, DC. 
Retrieved 11 03, 2021, from https://www.opengovpartnership.org/wp-content/uploads/2021/08/OGP-Data-Protec -
tion-Report.pdf  
Digital Competition Expert Panel. (2019).  Unlocking Digital Competition .  Retrieved 11 03, 2021, from https://assets.
publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/785547/unlocking_digital_
competition_furman_review_web.pdf
Dixon, P. (2019). Roundtable of African Data Protection Authorities. Retrieved 11 15, 2021, from https://www.id4africa.
com/2019/files/RADPA2019_Report_Blog_En.pdf  
European Commission. (2020).  Data protection as a pillar of citizens’ empowerment and the EU’s approach to the digital transi -
tion—two years of application of the General Data Protection Regulation . Retrieved 11 03, 2021, from https://eur-lex.europa.
eu/legal-content/EN/TXT/?uri=CELE
Fazlioglu, M. (2018).  How DPA Budget and Staffing Levels Mirror National Differences in GDP and Population . International As -
sociation of Privacy Professionals, Portsmouth, NH. Retrieved 11 03, 2021, from https://iapp.org/media/pdf/re -
source_center/DPA-Budget-Staffing-Whitepaper-FINAL.pdf
Gellman, B. (2021). Fair Information Practices: A Basic History. Retrieved 11 18, 2021, from https://bobgellman.com/
rg-docs/rg-FIPShistory.pdf  
Greenleaf, G. (2019). Global tables of data privacy laws and bills (6th Ed January 2019). SSRN. Retrieved 11 03, 2021, 
from https://ssrn.com/abstract=3380794
Greenleaf, G., & Cottier, B. (2020). 2020 ends a decade of 62 new data privacy laws.  Privacy Laws & Business International , 
163, 24–26. Retrieved 11 03, 2021, from https://ssrn.com/abstract=3572611
Hammer, C., Kaushik, S., Hwa Song, S., & Ricketts, L. (2021). Putting data and innovation to work for the SDGs: The 
Data Innovation Fund.  World Bank Data Blog . Retrieved 11 03, 2021, from https://blogs.worldbank.org/opendata/put -
ting-data-and-innovation-work-sdgs-data-innovation-fund
Hardinges, J., Wells, P., Blandford, A., Tennison, J., & Scott, A. (2019).  Data Trusts: Lessons from Three Pilots . Open Data 
Institute, London. Retrieved 11 03, 2021, from https://theodi.org/article/odi-data-trusts-report/  
Levite, A., & Kalwani, G. (2020). Cloud Governance Challenges: A Survey of Policy and Regulatory Issues . Carnegie Endowment 
for International Peace, Washington, DC. Retrieved 11 22, 2021, from https://carnegieendowment.org/2020/11/09/
cloud-governance-challenges- survey-of-policy-and-regulatory-issues-pub-83124 
McCann, D., Patel, O., & Ruiz, J. (2020). The Cost of Data Inadequacy. New Economics Foundation/UCL Europe Insti -
tute, London. Retrieved 11 03, 2021, from https://neweconomics.org/2020/11/the-cost-of-data-inadequacy  9
WHY DATA PROTECTION MATTERS FOR DEVELOPMENTMedine, D., & Murthy, G. (2020).  Making Data Work for the Poor: New Approaches to Data Protection and Privacy.  Consultative 
Group to Assist the Poor , Washington, DC. Retrieved 11 03, 2021, from https://www.cgap.org/sites/default/files/
publications/2020_01_Focus_Note_Making_Data_Work_for_Poor_0.pdf
Moerel, L., & Prins, C. (2016). Privacy for the homo digitalis: Proposal for a new regulatory framework for data pro -
tection in the light of big data and the Internet of Things.  Cybersecurity . Retrieved 11 03, 2021, from https://papers.
ssrn.com/sol3/papers.cfm?abstract_id=2784123
Mungan, M. (2019).  Seven Costs of Data Regulation Uncertainty . Data Catalyst, Washington, DC. Retrieved 11 03, 2021, from 
https://datacatalyst.org/reports/seven-costs-of-data- regulation-uncertainty/
Pisa, M., & Nwankwo, U. (2021).  Are Current Models of Data Protection Fit for Purpose? Understanding the Consequences for Eco -
nomic Development . Center for Global Development, Washington, DC. Retrieved 11 03, 2021, from https://www.cg -
dev.org/publication/are-current-models-data-protection-fit-purpose-understanding-consequences-economic
Pisa, M., Dixon, P., Ndulu, B., & Nwankwo, U. (2020). Governing data for development: Trends, challenges, and 
Opportunities. In  CDG Policy Paper . Center for Global Development, Washington, DC. Retrieved 11 03, 2021, from 
https://www.cgdev.org/sites/default/files/governing-data-development-trends-challenges-and-opportunities.
pdf
Regulation (EU) 2016/679 of the European Parliament and of the Council on the protection of natural persons with 
regard to the processing of personal data and on the free movement of such data, and repealing Directive 95/46/EC 
(General Data Protection Regulation), Official Journal of the European Union L 119/1, p. 19–20
Selinger, E., & Hartzog, W. (2020). The inconsentability of facial surveillance.  Loyola Law Review , 66. Retrieved 11 03, 
2021, from https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3557508
Tisné, M., & Schaake, M. (2020).  The Data Delusion: Protecting Individual Data is Not Enough When the Harm is Collective .  
Retrieved 11 03, 2021, from https://fsi-live.s3.us-west-1.amazonaws.com/s3fs-public/the_data_delusion_format -
ted-v3.pdf  
UNCTAD. (2013). Information Economy Report 2013—The Cloud Economy and Developing Countries. United Nations Confer -
ence on Trade and Development (UNCTAD), Geneva. Retrieved 11 03, 2021, from https://unctad.org/system/files/
official- document/ier2013_en.pdf 
UNCTAD. (2020).  Data Protection and Privacy Legislation Worldwide (webpage) . United Nations Conference on Trade and 
Development (UNCTAD), Geneva. Retrieved 11 03, 2021, from https://unctad.org/page/data-protection-and-pri -
vacy-legislation-worldwide
UNCTAD. (2021).  Digital Economy Report 2021—Cross-border Data Flows and Development: For Whom the Data Flow.  United Na -
tions Conference on Trade and Development (UNCTAD), Geneva. Retrieved 10 19, 2021, from https://unctad.org/
system/files/official-document/der2021_overview_en_0.pdf
Voss , A. (2021). Fixing the GDPR: Towards Version 2.0. Retrieved 11 03, 2021, from https://www.axel-voss-europa.de/
wp-content/uploads/2021/05/GDPR-2.0-ENG.pdf
World Bank. (2021).  World Development Report 2021: Data for Better Lives.  World Bank, Washington, DC. doi: https://doi.
org/10.1596/978-1-4648-1600-0  
World Economic Forum. (2019).  Data Collaboration for the Common Good: Enabling Trust and Innovation Through Public-Private 
Partnerships.  Retrieved 11 03, 2021, from https://www3.weforum.org/docs/WEF_Data_Collaboration_for_the_Com -
mon_Good.pdf
Wylie, B., & McDonald, S. (2018). What Is a data trust?  Centre for International Governance. Retrieved 11 03, 2021, from 
https://www.cigionline.org/articles/what-data-trust/  WWW.CGDEV.ORG
This work is made available under the terms of the Creative Commons  
Attribution-NonCommercial 4.0 license.Ideas to Action: Independent 
research for global prosperity
MICHAEL PISA is a policy fellow at the 
Center for Global Development. 
PAM DIXON is the founder and executive 
director of the World Privacy Forum.
UGONMA NWANKWO is a research 
assistant at the Center for Global 
Development.APPENDIX 
Working Group Co-Chairs 
Pam Dixon , World Privacy Forum
Benno Ndulu (in memoriam), Oxford University (Former Central Bank Governor of Tanzania)
Working Group Members 
Adedeji Adeniran , Center for the Study of Economies of Africa (CSEA)
Teki Akuetteh Falconer , Africa Digital Rights Hub
Shaida Badiee , Open Data Watch
Donatien Beguy , United Nations Human Settlements Program (UN-Habitat)
Stephen Chacha , Tanzania Data Lab
Somsak Chunharas , National Health Foundation (Thailand)
Vyjayanti Desai , Identification for Development (ID4D), World Bank
Drudeisha Madhub , Data Protection Office of Mauritius
David Medine , Consultative Group to Assist the Poor (CGAP)
Santosh K Misra , Government of Tamil Nadu
Tom Orrell , DataReady
Olasupo Oyedepo , African Alliance of Digital Health Networks
Isaac Rutenberg , Center for Intellectual Property and Information Technology Law
Omar Seidu , Ghanaian Statistical Service
Rachel Sibande , Data for Development at Digital Impact Alliance (DIAL)"
DataPrivacy,3018.txt," 
Revista de Derecho. Vol. 10 (I I) (2021), pp.  87-100. ISSN: 1390 -440X — eISSN: 1390 -7794  
Recepción: 24-10-2021. Aceptación: 4-12-2021. Publicación electrónica: 8-12-2021 
https://doi.org/10.31207/ih.v 10i2.288 
 
vol. 10 (I I) (2021 ), p. 87 
 
 
 
EXPERIENCING PERSONAL DATA PROTECTION ON THE 
INTERNET AND ITS POSSIBILITIES OF RECOGNITION 
AND ENFORCEMENT IN UKRAINE  
EXPERIMENTANDO LA PROTECCIÓN DE DATOS PERSONALES EN INTERNET Y SUS 
POSIBILIDADES DE RECONOCIMIENTO Y EJECUCIÓN EN UCRANIA  
 
 
Petro Melnyk* 
Oleksii Volodymirovich Kostenko** 
Hanna Oleksandrivna Blinova*** 
Iryna Igorivna Shynkarenko**** 
 
 
Abstract:  The purpose of this article is to find the most successful ways, 
forms and methods of personal data protection on the Internet among foreign 
countries for domestic political and legal realities. The following methods 
were used in the article: dialectical, logical -semantic, comparative -legal, 
documentary analysis, analytical, information -analytical. Issues related to 
the adaptation of the successful experience of a number of developed 
countries in the field of personal data protection on the Internet are bro ught 
up for discussion. Some options are covered and specified, which include 
effective methods and ways to implement an effective mechanism for 
 
* Candidate of Law. Associate Professor. Deputy Director of the Kyiv Institute of 
Intellectual Property and Law of the National University «Odesa Law Academy » (Kiev, 
Ukraine) . https://orcid.org/0000 -0003-1928 -9660 . melnyk.petro@gmail.com  
** Ph.D . Head of the  Research Laboratory of Theory and Law of Digital Transformations of 
the Research Institute of Informatics and Law of the National Academy of Legal Sciences 
of Ukraine (Kiev, Ukraine).  https://orcid.org/0000 -0002-2131 -0281. antizuk @gmail.com  
*** Doctor of Jurisprudence . Professor of the Department of Administrative Law, Process 
and Administrative Activity of Dnipropetrovsk State University of Internal Affairs (Dnipro, 
Ukraine) . https://orcid.org/0000 -0002 -3320 -585X. blinovahanna @i.ua 
**** Candidat e of Jurisprudence . Associate Professor of the Department of  Operative 
Investigative Activity and Crimes Investigation of the Kharkiv National University of Internal 
Affairs (Kharkiv, Ukraine) . https://orcid.org/0000 -0001 -7136 -3333 . i_shinkarenko @ukr.net Petro Melnyk, Oleksii Volodymirovich Kostenko, Hanna Oleksandrivna Blinova & Iryna...  
  | v. 10 (II) (2021 ), p. 88 
personal data protection on the Internet in Ukraine. It is emphasized that the 
protection and proper confidenti ality of personal data of individuals is one 
of the key tasks currently facing modern jurists. It is also added that the 
nature and specifics of the use and protection of personal data of individuals, 
including on the Internet, are extremely closely relate d to the institution of 
intellectual property. Emphasis is placed on the fact that the level of 
protection of personal data of individuals in a country is an indicator of the 
extent to which such a state meets the criteria of freedom, democracy , and 
the ru le of law.  
 
Keywords:  Personal Data, Private Property,  Legal Regulation,  Confidential 
Information, Intellectual Property  
 
 
Resumen : El propósito de este artículo es encontrar las formas  y métodos 
más exitosos de protección de datos personales en Internet entre países 
extranjeros en cuanto a  sus realidades políticas y legales nacionales. En el 
artículo se utilizaron los siguientes métodos: dialéctico, lógico -semántico, 
comparativo -legal, a nálisis documental, analítico  y de análisis informático . 
Se plantean para el debate cuestiones relacionadas con la adaptación de la 
experiencia exitosa de varios países desarrollados en el campo de la 
protección de datos personales en Internet. Algunas opciones están 
cubiertas y especificadas, incluyen do métodos y formas de implementar un 
mecanismo efectivo para la protección de datos personales en Internet en 
Ucrania. Se destaca que la protección y la debida confidencialidad de los 
datos persona les es una de las tareas clave a las que se enfrentan 
actualmente los juristas modernos. También se agrega que la naturaleza y 
los detalles del uso y la protección de los datos personales, incluso en 
Internet, están extremadamente relacionados con la insti tución de la 
propiedad intelectual. Se hace hincapié en el hecho de que el nivel de 
protección de los datos personales en un país es un indicador del grado en 
que dicho Estado cumple los criterios de libertad, democracia y Estado de 
Derecho.  
 
Palabras clav e: Datos personales , propiedad privada , regulación legal, 
información confidencial , propiedad intelectual  
 
 
Sum mary . I. Introduction . II. Methodology . III. Results and Discussion . III.1. The 
Necessity and Essence of Acquiring an Efficient Internet Platform in Ukraine . III.2. 
Legislation of Ukraine on Personal Data Protection, Including on the Internet . IV. 
Conclusions . References . 
 Experiencing Personal Data Protection of the Internet and Its Possibilities of…  
 
| v. 10 (I I) (2021), p. 89 
 
I. INTRODUCTION  
The application of modern information and communication 
technologies is an indispensable factor in improving public safety. At the 
same time, the development of modern information and communication 
technologies has acted the complexity of security risks po sed by such 
systems. On the other hand, citizens rightly expect an adequate response and 
protection mechanism from modern forms of endangering public safety and 
legally recognized freedoms and rights of citizens. Bearing this in mind, the 
basic task is to engage all the necessary resources starting from the collection 
of data and information, through assessment, processing and analysis. If 
such an answer is not forthcoming, we risk the trust of the citizens and the 
foundations of democracy itself. The chall enge is great, because modern 
forms of endangering public safety are constantly changing, especially due 
to the accelerated development of information technology and means of 
communication (Gligorijević , et al. , 2020).  
In modern realities, which  govern the  entire world community, it is 
impossible not to mention the importance of proper protection and 
confidentiality of the private sphere of people ’s lives. In particular, it 
concerns the personal data protection on the Internet, as it is well known that 
in the XXI century, which is rightly called “information ”, control over 
information, the ways of its collection and dissemination are a powerful 
element of political and economic influence.  
Internet penetration in Ukraine annually increases by an average of 
5%. In 2015, the figure was 49%. As of February 2016, 63% of households 
(excluding Crimea) are connected to the Internet in Ukraine. Regularly, once 
a month or more, 62% of families use the Internet (Chaikovska, 2016). 
According to the results of 2018, the n umber of Internet users in Ukraine is 
70% of Ukrainians (compared to 63% as of December 2017). Factum Group 
research company claims that the number of Internet users in Ukraine 
reached 73% of the total number of Ukrainians in 2019, and the number of 
regula r Internet users at the beginning of the 4th quarter 2019 was 22.96 
million ( Dubinskiy , 2019 ; Shymon , et al. , 2020).  
The issues outlined in this article are quite deep and not fully explored. 
It is recognized that the study and substantive analysis of the nature and 
characteristic features of the processes associated with the proper respect for 
the safety and confidentiality of personal data of individuals on the Internet, 
should remain relevant for domestic and foreign scientific community.  The 
issue o f personal data protection on the Internet is available around the 
world. Thus, for example, an estimated 26 million American citizens per Petro Melnyk, Oleksii Volodymirovich Kostenko, Hanna Oleksandrivna Blinova & Iryna...  
  | v. 10 (II) (2021 ), p. 90 
year have been victims of an identity -based crime (Piquero , et al., 2021). At 
the same time, it should be noted that in the developed countries of the West, 
the above processes of personal data protection are developing and 
improving much more dynamically than in Ukraine. That is why this article 
will focus on what specific methods and ways to develop the field of 
compli ance and protection of personal data of individuals on the Internet can 
be adopted and further used in the domestic political and legal sphere.  
 
 
II. METHODOLOGY  
At present, any research should be based on the use of appropriate 
scientific methods, the app lication of which makes it possible to achieve the 
goal, scientifically substantiate the conclusions and propose appropriate 
solutions to the problem under study. The methodological basis is an 
interdisciplinary approach, in which the theoretical and metho dological 
component is based on the fundamental provisions of the theory of law. The 
methodological base includes a set of general and specific methods of scientific 
knowledge.  An objective analysis of the subject became possible due to the use 
of a complex of methods of general and special scientific knowledge.  
In the study with the help of the dialectical method of cognition, as 
well as the logical -semantic method it was possible to define and reveal such 
theoretical concepts as “personal data”, “pe rsonal data protection” and “the 
use of personal data”.  The experience of developed countries of the Western 
democratic world, in particular, such as the United States of America, 
Canada, Great Britain and the countries of the European Union, is studied 
using the comparative legal method.  
When scientifically substantiating the theoretical conclusions of the 
author, the method of documentary analysis and the analytical method were 
used, in particular, it was concluded that the dynamic development of 
computer  and digital technology requires constant supervision by the state 
in the field of personal data protection on the Internet. The most effective 
way to do this is to develop programs that make it possible to block pirated 
sites; identify persons who, using computer programs, interfere with the 
personal data of other persons, enterprises, organizations, etc.  
The information and analytical basis of the scientific research was the 
legal framework governing relations in the field of personal data protection 
on the Internet, as well as the scientific achievements of Ukrainian and 
foreign scientists.  
All of these methods are applied in interdependence and interrelationship.  
It’s great and efficient that there exist methods of ensuring effective Experiencing Personal Data Protection of the Internet and Its Possibilities of…  
 
| v. 10 (I I) (2021), p. 91 
protection of user’s  personal data from the internet, but this method  
continues to be questionable as they continue to be violations on personal 
data and services provided by the internet. Most people data are not secured 
and protected from the ills or criminal invasion of in ternet criminal. Our 
main concern here is to guarantee to personal users of the internet that their 
information and other activities are secured and protected. There is no use 
of putting in place methods and strategies were user rights and data will 
contin ue to be infringed and violated. It is there the responsibility of the State 
of Ukraine in ensuring the effective protection of personal data of its internet 
user, since they have the right to their individual privacy. Ensuring and 
following up the measure  used become of prime importance.  
 
 
III. RESULTS AND DISCUSSION  
The democratic countries strictly adhere to the doctrine of privacy and 
private property, to the most radical approach in China. The settlement of 
any of the aspects does not mean the solution of the problem in the whole. 
The matter of its fundamental solu tion lies in the legal plane, and since 
organized crime in the context of global digitalization of the world 
community is becoming increasingly transnational in nature, it is primarily 
a field of international law. The complete solution of the problem has a 
complex character and should contain a number of normative and legal acts 
at the level of international and national law, as well as certain technical 
measures that ensure the legality of personal data collection procedures used 
by police in the most mod ern high -tech, toolmaking , analytical systems 
(Hnusov, Strukov & Mozhayev, 2021) . 
The expansion of the Internet and technologies based on it has enabled 
the development of the digital economy and the emergence of new and 
innovative business models. The Int ernet has transformed how goods and 
services have been produced, delivered and consumed. Using Internet 
platforms (such as eBay, Alibaba and Google Search) allows consumers to 
globally search, locate and buy goods from anywhere in the world. 
Simultaneously  with the rapid development of technology, there was a need 
for regulatory intervention in various area of law, including the Intellectual 
Property Law. Increasingly diverse ways of using Intellectual Property 
works on the Internet have contributed to the frequency and extent of their 
abuse (Maltzer, 2016; Perišić, 2020).  
The treatment of personal data of individuals solely in the spirit of the 
rule of law and on the basis of clear legal regulation is an important step 
towards the normalization of the funct ioning of intellectual property. Even Petro Melnyk, Oleksii Volodymirovich Kostenko, Hanna Oleksandrivna Blinova & Iryna...  
  | v. 10 (II) (2021 ), p. 92 
though with all these strategies and measures put in place in ensuring the 
effective use of personal data on the Internet, it ’s accessibility and protection 
has become problematic. Most users of the internet experience  violations on 
their personal data, and this has affected the confidence bestow in the usage 
of the internet. The question one should be asking here is in verifying how 
secured the personal data of users of the internet are  protecting their personal 
data. This has been a serious debate and deficiencies affecting users in 
aspects related to the internet.  
It is very important today to understand that the parameter of data 
openness and accessibility is a fundamental vector for the existence of a 
modern informa tion society. Open data, in this vein, should be studied from 
the standpoint of exclusively public information  to which government 
authorities have provided online access to anyone who wishes. That is, the 
basic aspect is free access. Alternatively, such d ata can be found on the 
websites of ministries, various departments, state bodies of the region, cities, 
state registers, and platforms that have been created by activists and 
journalists (Shevchenko , et al. , 2020).  
The Internet, the World Wide Web, and their successors are evolving 
rapidly into a global digital network, a “cybersphere ” interrelating people 
and their activities through robust, albeit ubiquitous, computers, networks, 
and intelligent hardware and software. The internet has become an integral 
part of everyday life across diverse parts of society (Atkins, Duderstadt & 
Houwel ing, 2002 ; Pew Research Center , 2005 ). The Internet builds a 
parallel world from conversion of analog practices to new experimental 
spaces. Dissensus is an intrinsic  feature of the cybersphere ; difference not 
identity is its connecting work. Communicative activism potentially renders 
all practices, rules, and norms of practice controversial. Online  activism 
feeds network change that serves the economic driven interests of the 
industry. Media oligopolies build out networks to reach audiences through 
filterin g preferences that serve the ends of mass media (Thomas , 2016 ). 
The present unbridled advancement in the field of information and 
communication technology has resulted in individuals being thrust at a 
crossroad, where refusing to sacrifice one’s privacy wo uld mean the denial 
of technological benefits. Concern for privacy begins once a child is born 
into this world where the right to privacy could now be argued needs to be 
considered as one of the basic human rights similar to other inalienable 
rights such a s the right to life and liberties (Dhali, Zulhuda & Fadhilah, 
2021).  
Ukraine should definitely take lessons from Western Europe and 
North America in the development of systems related to the protection and 
secure storage of personal data. In particular, th is can be explained by the fact Experiencing Personal Data Protection of the Internet and Its Possibilities of…  
 
| v. 10 (I I) (2021), p. 93 
that in these countries, such as the United States, the first real steps in world 
history were taken to create systems and methods of personal data protection.  
The United States is considered a “pioneer ” in the introduction of 
confidential processing and storage of personal data on the Internet. Experts 
argue that the especially serious attitude of American society to the problems 
and issues, one way or another related to personal data protection, began 
somewhere in the middl e of the XX century.  
The prevention of identity theft requires the responsibility, 
cooperation, and actions of three major groups: individuals, businesses, and 
government. Individuals have the information that must be protected and 
need to take care to gua rd their personally identifiable information (PII). To 
help them do so, agencies such as the Federal Trade Commission 
recommend a plethora of preventive measures, such as shredding sensitive 
documents and regularly reviewing credit reports and credit card statements, 
which individuals are encouraged to undertake for their own protection 
(Piquero , et al. , 2021).  
As we can see, without the effective and stable influence of public 
authorities and representatives of society (public organizations) on the 
quality  of compliance in the country with all the necessary rules for the 
treatment of personal data will be considered impossible. In turn, the 
development of various data confidentiality programs, introduced and 
successfully operated under the administration of  private entities and 
companies, is a good basis for the emergence of effective and consistent 
public policy in this area. Therefore, it is extremely important that there is a 
clear and long -term mutual cooperation (relationship) in society between 
governm ent agencies, institutions and private commercial and non -
commercial firms, as well as civil society organizations.  
Experts in the field of cyber protection from the United Kingdom 
confidently state in their official reports that hacker cyberattacks are 
currently the most widespread and dangerous threat to the daily operation of 
servers that work with personal data. British experts emphasize that almost 
every piece of software of any institution or organization can become a 
potential target of a cyberattack . For example, according to a British report, 
in 2017 the computer servers of the U nited Kingdom  National Health 
Service  (NHS)  were attacked. In particular, a criminal organization of 
hackers attacked more than two hundred thousand personal computers of 
representatives of the health service, which were located directly in the UK 
and other member countries of the British community . 
Security officials, along with British police, found that the criminal 
computer attack was due to the using of the infamous hacking program 
«WannaCrypt» by cybercriminals . The criminal result of this attack on NHS Petro Melnyk, Oleksii Volodymirovich Kostenko, Hanna Oleksandrivna Blinova & Iryna...  
  | v. 10 (II) (2021 ), p. 94 
servers was the taking by cybercriminals  of passwords and encryption keys 
of employees, including those in senior positions. As a result, criminals 
returned their access codes to UK health workers only after receiving 
significant amounts of money from them. It should be noted that this 
example i s just one of many similar cyber -attacks  that take place dozens of 
times a year.  
Given these circumstances, British researchers, in our opinion, have 
come to the logical conclusion that even in the developed world, which 
includes the United Kingdom, it is essential to continue reforms and other 
significant changes. This is somehow related to ensuring the proper and 
secure day -to-day operation of systems that use personal data of individuals 
(Appleby Global Group , 2017, pp. 1 -5). Such a negative example shou ld 
serve as a “lesson ” for the Ukrainian Government, since recently in our 
country electronic document management is being introduced more and 
more often, records in the form of an “electronic queue ” and others are being 
introduced in public and private in stitutions. Of course, such virtual 
computer programs require a person to provide their personal data, and 
therefore must be effectively protected from outside interference. That is 
why private companies should be more careful about issues and processes 
related to the transfer (even temporary) of personal data or other information 
to third parties or organizations. Offshore companies and institutions that 
offer their services to other legal entities in the field of temporary or long -
term storage of certain amounts of information, including personal data of 
employees of these legal entities, may act as such “third ” organizations. 
Given that, offshore companies are a “tasty ” target for illegal attacks by 
cybercriminals from around the world.  
In order to signif icantly minimize the processes of transferring 
personal data to offshore companies, British experts suggest that legal 
entities increase their own scientific and technical potential, raise the general 
level of high technology in production and try to creat e a product or provide 
certain services based on their own capabilities and resources. Regarding the 
use of high -tech tools and implements, scientists recommend that all 
institutions, enterprises and organizations, regardless of their form of 
ownership or field of activity, gradually move to the use of blockchain 
technology in their daily work. This technology, according to experts, is a 
powerful tool in countering financial and computer crimes and fraud. It is 
clear that along with the introduction within the country of a number of the 
latest technological advances, steps should also be taken to ensure a proper 
law enforcement system, which should act to prevent hackers, 
cybercriminals, etc.  Experiencing Personal Data Protection of the Internet and Its Possibilities of…  
 
| v. 10 (I I) (2021), p. 95 
Nowadays, it is no less important to establish a proper regime of legal 
regulation in the country, which will ensure the effective storage of 
confidential information and personal data. To do that, it is necessary that 
the legislative body of state power (in Ukraine it is the Verkhovna Rada), 
adopt a number of legal acts  that would adequately ensure effective and 
efficient legal regulation in the field of protection and promotion of personal 
data, including the Internet. In this context, both domestic and foreign legal 
experts point out that without a reliable system (mec hanism) in a particular 
state of clear and unambiguous legal regulation of compliance with 
international standards and requirements to ensure the protection of personal 
data, it will be impossible.  
A number of statements made by jurists and experts in cybe rnetics and 
computer technology from many developed countries should be identified 
as a separate point of view on this issue. This refers to the fact that the legal 
(legislative) regulation of such a specific and dynamic sphere of public 
relations as cyber netics, should be carried out exclusively at the national 
level, that is, at the State level. Instead, all other types of legal regulation, 
such as local or regional, are considered ineffective or even harmful. If such 
legal regulation is carried out by ea ch region or each municipality separately, 
it will of course significantly slow down the quality and efficiency of public 
relations in the field of personal data protection on the Internet. It will also 
significantly harm both the development of computer t echnology and the 
development of the economy of a particular city, region or country as a 
whole. It is based on these logical statements;  we note that regulation should 
take place at the State level.  
 
III.1. The Necessity and Essence of Acquiring an Efficient Internet 
Platform in Ukraine  
Developed and democratic states with true rule of law continue to be 
leaders and role models for other countries in further improving and 
developing the norms and principles of international law in the field of 
proper  confidential treatment and protection of personal data on the Internet. 
To ensure the effectiveness of these processes, it is recommended that 
economically, culturally and politically developed countries help less 
developed countries to improve the effici ency of their state mechanism, as 
well as to create clearer, transparent and unambiguous legislation. In 
addition, it is appropriate for states to act together in an effort to improve the 
effectiveness of international legal norms and institutions, as it i s known that 
the development of the Internet and computer technology is dynamic.  Petro Melnyk, Oleksii Volodymirovich Kostenko, Hanna Oleksandrivna Blinova & Iryna...  
  | v. 10 (II) (2021 ), p. 96 
Considering the above opinion more broadly, we can argue that today 
the Internet is such that it is developing extremely rapidly (faster than other 
spheres of public life). Moreover, the peculiarity of the phenomenon of the 
Internet is that it is not only quite rapid, dynamic and rapidly evolving, but also 
significantly helps to progress and develop all other spheres  of public life. In 
particular, the Internet and high technolo gy have become an advanced part of 
societal progress. Therefore, through the rapid nature of the above  phenomena, 
it can be noted that they should be subject to clear legal regulation.  
State control over the relevance of national legislation in the field o f 
personal data protection on the Internet should not be carried out through the 
creation of any new bodies or state commissions, while “inflating ” the 
bureaucracy and creating an additional burden on the domestic state budget 
and government mechanisms. A much more useful solution may be the 
introduction of simplified procedures for changing or correcting the existing 
regulatory legal acts governing public relations in the field of personal data 
protection, as well as simplified and rapid procedures for the  development 
and adoption of new legal acts.  
We completely agree with this way of thinking, because it is clear that 
in the context of considering this issue there are two sides to a common 
problem. On the one hand, such a powerful and influential branch o f public 
life as the Internet and the high -tech sector cannot be left unregulated by law 
(Kittichaisaree & Kuner , 2015, pp. 2 -4). On the other hand, it can be argued 
that excessive legal regulation in this area or its slow introduction and 
implementation c an significantly harm the whole mechanism of proper 
protection of private personal data, for example, artificially slowing down 
the natural dynamics of their development and functioning.  
Economically developed countries should be considered as priority 
countries for cooperation and mutually effective exchange of experience and 
practical successful examples of work. This is explained by the fact that they 
have a high level of political and social culture, as well as have in their 
territory stable state insti tutions of power, which demonstrate their own 
stable efficiency for many years or even centuries ( United Nations,  2013 , 
Resolution  No. 68/167). To date, several Western countries are subject to 
the characteristic requirements outlined above. In particular,  among them 
the United States as the leading “superpower ” of the modern world, which 
has the world ’s greatest influence and weight in politics, economics, culture, 
education and scientific and technological progress. Also, among the 
countries with which Ukraine can carry out fruitful cooperation, it is 
necessary to add the Member States of the European Union, which are 
extremely close to Ukraine in terms of common family, political and cultural 
values.  Experiencing Personal Data Protection of the Internet and Its Possibilities of…  
 
| v. 10 (I I) (2021), p. 97 
As is known, these countries follow the same principles and 
approaches in terms of protecting the privacy of every person as a subject of 
public relations . It is on the basis of these beliefs and convictions of 
Americans and representatives of Western European countries in the 
inviolability of property rights of each person, in respect for the 
confidentiality of their private life in general, as well as for  certain, separate 
parts of confidential information about these persons or their lives, the 
appropriate legal regulation is formed.  
 
III.2. Legislation of Ukraine on Personal Data Protection, Including on 
the Internet  
It will be important to emphasize tha t the general content of legal 
regulation of personal data protection is based on such well -known 
international documents as the Universal Declaration of Human Rights of 
1948 and the International Covenant on Civil and Political Rights of 1966. 
The current  legislation of Ukraine has been developed taking into account 
the requirements of these international documents, as they have been ratified 
by the Ukrainian Government (Verkhovna Rada) and are therefore binding. 
At present, the legislation on personal dat a protection consists of the 
Constitution of Ukraine, the Law of Ukraine “On Personal Data Protection ”, 
other laws and by -laws, international treaties of Ukraine which were 
approved as binding by the Verkhovna Rada of Ukraine . 
The Member States of the European Economic Area, as well as the 
states that have signed the Council of Europe Convention for the Protection 
of Individuals with Regard to Automatic Processing of Personal Data, are 
recognized as ensuring an adequate level of protection of personal d ata. 
Current regulations in the field of personal data protection regulate 
legal relations related to the protection and processing of personal data and  
are aimed at protecting fundamental human and civil rights and freedoms, in 
particular the right to pri vacy, in connection with the processing of personal 
data. The current legislation applies to the processing of personal data, which 
is carried out in whole or in part using automated means, as well as to the 
processing of personal data contained in the fil e or are subject to entering 
into the file, using non -automated means. If an international treaty of 
Ukraine, approved by the Verkhovna Rada of Ukraine, establishes rules 
other than those provided by the legislation of Ukraine, the rules of the 
internation al treaty of Ukraine shall be applied.  
According to Art. 2 of the Law of Ukraine “On Personal Data 
Protection ”, personal data is defined as “information or aggregate 
information about a natural person who is identified or may be identified ”. Petro Melnyk, Oleksii Volodymirovich Kostenko, Hanna Oleksandrivna Blinova & Iryna...  
  | v. 10 (II) (2021 ), p. 98 
Meanwhile, in accordance with Part 1 of Art. 5 of this Law , personal data 
relating to the exercise by a person authorized to perform the functions of state 
or local government, official or service powers is not confidential information . 
In our opinion, such a legislative interpretation of this concept is 
somewhat narrowed, as it includes only an indication of an individual, while 
neglecting the legal entity. At the same time, the owner of personal data may 
be both an individual and a legal entity, which determi nes the purpose of 
personal data processing, establishes the composition of these data and the 
procedures for their processing, unless otherwise provided by law. In 
addition, the legislator unjustifiably narrowed the concept of a subject of 
personal data, understanding it only as an individual whose personal data is 
processed.  However, personal data of an individual can also be processed, 
as evidenced by the fact that nowadays all tax reports are submitted in 
electronic form, and for the current year of the ir activity, business entities 
submit appropriate declarations, which can be found in the public domain 
on the Internet.  
The procedure for access of third parties to personal data possessed by 
the public information manager is determined by the Law of Ukra ine “On 
Access to Public Information ”, except for data received from other bodies 
by the central executive body that ensures the formation and implementation 
of state financial and budgetary policy during the verification and 
monitoring of state payments.  
It should be pointed out that the nature and content of legal norms 
protecting the confidentiality of personal data of individuals and their proper 
treatment are closely related to the institution of private property. In our 
opinion, this state of affairs can be explained by the fact that the personal 
data of individuals, as well as specific information about them, is their 
exclusive personal value, and therefore must be properly protected. The 
obtaining of personal data of individuals should be understood as a direct 
unlawful encroachment on personal freedoms, rights and interests of a 
person. It ’s a beautiful scenario that the country has put in place and enacted 
credible laws and legislation in ensuring the security and accessibility of its 
internet usage in depriving the violations of personal data rights. This is 
beautiful and laudable in all its initiation. The problem will be posing here 
is as to what use will this legislation be if effective methods and measures 
are not used to ensure i ts enforcement and application. It is one thing in 
enacting laws, and the other in ensuring it ’s application. The State of 
Ukraine can be credited in all its efforts in guaranteeing personal user’s  data, 
but more efforts still need to be done as this issue  of internet crackers and 
hackers continues to be a serious threat affecting not only particular persons, 
but the entire Ukrainian system . Experiencing Personal Data Protection of the Internet and Its Possibilities of…  
 
| v. 10 (I I) (2021), p. 99 
 
IV. CONCLUSIONS  
1. The study showed that the nature and content of the processes of 
protection and proper provision of personal data on the Internet are closely 
linked with the institution of private property, as well as in general with the 
private part of life of individuals. However, there are some exceptions in this 
context, for example, some of the personal data is information that is socially 
important.  
2. Given the many unique features of the Internet, as well as related 
computer and high -tech areas, the legal regulation of this area of public life 
is a difficult and responsible matter. In particular, the legal protection of 
personal data on the Internet sh ould be carried out with the help of special 
“simplified ” procedures for amending and adopting new legal acts. Instead, 
the creation of additional bodies or agencies to oversee the use of personal 
data of individuals is considered unnecessary and creates a n excessive 
burden on the state system and budget.  
3. The United States and the E uropean Union  Member States are the 
best role models for Ukraine in the field of proper protection and ensuring 
the confidentiality of personal data on the Internet, as these countries have 
common values regarding the priority of human rights, freedoms and 
interests of the individual over the State, as well as developed means of 
successful protection of these values.  
 
 
REFERENCES  
Appleby Global Group (2017). Protecting Personal  Data from Cyber -Attacks . Appleby 
Global Group Services Limited.  
Atkins, D. E.,  Duderstadt, J. J., & Houweling, D. V. (2002). Higher Education in the 
Digital Age: Technology, Issues and Strategies for American Colleges and 
Universities . Greenwood Press.  
Chaikovska, V.  P. (2016). E -Commerce in Ukraine: Current State and Trends. Intelligence  
XXI, 3, 38-48. 
Dhali, M ., Zulhuda, S ., & Fadhilah, S . (2021). The Digital Economy and the Quest for 
Privacy Protection in Bangladesh: A Comparative Legal Analysi s. IIUM Law 
Journal , 28 (2), 567 -596. Doi: https://doi.org/10.31436/iiumlj.v28i2.451  
Dubinskiy, I.  (11-X-2019).  Internet Penetration in Ukraine: Research Methodology. 
Factum Group . https://bit.ly/3rznGjf  
Gligorijevi ć, M., Pećanac, M., Vulevi ć, O., & Maksimovi ć, A. (2020 ). Analysis of 
Aspects of Personal Data Protection Risk Assessment Using Modern 
Technologies in the Ministry of Internal Affairs. International Scientific Petro Melnyk, Oleksii Volodymirovich Kostenko, Hanna Oleksandrivna Blinova & Iryna...  
  | v. 10 (II) (2021 ), p. 100 
Conference “Archibald Reiss Days”: Thematic Conference Proceedings of 
International Significance , 10 (1), 593 -603. 
Goodnight , G. T. (2016 ). Argumentation and the Cybersphere. In Ron von Burg (Ed.), 
Windsor Studies in Argumentation  [Vol. III]. University of Windsor.  
Hnusov, Y. V., Strukov, V. M., & Mozhayev, O. O. (2021). Problem of Harmonization of 
Legal Norms with the Needs of Police Investigations by Using High -Tech 
Instruments for Searching Information.  Law and Safety , 80 (1), 78 -85. Doi: 
https://doi.org/10.32631/pb.2021.1.11  
Kittichaisaree, K., & Kuner, C. (14 -X-2015). The Growing Importance of Data Protection 
in Public International Law. Blog of the European Journal of International 
Law. https://www.ejiltalk.org/the -growing -importance -of-data-protection -in-
public -international -law/ 
Meltzer, J. P. (2016). Maximizing the Opportunities of the Internet for International 
Trade. E15 Expert Group on the Digital Economy - Policy Options Paper. E15  
Initiative . International Centre for Trade and Sus tainable Development 
(ICTSD) & World Economic Forum.  
Perišić, J. Ć. (2020). The Sale of Counterfeit  Goods Via the Internet as a Contemporary 
Security Challenge – Legal Aspects. International Scientific Conference 
“Archibald Reiss Days”: Thematic Conference Proceedings of International 
Significance , 10 (1), 195 -203. 
Pew Research Center  (2005).  Internet: The Mainstreaming of Online Life . 
http://www.pewinternet.org/pdfs/Internet_Status_2005.pdf  
Piquero, N. L., Piquero, A. R., Gies, S., Green, B., Bobnis, A. , & Velasquez, E. (2021). 
Preventing Identity Theft: Perspectives on Technological Solutions from 
Industry Insiders. Victims & Offenders , 16 (3), 444 -463. Doi: 
10.1080/15564886.2020.1826023.  
Shevchenko, V., Dosenko, A., Iuksel, G., Synowiec, A., & Valentyna, D. (2020). Use of 
Open Data in Ukraine: Some Important Aspects. Revista San Gregorio , 42, 
319-329. Doi: http://dx.doi.org/10.36097/rsan.v1i42.1564  
Shymon, S. , Baliuk, I., Kykot, P., Shatalova, L., & Harust, Y. (2020). Administrative and 
Legal Bases of Consumer Rights Protection on the Internet.  Gênero e Direito , 9 
(5), 72 -92. 
United Nations (1948). Universal Declaration of Human Rights . United Nations  General 
Assembly Resolution 217A (III).  
United Nations (1966). International Covenant on Civil and Political Rights . United 
Nations General Assembly Resolution 2200A (XXI).  
United Nations (2013). The Right to Privacy in the Digital Age . United Nations Hig h 
Commissioner for Human Rights.  "
